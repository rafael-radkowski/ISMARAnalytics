@INPROCEEDINGS{6402510,
author={C. {Perey} and C. {Stapleton} and M. {Billinghurst}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={BOF},
year={2012},
volume={},
number={},
pages={1-1},
abstract={Tradition is alive and well, but there also should be opportunities for innovation at ISMAR.},
keywords={},
doi={10.1109/ISMAR.2012.6402510},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402512,
author={},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Committee},
year={2012},
volume={},
number={},
pages={1-2},
abstract={Provides a listing of current committee members.},
keywords={},
doi={10.1109/ISMAR.2012.6402512},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402513,
author={W. {Chinthammit} and I. {Radu}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Demos},
year={2012},
volume={},
number={},
pages={1-1},
abstract={We are proud to present the ISMAR 2012 Demonstrations Program. Research in Mixed and Augmented Reality is expanding into broad and diverse horizons. This year's demonstrations touch on a wide variety of topics, from marketing to education to public engagement and beyond; and showcase a multitude of technological implementations, from outdoor feature tracking devices to robotic-assisted camera manipulation to vibration-tolerant HUD displays and many other unique innovations. We are certain that hands-on engagement with the exciting demo selections will spark the enthusiasm and imagination of our diverse community, leading to a memorable conference experience and serving as inspiration for future evolution of the exciting medium of Mixed and Augmented Reality.},
keywords={},
doi={10.1109/ISMAR.2012.6402513},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402514,
author={ {Si Jung Kim} and B. H. {Thomas}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Doctoral Consortium},
year={2012},
volume={},
number={},
pages={1-1},
abstract={We are very happy to present the inaugural ISMAR Doctoral Consortium (DC). The goal of the DC is to create an opportunity for doctoral students to facilitate their research ideas, present their current progress and future plans, and receive constructive criticism and insights related to their future work and career perspectives. A total of seven doctoral students have been selected to the DC from the world, and their research projects in the area of AR, VR and HCI are scheduled to present all day on Monday 5th November from 10am. All ISMAR attendees are welcome to come and interaction with the students as well as foster the future event of DC.},
keywords={},
doi={10.1109/ISMAR.2012.6402514},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402511,
author={B. {MacIntyre} and G. {Welch}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={General chairs},
year={2012},
volume={},
number={},
pages={1-2},
abstract={Welcome to the Eleventh IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR 2012)! We are excited that this year's symposium is being held on the campus of the Georgia Institute of Technology, at Georgia Tech's Hotel and Conference Center. Located in the center of Altanta, Georgia, USA, Georgia Tech is one of the top public research universities in the United States, and a nationally ranked leader in many of the academic disciplines that form the heart of ISMAR: Computer Science, Engineering, HCI, Robotics & Computer Vision, and Digital Media. Atlanta is one of the largest cities in the United States, often considered the “capital of the South” and played a central role in the American Civil War and the Civil Rights movement. With a diverse and young population, Atlanta is a dynamic city with many cultural and historic attractions, restaurants, shopping, sports and outdoor activities. Atlanta is served by Hartsfield-Jackson International Airport, the worlds busiest airport.},
keywords={},
doi={10.1109/ISMAR.2012.6402511},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402515,
author={P. {Hoberman}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Portable reality: Expanding available space},
year={2012},
volume={},
number={},
pages={1-1},
abstract={All of the components for creating fully immersive virtual worlds have suddenly become ubiquitous and cheap, often built into devices that we have already in our pockets and on our desktops. These devices have everything they need to become state-of-the-art platforms for immersive games and virtual reality: powerful graphics, high-resolution displays, and precision sensors. Just add some optics, and now a responsive, fully immersive virtual reality platform can be built for next to nothing. And once we can do that, there's nothing to stop us from unleashing a flood of alternate and augmented worlds that can be colocated with our physical surroundings, anywhere and any time. More than just information or annotation, we can begin to imagine a multiplicity of inhabitable, immersive, interactive, networked environments that can be coordinated with our everyday lives. The coming proliferation of virtual and augmented worlds will make manifest the idea that there is far more to reality than we are normally aware of, and that there are countless virtual realms that can now be brought into conscious experience.},
keywords={virtual reality;portable reality;available space expansion;virtual worlds;immersive games;powerful graphics;high resolution displays;precision sensors;immersive virtual reality platform},
doi={10.1109/ISMAR.2012.6402515},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402516,
author={S. {Izadi}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={A new era of Human Computer Interaction},
year={2012},
volume={},
number={},
pages={1-2},
abstract={For most researchers outside of the field, Human Computer Interaction (or HCI) is the study and evaluation of interactive systems and techniques. While this is an important part of our discipline, nowadays HCI is as much about “building” the underlying technologies and systems as it is studying their use. In this talk I will demonstrate why it is an exciting time to be a computer science researcher in this discipline. You can play with the newest technologies, such as exotic cameras, displays and sensing hardware; readily embrace approaches outside of your discipline (e.g. within computer vision, machine learning, signal processing, or computer graphics); and even invent technologies and algorithms along the way. However, ultimately, you'll build working systems that are grounded by real-world problems that have direct impact on users.},
keywords={human computer interaction;interactive systems;human computer interaction;HCI;interactive systems;computer science researcher;exotic cameras;sensing hardware;real-world problems},
doi={10.1109/ISMAR.2012.6402516},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402518,
author={M. {Gandy} and K. {Kiyokawa} and G. {Reitmayr}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Preface},
year={2012},
volume={},
number={},
pages={1-3},
abstract={We are delighted to welcome you to ISMAR 2012, the 11th symposium on Mixed and Augmented Reality! This yearâ €™s symposium continues a long tradition of ISMAR meetings, a series that itself followed a related series of IWAR, ISMR, and ISAR meetings.},
keywords={},
doi={10.1109/ISMAR.2012.6402518},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402519,
author={},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Program Committee]},
year={2012},
volume={},
number={},
pages={1-1},
abstract={The following topics are dealt with: mixed reality; and augmented reality.},
keywords={augmented reality;mixed reality;augmented reality},
doi={10.1109/ISMAR.2012.6402519},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402520,
author={},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Reviewers},
year={2012},
volume={},
number={},
pages={1-6},
abstract={The conference offers a note of thanks and lists its reviewers.},
keywords={},
doi={10.1109/ISMAR.2012.6402520},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402517,
author={},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Steering Committee]},
year={2012},
volume={},
number={},
pages={1-1},
abstract={Provides a listing of current committee members.},
keywords={},
doi={10.1109/ISMAR.2012.6402517},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402521,
author={ {Changhyun Choi} and H. {Christensen} and M. {Huber}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Tracking competition},
year={2012},
volume={},
number={},
pages={1-1},
abstract={This is the fifth year that ISMAR is organizing a tracking competition. It is encouraging to see how computing, imaging and new algorithms are coming together to enable increasingly complex tasks on mobile platforms.},
keywords={},
doi={10.1109/ISMAR.2012.6402521},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402522,
author={N. {Stojanovic} and A. {Damala} and T. {Schuchert} and L. {Stojanovic} and S. {Fairclough} and J. {Moores}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Tutorial 1: Adaptive augmented reality (A2R): Where AR meets user's interest},
year={2012},
volume={},
number={},
pages={1-1},
abstract={Adaptive behavior is one of the main challenges in building computerized systems, especially in the case of systems which are delivering information to the end users. Indeed, since the information overload has become the main drawback for the future development of such systems (cf. Big Data challenge), there is a huge movement in the research community to develop concepts for better adaptation of the form and size of information that will be delivered to a user (usually taking different forms of the personalization). However, the main effort has been dedicated to the contextualization of the user's task in order to determine what is the best way to tailor/adapt the presentation of information to the user, neglecting the role of the user's internal context, expressed as the user's (short-term) interest. The same is valid for the AR systems. In this tutorial we present novel results in modeling users' interest in the context of AR systems and demonstrate some practical results in realizing such an approach in a multisensor AR system based on the usage of the see-through AR glasses. Due to the need for continuously adapt the AR content to the user's interest, such models are facing many challenges in sensing the user's behavior (using acoustic-, video-, gesture- and bio-sensors), interpreting it as an interest and deciding in real-time what kind of the adaptation to perform. We argue that this lead to a new class of AR system that we coined as adaptive AR (AR) systems. This work has been partially realized within the scope of the FP7 ICT research project ARtSENSE (www.artsense.eu), that is developing new AR concepts for improving personalized museum's experience. The tutorial will present practical results from applying the approach in three cultural heritage institutions in Europe (Paris, Madrid and Liverpool).},
keywords={},
doi={10.1109/ISMAR.2012.6402522},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402523,
author={E. {Barba} and J. {Bolter} and M. {Engberg} and I. {Kulka} and R. {Rouse}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Tutorial 2: Integrating and using panoramas and photographic images in AR experiences},
year={2012},
volume={},
number={},
pages={1-1},
abstract={Some AR browsers and other mobile phone apps (e.g. Argon, Photosynth, 360 Cities, Tourwrist) allow the user to create, display or interact with panoramas and other forms of historical and contemporary imagery. These technologies open up exciting possibilities for cultural heritage, entertainment and other uses in location-based experiences. Full panoramas or historical photographs merged into the visual field can provide the user with a perspective on a place as it looked in the past or might look in a possible future. We propose to offer the participants in this tutorial an introduction to the technical issues involved in creating and integrating such imagery into an AR/MR application. We will also provide relevant historical background regarding panoramas and consider issues of aesthetics and user experience design.},
keywords={},
doi={10.1109/ISMAR.2012.6402523},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402524,
author={J. {Chastine} and J. A. {Preston} and T. {Tseng}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Tutorial 3: AR mobile game development: Getting started},
year={2012},
volume={},
number={},
pages={1-1},
abstract={This tutorial is a half-day project based tutorial to demonstrate how to create an AR mobile game prototype from game design to art, animation and technical production. Tools such as Unity, Maya and Vuforia will be used in this tutorial. Standard game development topics which can be applied to all digital game projects such as the game design process, pre-production planning, 3D modeling, rigging and animation techniques, game engine workflow as well as the unique elements of AR mobile game development will be covered in this tutorial.},
keywords={},
doi={10.1109/ISMAR.2012.6402524},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402525,
author={M. {Adcock} and J. {Quarles} and A. {Simon}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Tutorials},
year={2012},
volume={},
number={},
pages={1-1},
abstract={It is our great pleasure to present the ISMAR Tutorials. We proudly host four tutorials that provide sharing of knowledge from seasoned researchers. Our tutorials cover a wide range of topics including evaluation techniques, game design, adaptive augmented reality, and panoramas. Through these exciting tutorials we hope to expand the minds of ISMAR 2012 attendees and help to foster the next generation of Mixed and Augmented Reality researchers, practitioners, and artists.},
keywords={},
doi={10.1109/ISMAR.2012.6402525},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402526,
author={D. {Kalkofen} and C. {Reynolds} and H. {Seichter}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Workshop},
year={2012},
volume={},
number={},
pages={1-1},
abstract={It is our great pleasure to present this year's ISMAR Workshops. These events provide a chance to thoroughly examine specific research areas in the exciting field of Mixed and Augmented Reality.},
keywords={},
doi={10.1109/ISMAR.2012.6402526},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402527,
author={M. {Billinghurst} and T. {Langlotz} and B. {MacIntyre} and H. {Seichter}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Workshop 1: 2nd IEEE ISMAR workshop on authoring solutions for augmented reality},
year={2012},
volume={},
number={},
pages={1-2},
abstract={The motivation of this workshop is to discuss future direction of content authoring in the field of Augmented Reality, as well as to discuss the current state of art on content creation and content authoring for augmented reality. The workshop will comprise of a paper session where authoring papers, late-breaking results and overviews over state-of-the-art are presented. In the afternoon, we will follow up with discussion sessions on different topics ranging from content creation and authoring to content distribution for AR and a short closing session.},
keywords={},
doi={10.1109/ISMAR.2012.6402527},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402528,
author={S. {Feiner} and K. {Kiyokawa} and G. {Klinker} and M. {Dennis} and C. {Woodward}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Workshop 2: Classifying the AR presentation space},
year={2012},
volume={},
number={},
pages={1-2},
abstract={Already 3D visualization environments provide a large design space not being investigated to the same extent as traditional WIMP-spaces. When using this design space in combination with AR, the design space even further grows. Information can not only be presented in a 3D space, AR also puts virtual information in relation to real objects, locations or events. The different properties of presentation in AR need to be investigated to develop a comprehensive set of dimensions of presentation principles.},
keywords={},
doi={10.1109/ISMAR.2012.6402528},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402529,
author={D. {Wagner} and J. {Ventura} and G. {Reitmayr} and H. {Saito} and S. {Benhimane}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Workshop 3: IEEE ISMAR 2012 workshop on tracking methods and applications (TMA)},
year={2012},
volume={},
number={},
pages={1-2},
abstract={The focus of this workshop is on presenting, discussing and demonstrating recent tracking methods and applications that work well in practice and that show some superiority over state-of-the-art methods. Rather than focusing on pure novelty, this workshop encourages presentations that concentrate on complete systems and integrated approaches. The TMA workshop looks at pose tracking from an end-to-end point of view.},
keywords={},
doi={10.1109/ISMAR.2012.6402529},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402530,
author={},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Science technology papers},
year={2012},
volume={},
number={},
pages={1-2},
abstract={Start of the above-titled section of the conference proceedings record.},
keywords={},
doi={10.1109/ISMAR.2012.6402530},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402531,
author={J. {Ventura} and T. {Höllerer}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Wide-area scene mapping for mobile visual tracking},
year={2012},
volume={},
number={},
pages={3-12},
abstract={We propose a system for easily preparing arbitrary wide-area environments for subsequent real-time tracking with a handheld device. Our system evaluation shows that minimal user effort is required to initialize a camera tracking session in an unprepared environment. We combine panoramas captured using a handheld omnidirectional camera from several viewpoints to create a point cloud model. After the offline modeling step, live camera pose tracking is initialized by feature point matching, and continuously updated by aligning the point cloud model to the camera image. Given a reconstruction made with less than five minutes of video, we achieve below 25 cm translational error and 0.5 degrees rotational error for over 80% of images tested. In contrast to camera-based simultaneous localization and mapping (SLAM) systems, our methods are suitable for handheld use in large outdoor spaces.},
keywords={augmented reality;image matching;image reconstruction;pose estimation;tracking;video signal processing;wide-area scene mapping;mobile visual tracking;subsequent real-time tracking;handheld device;system evaluation;camera tracking session;panorama;handheld omnidirectional camera;point cloud model;offline modeling step;live camera pose tracking;feature point matching;wide-area augmented reality;image reconstruction;Cameras;Mobile handsets;Image reconstruction;Real-time systems;Streaming media;Servers;Accuracy;I.2.10 [Artificial Intelligence]: Vision and Scene Understanding—3D/stereo scene analysis;I.4.8 [Image Processing and Computer Vision]: Scene Analysis—Tracking;I.5.4 [Pattern Recognition]: Applications—Computer Vision;C.5.3 [Computer System Implementation]: Microcomputers—Portable Devices (e.g., laptops, personal digital assistants)},
doi={10.1109/ISMAR.2012.6402531},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402532,
author={S. {Gauglitz} and C. {Sweeney} and J. {Ventura} and M. {Turk} and T. {Höllerer}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Live tracking and mapping from both general and rotation-only camera motion},
year={2012},
volume={},
number={},
pages={13-22},
abstract={We present an approach to real-time tracking and mapping that supports any type of camera motion in 3D environments, that is, general (parallax-inducing) as well as rotation-only (degenerate) motions. Our approach effectively generalizes both a panorama mapping and tracking system and a keyframe-based Simultaneous Localization and Mapping (SLAM) system, behaving like one or the other depending on the camera movement. It seamlessly switches between the two and is thus able to track and map through arbitrary sequences of general and rotation-only camera movements. Key elements of our approach are to design each system component such that it is compatible with both panoramic data and Structure-from-Motion data, and the use of the `Geometric Robust Information Criterion' to decide whether the transformation between a given pair of frames can best be modeled with an essential matrix E, or with a homography H. Further key features are that no separate initialization step is needed, that the reconstruction is unbiased, and that the system continues to collect and map data after tracking failure, thus creating separate tracks which are later merged if they overlap. The latter is in contrast to most existing tracking and mapping systems, which suspend tracking and mapping, thus discarding valuable data, while trying to relocalize the camera with respect to the initial map. We tested our system on a variety of video sequences, successfully tracking through different camera motions and fully automatically building panoramas as well as 3D structures.},
keywords={cameras;image sequences;live tracking;live mapping;rotation-only camera motion;real-time tracking;real-time mapping;3D environments;keyframe-based simultaneous localization and mapping system;structure-from-motion data;geometric robust information criterion;homography H;video sequences;automatically building panoramas;Cameras;Tracking;Simultaneous localization and mapping;Real-time systems;Robustness;Merging;Data models},
doi={10.1109/ISMAR.2012.6402532},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402533,
author={P. {McIlroy} and S. {Izadi} and A. {Fitzgibbon}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Kinectrack: Agile 6-DoF tracking using a projected dot pattern},
year={2012},
volume={},
number={},
pages={23-29},
abstract={We present Kinectrack, a new six degree-of-freedom (6-DoF) tracker which allows real-time and low-cost pose estimation using only commodity hardware. We decouple the dot pattern emitter and IR camera of the Kinect. Keeping the camera fixed and moving the IR emitter in the environment, we recover the 6-DoF pose of the emitter by matching the observed dot pattern in the field-of-view of the camera to a pre-captured reference image. We propose a novel matching technique to obtain dot pattern correspondences efficiently in wide- and adaptive-baseline scenarios. We also propose an auto-calibration method to obtain the camera intrinsics and dot pattern reference image. The performance of Kinectrack is evaluated and the rotational and translational accuracy of the system is measured relative to ground truth for both planar and multi-planar scene geometry. Our system can simultaneously recover the 6-DoF pose of the device and also recover piecewise planar 3D scene structure, and can be used as a low-cost method for tracking a device without any on-board computation, with small size and only simple electronics.},
keywords={augmented reality;Kinectrack;agile 6-DoF tracking;projected dot pattern;six degree-of-freedom;dot pattern emitter;IR camera;6-DoF pose;adaptive-baseline scenarios;auto-calibration method;camera intrinsics;translational accuracy;rotational accuracy;multiplanar scene geometry;piecewise planar 3D scene structure;on-board computation;Cameras;Calibration;Pattern matching;Accuracy;Runtime;Table lookup;Robustness},
doi={10.1109/ISMAR.2012.6402533},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402534,
author={},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Blank page]},
year={2012},
volume={},
number={},
pages={30-30},
abstract={This page or pages intentionally left blank.},
keywords={},
doi={10.1109/ISMAR.2012.6402534},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402535,
author={A. {Roussos} and C. {Russell} and R. {Garg} and L. {Agapito}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Dense multibody motion estimation and reconstruction from a handheld camera},
year={2012},
volume={},
number={},
pages={31-40},
abstract={Existing approaches to camera tracking and reconstruction from a single handheld camera for Augmented Reality (AR) focus on the reconstruction of static scenes. However, most real world scenarios are dynamic and contain multiple independently moving rigid objects. This paper addresses the problem of simultaneous segmentation, motion estimation and dense 3D reconstruction of dynamic scenes. We propose a dense solution to all three elements of this problem: depth estimation, motion label assignment and rigid transformation estimation directly from the raw video by optimizing a single cost function using a hill-climbing approach. We do not require prior knowledge of the number of objects present in the scene - the number of independent motion models and their parameters are automatically estimated. The resulting inference method combines the best techniques in discrete and continuous optimization: a state of the art variational approach is used to estimate the dense depth maps while the motion segmentation is achieved using discrete graph-cut based optimization. For the rigid motion estimation of the independently moving objects we propose a novel tracking approach designed to cope with the small fields of view they induce and agile motion. Our experimental results on real sequences show how accurate segmentations and dense depth maps can be obtained in a completely automated way and used in marker-free AR applications.},
keywords={augmented reality;cameras;graph theory;image reconstruction;image segmentation;image sequences;inference mechanisms;motion estimation;natural scenes;object tracking;optimisation;robot vision;variational techniques;video signal processing;dense multibody rigid motion estimation;handheld camera tracking;augmented reality;static scene reconstruction;dynamic scene segmentation;dynamic scene motion estimation;dynamic scene dense 3D reconstruction;depth estimation;motion label assignment;rigid transformation estimation;raw video;cost function optimization;hill-climbing approach;inference method;discrete-continuous optimization;variational approach;dense depth map estimation;discrete graph-cut based optimization;independently moving rigid objects;agile motion;field-of-view;image sequences;marker-free AR applications;Cameras;Optimization;Motion segmentation;Image reconstruction;Tracking;Estimation;Motion estimation},
doi={10.1109/ISMAR.2012.6402535},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402536,
author={W. {Yii} and {Wai Ho Li} and T. {Drummond}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Distributed visual processing for augmented reality},
year={2012},
volume={},
number={},
pages={41-48},
abstract={Recent advances have made augmented reality on smartphones possible but these applications are still constrained by the limited computational power available. This paper presents a system which combines smartphones with networked infrastructure and fixed sensors and shows how these elements can be combined to deliver real-time augmented reality. A key feature of this framework is the asymmetric nature of the distributed computing environment. Smartphones have high bandwidth video cameras but limited computational ability. Our system connects multiple smartphones through relatively low bandwidth network links to a server with large computational resources connected to fixed sensors that observe the environment. By contrast to other systems that use preprocessed static models or markers, our system has the ability to rapidly build dynamic models of the environment on the fly at frame rate. We achieve this by processing data from a Microsoft Kinect, to build a trackable point cloud model of each frame. The smartphones process their video camera data on-board to extract their own set of compact and efficient feature descriptors which are sent via WiFi to a server. The server runs computationally intensive algorithms including feature matching, pose estimation and occlusion testing for each smartphone. Our system demonstrates real-time performance for two smartphones.},
keywords={augmented reality;data visualisation;feature extraction;pattern matching;pose estimation;smart phones;video cameras;wireless LAN;distributed visual processing;smartphone;computational power;networked infrastructure;fixed sensors;real-time augmented reality;distributed computing environment;computational ability;bandwidth network link;preprocessed static model;dynamic model;Microsoft Kinect;point cloud model;video camera data;feature descriptor extraction;WiFi;server;feature matching;pose estimation;occlusion testing;Smart phones;Cameras;Servers;Sensors;Augmented reality;Visualization;Hip},
doi={10.1109/ISMAR.2012.6402536},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402537,
author={ {Xin Yang} and {Kwang-Ting Cheng}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={LDB: An ultra-fast feature for scalable Augmented Reality on mobile devices},
year={2012},
volume={},
number={},
pages={49-57},
abstract={The efficiency, robustness and distinctiveness of a feature descriptor are critical to the user experience and scalability of a mobile Augmented Reality (AR) system. However, existing descriptors are either too compute-expensive to achieve real-time performance on a mobile device such as a smartphone or tablet, or not sufficiently robust and distinctive to identify correct matches from a large database. As a result, current mobile AR systems still only have limited capabilities, which greatly restrict their deployment in practice. In this paper, we propose a highly efficient, robust and distinctive binary descriptor, called Local Difference Binary (LDB). LDB directly computes a binary string for an image patch using simple intensity and gradient difference tests on pairwise grid cells within the patch. A multiple gridding strategy is applied to capture the distinct patterns of the patch at different spatial granularities. Experimental results demonstrate that LDB is extremely fast to compute and to match against a large database due to its high robustness and distinctiveness. Comparing to the state-of-the-art binary descriptor BRIEF, primarily designed for speed, LDB has similar computational efficiency, while achieves a greater accuracy and 5x faster matching speed when matching over a large database with 1.7M+ descriptors.},
keywords={augmented reality;gradient methods;mobile computing;notebook computers;smart phones;ultra-fast feature;scalable augmented reality;mobile devices;mobile augmented reality system;smartphone;tablet;mobile AR systems;local difference binary;LDB;image patch;gradient difference tests;multiple gridding strategy;BRIEF;Robustness;Abstracts;Virtual reality;Performance evaluation;Lighting;Correlation;Image coding;Augmented reality;binary feature descriptor;mobile devices;object recognition;tracking},
doi={10.1109/ISMAR.2012.6402537},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402538,
author={},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Blank page]},
year={2012},
volume={},
number={},
pages={58-58},
abstract={This page or pages intentionally left blank.},
keywords={},
doi={10.1109/ISMAR.2012.6402538},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402539,
author={G. {Woo} and A. {Lippman} and R. {Raskar}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={VRCodes: Unobtrusive and active visual codes for interaction by exploiting rolling shutter},
year={2012},
volume={},
number={},
pages={59-64},
abstract={We show a new visible tagging solution for active displays which allows a rolling-shutter camera to detect active tags from a relatively large distance in a robust manner. Current planar markers are visually obtrusive for the human viewer. In order for them to be read from afar and embed more information, they must be shown larger thus occupying valuable physical space on the design. We present a new active visual tag which utilizes all dimensions of color, time and space while remaining unobtrusive to the human eye and decodable using a 15fps rolling-shutter camera. The design exploits the flicker fusion-frequency threshold of the human visual system, which due to the effect of metamerism, can not resolve metamer pairs alternating beyond 120Hz. Yet, concurrently, it is decodable using a 15fps rolling-shutter camera due to the effective line-scan speed of 15×400 lines per second. We show an off-the-shelf rolling-shutter camera can resolve the metamers flickering on a television from a distance over 4 meters. We use intelligent binary coding to encode digital positioning and show potential applications such as large screen interaction. We analyze the use of codes for locking and tracking encoded targets. We also analyze the constraints and performance of the sampling system, and discuss several plausible application scenarios.},
keywords={augmented reality;computer vision;image coding;VRCodes;unobtrusive code;active visual codes;rolling shutter;visible tagging solution;active display;planar marker;active visual tag;flicker fusion-frequency threshold;human visual system;metamers flickering;intelligent binary coding;digital positioning;large screen interaction;sampling system;Image color analysis;Cameras;Humans;Colored noise;Decoding;Visualization;Solids;Augmented Reality [Tracking]: Barcodes—Visual Markers;Human Visual System [Metamerism]: Fusion Flicker Threshold—},
doi={10.1109/ISMAR.2012.6402539},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402540,
author={D. {Kurz} and T. {Olszamowski} and S. {Benhimane}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Representative feature descriptor sets for robust handheld camera localization},
year={2012},
volume={},
number={},
pages={65-70},
abstract={We present a method to automatically determine a set of feature descriptors that describes an object such that it can be localized under a variety of viewpoints. Based on a set of synthetically generated views, local image features are detected, described and aggregated in a database. Our proposed method evaluates matches between these database features to eventually find a set of the most representative descriptors from the database. Using this scalable offline process, the localization success rate is significantly increased without adding computational load to the runtime method. Moreover, if camera localization is performed with respect to objects at a known gravity orientation, we propose to create multiple reference descriptor sets for different angles between the camera's principal axis and the gravity vector. This approach is particularly suited for handheld devices with built-in inertial sensors and enables matching against a reference dataset only containing the information relevant for camera poses that are consistent with the measured gravity. Comprehensive evaluations of the proposed methods using a large quantity of real camera images, a variety of objects, different cameras and different kinds of feature descriptors confirm that our approaches outperform standard feature descriptor-based methods.},
keywords={cameras;computational complexity;image processing;realistic images;set theory;vectors;visual databases;representative feature descriptor sets;robust handheld camera localization;local image features;database features;representative descriptors;scalable offline process;localization success rate;computational load;runtime method;gravity orientation;multiple reference descriptor sets;principal axis;gravity vector;handheld devices;built-in inertial sensors;reference dataset;camera poses;measured gravity;comprehensive evaluations;real camera images;feature descriptor-based methods;Cameras;Gravity;Vectors;Feature extraction;Sensors;Databases;Mobile handsets},
doi={10.1109/ISMAR.2012.6402540},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402541,
author={T. {Oskiper} and S. {Samarasekera} and R. {Kumar}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Multi-sensor navigation algorithm using monocular camera, IMU and GPS for large scale augmented reality},
year={2012},
volume={},
number={},
pages={71-80},
abstract={Camera tracking system for augmented reality applications that can operate both indoors and outdoors is described. The system uses a monocular camera, a MEMS-type inertial measurement unit (IMU) with 3-axis gyroscopes and accelerometers, and GPS unit to accurately and robustly track the camera motion in 6 degrees of freedom (with correct scale) in arbitrary indoor or outdoor scenes. IMU and camera fusion is performed in a tightly coupled manner by an error-state extended Kalman filter (EKF) such that each visually tracked feature contributes as an individual measurement as opposed to the more traditional approaches where camera pose estimates are first extracted by means of feature tracking and then used as measurement updates in a filter framework. Robustness in feature tracking and hence in visual measurement generation is achieved by IMU aided feature matching and a two-point relative pose estimation method, to remove outliers from the raw feature point matches. Landmark matching to contain long-term drift in orientation via on the fly user generated geo-tiepoint mechanism is described.},
keywords={accelerometers;augmented reality;cameras;Global Positioning System;gyroscopes;image matching;Kalman filters;microsensors;nonlinear filters;pose estimation;sensor fusion;target tracking;multisensor navigation algorithm;monocular camera;large scale augmented reality application;camera tracking system;MEMS-type inertial measurement unit;MEMS-type IMU;3-axis gyroscopes;accelerometers;GPS unit;camera motion;outdoor scenes;indoor scenes;error-state extended Kalman filter;error-state EKF;visually tracked feature;individual measurement;camera pose estimation;feature tracking;measurement updates;filter framework;visual measurement generation;IMU aided feature matching;two-point relative pose estimation method;raw feature point matching;landmark matching;long-term drift;fly user generated geo-tiepoint mechanism;Cameras;Vectors;Current measurement;Measurement uncertainty;Kalman filters;Mathematical model;Feature extraction;MEMS IMU;monocular camera;GPS;inertial navigation;sensor fusion;EKF},
doi={10.1109/ISMAR.2012.6402541},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402542,
author={D. {Pustka} and J. {Hülß} and J. {Willneff} and F. {Pankratz} and M. {Huber} and G. {Klinker}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Optical outside-in tracking using unmodified mobile phones},
year={2012},
volume={},
number={},
pages={81-89},
abstract={Marker-based optical outside-in tracking is a mature and robust technology used by many AR, VR and motion capture applications. However, in small environments the tracking cameras are often difficult to install. An example scenario are ergonomic studies in car manufacturing, where the motion of a worker needs to be tracked in small spaces such as the trunk of a car. In this paper, we describe how to extend the tracking volume in small, cluttered environments using small and flexible wireless cameras in form of unmodified mobile phones that can quickly be installed. Since those small cameras are not synchronized with the main tracking cameras, we describe several modifications to the tracking algorithms, such as inter-frame interpolation, the replacement of the least-squares adjustment by a Kalman filter and the integration of rolling-shutter compensation. To support the quick setup of additional cameras while the tracking system is running, the system is extended by an on-line calibration technique that determines the extrinsic camera parameters without requiring a dedicated calibration step.},
keywords={calibration;cameras;compensation;image motion analysis;interpolation;Kalman filters;least squares approximations;mobile computing;mobile radio;object tracking;unmodified mobile phone;marker-based optical outside-in tracking;AR;VR;motion capture application;tracking camera;tracking volume;cluttered environment;wireless camera;tracking algorithm;interframe interpolation;least-squares adjustment;Kalman filter;rolling-shutter compensation;tracking system;on-line calibration technique;camera parameter;Cameras;Target tracking;Kalman filters;Mobile handsets;Synchronization;Calibration;I.4.8 [Image Processing and Computer Vision]: Scene Analysis—Tracking;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities},
doi={10.1109/ISMAR.2012.6402542},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402543,
author={},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Blank page]},
year={2012},
volume={},
number={},
pages={90-90},
abstract={This page or pages intentionally left blank.},
keywords={},
doi={10.1109/ISMAR.2012.6402543},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402544,
author={J. {Jachnik} and R. A. {Newcombe} and A. J. {Davison}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Real-time surface light-field capture for augmentation of planar specular surfaces},
year={2012},
volume={},
number={},
pages={91-97},
abstract={A single hand-held camera provides an easily accessible but potentially extremely powerful setup for augmented reality. Capabilities which previously required expensive and complicated infrastructure have gradually become possible from a live monocular video feed, such as accurate camera tracking and, most recently, dense 3D scene reconstruction. A new frontier is to work towards recovering the reflectance properties of general surfaces and the lighting configuration in a scene without the need for probes, omni-directional cameras or specialised light-field cameras. Specular lighting phenomena cause effects in a video stream which can lead current tracking and reconstruction algorithms to fail. However, the potential exists to measure and use these effects to estimate deeper physical details about an environment, enabling advanced scene understanding and more convincing AR. In this paper we present an algorithm for real-time surface light-field capture from a single hand-held camera, which is able to capture dense illumination information for general specular surfaces. Our system incorporates a guidance mechanism to help the user interactively during capture. We then split the light-field into its diffuse and specular components, and show that the specular component can be used for estimation of an environment map. This enables the convincing placement of an augmentation on a specular surface such as a shiny book, with realistic synthesized shadow, reflection and occlusion of specularities as the viewpoint changes. Our method currently works for planar scenes, but the surface light-field representation makes it ideal for future combination with dense 3D reconstruction methods.},
keywords={augmented reality;cameras;image representation;real-time surface light-field capture;planar specular surfaces augmentation;single hand-held camera;live monocular video;3D scene reconstruction;reflectance properties;lighting configuration;specialised light-field cameras;reconstruction algorithms;tracking algorithms;AR;augmented reality;single hand-held camera;general specular surfaces;specular components;viewpoint changes;surface light-field representation;3D reconstruction methods;Cameras;Lighting;Real-time systems;Surface treatment;Image color analysis;Surface texture;Light sources;Real-Time;Light-Fields;Illumination Estimation;GPU;SLAM;AR},
doi={10.1109/ISMAR.2012.6402544},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402545,
author={},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Blank page]},
year={2012},
volume={},
number={},
pages={98-98},
abstract={This page or pages intentionally left blank.},
keywords={},
doi={10.1109/ISMAR.2012.6402545},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402546,
author={P. {Kán} and H. {Kaufmann}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={High-quality reflections, refractions, and caustics in Augmented Reality and their contribution to visual coherence},
year={2012},
volume={},
number={},
pages={99-108},
abstract={In this paper we present a novel high-quality rendering system for Augmented Reality (AR). We study ray-tracing based rendering techniques in AR with the goal of achieving real-time performance and improving visual quality as well as visual coherence between real and virtual objects in a final composited image. A number of realistic and physically correct rendering effects are demonstrated, that have not been presented in real-time AR environments before. Examples are high-quality specular effects such as caustics, refraction, reflection, together with a depth of field effect and anti-aliasing. We present a new GPU implementation of photon mapping and its application for the calculation of caustics in environments where real and virtual objects are combined. The composited image is produced on-the-fly without the need of any preprocessing step. A main contribution of our work is the achievement of interactive rendering speed for high-quality ray-tracing algorithms in AR setups. Finally we performed an evaluation to study how users perceive visual quality and visual coherence with different realistic rendering effects. The results of our user study show that in 40.1% cases users mistakenly judged virtual objects as real ones. Moreover we show that high-quality rendering positively affects the perceived visual coherence.},
keywords={augmented reality;graphics processing units;ray tracing;rendering (computer graphics);augmented reality;visual coherence;high-quality rendering system;ray-tracing based rendering technique;visual quality;caustics effect;refraction effect;reflection effect;depth of field effect;antialiasing;GPU;photon mapping;interactive rendering speed;Rendering (computer graphics);Photonics;Ray tracing;Lighting;Visualization;Cameras;Kernel;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems —Artificial, augmented, and virtual realities;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism —Raytracing},
doi={10.1109/ISMAR.2012.6402546},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402547,
author={P. {Lensing} and W. {Broll}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Instant indirect illumination for dynamic mixed reality scenes},
year={2012},
volume={},
number={},
pages={109-118},
abstract={For seamless integration of virtual content into real scenes, realizing mutual global lighting effects between both worlds belongs to the most important and challenging goals. Therefore, plenty of global illumination approaches exist, which mostly share the same restriction: the real scene is approximated by a static model, which was built in advance and thus has to remain static. In our paper, we propose an image-space global illumination approach, based on reflective shadow maps, combined with the use of an RGB-D camera, to simulate first bounce diffuse indirect illumination without any pre-computations. Our approach supports indirect illumination in both directions (real to virtual and vice versa) and runs in real-time. Furthermore, it does not require advanced shader properties, since we developed an implementation making efficient usage of the Z-Buffer algorithm for calculating indirect illumination.},
keywords={augmented reality;image processing;instant indirect illumination;dynamic mixed reality scene;mutual global lighting effect;static model;image-space global illumination;reflective shadow maps;RGB-D camera;diffuse indirect illumination;Z-Buffer algorithm;Lighting;Virtual reality;Real-time systems;Image resolution;Cameras;Filtering;Rendering (computer graphics);Mixed Reality;Augmented Reality;real-time global illumination;image based lighting;occlusions;depth camera;kinect},
doi={10.1109/ISMAR.2012.6402547},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402548,
author={L. {Gruber} and T. {Richter-Trummer} and D. {Schmalstieg}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Real-time photometric registration from arbitrary geometry},
year={2012},
volume={},
number={},
pages={119-128},
abstract={Visually coherent rendering for augmented reality is concerned with seamlessly blending the virtual world and the real world in real-time. One challenge in achieving this is the correct handling of lighting. We are interested in applying real-world light to virtual objects, and compute the interaction of light between virtual and real. This implies the measurement of the real-world lighting, also known as photometric registration. So far, photometric registration has mainly been done through capturing images with artificial light probes, such as mirror balls or planar markers, or by using high dynamic range cameras with fish-eye lenses. In this paper, we present a novel non-invasive system, using arbitrary scene geometry as a light probe for photometric registration, and a general AR rendering pipeline supporting real-time global illumination techniques. Based on state of the art real-time geometric reconstruction, we show how to robustly extract data for photometric registration to compute a realistic representation of the real-world diffuse lighting. Our approach estimates the light from observations of the reconstructed model and is based on spherical harmonics, enabling plausible illumination such as soft shadows, in a mixed virtual-real rendering pipeline.},
keywords={augmented reality;cameras;image registration;rendering (computer graphics);real time photometric registration;visually coherent rendering;augmented reality;virtual world;artificial light probes;mirror balls;planar markers;dynamic range cameras;fish eye lenses;noninvasive system;arbitrary scene geometry;real time global illumination;real time geometric reconstruction;realistic representation;real world diffuse lighting;reconstructed model;spherical harmonics;mixed virtual real rendering pipeline;Lighting;Rendering (computer graphics);Estimation;Geometry;Cameras;Mathematical model;Real-time systems;H.5.1 [Information Interfaces and Presentation]: Artificial, augmented,Virtual Realities—;I.4.8 [Image Processing and Computer Vision]: Photometric registration—3D Reconstruction;I.3.3 [Computer Graphics]: Image Generation—Spherical Harmonics},
doi={10.1109/ISMAR.2012.6402548},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402549,
author={T. {Fukiage} and T. {Oishi} and K. {Ikeuchi}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Reduction of contradictory partial occlusion in mixed reality by using characteristics of transparency perception},
year={2012},
volume={},
number={},
pages={129-139},
abstract={One of the challenges in mixed reality (MR) applications is handling contradictory occlusions between real and virtual objects. The previous studies have tried to solve the occlusion problem by extracting the foreground region from the real image. However, real-time occlusion handling is still difficult since it takes too much computational cost to precisely segment foreground regions in a complex scene. In this study, therefore, we proposed an alternative solution to the occlusion problem that does not require precise foreground-background segmentation. In our method, a virtual object is blended with a real scene so that the virtual object can be perceived as being behind the foreground region. For this purpose, we first investigated characteristics of human transparency perception in a psychophysical experiment. Then we made a blending algorithm applicable to real scenes based on the results of the experiment.},
keywords={augmented reality;image segmentation;realistic images;real-time systems;contradictory partial occlusion reduction;mixed reality applications;contradictory occlusions;real objects;virtual objects;real image;real-time occlusion handling;computational cost;foreground regions;foreground-background segmentation;human transparency perception;psychophysical experiment;blending algorithm;real scenes;Virtual reality;Predictive models;Mathematical model;Humans;Equations;Observers;Real-time systems;Mixed Reality;Augmented Reality;transparency perception},
doi={10.1109/ISMAR.2012.6402549},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402550,
author={},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Blank page]},
year={2012},
volume={},
number={},
pages={140-140},
abstract={This page or pages intentionally left blank.},
keywords={},
doi={10.1109/ISMAR.2012.6402550},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402551,
author={J. {Herling} and W. {Broll}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={PixMix: A real-time approach to high-quality Diminished Reality},
year={2012},
volume={},
number={},
pages={141-150},
abstract={Diminished Reality (DR) allows to remove objects from a video stream while preseving a frame to frame coherence. Some approaches apply a pseudo-DR, allowing for the removal of objects only, while their background can be observed by a second camera. Most real DR approaches are highly computational expensive, not even allowing for interactive rates and/or apply significant restrictions regarding the uniformity of the background, or allow linear camera movements or even a static camera only. In this paper we will present a real-time capable Diminished Reality approach for high-quality image manipulation. Our approach achieves a significantly better performance and image quality for almost planar but non-trivial image backgrounds. Our Diminished Reality pipeline provides coherent video streams even for nonlinear camera movements due to the integration of homography based object tracking.},
keywords={cameras;image sequences;object tracking;video signal processing;PixMix approach;object background removal;pseudoDR;high-quality image manipulation;planar nontrivial image backgrounds;high-quality diminished reality pipeline;coherent video stream preservation;nonlinear camera movements;homography-based object tracking integration;Streaming media;Real-time systems;Cameras;Coherence;Visualization;Cost function;Image segmentation;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities;I.3.6 [Computing Methodologies]: Computer Graphics—Methodology and Techniques;I.4.9 [Computing Methodologies]: Image Processing and Computer Vision—Applications},
doi={10.1109/ISMAR.2012.6402551},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402552,
author={J. {Chen} and G. {Turk} and B. {MacIntyre}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={A non-photorealistic rendering framework with temporal coherence for augmented reality},
year={2012},
volume={},
number={},
pages={151-160},
abstract={Many augmented reality (AR) applications require a seamless blending of real and virtual content as key to increased immersion and improved user experiences. Photorealistic and non-photorealistic rendering (NPR) are two ways to achieve this goal. Compared with photorealistic rendering, NPR stylizes both the real and virtual content and makes them indistinguishable. Maintaining temporal coherence is a key challenge in NPR. We propose a NPR framework with support for temporal coherence by leveraging model-space information. Our systems targets painterly rendering styles of NPR. There are three major steps in this rendering framework for creating coherent results: tensor field creation, brush anchor placement, and brush stroke reshaping. To achieve temporal coherence for the final rendered results, we propose a new projection-based surface sampling algorithm which generates anchor points on model surfaces. The 2D projections of these samples are uniformly distributed in image space for optimal brush stroke placement. We also propose a general method for averaging various properties of brush stroke textures, such as their skeletons and colors, to further improve the temporal coherence. We apply these methods to both static and animated models to create a painterly rendering style for AR. Compared with existing image space algorithms our method renders AR with NPR effects with a high degree of coherence.},
keywords={augmented reality;rendering (computer graphics);sampling methods;nonphotorealistic rendering;temporal coherence;augmented reality;NPR;model-space information;tensor field creation;brush anchor placement;brush stroke reshaping;projection-based surface sampling algorithm;anchor point;2D projection;brush stroke texture;static model;animated model;Coherence;Tensile stress;Rendering (computer graphics);Solid modeling;Computational modeling;Brushes;Algorithm design and analysis;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, Augmented, and Virtual Realities;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism—Virtual Reality},
doi={10.1109/ISMAR.2012.6402552},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402553,
author={W. {Lu} and B. H. {Duh} and S. {Feiner}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Subtle cueing for visual search in augmented reality},
year={2012},
volume={},
number={},
pages={161-166},
abstract={Visual search in augmented reality environments is an important task that can be facilitated through different cueing methods. Current cueing methods rely on explicit cueing, which can potentially reduce visual search performance. In comparison, this paper proposes a subtle cueing method that improves visual search performance while being clutter-neutral. Two empirical user studies were conducted to evaluate our subtle cueing method in outdoor scenes. The results show that subtle cueing functions well within a narrow Feature Congestion range, and could be a feasible alternative to explicit cueing.},
keywords={augmented reality;clutter;augmented reality environment;visual search performance;subtle cueing method;clutter-neutral;outdoor scene;feature congestion range;explicit cueing;Visualization;Clutter;Erbium;Protocols;Augmented reality;Educational institutions;Humans;Outdoor scenes;Subtle visual cueing;Visual search},
doi={10.1109/ISMAR.2012.6402553},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402554,
author={S. {Zollmann} and D. {Kalkofen} and C. {Hoppe} and S. {Kluckner} and H. {Bischof} and G. {Reitmayr}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Interactive 4D overview and detail visualization in augmented reality},
year={2012},
volume={},
number={},
pages={167-176},
abstract={In this paper we present an approach for visualizing time-oriented data of dynamic scenes in an on-site AR view. Visualizations of time-oriented data have special challenges compared to the visualization of arbitrary virtual objects. Usually, the 4D data occludes a large part of the real scene. Additionally, the data sets from different points in time may occlude each other. Thus, it is important to design adequate visualization techniques that provide a comprehensible visualization. In this paper we introduce a visualization concept that uses overview and detail techniques to present 4D data in different detail levels. These levels provide at first an overview of the 4D scene, at second information about the 4D change of a single object and at third detailed information about object appearance and geometry for specific points in time. Combining the three levels of detail with interactive transitions such as magic lenses or distorted viewing techniques enables the user to understand the relationship between them. Finally we show how to apply this concept for construction site documentation and monitoring.},
keywords={augmented reality;civil engineering computing;construction industry;data visualisation;document handling;interactive 4D overview;detail visualization;augmented reality;time-oriented data visualization;arbitrary virtual object;comprehensible visualization;overview technique;detail technique;object appearance;object geometry;construction site documentation;construction site monitoring;Data visualization;Context;Abstracts;Visualization;Image color analysis;Geometry;Rendering (computer graphics);Augmented Reality Visualization;Time-Oriented Visualization;Overview and Detail},
doi={10.1109/ISMAR.2012.6402554},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402555,
author={R. {Grasset} and T. {Langlotz} and D. {Kalkofen} and M. {Tatzgern} and D. {Schmalstieg}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Image-driven view management for augmented reality browsers},
year={2012},
volume={},
number={},
pages={177-186},
abstract={In this paper, we introduce a novel view management technique for placing labels in Augmented Reality systems. A common issue in many Augmented Reality applications is the absence of knowledge of the real environment, limiting the efficient representation and optimal layout of the digital information augmented onto the real world. To overcome this problem, we introduce an image-based approach, which combines a visual saliency algorithm with edge analysis to identify potentially important image regions and geometric constraints for placing labels. Our proposed solution also includes adaptive rendering techniques that allow a designer to control the appearance of depth cues. We describe the results obtained from a user study considering different scenarios, which we performed for validating our approach. Our technique will provide special benefits to Augmented Reality browsers that usually lack scene knowledge, but also to many other applications in the domain of Augmented Reality such as cultural heritage and maintenance applications.},
keywords={augmented reality;edge detection;geometry;rendering (computer graphics);image-driven view management;augmented reality browser;view management technique;augmented reality system;digital information;image-based approach;visual saliency algorithm;edge analysis;image region;geometric constraint;labels;adaptive rendering technique;depth cue appearance;scene knowledge;cultural heritage;maintenance application;Layout;Visualization;Image edge detection;Labeling;Image color analysis;Augmented reality;Linear programming;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, augmented and virtual realities;H.5.2 [Information Interfaces and Presentation]: User Interface—User interface management system (UIMS)},
doi={10.1109/ISMAR.2012.6402555},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402556,
author={A. {Dey} and G. {Jarvis} and C. {Sandor} and G. {Reitmayr}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Tablet versus phone: Depth perception in handheld augmented reality},
year={2012},
volume={},
number={},
pages={187-196},
abstract={Augmented Reality (AR) applications on mobile devices like smartphones and tablet computers have become increasingly popular. In this paper, for the first time in the AR domain, we present: (1) the influence of different handheld displays and (2) the exocentric depth perception. Unlike egocentric depth perception, exocentric depth perception has not been investigated in AR. We have selected a suitable vision-based tracking method for our user studies based on a set of evaluations. Then we have investigated the effect of display size and resolution through two user studies. One study investigated the effect of different displays on egocentric depth perception. The other study investigated the effect of displays on exocentric and ordinal depth perception. Interestingly, we noticed depth compression is less when using a mobile phone, while participants subjectively preferred a tablet. A similar effect was also noticed in exocentric depth perception. The tablet provided significantly better ordinal depth perception and faster response time than the mobile phone. In both of the studies, we found no effect of the AR X-ray visualization on depth perception. Both egocentric and exocentric distances were underestimated.},
keywords={augmented reality;mobile handsets;notebook computers;handheld augmented reality application;mobile device;smartphones;tablet computers;handheld displays;exocentric depth perception;egocentric depth perception;vision based tracking method;ordinal depth perception;mobile phone;augmented reality X-ray visualization;Visualization;Mobile handsets;Cameras;Target tracking;Mobile communication;Estimation;Augmented Reality;User Evaluation;X-ray Visualization;Handheld Displays;Outdoor Environment;Depth Perception},
doi={10.1109/ISMAR.2012.6402556},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402557,
author={D. {Baričević} and C. {Lee} and M. {Turk} and T. {Höllerer} and D. A. {Bowman}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={A hand-held AR magic lens with user-perspective rendering},
year={2012},
volume={},
number={},
pages={197-206},
abstract={In this paper we present a user study evaluating the benefits of geometrically correct user-perspective rendering using an Augmented Reality (AR) magic lens. In simulation we compared a user-perspective magic lens against the common device-perspective magic lens on both phone-sized and tablet-sized displays. Our results indicate that a tablet-sized display allows for significantly faster performance of a selection task and that a user-perspective lens has benefits over a device-perspective lens for a selection task. Based on these promising results, we created a proof-of-concept prototype, engineered with current off-the-shelf devices and software. To our knowledge, this is the first geometrically correct user-perspective magic lens.},
keywords={augmented reality;lenses;rendering (computer graphics);hand-held AR magic lens;user-perspective rendering;augmented reality;geometrically correct user-perspective magic lens;phone-sized display;tablet-sized display;off-the-shelf device;Lenses;Cameras;Games;Performance evaluation;Visualization;Virtual reality;Sensors;User-perspective view;magic lens;user study;prototype;MR simulation;augmented reality},
doi={10.1109/ISMAR.2012.6402557},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402558,
author={O. {Oda} and S. {Feiner}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={3D referencing techniques for physical objects in shared augmented reality},
year={2012},
volume={},
number={},
pages={207-215},
abstract={We introduce an augmented reality referencing technique for shared environments that is designed to improve the accuracy with which one user can point out a real physical object to another user. Our technique, GARDEN (Gesturing in an Augmented Reality Depth-mapped ENvironment), is intended for use in otherwise unmodeled environments in which objects in the environment, and the hand of the user performing a selection, are interactively observed by a depth camera, and users wear tracked see-through displays. We present the results of a user study that compares GARDEN against existing augmented reality referencing techniques, as well as the use of a physical laser pointer. GARDEN performed significantly more accurately than all the comparison techniques when the participating users have sufficiently different views of the scene, and significantly more accurately than one of these techniques when the participating users have similar perspectives.},
keywords={augmented reality;display devices;3D referencing techniques;physical objects;shared augmented reality;augmented reality referencing technique;shared environments;GARDEN;gesturing in an augmented reality depth-mapped environment;unmodeled environments;depth camera;wear tracked see-through displays;physical laser pointer;Cameras;Laser theory;Streaming media;Augmented reality;Accuracy;Animation;Collaborative mixed/augmented reality;referencing technique},
doi={10.1109/ISMAR.2012.6402558},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402559,
author={},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Blank page]},
year={2012},
volume={},
number={},
pages={216-216},
abstract={This page or pages intentionally left blank.},
keywords={},
doi={10.1109/ISMAR.2012.6402559},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402560,
author={M. {Sukan} and S. {Feiner} and B. {Tversky} and S. {Energin}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Quick viewpoint switching for manipulating virtual objects in hand-held augmented reality using stored snapshots},
year={2012},
volume={},
number={},
pages={217-226},
abstract={Magic-lens style augmented reality applications allow users to control camera pose easily by manipulating a portable hand-held device and provide immediate visual feedback. However, strategic vantage points must often be revisited repeatedly, adding time and error and taxing memory. We describe a new approach that allows users to take snapshots of augmented scenes that can be virtually revisited at later times. The system stores still images of scenes along with camera poses, so that augmentations remain dynamic and interactive. Users can manipulate virtual objects while viewing snapshots, instead of moving to real-world views. We present a study comparing performance in snapshot and live mode conditions in a task in which a virtual object must be aligned with two pairs of physical objects. Proper alignment requires sequentially visiting two viewpoints. Participants completed the alignment task significantly faster and more accurately using snapshots than when using the live mode. Moreover, participants preferred manipulating virtual objects using snapshots to the live mode.},
keywords={augmented reality;user interfaces;quick viewpoint switching;virtual object manipulation;hand-held augmented reality;stored snapshots;magic-lens style augmented reality;visual feedback;augmented scene;snapshot condition;live mode condition;virtual object;physical object;viewpoint alignment task;Cameras;Switches;Handheld computers;Visualization;Arrays;Legged locomotion;Feeds;Augmented reality;quick viewpoint switching;virtual travel},
doi={10.1109/ISMAR.2012.6402560},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402561,
author={I. {Radu} and B. {MacIntyre}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Using children's developmental psychology to guide augmented-reality design and usability},
year={2012},
volume={},
number={},
pages={227-236},
abstract={Augmented reality (AR) designers have great potential to enrich children's lives through AR experiences in education and entertainment. A significant difficulty in designing for children is that tremendous physical and cognitive development occurs across the first 10 years of life, and the changes in children's capabilities and limitations impact how these users respond to AR designs. Currently, little is known about how developmental changes relate to AR designs, or what AR designs are effective for young children. In this work, we focus on children 6-9 years old, presenting several concepts from developmental psychology and discussing how these relate to AR designs. Specifically, we investigate children's skills in the categories of motor abilities, spatial cognition, attention, logic and memory, and we discuss the relationship of these skills to current and hypothetical AR designs. Through this work, we intend to strengthen the field's understanding of AR usability and design, resulting in the generation of effective AR experiences for young users.},
keywords={augmented reality;human factors;psychology;software reusability;children developmental psychology;augmented-reality design;augmented-reality usability;AR experiences;education;entertainment;physical development;cognitive development;AR designs;children skills;motor abilities;spatial cognition;attention;logic;memory;Games;Augmented reality;Psychology;Cameras;Guidelines;Performance evaluation;Muscles;Augmented Reality;Children;Psychology;Interaction Design;Mixed Reality},
doi={10.1109/ISMAR.2012.6402561},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402562,
author={N. {Petersen} and D. {Stricker}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Learning task structure from video examples for workflow tracking and authoring},
year={2012},
volume={},
number={},
pages={237-246},
abstract={We present a robust real-time capable and simple framework for segmenting video sequences and live-streams of manual workflows into the comprising single tasks. Using classifiers trained on these segments we can follow a user that is performing the workflow in real-time as well as learn task variants from additional video examples. Our proposed method neither requires object detection nor high-level features. Instead we propose a novel measure derived from image distance that evaluates image properties jointly without prior segmentation. Our method can cope with repetitive and free-hand activities and the results are in many cases comparable or equal to manual task segmentation. One important application of our method is the automatic creation of a step-by-step task documentation from a video demonstration. The entire process to automatically create a fully functional augmented reality manual will be explained in detail and results are shown.},
keywords={augmented reality;image classification;image segmentation;image sequences;video signal processing;learning task structure;video example;workflow tracking;authoring;video segmentation;video sequence;live-streams;classifier;task variant;image distance;image properties;manual task segmentation;task documentation;video demonstration;augmented reality;Image segmentation;Motion segmentation;Robustness;Manuals;Augmented reality;Training;Current measurement},
doi={10.1109/ISMAR.2012.6402562},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402563,
author={A. {Haugstvedt} and J. {Krogstie}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Mobile augmented reality for cultural heritage: A technology acceptance study},
year={2012},
volume={},
number={},
pages={247-255},
abstract={We have developed a mobile augmented reality application with historical photographs and information about a historical street. We follow a design science research methodology and use an extended version of the technology acceptance model (TAM) to study the acceptance of this application. A prototype has been developed in accordance with general principles for usability design, and two surveys have been conducted. A web survey with 200 participants that watched a short video demonstration of the application to validate the adapted acceptance model, and a street survey, where 42 participants got the opportunity to try the application in a live setting before answering a similar questionnaire and provide more concrete feedback. The results show that both perceived usefulness and perceived enjoyment has a direct impact on the intention to use mobile augmented reality applications with historical pictures and information. Further a number of practical recommendations for the development and deployment of such systems are provided.},
keywords={augmented reality;history;mobile computing;user centred design;mobile augmented reality;cultural heritage;technology acceptance model;historical photograph;historical street;design science research methodology;TAM;usability design;perceived usefulness;perceived enjoyment;Augmented reality;Mobile communication;Cultural differences;Cities and towns;Adaptation models;Cameras;Prototypes;H.5.1 [Multimedia Information Systems]: Artificial, augmented, and virtual realities—Evaluation/methodology},
doi={10.1109/ISMAR.2012.6402563},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402564,
author={},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Blank page]},
year={2012},
volume={},
number={},
pages={256-256},
abstract={This page or pages intentionally left blank.},
keywords={},
doi={10.1109/ISMAR.2012.6402564},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402565,
author={J. {Wang} and P. {Fallavollita} and L. {Wang} and M. {Kreiser} and N. {Navab}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Augmented reality during angiography: Integration of a virtual mirror for improved 2D/3D visualization},
year={2012},
volume={},
number={},
pages={257-264},
abstract={Visualizing 2D and 3D anatomical information simultaneously within an X-ray image is challenging. Multiple monitors are required in the operating room to enable visualization of anatomical data by the surgeon. Consequently, this results in an interruption during the operation for proper assessment of information. In this paper, we introduce an interactive visualization of the 3D data from new perspectives including visualizing a virtual mirror from the same viewpoint as the X-ray source. The main contribution is our development of a complete angiographic visualization system that displays simultaneous 2D X-ray and 3D anatomical information in a common monitor for the surgeon in the operating room. No previous works have conceived the integration of the virtual mirror into the projection geometry of a C-arm fluoroscope. In total, 24 participants were asked to assess the benefits of the angiographic virtual mirror with different colour-depth encodings. The results of our feasibility study show a clear improvement when deciphering the true positions of aneurysms in X-ray. Lastly, color depth encoding improves correspondence between the 3D vasculature displayed in the virtual mirror to their projection images in X-ray.},
keywords={augmented reality;data visualisation;diagnostic radiography;image coding;image colour analysis;medical image processing;surgery;three-dimensional displays;augmented reality;virtual mirror;3D anatomical information visualization;2D anatomical information visualization;X-ray image;multiple monitors;surgeon;3D data interactive visualization;X-ray source;angiographic visualization system;simultaneous 2D X-ray anatomical information displays;simultaneous 3D anatomical information displays;operating room;C-arm fluoroscope projection geometry;colour-depth encoding;deciphering;X-ray aneurysms;color depth encoding;3D vasculature;Mirrors;Image color analysis;X-ray imaging;Data visualization;Rendering (computer graphics);Encoding;Aneurysm;Augmented Reality;Angiography;Virtual Mirror;X-ray;Visualization;2D/3D Registration},
doi={10.1109/ISMAR.2012.6402565},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402566,
author={},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Posters},
year={2012},
volume={},
number={},
pages={265-266},
abstract={Start of the above-titled section of the conference proceedings record.},
keywords={},
doi={10.1109/ISMAR.2012.6402566},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402567,
author={Z. {Bai} and A. F. {Blackwell} and G. {Coulouris}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Making Pretense Visible and Graspable: An augmented reality approach to promote pretend play},
year={2012},
volume={},
number={},
pages={267-268},
abstract={Children with autism are often found to lack facility for pretend play. It is believed that this deficit is linked to linguistic, social and creativity competencies in autism. We observe that both Augmented Reality (AR) and pretend play involve processing of information that is coupled with real scenes while not necessarily being directly perceived. This research therefore examines the potential of using AR technologies to promote pretend play behaviors in children with autism. As an initial outcome, we present the design and implementation of an AR system that aims to enhance the comprehension and flexibility of object substitution during pretend play.},
keywords={augmented reality;handicapped aids;augmented reality;pretend play behavior;autism;linguistic competency;social competency;creativity competency;AR technology;Autism;Augmented reality;Airplanes;Mirrors;Bridges;Educational institutions;Cotton;Augmented Reality;pretend play;autistic children},
doi={10.1109/ISMAR.2012.6402567},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402568,
author={B. {Besbes} and S. N. {Collette} and M. {Tamaazousti} and S. {Bourgeois} and V. {Gay-Bellile}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={An interactive Augmented Reality system: A prototype for industrial maintenance training applications},
year={2012},
volume={},
number={},
pages={269-270},
abstract={In this paper, we present an innovative Augmented Reality prototype designed for industrial education and training applications. The system uses an Optical See-Through HMD integrating a calibrated camera and a laser pointer to interactively augment an industrial object with virtual sequences designed to train a user for specific maintenance tasks. The training leverages user interactions by simply pointing on a specific object component. The architecture of our prototype involves two main vision-based modules : camera localization and user-interaction handling. The first module includes markerless trackers for camera localization, which can deal with partial occlusions and specular reflections on the metallic object surfaces. In the second module, we developed fast image processing methods for red laser dot tracking. By combining these processing elements, the proposed system is able to interactively augment in real time an industrial object making the learning process more interesting and intuitive.},
keywords={augmented reality;cameras;computer based training;helmet mounted displays;human computer interaction;image processing;industrial training;laser beam applications;maintenance engineering;production engineering computing;target tracking;interactive augmented reality system;industrial maintenance training;industrial education;optical see-through HMD;calibrated camera;laser pointer;virtual sequences;maintenance tasks;object component;vision-based modules;camera localization;user-interaction handling;partial occlusions;specular reflections;metallic object surfaces;image processing methods;red laser dot tracking;industrial object;Prototypes;Training;Cameras;Augmented reality;Laser modes;Laser applications;Real-time systems;MR/AR applications [industrial and military MR/AR applications];—[Sensors]: vision-based registration and tracking—;User interaction [interaction techniques for MR/AR];—[MR/AR applications]: MR/AR for art, cultural heritage, or education and training—},
doi={10.1109/ISMAR.2012.6402568},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402569,
author={T. {Blum} and R. {Stauder} and E. {Euler} and N. {Navab}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Superman-like X-ray vision: Towards brain-computer interfaces for medical augmented reality},
year={2012},
volume={},
number={},
pages={271-272},
abstract={This paper describes first steps towards a Superman-like X-ray vision where a brain-computer interface (BCI) device and a gaze-tracker are used to allow the user controlling the augmented reality (AR) visualization. A BCI device is integrated into two medical AR systems. To assess the potential of this technology first feedback from medical doctors is gathered. While in this pilot study not the full range of available signals but only electromyographic signals are used, the medical doctors provided very positive feedback on the use of BCI for medical AR.},
keywords={augmented reality;brain-computer interfaces;data visualisation;electromyography;medical signal processing;object tracking;visual evoked potentials;brain-computer interface;medical augmented reality;Superman-like X-ray vision;gaze-tracker;AR visualization;BCI device;medical AR system;electromyographic signal;Biomedical imaging;Surgery;Visualization;Electromyography;Augmented reality;X-ray imaging;Monitoring;H.5.1 [Information Interfaces and Presentation]: Artificial, augmented, and virtual realities—},
doi={10.1109/ISMAR.2012.6402569},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402570,
author={P. {Bunnun} and D. {Damen} and A. {Calway} and W. {Mayol-Cuevas}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Integrating 3D object detection, modelling and tracking on a mobile phone},
year={2012},
volume={},
number={},
pages={273-274},
abstract={This paper presents a complete system on a camera phone that integrates a texture less 3D object detector together with in-situ modelling and tracking. The result is a suite intended for AR applications on the move where objects can be captured, tracked and with the automated detection providing a bridge to either initialize tracking or resume it after measurement loss. The object detector training is online and in-situ and benefits from tight integration with the modelling and tracking processes.},
keywords={augmented reality;cameras;image texture;mobile handsets;object detection;object tracking;3D object detection integration;in-situ 3D object modelling integration;in-situ 3D object tracking integration;mobile camera phone;automatic texture-less 3D object detector;AR applications;object capturing;measurement loss;online object detector training;Image edge detection;Solid modeling;Constellation diagram;Detectors;Object detection;Computational modeling;Tracking},
doi={10.1109/ISMAR.2012.6402570},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402571,
author={J. H. {Chuah} and B. {Lok}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Hybrid virtual-physical entities},
year={2012},
volume={},
number={},
pages={275-276},
abstract={Mixed reality (MR) combines virtual elements with the real, physical world. Virtual elements can be more dynamic - easily moving around or changing appearance. Physical elements are more static but improve immersion and presence. The combination of the two creates dynamic, immersive MR environments. However, individual entities in MR environments, e.g. an object or a character, are typically either purely physical or purely virtual. We propose using both virtual and physical elements in a hybrid virtual-physical entity. We implemented this idea with a hybrid virtual human and ran several user studies. These studies helped us develop guidelines for implementing hybrid entities.},
keywords={virtual reality;mixed reality;virtual element;real physical world;physical element;MR environment;hybrid virtual-physical entity;hybrid virtual human;Animatronics;Humans;Legged locomotion;Avatars;Face;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism—Virtual Reality;K.3.0 [Computing Milieux]: Computers and Education—General},
doi={10.1109/ISMAR.2012.6402571},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402572,
author={M. {Csongei} and L. {Hoang} and U. {Eck} and C. {Sandor}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={ClonAR: Rapid redesign of real-world objects},
year={2012},
volume={},
number={},
pages={277-278},
abstract={ClonAR enables users to rapidly clone and edit real-world objects. First, real-world objects can be scanned using KinectFusion. Second, users can edit the scanned objects in our visuo-haptic Augmented Reality environment. Our whole pipeline does not use mesh representation of objects, but rather Signed Distance Fields, which are the output of KinectFusion. We directly render Signed Distance Fields haptically and visually. We do direct haptic rendering of Signed Distance Fields, which is faster and more flexible than rendering meshes. Visual rendering is performed by our custom-built raymarcher, which facilitates realistic illumination effects like ambient occlusions and soft shadows. Our prototype demonstrates the whole pipeline. We further present several results of redesigned real-world objects.},
keywords={augmented reality;haptic interfaces;rendering (computer graphics);ClonAR;real-world object redesign;KinectFusion;visuo-haptic augmented reality;signed distance fields;haptic rendering;visual rendering;custom-built raymarcher;realistic illumination effect;ambient occlusion;soft shadow;Rendering (computer graphics);Haptic interfaces;USA Councils;Lighting;Real-time systems;Augmented reality;Cloning;H.5.1. [Information Interfaces and Presentation]: Multimedia Information Systems—[Artificial, augmented and virtual realities];H.5.2. [Information Interfaces and Presentation]: User Interfaces—[Haptic I/O ];H.1.2. [Information Systems]: Models and Principles—[Human factors]},
doi={10.1109/ISMAR.2012.6402572},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402573,
author={Y. {Fujimoto} and G. {Yamamoto} and T. {Taketomi} and J. {Miyazaki} and H. {Kato}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Relationship between features of augmented reality and user memorization},
year={2012},
volume={},
number={},
pages={279-280},
abstract={The objective of this study is to investigate the relationship between the features of augmented reality (AR) and human memorization ability. The basis of this relation is derived from the following features. The AR feature is that AR can provide information associated with specific locations in the real world. The feature of human memory is that humans can easily memorize information if the information is visually associated with specific locations. To investigate this relation, we conduct a pilot user study in which blocks are picked from some drawers. As a result, significant differences are found between a situation in which visual information is displayed at the location of each drawer in the real world and that in which textual information is displayed at an unrelated location.},
keywords={augmented reality;data visualisation;augmented reality;user memorization;human memorization ability;AR feature;real world;human memory;visual information;textual information;Humans;Visualization;Augmented reality;Accuracy;Assembly;Psychology;Abstracts;H.1.2 [Models and Principles]: User/Machine Systems—Human factors;H.5.1 [Information Interfaces and Presentation]: User Interfaces—Artificial, augmented and virtual reality},
doi={10.1109/ISMAR.2012.6402573},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402574,
author={C. {Gao} and Y. {Lin} and H. {Hua}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Occlusion capable optical see-through head-mounted display using freeform optics},
year={2012},
volume={},
number={},
pages={281-282},
abstract={Most state-of-the-art optical see-through head-mounted display (OST-HMD) lacks mutual occlusion capability between computer-rendered and real objects so that the virtual view through an OST-HMD appears “ghost-like”, floating in the real world. In this paper, we demonstrated a light-weight, compact OST-HMD with mutual occlusion capability by exploring a highly innovative optical approach based on emerging freeform optical design and fabrication technologies. Our approach enabled us to achieve an occlusion-capable OST-HMD system with a very compelling form factor and high optical performance. The proposed display technology is designed for highly efficient liquid crystal on silicon (LCoS) type spatial light modulator (SLM) and bright Organic LED (OLED) microdisplay, which is capable of working in both indoor and outdoor environments. Our current design offered a 1280×1024 color resolution with a field of view (FOV) of 40 degrees and lightweight optics about 30 grams per eye.},
keywords={computer graphics;computerised instrumentation;helmet mounted displays;hidden feature removal;liquid crystal displays;microdisplays;organic light emitting diodes;occlusion-capable optical see-through head-mounted display;freeform optics;mutual occlusion capability;ghost-like;compact OST-HMD;freeform optical design;fabrication technologies;occlusion-capable OST-HMD system;liquid crystal on silicon type spatial light modulator;LCoS type SLM;bright OLED microdisplay;bright organic LED icrodisplay;indoor environments;outdoor environments;lightweight optics;Optical device fabrication;Optical imaging;Biomedical optical imaging;Optical design;Microdisplays;Adaptive optics;Head Mounted Display;Optical See-through;Mutual Occlusion;Augmented Reality},
doi={10.1109/ISMAR.2012.6402574},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402575,
author={C. {Heinrichs} and A. {McPherson}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Recreating the parallax effect associated with Fishtank VR in a Real-Time telepresence system using head-tracking and a robotic camera},
year={2012},
volume={},
number={},
pages={283-284},
abstract={This project aims to simulate the effect of Fishtank virtual reality in a telepresence system by using head-tracking and real-time control of a robotic camera. Despite the use of the term telepresence, the focus is here not on two-way communication between persons in separate spaces, but in creating a strong sense of physical presence by making the observed scene seem to be an extension of the room. The observer can thus interact with a 3D stage as though the screen were a window into the remote space. The research carried out here is unprecedented in that, to the authors' knowledge, nobody has employed one of the fundamental aspects of the Fishtank concept, namely the alignment of viewer-screen distance to camera-stage distance, in conjunction with a head-coupled camera system. The implementation is described followed by a brief discussion of its limitations and suggestions for future developments.},
keywords={cameras;control engineering computing;position control;telerobotics;virtual reality;parallax effect;Fishtank VR;virtual reality;realtime telepresence system;head-tracking control;robotic camera;viewer-screen distance;camera-stage distance;Fishtank concept;head-coupled camera system;Cameras;Robot vision systems;Head;Real-time systems;Tracking},
doi={10.1109/ISMAR.2012.6402575},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402576,
author={A. {Herout} and M. {Zachariáš} and M. {Dubská} and J. {Havel}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Fractal marker fields: No more scale limitations for fiduciary markers},
year={2012},
volume={},
number={},
pages={285-286},
abstract={One limitation of existing fiduciary markers is that the camera motion is tightly limited: the marker (one of the markers) must be visible and it must be observed at a proper scale. This paper introduces a fractal structure of markers similar to matrix codes (such as QR-code or Data Matrix): the Fractal Marker Field. The FMF allows for embedding markers of a virtually unlimited number of scales. At the same time, for each of the scales it guarantees a constant density of markers at that scale over the whole marker field's surface. The Fractal Marker Field can provide unprecedented freedom of motion to camera-based augmented reality applications.},
keywords={augmented reality;cameras;fractals;image motion analysis;fiduciary marker;camera motion;fractal structure;matrix code;QR-code;data matrix;fractal marker field;FMF;embedding marker;marker constant density;marker field surface;camera-based augmented reality;Fractals;Cameras;Augmented reality;Reliability;Manganese;Image color analysis;Calibration},
doi={10.1109/ISMAR.2012.6402576},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402577,
author={T. N. {Hoang} and B. H. {Thomas}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Distance-based modeling and manipulation techniques using ultrasonic gloves},
year={2012},
volume={},
number={},
pages={287-288},
abstract={We present a set of distance-based interaction techniques for modeling and manipulation, enabled by a new input device called the ultrasonic gloves. The ultrasonic gloves are built upon the original design of the pinch glove device for virtual reality systems with a tilt sensor and a pair of ultrasonic transducers in the palms of the gloves. The transducers are distance-ranging sensors that allow the user to specify a range of distances by natural gestures such as facing the palms towards each other or towards other surfaces. The user is able to create virtual models of physical objects by specifying their dimensions with hand gestures. We combine the reported distance with the tilt orientation data to construct virtual models. We also map the distance data to create a set of affine transformation techniques, including relative and fixed scaling, translation, and rotation. Our techniques can be generalized to different sensor technologies.},
keywords={augmented reality;data gloves;sensors;solid modelling;user interfaces;distance-based modeling technique;distance-based manipulation technique;ultrasonic gloves;distance-based interaction technique;pinch glove device;virtual reality system;tilt sensor;ultrasonic transducer;distance-ranging sensor;virtual model;hand gesture;tilt orientation data;affine transformation technique;relative scaling technique;fixed scaling technique;translation technique;rotation technique;sensor technology;Acoustics;Solid modeling;Ultrasonic variables measurement;Thumb;Ultrasonic transducers;Augmented reality;ultrasonic gloves;distance-based techniques;modeling;manipulation},
doi={10.1109/ISMAR.2012.6402577},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402578,
author={R. {Hofmann} and H. {Seichter} and G. {Reitmayr}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={A GPGPU accelerated descriptor for mobile devices},
year={2012},
volume={},
number={},
pages={289-290},
abstract={We present a modified upright SURF feature descriptor for mobile phone GPUs. Our implementation called uSURF-ES is multiple times faster than a comparable CPU variant on the same device. Our results proof the feasibility of modern mobile graphics accelerators for GPGPU tasks especially for the detection phase in natural feature tracking used in Augmented Reality applications.},
keywords={computer graphics;graphics processing units;mobile computing;mobile handsets;GPGPU accelerated descriptor;mobile devices;SURF feature descriptor;mobile phone GPU;uSURF-ES;mobile graphics accelerators;detection phase;natural feature tracking;augmented reality applications;Feature extraction;Mobile handsets;Mobile communication;Graphics processing units;Augmented reality;Encoding;Androids;H.5.1 [INFORMATION INTERFACES AND PRESENTATION]: Multimedia Information Systems—Artificial, augmented, and virtual realities;K.7.m [IMAGE PROCESSING AND COMPUTER VISION]: Segmentation—Edge and feature detection},
doi={10.1109/ISMAR.2012.6402578},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402579,
author={A. M. {Howard} and L. {Roberts} and S. {Garcia} and R. {Quarells}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Using mixed reality to map human exercise demonstrations to a robot exercise coach},
year={2012},
volume={},
number={},
pages={291-292},
abstract={Obesity is a growing health problem in the United States, especially among children. Indicators show that the rate of obesity for children age 12-19 years old has risen from 5% percent to 18% over the last ten years. To deal with the obesity epidemic, a number of technology interventions, including the use of robotics and virtual reality games, have arisen to motivate youth to become physically active. The difficulty though lies in providing a tool for health professionals to embed established clinical health protocols into these technologies. As such, in this paper we present a mixed reality system that translates physical demonstrations of various exercise protocols into movements for a robotic agent. This is accomplished by mapping real-time data from an RGB-D sensor to a robotic exercise coach. Details of the system are discussed and results from evaluation with 20 human subjects are provided.},
keywords={augmented reality;computer games;humanoid robots;mobile robots;protocols;robot kinematics;sensors;human exercise demonstration mapping;robot exercise coach;health problem;united states;obesity epidemic rate;health professionals;clinical health protocols;mixed reality system;physical demonstration translation;exercise protocols;robotic agent;RGB-D sensor;virtual reality games;Robot sensing systems;Humans;Pediatrics;Robot kinematics;Obesity;Joints;1.2.9: Artificial Intelligence—Robotics},
doi={10.1109/ISMAR.2012.6402579},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402580,
author={N. {Kawai} and M. {Yamasaki} and T. {Sato} and N. {Yokoya}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={AR marker hiding based on image inpainting and reflection of illumination changes},
year={2012},
volume={},
number={},
pages={293-294},
abstract={This paper proposes a new method of diminished reality which removes AR markers from a user's view image in real time. To achieve natural marker hiding, assuming that an area around a marker is locally planar, the marker area in the first frame is inpainted using the rectified image to achieve high-quality inpainting. The unique inpainted texture is overlaid on the marker region in each frame according to camera motion for geometric consistency. Both global and local luminance changes around the marker are separately detected and reflected to the inpainted texture for photometric consistency.},
keywords={augmented reality;cameras;data encapsulation;image motion analysis;image texture;lighting;photometry;image inpainting-based AR marker hiding;illumination changes reflection;diminished reality;real time view image;natural marker hiding;marker area;image rectification;high-quality inpainting;marker region;camera motion;geometric consistency;local luminance changes;global luminance changes;inpainted texture reflection;inpainted texture detection;photometric consistency;Cameras;Real-time systems;Indexes;Augmented reality;Interpolation;Lighting;Measurement;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial;augmented;virtual realities},
doi={10.1109/ISMAR.2012.6402580},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402581,
author={N. H. {Lehment} and K. {Erhardt} and G. {Rigoll}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Interface design for an inexpensive hands-free collaborative videoconferencing system},
year={2012},
volume={},
number={},
pages={295-296},
abstract={In this paper an interaction framework for AR enhanced video conferencing is presented. The goal is to provide a cheap and portable system based on a combination of commodity Kinect cameras and regular computer screens. These conditions necessitate the use of contact free interaction methods. The interaction framework presented in this paper is specifically suited for remotely presenting, sharing and annotating visual data such as images, presentation slides and 3D objects. In the proposed system all data is represented by freely manipulable 3D objects which are augmented into the camera views. These representations are integrated into a differentiated ownership scheme, allowing for operations such as spatially managed data sharing. The suitability of different interaction paradigms with regards to this usage scenario is examined. Furthermore, occlusion and collision management between virtual objects and real obstacles is enabled by integrating basic models of the environment.},
keywords={augmented reality;telecommunication computing;teleconferencing;user interfaces;video communication;interface design;hands-free collaborative videoconferencing system;AR enhanced video conferencing;portable system;Kinect cameras;computer screens;contact free interaction methods;remote visual data presentation;remote visual data sharing;remote visual data annotation;images;presentation slides;3D objects;occlusion;collision management;virtual objects;Collaboration;Cameras;Teleconferencing;Augmented reality;Real-time systems;Visualization;H.4.3 [Information Systems Applications]: Communications Applications—Computer Conferencing;Teleconferencing;Videoconferencing},
doi={10.1109/ISMAR.2012.6402581},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402582,
author={ {João Paulo Lima} and H. {Uchiyama} and V. {Teichrieb} and E. {Marchand}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Texture-less planar object detection and pose estimation using Depth-Assisted Rectification of Contours},
year={2012},
volume={},
number={},
pages={297-298},
abstract={This paper presents a method named Depth-Assisted Rectification of Contours (DARC) for detection and pose estimation of texture-less planar objects using RGB-D cameras. It consists in matching contours extracted from the current image to previously acquired template contours. In order to achieve invariance to rotation, scale and perspective distortions, a rectified representation of the contours is obtained using the available depth information. DARC requires only a single RGB-D image of the planar objects in order to estimate their pose, opposed to some existing approaches that need to capture a number of views of the target object. It also does not require to generate warped versions of the templates, which is commonly needed by existing object detection techniques. It is shown that the DARC method runs in real-time and its detection and pose estimation quality are suitable for augmented reality applications.},
keywords={feature extraction;image matching;image texture;object detection;pose estimation;texture-less planar object detection;pose estimation;depth-assisted rectification of contours;DARC;RGB-D camera;contours matching;template contour;rotation distortion;scale distortion;perspective distortion;depth information;augmented reality;Estimation;Cameras;Object detection;Real-time systems;Augmented reality;Transforms;Shape;Pose estimation;texture-less objects;augmented reality;RGB-D cameras},
doi={10.1109/ISMAR.2012.6402582},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402583,
author={K. {Mitobe} and M. {Tomioka} and M. {Saito} and M. {Suzuki}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Development of a ubiquitous learning system for dexterous hand operation},
year={2012},
volume={},
number={},
pages={299-300},
abstract={It is not easy to learn the dexterous finger movements of a skilled person due to flexibility and complexity. Therefore, a lot of training and effort is needed for a novice in order to acquire proficiency. The role model and feedback information of experienced persons is important for learning skilled movements. In this paper, we evaluate a ubiquitous learning system for dexterous hand operation using the finger motion capture data measured by the Hand-MoCap system. Through this system, a novice can learn under the guidance of the virtual 3D hand reconstructed using the master's motion capture data. We quantitatively evaluate the effectiveness of the system.},
keywords={computer aided instruction;ubiquitous computing;virtual reality;ubiquitous learning system;dexterous hand operation;dexterous finger movement;skilled movement learning;finger motion capture data;Hand-MoCap system;virtual 3D hand;Thumb;Learning systems;Training;Educational institutions;Spatial resolution;Augmented reality;Dexterous finger movement;motion capture;training},
doi={10.1109/ISMAR.2012.6402583},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402584,
author={S. {Morishima} and T. {Mashita} and K. {Kiyokawa} and H. {Takemura}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={A waist-mounted ProCam system for remote collaboration},
year={2012},
volume={},
number={},
pages={301-302},
abstract={We propose a waist-mounted projector-camera (ProCam) system for asymmetric remote collaboration. A wearable camera is often used to transmit a worker's situation to a remote instructor, however 3D structure of the worker's environment is not always available and the instructor has a minimal flexibility in changing the camera's viewpoint. A stationary 3D measurement system is also commonly used for remote collaboration, however a narrow measurement area and occlusion from a worker's body can be a severe problem. Our waist-mounted ProCam system reconstructs worker's environment in real-time without occlusion from the worker's body. The remote instructor can give instructions simply by drawing annotations on the reconstructed environment on screen, and they are properly projected in front of the worker. Structured-light based reconstruction, vision-based localization, and visual annotation projection, are processed synchronously with a camera and modified shutter glasses so that both the camera and the worker observe only the information they need. Experimental results show that users prefer our system to a stationary ProCam system.},
keywords={augmented reality;cameras;computer aided instruction;groupware;measurement systems;solid modelling;waist-mounted ProCam system;projector-camera system;remote collaboration;wearable camera;worker environment;remote instructor;camera viewpoint;stationary 3D measurement system;structured-light based reconstruction;vision-based localization;visual annotation projection;augmented reality system;Cameras;Collaboration;Glass;Educational institutions;Solid modeling;Electronic mail;Area measurement;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial;augmented;virtual realities H.5.3 [Information Interfaces and Presentation]: Group and Organization Interfaces—Computer-Supported Cooperative Work},
doi={10.1109/ISMAR.2012.6402584},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402585,
author={M. {Nakevska} and {Jun Hu} and G. {Langereis} and M. {Rauterberg}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Alice's adventures in an immersive mixed reality environment},
year={2012},
volume={},
number={},
pages={303-304},
abstract={Immersive mixed reality environments offer new possibilities to reproduce reality or embodied presence with constructing elaborate fantasy worlds and provide to the user an intense and seemingly real experience. Mixed reality gives possibilities to create deeply dimensional narratives and simulations that put the user in the center of the action. In this work we describe an interactive mixed reality installation named Alice, consisting of six separate stages based on the narrative “Alice's Adventures in Wonderland”. To be able to achieve the intended experience we have to build complex and heterogeneous distributed system, composed of sensors, actuators, virtual reality, application components and variety of processing components that manage the flow of context information between the sensors/actuators and applications.},
keywords={distributed processing;humanities;virtual reality;immersive mixed reality environment;elaborate fantasy worlds;deeply dimensional narratives;Alices adventures in wonderland;distributed system;sensors;actuators;virtual reality;application components;Abstracts;Argon;Robots;Immersive mixed reality;cultural computing},
doi={10.1109/ISMAR.2012.6402585},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402586,
author={S. {Noh} and S. {Hashimoto} and D. {Yamanaka} and Y. {Kamiyama} and M. {Inami} and T. {Igarashi}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Lighty: A painting interface for room illumination by robotic light array},
year={2012},
volume={},
number={},
pages={305-306},
abstract={We propose an AR-based painting interface that enables users to design an illumination distribution for a real room using an array of computer-controlled lights. Users specify an illumination distribution of the room by painting on the image obtained by a camera mounted in the room. The painting result is overlaid on the camera image as contour lines of the target illumination intensity. The system runs an optimization interactively to calculate light parameters to deliver the requested illumination condition. In this implementation, we used actuated lights that can change the lighting direction to generate the requested illumination condition more accurately and efficiently than static lights. We built a miniature-scale experimental environment and ran a user study to compare our method with a standard direct manipulation method using widgets. The results showed that the users preferred our method for informal light control.},
keywords={augmented reality;control engineering computing;image sensors;lighting control;robots;user interfaces;Lighty;room illumination;robotic light array;AR-based painting interface;illumination distribution;computer-controlled lights;camera;contour lines;target illumination intensity;static lights;miniature-scale experimental environment;direct manipulation method;informal light control;Augmented reality;I.3.6 [Computer Graphics]: Methodology and Techniques—Interaction Techniques;I.6.3 [Computing Methodologies]: Simulation and Modeling—Applications;G.1.6 [Numerical Analysis]: Optimization—Constrained Optimization;H.5.2 [Information Interfaces and Presentation]: User Interfaces—Usercentered Design},
doi={10.1109/ISMAR.2012.6402586},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402587,
author={M. A. {Oikawa} and I. {de Souza Almeida} and T. {Taketomi} and G. {Yamamoto} and J. {Miyazaki} and H. {Kato}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Augmented prototyping of 3D rigid curved surfaces},
year={2012},
volume={},
number={},
pages={307-308},
abstract={This paper presents an application of Augmented Reality (AR) in Rapid Prototyping (RP) of non-textured rigid curved surfaces. By enhancing the prototypes with AR, evaluation of its design and aesthetic concepts in real-time becomes easier, saving time and production costs. In our application, no fiducial markers are required and the CAD model used to build the prototype is applied in an edge-based tracking system specially designed to deal with curved shapes. Results from a pilot user study comparing the use of a 3D software and the proposed application are also presented.},
keywords={augmented reality;computational geometry;augmented prototyping;3D rigid curved surfaces;rapid prototyping;nontextured rigid curved surface;CAD model;edge-based tracking system;curved shapes;Prototypes;Solid modeling;Augmented reality;Surface texture;Shape;Software;Visualization;product design;augmented prototyping;model-based tracking;non-textured curved surfaces},
doi={10.1109/ISMAR.2012.6402587},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402588,
author={ {Joonsuk Park} and {Donghyun Lee} and {Jun Park}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Digital map based pose improvement for outdoor Augmented Reality},
year={2012},
volume={},
number={},
pages={309-310},
abstract={With popularization of smart phones, needs for location based services (LBS), which is one of the most promising Augmented Reality applications, increased rapidly. However, accuracy of most commercially available Global Positioning Systems (GPS) is below levels for providing practically meaningful location based information. Especially when there are high building structures nearby, GPS location measurements are known to be erroneous and deviant. In this paper, we present a computer vision based method for improving user's position and orientation for outdoor Augmented Reality with initial values obtained from a GPS and a digital compass. Given a digital map, our goal was to determine corresponding buildings visible in the camera image and improve the user location and orientation. In average, our method improved (14.4m, 3.3m) in position and 2.8 degrees in orientation. Our method is suitable for mobile services in urban environments where tall buildings degrade GPS signals.},
keywords={augmented reality;cameras;cartography;computer vision;Global Positioning System;mobile computing;smart phones;structural engineering;digital map;pose improvement;outdoor augmented reality;smart phone;location based service;LBS;Global Positioning System;location based information;building structure;GPS location measurement;computer vision;user position;digital compass;camera image;user location;user orientation;mobile service;urban environment;tall building;GPS signal;Buildings;Global Positioning System;Image edge detection;Augmented reality;Compass;Cameras;Accuracy;GPS;outdoor tracking;location based service;depth map},
doi={10.1109/ISMAR.2012.6402588},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402589,
author={O. {Pauly} and A. {Katouzian} and A. {Eslami} and P. {Fallavollita} and N. {Navab}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Supervised classification for customized intraoperative augmented reality visualization},
year={2012},
volume={},
number={},
pages={311-312},
abstract={In this paper, we present a fusion algorithm supplemented with appropriate visualization by selecting relevant information from different modalities in mixed and augmented reality (AR). This encompasses a learning based method upon relevance of information, defined by an expert, which ultimately enables confident interventional decisions based on mixed reality (MR) images. The performance of our developed fusion and tailored visualization techniques was evaluated by employing X-ray/optical images during surgery and validated qualitatively using a 5-point Likert scale. Our observations indicated that the proposed technique provided semantic contextual information about underlying pixels and in general was preferred over the traditional pixel-wise linear alpha-blending method.},
keywords={augmented reality;biomedical optical imaging;data visualisation;diagnostic radiography;image classification;learning (artificial intelligence);medical image processing;sensor fusion;supervised classification;customized intraoperative augmented reality visualization;fusion algorithm;mixed reality;augmented reality;AR;MR;learning based method;information relevance;interventional decision;X-ray image;optical image;5-point Likert scale;surgery;pixel-wise linear alpha-blending method;Surgery;X-ray imaging;Biomedical optical imaging;Optical imaging;Optical mixing;Visualization;Augmented reality;Medical Augmented Reality;Fusion;Relevant Information;Visualization;X-ray;CamC},
doi={10.1109/ISMAR.2012.6402589},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402590,
author={I. {Radu}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Why should my students use AR? A comparative review of the educational impacts of augmented-reality},
year={2012},
volume={},
number={},
pages={313-314},
abstract={Augmented reality is increasingly reaching young users such as elementary-school and high-school children, as their parents and teachers become aware of the technology and its potential for education. Although research has shown that AR systems have the potential to improve student learning, the educator community does not clearly understand the educational impact of AR, nor the factors which impact the educational effectiveness of AR. In this poster, we analyse 32 publications that have previously compared learning effects of AR vs non-AR applications. We identify a list of positive and negative impacts of AR on student learning, and identify potential underlying causes for these effects. Our vision is that educational initiatives will exploit these factors, in order to realize the full potential of AR to enrich learner's lives.},
keywords={augmented reality;computer aided instruction;educational impacts;augmented-reality;elementary-school children;high-school children;parents;teachers;AR systems;student learning;educator community;educational effectiveness;AR learning effects;educational initiatives;Augmented reality;Education;Solid modeling;Media;Human factors;Computers;Machine learning;Augmented Reality;Education;Children;Human Factors},
doi={10.1109/ISMAR.2012.6402590},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402591,
author={J. {Sausman} and A. {Samoylov} and S. H. {Regli} and M. {Hopps}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Effect of eye and body movement on augmented reality in the manufacturing domain},
year={2012},
volume={},
number={},
pages={315-316},
abstract={Understanding the eye and body movements required to perform large vehicle assembly tasks is important for developing a mobile support system for mechanics, and tracking user movements with regard to the surrounding environment is critical for designing a wearable Augmented Reality (AR) systems. This poster summarizes a study measuring the eye and body movements of mechanics performing assembly activities in a live manufacturing environment. It reviews our quantitative analysis of eye movements and qualitative analysis of body movements and describes the implications of this data in terms of the feasibility and potential utility of using a mobile AR application to support manufacturing. We found that the mechanics' eye movements ranged over a slightly larger field than the eye movements reported in previous research because of constraints imposed by some body positions required in manufacturing tasks.},
keywords={assembling;augmented reality;biomechanics;computer aided manufacturing;eye;manufacturing processes;mobile computing;augmented reality system;manufacturing domain;mobile support system;user movements tracking;live manufacturing environment;body movements qualitative analysis;eye movements quantitative analysis;mobile AR application;eye movements mechanics;manufacturing tasks;Augmented reality;Assembly;Mobile communication;Cameras;Tracking;Vehicles;Augmented reality;manufacturing;production;eye tracking;saccadic eye movement;gaze shift;heads-up display;human factors},
doi={10.1109/ISMAR.2012.6402591},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402592,
author={S. {Sridhar} and V. {Ng-Thow-Hing}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Generation of virtual display surfaces for in-vehicle contextual augmented reality},
year={2012},
volume={},
number={},
pages={317-318},
abstract={In-vehicle contextual augmented reality (I-CAR) has the potential to provide novel visual feedback to drivers for an enhanced driving experience. To enable I-CAR, we present a parametrized road trench model (RTM) for dynamically extracting display surfaces from a driver's point of view that is adaptable to constantly changing road curvature and intersections. We use computer vision algorithms to analyze and extract road features that are used to estimate the parameters of the RTM. GPS coordinates are used to quickly compute lighting parameters for shading and shadows. Novel driver-based applications that use the RTM are presented.},
keywords={augmented reality;computer vision;driver information systems;feature extraction;lighting;parameter estimation;road traffic;virtual display surface generation;in-vehicle contextual augmented reality;I-CAR;visual feedback;driving experience;RTM model;road trench model;road curvature;road intersection;computer vision algorithm;road feature extraction;RTM parameter estimation;GPS coordinates;Global Positioning System;lighting parameter;driver-based application;Roads;Vehicles;Cameras;Sun;Image edge detection;Global Positioning System;Lighting},
doi={10.1109/ISMAR.2012.6402592},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402593,
author={I. {Szentandrási} and M. {Zachariáš} and J. {Havel} and A. {Herout} and M. {Dubská} and R. {Kajan}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Uniform Marker Fields: Camera localization by orientable De Bruijn tori},
year={2012},
volume={},
number={},
pages={319-320},
abstract={In various applications, a wider area needs to be covered by fiduciary markers but a large marker cannot be used because only a fraction of the area is to be viewed by the camera. Such an area can be covered by a number of small markers with unique identifiers. However, with the camera freely moving in the scene and with occluders present, it is difficult to ensure that at least one of the individual markers is completely visible, unless the markers are small and numerous. In that case, the markers are not recognizable from larger distances. In this paper we introduce the concept of Marker Fields which overcome this limitation. The Marker Field covers a large-scale planar (or non-planar) area and it is composed of mutually overlapping partial markers. We propose a particular arrangement of the Marker Field: a Uniform Checker-Board Marker Field, which is a black- and-white checkerboard whose square modules are defined by aperiodic 4-orientable binary n2-window arrays (De Bruijn tori). We propose a genetic algorithm for construction of 4-orientable n2window arrays. We used a supercomputer to synthesize large 4-orientable 42window arrays and offer them publicly for downloading. We prototyped an algorithm for detection of the checkerboard marker fields and measured its performance. When processing input video from a cellphone camera, the algorithm visits only about 5 % of image pixels for reliable detection and the processing time is about 1 ms on a mid-range PC processor. The Uniform Marker Field increases freedom of camera movement, especially with occluders present in the scene. The detection algorithm is efficient and real-time marker field detection will be feasible on ultramobile devices.},
keywords={genetic algorithms;graph theory;object detection;uniform checker-board marker field;camera localization;orientable De Bruijn tori;large-scale planar area;black-and-white checkerboard;aperiodic 4-orientable binary n2-window arrays;genetic algorithm;supercomputer;ultramobile device;fiducial marker;Cameras;Image edge detection;Feature extraction;Genetic algorithms;Supercomputers;Algorithm design and analysis;Reliability},
doi={10.1109/ISMAR.2012.6402593},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402594,
author={Y. {Taguchi} and {Yong-Dian Jian} and S. {Ramalingam} and {Chen Feng}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={SLAM using both points and planes for hand-held 3D sensors},
year={2012},
volume={},
number={},
pages={321-322},
abstract={We present a simultaneous localization and mapping (SLAM) algorithm for a hand-held 3D sensor that uses both points and planes as primitives. Our algorithm uses any combination of three point/plane primitives (3 planes, 2 planes and 1 point, 1 plane and 2 points, and 3 points) in a RANSAC framework to efficiently compute the sensor pose. As the number of planes is significantly smaller than the number of points in typical 3D scenes, our RANSAC algorithm prefers primitive combinations involving more planes than points. In contrast to existing approaches that mainly use points for registration, our algorithm has the following advantages: (1) it enables faster correspondence search and registration due to the smaller number of plane primitives; (2) it produces plane-based 3D models that are more compact than point-based ones; and (3) being a global registration algorithm, our approach does not suffer from local minima or any initialization problems. Our experiments demonstrate real-time, interactive 3D reconstruction of office spaces using a hand-held Kinect sensor.},
keywords={image reconstruction;image registration;image sensors;SLAM (robots);solid modelling;hand-held 3D sensors;simultaneous localization and mapping algorithm;SLAM algorithm;three point primitives;three plane primitives;RANSAC framework;sensor pose;3D scenes;RANSAC algorithm;correspondence search;plane-based 3D models;point-based 3D models;global registration algorithm;interactive 3D reconstruction;office spaces;hand-held Kinect sensor;Simultaneous localization and mapping;Real-time systems;Current measurement;Color;Three dimensional displays;Cameras;I.4.8 [Image Processing and Computer Vision]: Scene Analysis—Range Data;Tracking},
doi={10.1109/ISMAR.2012.6402594},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402595,
author={T. {Tasaki} and A. {Moriya} and A. {Hotta} and T. {Sasaki} and H. {Okumura}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Depth perception control by hiding displayed images based on car vibration for monocular head-up display},
year={2012},
volume={},
number={},
pages={323-324},
abstract={We have developed a novel depth perception control method for a monocular head-up display (HUD) in a car. However, it is difficult to achieve an accurate depth perception in the real world because of car vibration. To resolve this problem, we focus on a property that people complement hidden images by previous continuous observed images. We hide the image on the HUD when the car is vibrated. We aim to point at the accurate depth position by using HUD images with having users compliment the hidden image positions based on the continuous images before car vibration. We developed a car which detects big vibration by an acceleration sensor and is equipped with our monocular HUD. Our method pointed at the depth position within a 3.4 [m] error, which was 2 times more accurate than the previous method does.},
keywords={automobiles;head-up displays;image coding;position control;sensors;depth perception control;displayed image hiding;car vibration;monocular head-up display;HUD;continuous observed image;acceleration sensor;Vibrations;Acceleration;Navigation;Augmented reality;Position measurement;Image resolution;Vehicles;HUD;navigation;augmented reality;depth control},
doi={10.1109/ISMAR.2012.6402595},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402596,
author={A. {Umakatsu} and T. {Mashita} and K. {Kiyokawa} and H. {Takemura}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Touch-n-Paste: Direct texture transfer interaction in AR environments},
year={2012},
volume={},
number={},
pages={325-326},
abstract={Our Touch-n-Paste allows a user to touch one part of an object, copy and move its texture, and paste it onto another object, directly with his or her hand, in an augmented reality environment. To transfer texture appropriately from one part of an object to another, two texture images are generated by the Least Square Conformal Map (LSCM) technique. Two regions in the texture images corresponding to source and target areas of interest are then obtained using cross-boundary brushes. Target texel values are sampled from corresponding source texels by Moving Least Squares (MLS), and are finally mapped onto the target object.},
keywords={augmented reality;image texture;least squares approximations;tactile sensors;touch-n-paste;direct texture transfer interaction;AR environments;augmented reality environment;image texture;least square conformal map technique;LSCM technique;cross-boundary brushes;target areas of interest;target texel values;source texels;moving least squares;MLS;Clothing;Educational institutions;Electronic mail;Solid modeling;Image color analysis;Brushes;Computer graphics;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial;augmented;and virtual realities I.3.7 [Computer Graphics]: Three-dimensional Graphics and Realism—Color;shading;shadowing;texture},
doi={10.1109/ISMAR.2012.6402596},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402597,
author={ {Duc Nguyen Van} and T. {Mashita} and K. {Kiyokawa} and H. {Takemura}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Subjective evaluations on perceptual depth of stereo image and effective field of view of a wide-view head mounted projective display with a semi-transparent retro-reflective screen},
year={2012},
volume={},
number={},
pages={327-328},
abstract={We report two user studies on a wearable hyperboloidal head mounted projective display (HHMPD) with a semi-transparent retro-reflective screen. First experiment revealed that a virtual image is perceived at a similar distance as the real image only when the observation distance is within 2.5m with monocular vision, whereas its threshold is further than 3m with stereo (binocular) vision. Second experiment revealed that users are able to identify visual stimuli in the periphery of the visual field up to ±50 degrees in horizontal, while paying attention to a real object in frontal direction.},
keywords={computer vision;helmet mounted displays;stereo image processing;wearable computers;subjective evaluations;perceptual depth;field-of-view;semitransparent retro-reflective screen;wearable wide-view hyperboloidal head mounted projective display;HHMPD;perceived virtual stereo image;real image;observation distance;monocular vision;stereo binocular vision;visual stimuli;visual field periphery;frontal direction;Visualization;Head;Magnetic heads;Educational institutions;Electronic mail;Biomedical monitoring;Shape;B.4.2 [Input / Output and Data Communications]: Input / Output Devices—Image Display;H.1.2 [Models and Principles]: User / Machine Systems—Human Factors},
doi={10.1109/ISMAR.2012.6402597},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402598,
author={T. {Verbelen} and P. {Simoens} and F. {De Turck} and B. {Dhoedt}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={A component-based approach towards mobile distributed and collaborative PTAM},
year={2012},
volume={},
number={},
pages={329-330},
abstract={Having numerous sensors on-board, smartphones have rapidly become a very attractive platform for augmented reality applications. Although the computational resources of mobile devices grow, they still cannot match commonly available desktop hardware, which results in downscaled versions of well known computer vision techniques that sacrifice accuracy for speed. We propose a component-based approach towards mobile augmented reality applications, where components can be configured and distributed at runtime, resulting in a performance increase by offloading CPU intensive tasks to a server in the network. By sharing distributed components between multiple users, collaborative AR applications can easily be developed. In this poster, we present a component-based implementation of the Parallel Tracking And Mapping (PTAM) algorithm, enabling to distribute components to achieve a mobile, distributed version of the original PTAM algorithm, as well as a collaborative scenario.},
keywords={augmented reality;groupware;mobile computing;object-oriented programming;smart phones;component-based approach;mobile distributed PTAM;collaborative PTAM;sensors;smartphones;mobile devices;desktop hardware;mobile augmented reality applications;CPU intensive tasks;parallel tracking and mapping algorithm;Augmented reality;Mobile handsets;Collaboration;Mobile communication;Cameras;Servers;Runtime},
doi={10.1109/ISMAR.2012.6402598},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402599,
author={P. {Weir} and C. {Sandor} and M. {Swoboda} and {Thanh Nguyen} and U. {Eck} and G. {Reitmayr} and A. {Dey}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={BurnAR: Feel the heat},
year={2012},
volume={},
number={},
pages={331-332},
abstract={Augmented Reality systems that run interactively and in real time, using high quality graphical displays and sensational cues, can create the illusion of virtual objects appearing to be real. This paper presents the design and implementation of BurnAR, a demonstration which enables users to experience the illusion of seeing their own hands burning, which we achieve by overlaying virtual flames and smoke on their hands. Surprisingly, some users reported an involuntary warming sensation of their hands.},
keywords={augmented reality;flames;smoke;user interfaces;BurnAR;augmented reality system;graphical display;sensational cue;virtual object illusion;user experience;virtual flame;virtual smoke;involuntary warming sensation;Image color analysis;Streaming media;Cameras;Augmented reality;Image segmentation;USA Councils;H.5.1. [Information Interfaces and Presentation]: Multimedia Information Systems—[Artificial;augmented and virtual realities] H.1.2. [Information Systems]: Models and Principles—[Human factors]},
doi={10.1109/ISMAR.2012.6402599},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402600,
author={H. {Yasuda} and Y. {Ohama}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Toward a practical wall see-through system for drivers: How simple can it be?},
year={2012},
volume={},
number={},
pages={333-334},
abstract={This study specifically examines wall see-through visualization for drivers at blind corners to prevent crossing collisions. We believe that realizing the desired effect with the simplest visualization is a key to building practical systems, although previous studies mainly targeted rich visualization as if the wall were actually transparent. We compared several visualization levels using qualitative and quantitative measures based on performance of the driver's collision estimation and the meaning assignment to visual stimuli. The results revealed that displaying only the direction of the obscured vehicle by a small circle is sufficient for collision estimation, although it was perceived as less informative. We also obtained a preliminary result indicating that the meaning assignment performance is significantly lower in a peripheral region of the driver's view. Although both collision estimation and meaning assignment performance are necessary for building an effective system, these results clarify that future studies must specifically examine the meaning assignment performance of the stimuli.},
keywords={data visualisation;driver information systems;wall see-through visualization;rich visualization;qualitative measure;quantitative measure;driver collision estimation;visual stimuli;meaning assignment performance;blind corner driving;crossing collision prevention;Vehicles;Visualization;Estimation;Augmented reality;Videos;Buildings;Accidents;wall see-through;x-ray vision;advanced driver assistance systems;active safety;peripheral vision},
doi={10.1109/ISMAR.2012.6402600},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6402601,
author={ {Feng Zheng} and R. {Schubert} and G. {Welch}},
booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={A general approach for closed-loop registration in AR},
year={2012},
volume={},
number={},
pages={335-336},
abstract={Tracking and augmentation are usually handled in independent consecutive stages in augmented reality (AR). The result is that the real-virtual registration is “open loop”-inaccurate tracking leads to misregistration that is seen by the users but not the system. We propose a general approach to “close the loop” in the displayed appearance by using the visual feedback of registration for tracking. Specifically, a model-based method is introduced to simultaneously track and augment real objects in a closed-loop fashion, where the model is comprised of the combination of the real object to be tracked and the virtual object to be rendered. This method is applicable to paradigms including video-based AR, projector-based AR, and diminished reality.},
keywords={augmented reality;closed loop systems;feedback;image registration;rendering (computer graphics);closed-loop registration;augmented reality;tracking;augmentation;real-virtual registration;visual feedback;model-based method;rendering;virtual object;real object;video-based AR;projector-based AR;diminished reality;Augmented reality;Solid modeling;Optimization;Equations;Mathematical model;Computational modeling;Educational institutions;Closed-loop registration;visual feedback;tracking},
doi={10.1109/ISMAR.2012.6402601},
ISSN={},
month={Nov},}