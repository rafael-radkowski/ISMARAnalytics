%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for Rafael Radkowski at 2021-03-13 10:23:55 -0600 


%% Saved with string encoding Unicode (UTF-8) 



@inproceedings{7328047,
	Abstract = {In this tutorial we aim for a review of existing technologies to perform outdoor localization in urban environments at a global level in full 6DOF using visual sensors primarily. The goal is to provide a clear overview about the current state-of-the-art in global positioning and orientation estimation, which includes a wide range of methods and algorithms from both the Computer Vision and the Augmented Reality community. The main focus is put on methods that are real-time capable, or can at least be applied through a server-client infrastructure. Algorithms that are based on single images, panoramic images, as well as SLAM maps and sparse point cloud reconstructions from SfM will be discussed, together with mobile hardware considerations.The attendees will acquire an overview about the current landscape of technologies employed to facilitate outdoor localization for AR. The tutorial should enable them to get a feeling for the current state-of-the-art of methods for outdoor Augmented Reality.},
	Author = {C. {Arth} and D. {Schmalstieg}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.72},
	Keywords = {augmented reality;global-scale localization;outdoor environments;AR;urban environments;global level;6DOF;visual sensors;global positioning;orientation estimation;computer vision;augmented reality community;server-client infrastructure;panoramic images;SLAM maps;sparse point cloud reconstructions;SfM;mobile hardware considerations;Sensors;Augmented reality;Tutorials;Computer vision;Mobile communication;Hardware;Visualization},
	Month = {Sep.},
	Pages = {xxx-xxx},
	Title = {Tutorial 1: Global-scale Localization in Outdoor Environments for AR},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.72}}

@inproceedings{7328048,
	Abstract = {Summary form only given. In this tutorial, we will introduce emerging technologies on computational imaging and light field projection to AR/MR researchers.Light is the most important medium in AR/VR technologies to not only obtain information but also show and modify visual cue in the real scenes. Therefore in this area, latest techniques on optics, imaging and lighting have played an important role to make a next step toward the sophisticated experiences. Computational photography is one of the most influential technology in computer vision and optical engineering areas, and we think most techniques in computational imaging and projection can be applied to common problems in mixed reality, such as scene modeling, modification of the appearances of actual objects and user interactions.},
	Author = {S. {Hiura} and H. {Nagahara} and D. {Iwai} and T. {Amano}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.71},
	Keywords = {augmented reality;computer vision;computational imaging;light field projection;AR/MR research;augmented reality;mixed reality;AR-VR technology;virtual reality;computer vision;optical engineering;Photography;Cities and towns;Computer vision;Cameras;Virtual reality;Tutorials},
	Month = {Sep.},
	Pages = {xxxi-xxxi},
	Title = {Tutorial 2: Computational Imaging and Projection},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.71}}

@inproceedings{7328049,
	Abstract = {IUI - Intelligent User Interfaces: will introduce you to the design and implementation of Intelligent User Interfaces (IUIs). IUIs aim to incorporate intelligent automated capabilities in human computer interaction, where the net impact is a human-computer interaction that improves performance or usability in critical ways. It also involves designing and implementing an artificial intelligence (AI) component that effectively leverages human skills and capabilities, so that human performance with an application excels. IUIs embody capabilities that have traditionally been associated more strongly with humans than with computers: how to perceive, interpret, learn, use language, reason, plan, and decide.},
	Author = {D. {Sonntag}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.74},
	Keywords = {artificial intelligence;human computer interaction;human factors;user interfaces;intelligent user interface;IUI;human computer interaction;artificial intelligence;Artificial intelligence;Computers;Virtual reality;Electronic mail;Human computer interaction;Usability},
	Month = {Sep.},
	Pages = {xxxii-xxxii},
	Title = {Tutorial 3: Intelligent User Interfaces},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.74}}

@inproceedings{7328050,
	Abstract = {A variety of cases uses of AR in informal learning environments. The cases uses are drawn from a variety of different contexts. There will be examples of AR use in education, tourism, event organizing, and others. This is mainly geared to people creating learning environments in any industry a foundation to start implementation AR. The featured case use will be how AR was used at TEDxKyoto to engage participants. There will also be several student projects that use AR presented and available for demo.},
	Author = {E. {Hawkinson} and M. {Stack} and J. {Klaphake} and S. {Jacoby}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.73},
	Keywords = {augmented reality;learning (artificial intelligence);informal learning environments;AR implementations;education;tourism;event organizing;TEDxKyoto;Jacobian matrices;Context;Industries;Tutorials;Cultural differences;Schedules},
	Month = {Sep.},
	Pages = {xxxiii-xxxiii},
	Title = {Tutorial 4: AR Implementations in Informal Learning},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.73}}

@inproceedings{7328051,
	Abstract = {Augmented Reality (AR) in microscopic surgery has been subject of several studies in the past two decades. Nevertheless, AR has not found its way into everyday microsurgical workflows. The introduction of new surgical microscopes equipped with Optical Coherence Tomography (OCT) enables the surgeons to perform multimodal (optical and OCT) imaging in the operating room. Taking full advantage of such elaborate source of information requires sophisticated intraoperative image fusion, information extraction, guidance and visualization methods. Medical AR is a unique approach to facilitate utilization of multimodal medical imaging devices. Here we propose a novel medical AR solution to the long-known problem of determining the distance between the surgical instrument tip and the underlying tissue in ophthalmic surgery to further pave the way of AR into the surgical theater. Our method brings augmented reality to OCT for the first time by augmenting the surgeon's view of the OCT images with an estimated instrument cross-section shape and distance to the retinal surface using only information from the shadow of the instrument in intraoperative OCT images. We demonstrate the applicability of our method in retinal surgery using a phantom eye and evaluate the accuracy of the augmented information using a micromanipulator.},
	Author = {H. {Roodaki} and K. {Filippatos} and A. {Eslami} and N. {Navab}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.15},
	Keywords = {augmented reality;data visualisation;medical image processing;micromanipulators;optical tomography;surgery;augmented reality;optical coherence tomography;ophthalmic microsurgery;microscopic surgery;surgical microscope;image fusion;information extraction;multimodal medical imaging device;medical AR solution;surgical instrument tip;ophthalmic surgery;intraoperative OCT image;retinal surgery;micromanipulator;Surgery;Microscopy;Retina;Estimation;Optical microscopy;Biomedical optical imaging;Augmented reality;optical coherence tomography;ophthalmic surgery;instrument cross-section},
	Month = {Sep.},
	Pages = {1-6},
	Title = {Introducing Augmented Reality to Optical Coherence Tomography in Ophthalmic Microsurgery},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.15}}

@inproceedings{7328052,
	Abstract = {Image-guided medical interventions more frequently rely on Augmented Reality (AR) visualization to enable surgical navigation. Current systems use 2-D monitors to present the view from external cameras, which does not provide an ideal perception of the 3-D position of the region of interest. Despite this problem, most research targets the direct overlay of diagnostic imaging data, and only few studies attempt to improve the perception of occluded structures in external camera views. The focus of this paper lies on improving the 3-D perception of an augmented external camera view by combining both auditory and visual stimuli in a dynamic multi-sensory AR environment for medical applications. Our approach is based on Temporal Distance Coding (TDC) and an active surgical tool to interact with occluded virtual objects of interest in the scene in order to gain an improved perception of their 3-D location. Users performed a simulated needle biopsy by targeting virtual lesions rendered inside a patient phantom. Experimental results demonstrate that our TDC-based visualization technique significantly improves the localization accuracy, while the addition of auditory feedback results in increased intuitiveness and faster completion of the task.},
	Author = {F. {Bork} and B. {Fuers} and A. {Schneider} and F. {Pinto} and C. {Graumann} and N. {Navab}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.16},
	Keywords = {augmented reality;cameras;data visualisation;medical computing;patient diagnosis;surgery;auditory;visio-temporal distance coding;3-Dimensional Perception;medical augmented reality visualization;image-guided medical intervention;surgical navigation;diagnostic imaging data;camera;dynamic multisensory AR environment;active surgical tool;patient phantom;TDC-based visualization technique;Biopsy;Shape;Needles;Visualization;Encoding;Accuracy;Lesions;Medical Augmented Reality;Multi-Sensory Environment;Temporal Distance Coding;Auditory and Visual Stimuli},
	Month = {Sep.},
	Pages = {7-12},
	Title = {Auditory and Visio-Temporal Distance Coding for 3-Dimensional Perception in Medical Augmented Reality},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.16}}

@inproceedings{7328053,
	Abstract = {This paper presents the first design of a mirror based RGBD X-ray imaging system and includes an evaluation study of the depth errors induced by the mirror when used in combination with an infrared pattern-emission RGBD camera. Our evaluation consisted of three experiments. The first demonstrated almost no difference in depth measurements of the camera with and without the use of the mirror. The final two experiments demonstrated that there were no relative and location-specific errors induced by the mirror showing the feasibility of the RGBDX-ray imaging system. Lastly, we showcase the potential of the RGBDX-ray system towards a visualization application in which an X-ray image is fused to the 3D reconstruction of the surgical scene via the RGBD camera, using automatic C-arm pose estimation.},
	Author = {S. {Habert} and J. {Gardiazabal} and P. {Fallavollita} and N. {Navab}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.17},
	Keywords = {data visualisation;diagnostic radiography;image colour analysis;image reconstruction;medical image processing;pose estimation;RGBDX system;mirror-based RGBD X-ray imaging system;red-green-blue depth imaging;infrared pattern-emission RGBD camera;visualization application;automatic C-arm pose estimation;surgical scene reconstruction;Mirrors;Three-dimensional displays;X-ray imaging;Cameras;Surface reconstruction;Surgery;Rendering (computer graphics);Medical Augmented Reality;X-ray imaging;Range Imaging;Multi-modal Visualization},
	Month = {Sep.},
	Pages = {13-18},
	Title = {RGBDX: First Design and Experimental Validation of a Mirror-Based RGBD X-ray Imaging System},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.17}}

@inproceedings{7328054,
	Abstract = {We propose a novel formulation for determining the absolute pose of a single or multi-camera system given a known vertical direction. The vertical direction may be easily obtained by detecting the vertical vanishing points with computer vision techniques, or with the aid of IMU sensor measurements from a smartphone. Our solver is general and able to compute absolute camera pose from two 2D-3D correspondences for single or multi-camera systems. We run several synthetic experiments that demonstrate our algorithm's improved robustness to image and IMU noise compared to the current state of the art. Additionally, we run an image localization experiment that demonstrates the accuracy of our algorithm in real-world scenarios. Finally, we show that our algorithm provides increased performance for real-time model-based tracking compared to solvers that do not utilize the vertical direction and show our algorithm in use with an augmented reality application running on a Google Tango tablet.},
	Author = {C. {Sweeney} and J. {Flynn} and B. {Nuernberger} and M. {Turk} and T. {H{\"o}llerer}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.20},
	Keywords = {augmented reality;real-time systems;smart phones;gravity-aware augmented reality;multicamera system;absolute pose;vertical direction;vertical vanishing points;computer vision techniques;sensor measurements;smartphone;2D correspondences;3D correspondences;IMU noise;real-time model-based tracking;Google Tango tablet;image localization experiment;Cameras;Three-dimensional displays;Gravity;Augmented reality;Accuracy;Robustness;Google;Absolute pose;multi-camera system;gravity-aware augmented reality;inertial sensor;model-based tracking},
	Month = {Sep.},
	Pages = {19-24},
	Title = {Efficient Computation of Absolute Pose for Gravity-Aware Augmented Reality},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.20}}

@inproceedings{7328055,
	Abstract = {In this paper we present a dual, wide area, collaborative augmented reality (AR) system that consists of standard live view augmentation, e.g., from helmet, and zoomed-in view augmentation, e.g., from binoculars. The proposed advanced scouting capability allows long range high precision augmentation of live unaided and zoomed-in imagery with aerial and terrain based synthetic objects, vehicles, people and effects. The inserted objects must appear stable in the display and not jitter or drift as the user moves around and examines the scene. The AR insertions for the binocs must work instantly when they are picked up anywhere as the user moves around. The design of both AR modules is based on using two different cameras with wide and narrow field of view (FoV) lenses. The wide FoV gives context and enables the recovery of location and orientation of the prop in 6 degrees of freedom (DoF) much more robustly, whereas the narrow FoV is used for the actual augmentation and increased precision in tracking. Furthermore, narrow camera in unaided eye and wide camera on the binoculars are jointly used for global yaw (heading) correction. We present our navigation algorithms using monocular cameras in combination with IMU and GPS in an Extended Kalman Filter (EKF) framework to obtain robust and real-time pose estimation for precise augmentation and cooperative tracking.},
	Author = {T. {Oskiper} and M. {Sizintsev} and V. {Branzoi} and S. {Samarasekera} and R. {Kumar}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.11},
	Keywords = {augmented reality;computer based training;groupware;Kalman filters;nonlinear filters;augmented reality scout;immersive team training;collaborative augmented reality system;AR modules;field of view lenses;FoV lenses;monocular cameras;navigation algorithms;IMU;GPS;extended Kalman filter;EKF framework;pose estimation;cooperative tracking;telescopic-zoom system;unaided-eye system;Cameras;Databases;Global Positioning System;Visualization;Robustness;Three-dimensional displays;Buildings;visual-inertial navigation;monocular wide and narrow field of view camera;GPS;sensor fusion;EKF},
	Month = {Sep.},
	Pages = {25-30},
	Title = {Augmented Reality Scout: Joint Unaided-Eye and Telescopic-Zoom System for Immersive Team Training},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.11}}

@inproceedings{7328056,
	Abstract = {Omnidirectional videos of real world environments viewed on head-mounted displays with real-time head motion tracking can offer immersive visual experiences. For live streaming applications, compression is critical to reduce the bitrate. Omnidirectional videos, which are spherical in nature, are mapped onto one or more planes before encoding to interface with modern video coding standards. In this paper, we consider the problem of evaluating the coding efficiency in the context of viewing with a head-mounted display. We extract viewport based head motion trajectories, and compare the original and coded videos on the viewport. With this approach, we compare different sphere-to-plane mappings. We show that the average viewport quality can be approximated by a weighted spherical PSNR.},
	Author = {M. {Yu} and H. {Lakshman} and B. {Girod}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.12},
	Keywords = {feature extraction;helmet mounted displays;video coding;omnidirectional video coding schemes;head-mounted display;viewport extraction;head motion trajectories;sphere-to-plane mappings;viewport quality;weighted spherical PSNR;peak signal-to-noise ratio;Encoding;Bit rate;Approximation methods;Head;Video coding;Streaming media;Trajectory},
	Month = {Sep.},
	Pages = {31-36},
	Title = {A Framework to Evaluate Omnidirectional Video Coding Schemes},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.12}}

@inproceedings{7328057,
	Abstract = {Mobile devices are part of our everyday life and allow augmented reality (AR) with their integrated camera image. Recent research has shown that even photorealistic augmentations with consistent illumination are possible. A method, achieving this first, distributed lighting computations and the extraction of the important light sources. To reach real-time frame rates on a mobile device, the number of these extracted light sources must be low, limiting the scope of possible illumination scenarios and the quality of shadows. In this paper, we show how to reduce the computational cost per light using a combination of tile-based rendering and frustum culling techniques tailored for AR applications. Our approach runs entirely on the GPU and does not require any precomputation. Without reducing the displayed image quality, we achieve up to 2.2Ã— speedup for typical AR scenarios.},
	Author = {K. {Rohmer} and T. {Grosch}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.13},
	Keywords = {augmented reality;graphics processing units;mobile computing;rendering (computer graphics);tiled frustum culling;differential rendering;mobile devices;augmented reality;AR;photorealistic augmentation;distributed lighting computation;light source extraction;illumination scenario;shadow quality;GPU;graphics processing unit;image quality;Rendering (computer graphics);Lighting;Light sources;Cameras;Face;Radiation detectors;Timing;Mixed/Augmented Reality;Mobile;Real-time Global Illumination;Differential Rendering;Culling;Light Management},
	Month = {Sep.},
	Pages = {37-42},
	Title = {Tiled Frustum Culling for Differential Rendering on Mobile Devices},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.13}}

@inproceedings{7328058,
	Abstract = {In Augmented Reality (AR) with an Optical See-Through Head-Mounted Display (OST-HMD), the spatial calibration between a user's eye and the display screen is a crucial issue in realizing seamless AR experiences. A successful calibration hinges upon proper modeling of the display system which is conceptually broken down into an eye part and an HMD part. This paper breaks the HMD part down even further to investigate optical aberration issues. The display optics causes two different optical aberrations that degrade the calibration quality: the distortion of incoming light from the physical world, and that of light from the image source of the HMD. While methods exist for correcting either of the two distortions independently, there is, to our knowledge, no method which corrects for both simultaneously. This paper proposes a calibration method that corrects both of the two distortions simultaneously for an arbitrary eye position given an OST-HMD system. We expand a light-field (LF) correction approach [8] originally designed for the former distortion. Our method is camera-based and has an offline learning and an online correction step. We verify our method in exemplary calibrations of two different OST-HMDs: a professional and a consumer OST-HMD. The results show that our method significantly improves the calibration quality compared to a conventional method with the accuracy comparable to 20/50 visual acuity. The results also indicate that only by correcting both the distortions simultaneously can improve the quality.},
	Author = {Y. {Itoh} and G. {Klinker}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.14},
	Keywords = {aberrations;augmented reality;calibration;helmet mounted displays;learning (artificial intelligence);augmented view distortion calibration;optical see-through head-mounted displays;direct view distortion calibration;OST-HMD;augmented reality;spatial calibration;user eye;display screen;seamless AR experiences;optical aberration issues;light-field correction approach;LF;offline learning;online correction step;visual acuity;Optical distortion;Distortion;Calibration;DVD;Cameras;Three-dimensional displays;Optical imaging},
	Month = {Sep.},
	Pages = {43-48},
	Title = {Simultaneous Direct and Augmented View Distortion Calibration of Optical See-Through Head-Mounted Displays},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.14}}

@inproceedings{7328059,
	Abstract = {An effective interaction in augmented reality (AR) requires utilization of different modalities. In this study, we investigated orienting the user in bimodal AR. Using auditory perception to support visual perception provides a useful approach for orienting the user to directions that are outside of the visual field-of-view (FOV). In particular, this is important in path-finding, where points-of-interest (POIs) can be all around the user. However, the ability to perceive the audio POIs is affected by the ventriloquism effect (VE), which means that audio POIs are captured by visual POIs. We measured the spatial limits for the VE in AR using a video see-through head-worn display. The results showed that the amount of the VE in AR was approx. 5$\,^{\circ}$--15$\,^{\circ}$ higher than in a real environment. In AR, spatial disparity between an audio and visual POI should be at least 30$\,^{\circ}$ of azimuth angle, in order to perceive the audio and visual POIs as separate. The limit was affected by azimuth angle of visual POI and magnitude of head rotations. These results provide guidelines for designing bimodal AR systems.},
	Author = {M. {Kyt{\"o}} and K. {Kusumoto} and P. {Oittinen}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.18},
	Keywords = {augmented reality;helmet mounted displays;ventriloquist effect;augmented reality;bimodal AR system;auditory perception;visual perception support;visual field-of-view;FOV;VE;video see-through head-worn display;audio POI;visual POI;points-of-interest;Visualization;Azimuth;Augmented reality;Speech;Visual perception;Navigation;Uncertainty},
	Month = {Sep.},
	Pages = {49-53},
	Title = {The Ventriloquist Effect in Augmented Reality},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.18}}

@inproceedings{7328060,
	Abstract = {Current methods dealing with non-rigid augmented reality only provide an augmented view when the topology of the tracked object is not modified, which is an important limitation. In this paper we solve this shortcoming by introducing a method for physics-based non-rigid augmented reality. Singularities caused by topological changes are detected by analyzing the displacement field of the underlying deformable model. These topological changes are then applied to the physics-based model to approximate the real cut. All these steps, from deformation to cutting simulation, are performed in real-time. This significantly improves the coherence between the actual view and the model, and provides added value.},
	Author = {C. J. {Paulus} and N. {Haouchine} and D. {Cazier} and S. {Cotin}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.19},
	Keywords = {augmented reality;object tracking;deformable object cutting;deformable object tearing;augmented view;object tracking;physics-based nonrigid augmented reality;deformable model;physics-based model;cutting simulation;Computational modeling;Feature extraction;Deformable models;Augmented reality;Finite element analysis;Real-time systems;Surgery;Augmented Reality;Deformation;Cutting;Tearing},
	Month = {Sep.},
	Pages = {54-59},
	Title = {Augmented Reality during Cutting and Tearing of Deformable Objects},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.19}}

@inproceedings{7328061,
	Abstract = {C-arm fluoroscopes are frequently used during surgeries for intraoperative guidance. Unfortunately, due to X-ray emission and scattering, increased radiation exposure occurs in the operating theatre. The objective of this work is to sensitize the surgeon to their radiation exposure, enable them to check on their exposure over time, and to help them choose their best position related to the C-arm gantry during surgery. First, we aim at simulating the amount of radiation that reaches the surgeon using the Geant4 software, a toolkit developed by CERN. Using a flexible setup in which two RGB-D cameras are mounted to the mobile C-arm, the scene is captured and modeled respectively. After the simulation of particles with specific energies, the dose at the surgeon's position, determined by the depth cameras, can be measured. The validation was performed by comparing the simulation results to both theoretical values from the C-arms user manual and real measurements made with a QUART didoSVM dosimeter. The average error was 16.46% and 16.39%, respectively. The proposed flexible setup and high simulation precision without a calibration with measured dosimeter values, has great potential to be directly used and integrated intraoperatively for dose measurement.},
	Author = {N. {Leucht} and S. {Habert} and P. {Wucherer} and S. {Weidert} and N. {Navab} and P. {Fallavollita}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.21},
	Keywords = {augmented reality;diagnostic radiography;medical computing;surgery;augmented reality;radiation awareness;C-arm fluoroscopes;intraoperative surgery guidance;radiation exposure;C-arm gantry;Geant4 software;RGB-D camera;red-green-blue-depth camera;QUART didoSVM dosimeter;dose measurement;Surgery;Cameras;X-ray imaging;Photonics;Manuals;Computational modeling;Calibration;radiation exposure;dose measurement;C-arm fluoroscopy;augmented reality;visualization},
	Month = {Sep.},
	Pages = {60-63},
	Title = {[POSTER] Augmented Reality for Radiation Awareness},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.21}}

@inproceedings{7328062,
	Abstract = {Mixed Reality (MR) can merge real and virtual worlds seamlessly. This paper proposes a method to realize smooth collaboration using a remote MR, which makes it possible for geographically distributed users to share the same objects and communicate in real time as if they are at the same place. In this paper, we consider a situation where the users at local and remote sites perform a collaborative work, and real objects to be operated exist only at the local site. It is necessary to share the real objects between the two sites. However, prior studies have shown sharing real objects by duplication is either too costly or unrealistic. Therefore, we propose a method to share the objects by virtualizing the real objects using Computer Vision (CV) and then rendering the virtualized objects using MR. We have proposed a remote collaborative work system to create a smoother user experience for collaborative work with virtualized objects for remote users. Through experiments, we confirmed the effectiveness of our approach.},
	Author = {P. {Yang} and I. {Kitahara} and Y. {Ohta}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.22},
	Keywords = {computer vision;groupware;user interfaces;virtual reality;remote mixed reality system;virtualized objects;MR;geographically distributed users;local sites;remote sites;computer vision;CV;remote collaborative work system;user experience;Collaboration;Virtual reality;Collaborative work;Rendering (computer graphics);Three-dimensional displays;Cameras;Real-time systems;Mixed reality;virtualized object;3D interaction;remote collaboration;RGB-D camera},
	Month = {Sep.},
	Pages = {64-67},
	Title = {[POSTER] Remote Mixed Reality System Supporting Interactions with Virtualized Objects},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.22}}

@inproceedings{7328063,
	Abstract = {This paper aims at accurate and efficient pose tracking of planar targets on modern smartphones. Existing methods, relying on either visual features or motion sensing based on built-in inertial sensors, are either too computationally expensive to achieve realtime performance on a smartphone, or too noisy to achieve sufficient tracking accuracy. In this paper we present a hybrid tracking method which can achieve real-time performance with high accuracy. Based on the same framework of a state-of-the-art visual feature tracking algorithm [5] which ensures accurate and reliable pose tracking, the proposed hybrid method significantly reduces its computational cost with the assistance of a phone's built-in inertial sensors. However, noises in inertial sensors and abrupt errors in feature tracking due to severe motion blurs could result in instability of the hybrid tracking system. To address this problem, we propose to employ an adaptive Kalman filter with abrupt error detection to robustly fuse the inertial and feature tracking results. We evaluated the proposed method on a dataset consisting of 16 video clips with synchronized inertial sensing data. Experimental results demonstrated our method's superior performance and accuracy on smartphones compared to a state-of-the-art vision tracking method [5]. The dataset will be made publicly available with the publication of this paper.},
	Author = {X. {Yang} and X. {Si} and T. {Xue} and K. T. {Cheng}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.23},
	Keywords = {adaptive Kalman filters;error detection;feature extraction;motion estimation;pose estimation;smart phones;inertial sensing;pose tracking;smartphones;planar targets;motion sensing;hybrid tracking method;visual feature tracking algorithm;computational cost;phone built-in inertial sensors;motion blurs;adaptive Kalman filter;error detection;synchronized inertial sensing data;vision tracking method;Tracking;Visualization;Sensors;Feature extraction;Cameras;Accuracy;Smart phones;Pose tracking;visual features;inertial sensing;fusion;adaptive Kalman filtering},
	Month = {Sep.},
	Pages = {68-71},
	Title = {[POSTER] Fusion of Vision and Inertial Sensing for Accurate and Efficient Pose Tracking on Smartphones},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.23}}

@inproceedings{7328064,
	Abstract = {Fusing intraoperative X-ray data with real-time video in a common reference frame is not trivial since both modalities have to be acquired from the same viewpoint. The goal of this work is to design a flexible system comprising two RGBD sensors that can be attached to any mobile C-arm, with the objective of synthesizing projective color images from the X-ray source viewpoint. To achieve this, we calibrate the RGBD sensors to the X-ray source with a 3D calibration object. Then, we synthesize the projective color image from the X-ray viewpoint by applying a volumetric-based rendering method. Finally, the X-ray image is overlaid on the projective image without any further registration, offering a multimodal visualization of X-ray and color images. In this paper we present the different steps of development (i.e. hardware setup, calibration and rendering algorithm) and discuss clinical applications for the new video augmented C-arm. By placing X-ray markers on a hand patient and a spine model, we show that the overlay accuracy between the X-ray image and the synthetized image is in average 1.7 mm.},
	Author = {S. {Habert} and M. {Meng} and W. {Kehl} and X. {Wang} and F. {Tombari} and P. {Fallavollita} and N. {Navab}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.24},
	Keywords = {data visualisation;diagnostic radiography;image colour analysis;image fusion;image sensors;medical image processing;rendering (computer graphics);mobile C-arm fluoroscopes;stereo-RGBD sensor;red-green-blue-depth sensors;multimodal visualization;intraoperative X-ray data fusion;projective color image synthesis;X-ray source viewpoint;3D calibration object;volumetric-based rendering method;hand patient;spine model;X-ray imaging;Cameras;Calibration;Sensors;Three-dimensional displays;Color;Image color analysis;Medical Augmented Reality;X-ray imaging;Range Imaging;Multi-modal Visualization},
	Month = {Sep.},
	Pages = {72-75},
	Title = {[POSTER] Augmenting Mobile C-arm Fluoroscopes via Stereo-RGBD Sensors for Multimodal Visualization},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.24}}

@inproceedings{7328065,
	Abstract = {To help the computing device always understand the spacial relationship between the user's gesture and the ambient objects, a methodology is proposed to find the user's virtual eye center in the wearable camera coordinate system and then calculate accurately where a user is pointing at to perform the natural interaction. First, the wearable RGB-D sensor is affixed around the user forehead. A tool-free calibration is done by having the user move their fingers along their lines of sight from his eye center to the random selected targets. The fingertips are detected in the depth camera and then the interaction of these lines of sight is calculated. Then we present how to find where the user is pointing at in different scenarios with a depth map, a detected object and a controlled virtual element. To validate our methods, we perform a point-to-screen experiment. Results demonstrate that when a user is interacting with a display up to 1.5 meters away, our natural gesture interface has an average error of 2.1cm. In conclusion, the presented technique is a viable option for a reliable user interaction.},
	Author = {M. {Ma} and K. {Merckx} and P. {Fallavollita} and N. {Navab}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.25},
	Keywords = {gesture recognition;human computer interaction;image colour analysis;image sensors;user interfaces;natural user interface;ambient objects;spacial relationship;user gesture;virtual eye center;wearable camera coordinate system;wearable RGB-D sensor;user forehead;eye center;random selected targets;controlled virtual element;point-to-screen experiment;natural gesture interface;reliable user interaction;Three-dimensional displays;Cameras;Calibration;Color;Human computer interaction;Gesture recognition},
	Month = {Sep.},
	Pages = {76-79},
	Title = {[POSTER] Natural User Interface for Ambient Objects},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.25}}

@inproceedings{7328066,
	Abstract = {Augmented reality does not make any sense for fixed cameras. Or does it? In this work, we are dealing with static cameras and their usability for interactive augmented reality applications. Knowing that the camera does not move makes camera pose estimation both less and more difficult - one does not have to deal with pose change in time, but on the other hand, obtaining some level of understanding of the scene from a single viewpoint is challenging. We propose several ways how to gain advantage from the camera being static and a pipeline of a system for broadcasting a video stream enriched by information needed for its interactive visual augmenting - Interactive Camera Streams, INCAST. We present a proof-of-concept system showing the usability of INCAST on several use-cases - non-interactive demos and simple AR games.},
	Author = {I. {Szentandr{\'a}si} and M. {Zachari{\'a}} and R. {Kajan} and J. {Tinka} and M. {Dubsk{\'a}} and J. {Sochor} and A. {Herout}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.26},
	Keywords = {augmented reality;cameras;pose estimation;INCAST system;interactive camera stream;surveillance cams AR;interactive augmented reality application;camera pose estimation;Cameras;Streaming media;Augmented reality;Surveillance;Games;Three-dimensional displays;Real-time systems},
	Month = {Sep.},
	Pages = {80-83},
	Title = {[POSTER] INCAST: Interactive Camera Streams for Surveillance Cams AR},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.26}}

@inproceedings{7328067,
	Abstract = {In this paper, we propose an interaction system in which the appearance of the image displayed on a mobile display is consistent with that of the real space and that enables a user to interact with virtual objects overlaid on the image using the user's hand. The three-dimensional scene obtained by a depth camera is projected according to the user's viewpoint position obtained by face tracking, and the see-through image whose appearance is consistent with that outside the mobile display is generated. Interaction with virtual objects is realized by using the depth information obtained by the depth camera. To move virtual objects as if they were in real space, virtual objects are rendered in the world coordinate system that is fixed to a real scene even if the mobile display moves, and the direction of gravitational force added to virtual objects is made consistent with that of the world coordinate system. The former is realized by using the ICP (Iterative Closest Point) algorithm and the latter is realized by using the information obtained by an accelerometer. Thus, natural interaction with virtual objects using the user's hand is realized.},
	Author = {Y. {Unuma} and T. {Komuro}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.27},
	Keywords = {augmented reality;iterative methods;user interfaces;natural 3D interaction;see-through mobile AR system;augmented reality;mobile display;virtual objects;three-dimensional scene;depth camera;gravitational force;world coordinate system;ICP algorithm;iterative closest point algorithm;user hand;Cameras;Mobile communication;Three-dimensional displays;Face;Mobile handsets;Augmented reality;Iterative closest point algorithm;Augmented reality;mobile device;geometric consistency;depth camera},
	Month = {Sep.},
	Pages = {84-87},
	Title = {[POSTER] Natural 3D Interaction Using a See-Through Mobile AR System},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.27}}

@inproceedings{7328068,
	Abstract = {Within modern manufacturing, digital solutions are needed to optimize and aid shop floor processes. This includes user-centered technologies that can be appropriately integrated into factory environments to assist in the efficiency of manufacturing tasks. In this paper, we present a dynamic system to support the electrical wiring assembly of commercial aircraft. Specifically, we describe the system design, which aims to improve the productivity of factory operators through the integration of wearable and mobile solutions. An evaluation of the augmented reality component of our system using a pair of smart glasses is reported with 12 participants, as we describe important interaction issues in the ongoing development of this work.},
	Author = {M. {Rice} and H. H. {Tay} and J. {Ng} and C. {Lim} and S. K. {Selvaraj} and E. {Wu}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.28},
	Keywords = {aircraft manufacture;assembling;augmented reality;computer aided manufacturing;mobile computing;productivity;wearable computers;wires (electric);augmented wire routing navigation;shop floor process;digital solution optimization;user-centered technologies;factory environments;manufacturing tasks;electrical wiring assembly;commercial aircraft;factory operator productivity;mobile solutions;wearable solutions;augmented reality component;smart glasses;Wires;Assembly;Routing;Glass;Augmented reality;Navigation;Aircraft;Augmented reality;wire assembly;indoor location tracking;GPS navigation},
	Month = {Sep.},
	Pages = {88-91},
	Title = {[POSTER] Augmented Wire Routing Navigation for Wire Assembly},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.28}}

@inproceedings{7328069,
	Abstract = {For the clinical community, there is great need for objective, quantitative and valid measures of the factors contributing to motor dysfunction. Currently, there are no standard protocols to assess motor dysfunction in various patient groups, where each medical discipline uses subjectively scored clinical tests, qualitative video analysis, or marker-based motion capturing. We investigate the potential of Augmented Reality (AR) combined with serious gaming and marker-less tracking of the hand to facilitate efficient, cost-effective and patient-friendly methods for evaluation of upper extremity motor dysfunction in different patient groups. First, the design process of the game and the system architecture of the AR framework are described. To provide unhindered assessment of motor dysfunction, patients should operate with the system in a natural way and be able to understand their actions in the virtual AR world. To test this in our system, we conducted a usability study with five healthy people (aged between 57-63) on three different modalities of visual feedback for natural hand interaction with AR objects. These modalities are: no augmented hand, partial augmented hand (tip of index finger and tip of thumb) and a full augmented hand model. The results of the study show that a virtual representation of the fingertips or hand improves the usability of natural hand interaction.},
	Author = {M. A. {Cidota} and R. M. S. {Clifford} and P. {Dezentje} and S. G. {Lukosch} and P. J. M. {Bank}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.29},
	Keywords = {augmented reality;diseases;human computer interaction;serious games (computing);visual feedback;natural hand interaction;motor dysfunction;qualitative video analysis;marker-based motion capturing;augmented reality;serious gaming;marker-less tracking;patient-friendly method;cost-effective method;system architecture;virtual AR world;virtual representation;Games;Thumb;Tracking;Cameras;Augmented reality;Usability;Augmented Reality;Optical See-Through HMD;Natural Hand Interaction;Serious Gaming;Upper Extremity Motor Dysfunction;Assessment},
	Month = {Sep.},
	Pages = {92-95},
	Title = {[POSTER] Affording Visual Feedback for Natural Hand Interaction in AR to Assess Upper Extremity Motor Dysfunction},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.29}}

@inproceedings{7328070,
	Abstract = {In optical motion capture systems, it is difficult to correctly recognize markers based on their unique identifiers (IDs) in a single frame. In this paper, we propose two types of light-emitting diodes (LEDs) and cameras, infrared (IR) and RGB, in order to correctly detect and identify all markers tracking objects in a given system. To detect and estimate the three-dimensional (3D) position of the marker, we measure IR LEDs using IR stereo cameras. Furthermore, in order to identify each marker, we calculate and compare the RGB color descriptor in the vicinity of its center. Our system consists of general IR and RGB cameras, and is easy to extend by increasing the number of cameras. We implemented an IR/RGB LED marker circuit and constructed a simple motion capture system to test the effectiveness of our system. The results show that our system can detect the 3D positions and unique IDs of markers in one frame.},
	Author = {G. {Koutaki} and S. {Hirata} and H. {Sato} and K. {Uchimura}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.30},
	Keywords = {image capture;image colour analysis;image motion analysis;infrared imaging;light emitting diodes;object tracking;optical tracking;stereo image processing;marker identification;RGB color descriptors;light-emitting diodes;infrared stereo cameras;optical motion capture systems;IR stereo cameras;RGB cameras;IR/RGB LED marker circuit;object tracking;Light emitting diodes;Cameras;Image color analysis;Three-dimensional displays;Target tracking;Optical sensors},
	Month = {Sep.},
	Pages = {96-99},
	Title = {[POSTER] Marker Identification Using IR LEDs and RGB Color Descriptors},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.30}}

@inproceedings{7328071,
	Abstract = {Calibration and registration are the first steps for augmented reality and mixed reality applications. In the medical field, the calibration between an RGB-D camera and a mobile C-arm fluoroscope is a new topic which introduces challenges. In this paper, we propose a precise 3D/2D calibration method to achieve a video augmented fluoroscope. With the design of a suitable calibration phantom for RGB-D/C-arm calibration, we calculate the projection matrix from the depth camera coordinates to the X-ray image. Through a comparison experiment by combining different steps leading to the calibration, we evaluate the effect of every step of our calibration process. Results demonstrated that we obtain a calibration RMS error of 0.54$\pm$1.40 mm which is promising for surgical applications. We conclude this paper by showcasing two clinical applications. One is a markerless registration application, the other is an RGB-D camera augmented mobile C-arm visualization.},
	Author = {X. {Wang} and S. {Habert} and M. {Ma} and C. {Huang} and P. {Fallavollita} and N. {Navab}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.31},
	Keywords = {augmented reality;diagnostic radiography;image colour analysis;image registration;medical image processing;surgery;RGB-D/C-arm calibration;medical augmented reality;mixed reality application;RGB-D camera;red-green-blue-depth camera;mobile C-arm fluoroscope;3D-2D calibration method;video augmented fluoroscope;X-ray image;surgical application;markerless registration application;X-ray imaging;Calibration;Three-dimensional displays;Cameras;Sensors;Biomedical imaging;Distortion},
	Month = {Sep.},
	Pages = {100-103},
	Title = {[POSTER] RGB-D/C-arm Calibration and Application in Medical Augmented Reality},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.31}}

@inproceedings{7328072,
	Abstract = {In this extended poster, we present a model that aims to provide developers with an extensive and extensible set of context-aware interaction techniques, greatly facilitating the creation of meaningful AR-based user experiences. To provide a complete view of the model, we detail the different aspects that form its theoretical foundations, while also discussing several considerations for its correct implementation.},
	Author = {M. {Salazar} and C. {Laorden} and P. G. {Bringas}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.32},
	Keywords = {augmented reality;human computer interaction;user interfaces;AR system;augmented reality;context-aware interaction technique;AR-based user experience;human-computer interaction;Computational modeling;Computers;User interfaces;Context;Augmented reality;Context modeling;Unified modeling language;Interaction model;Augmented Reality},
	Month = {Sep.},
	Pages = {104-107},
	Title = {[POSTER] A Comprehensive Interaction Model for AR Systems},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.32}}

@inproceedings{7328073,
	Abstract = {In this paper we present FastAR, a software component capable of transforming Joomla based websites into AR-channels compatible with the most popular augmented reality browsers (i.e. Junaio, Layar, Wikitude). FastAR exploits the consistency of the data structure across multiple sites that have been developed using the same content management system, so as to automate the transformation process of an internet website to an augmented reality channel. The proposed component abstracts all related programming tasks and significantly reduces the time required to generate and publish AR-content, making the entire process manageable by non-experts. In verifying the usefulness and effectiveness of FastAR, we conducted a survey to solicit the opinion of users who carried out the installation and transformation process.},
	Author = {D. {Ververidis} and S. {Nikolopoulos} and I. {Kompatsiaris}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.33},
	Keywords = {augmented reality;user interfaces;Web sites;Web site transformation;augmented reality view;FastAR software component;augmented reality browsers;content management system;user installation process;user transformation process;Browsers;Databases;Companies;Servers;HTML;Google;Augmented reality},
	Month = {Sep.},
	Pages = {108-111},
	Title = {[POSTER] Transforming Your Website to an Augmented Reality View},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.33}}

@inproceedings{7328074,
	Abstract = {In Spatial Augmented Reality (SAR) applications, real world objects are augmented with virtual content by means of a calibrated camera-projector system. A computer generated model (CAD) of the real object is used to plan the positions where the virtual content is to be projected. It is often the case that the real object deviates from its CAD model, this resulting in misregistered augmentations. We propose a new method to dynamically correct the planned augmentation by accommodating for the unknown deviations in the object geometry. We use a closed loop approach where the projected features are detected in the camera image and deployed as feedback. As a result, the registration misalignment is identified and the augmentations are corrected in the areas affected by the deviation. Our work is especially focused on SAR applications related to the industrial domain, where this problem is omnipresent. We show that our method is effective and beneficial for multiple industrial applications.},
	Author = {H. {Naik} and F. {Tombari} and C. {Resch} and P. {Keitler} and N. {Navab}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.34},
	Keywords = {augmented reality;cameras;closed loop systems;computational geometry;feature extraction;image registration;optical projectors;closed loop dynamic registration correction;spatial augmented reality applications;SAR applications;virtual content;camera-projector system;computer generated model;CAD model;misregistered augmentations;object geometry;closed loop approach;camera image;feature detection;Solid modeling;Three-dimensional displays;Geometry;Accuracy;Design automation;Calibration;Cameras;Closed loop AR;Registration Correction;3D Verification;Industrial SAR},
	Month = {Sep.},
	Pages = {112-115},
	Title = {[POSTER] A Step Closer To Reality: Closed Loop Dynamic Registration Correction in SAR},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.34}}

@inproceedings{7328075,
	Abstract = {An important yet unsolved problem in computer vision and Augmented Reality (AR) is to compute the 3D shape of nonrigid objects from live 2D videos. When the object's shape is provided in a rest pose, this is the Shape-from-Template (SfT) problem. Previous realtime SfT methods require simple, smooth templates, such as flat sheets of paper that are densely textured, and which deform in simple, smooth ways. We present a realtime SfT framework that handles generic template meshes, complex deformations and most of the difficulties present in real imaging conditions. Achieving this has required new, fast solutions to the two core sub-problems: robust registration and 3D shape inference. Registration is achieved with what we call Deformable Render-based Block Matching (DRBM): a highly-parallel solution which densely matches a time-varying render of the object to each video frame. We then combine matches from DRBM with physical deformation priors and perform shape inference, which is done by quickly solving a sparse linear system with a Geometric Multi-Grid (GMG)-based method. On a standard PC we achieve up to 21fps depending on the object. Source code will be released.},
	Author = {T. {Collins} and A. {Bartoli}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.35},
	Keywords = {augmented reality;computer vision;image matching;image registration;rendering (computer graphics);realtime shape-from-template problem;computer vision;augmented reality;AR;realtime SfT framework;robust registration problem;3D shape inference problem;DRBM;deformable render-based block matching;geometric multigrid based method;GMG-based method;Three-dimensional displays;Videos;Shape;Cameras;Robustness;Transforms;Dairy products},
	Month = {Sep.},
	Pages = {116-119},
	Title = {[POSTER] Realtime Shape-from-Template: System and Applications},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.35}}

@inproceedings{7328076,
	Abstract = {Most work about instructions in Augmented Reality (AR) does not follow established patterns or design rules -- each approach defines its own method on how to convey instructions. This work describes our initial results and experiences towards defining design guidelines for AR instructions. The guidelines were derived from a survey of the most common visualization techniques and instruction types applied in AR. We studied about how 2D and 3D instructions can be applied in the AR context.},
	Author = {C. {Rolim} and D. {Schmalstieg} and D. {Kalkofen} and V. {Teichrieb}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.36},
	Keywords = {augmented reality;augmented reality instructions;design rules;3D instructions;2D instructions;Augmented reality;Visualization;Three-dimensional displays;Real-time systems;Assembly;Guidelines;Visualization;Mixed Reality;Instructions},
	Month = {Sep.},
	Pages = {120-123},
	Title = {[POSTER] Design Guidelines for Generating Augmented Reality Instructions},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.36}}

@inproceedings{7328077,
	Abstract = {We introduce a novel finger worn ring interface that enables complex spatial interactions through 3D hand movement in virtual reality environment. Users receive physical feedback in the form of vibrations from the wearable ring interface as their finger reaches a certain 3D position. The positions of the fingertip are extracted, linked, and then reconstructed as a trajectory. This system allows the wearer to write characters in midair as if they were using an imaginary whiteboard. User can freely write in the air using Korean characters, English letters, both upper and lower case, and digits in real time with over 92% accuracy rate. Thus, it is now conceivable that anything people can do on contemporary touch based devices, they could do in midair with a pseudocontact interface.},
	Author = {K. {Yeom} and J. {Kwon} and J. {Maeng} and B. {You}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.37},
	Keywords = {feature extraction;graphical user interfaces;haptic interfaces;natural language processing;vibrations;virtual reality;haptic ring interface;air-writing;virtual reality environment;finger worn ring interface;complex spatial interactions;3D hand movement;fingertip position extraction;trajectory reconstruction;imaginary whiteboard;Korean characters;English letters;touch based devices;pseudocontact interface;Writing;Vibrations;Trajectory;Three-dimensional displays;Character recognition;Handwriting recognition;Image color analysis},
	Month = {Sep.},
	Pages = {124-127},
	Title = {[POSTER] Haptic Ring Interface Enabling Air-Writing in Virtual Reality Environment},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.37}}

@inproceedings{7328078,
	Abstract = {This paper proposes a remote welding robot manipulation system by using multi-view images. After an operator specifies two-dimensional path on images, the system transforms it into three-dimensional path and displays the movement of the robot by overlaying graphics with images. The accuracy of our system is sufficient to weld objects when combining with a sensor in the robot. The system allows the non-expert operator to weld objects remotely and intuitively, without the need to create a 3D model of a processed object beforehand.},
	Author = {Y. {Hiroi} and K. {Obata} and K. {Suzuki} and N. {Ienaga} and M. {Sugimoto} and H. {Saito} and T. {Takamaru}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.38},
	Keywords = {augmented reality;robotic welding;sensors;solid modelling;telerobotics;multiview images;remote welding robot manipulation system;two-dimensional path;three-dimensional path;robot movement;sensor;nonexpert operator;3D model;Cameras;Welding;Robot vision systems;Three-dimensional displays;Robot kinematics;Remote robot control;multi-view images;industrial robot;augmented reality},
	Month = {Sep.},
	Pages = {128-131},
	Title = {[POSTER] Remote Welding Robot Manipulation Using Multi-view Images},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.38}}

@inproceedings{7328079,
	Abstract = {We propose an outdoor localization system using a particle filter. In our approach, a textured, geo-registered model of the outdoor environment is used as a reference to estimate the pose of a smartphone. The device position and the orientation obtained from a Global Positioning System (GPS) receiver and an inertial measurement unit (IMU) are used as a first estimation of the true pose. Then, multiple pose hypotheses are randomly distributed about the GPS/IMU measurement and use to produce renderings of the virtual model. With vision-based methods, the rendered images are compared with the image received from the smartphone, and the matching scores are used to update the particle filter. The outcome of our system improves the camera pose estimate in real time without user assistance.},
	Author = {C. {Poglitsch} and C. {Arth} and D. {Schmalstieg} and J. {Ventura}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.39},
	Keywords = {Global Positioning System;image matching;particle filtering (numerical methods);pose estimation;rendering (computer graphics);smart phones;particle filter approach;outdoor localization;image-based rendering;outdoor localization system;geo-registered model;smartphone;global positioning system receiver;GPS receiver;inertial measurement unit;IMU;vision-based method;camera;Sensors;Rendering (computer graphics);Cameras;Three-dimensional displays;Augmented reality;Global Positioning System;Computational modeling},
	Month = {Sep.},
	Pages = {132-135},
	Title = {[POSTER] A Particle Filter Approach to Outdoor Localization Using Image-Based Rendering},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.39}}

@inproceedings{7328080,
	Abstract = {In this work, we propose a multi-user system for tracking and mapping, which accommodates mobile clients with different capabilities, mediated by a server capable of providing real-time structure from motion. Clients share their observations of the scene according to their individual capabilities. This can involve only keyframe tracking, but also mapping and map densification, if more computational resources are available. Our contribution is a system architecture that lets heterogeneous clients contribute to a collaborative mapping effort, without prescribing fixed capabilities for the client devices. We investigate the implications that the clients' capabilities have on the collaborative reconstruction effort and its use for AR applications.},
	Author = {P. {Fleck} and C. {Arth} and C. {Pirchheim} and D. {Schmalstieg}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.40},
	Keywords = {augmented reality;groupware;image reconstruction;mobile computing;object tracking;multiuser system;collaborative mapping;collaborative reconstruction effort;AR applications;augmented reality;tracking;mobile client swarms;Servers;Simultaneous localization and mapping;Three-dimensional displays;Collaboration;Cameras;Mobile handsets;Image reconstruction},
	Month = {Sep.},
	Pages = {136-139},
	Title = {[POSTER] Tracking and Mapping with a Swarm of Heterogeneous Clients},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.40}}

@inproceedings{7328081,
	Abstract = {AR systems have been developed for many years now, ranging from systems consisting of a single sensor and output device to systems with a multitude of sensors and/or output devices. With the increasing complexity of the setup, the complexity of handling the different sensors as well as the necessary calibrations and registrations increases accordingly. A much needed (yet missing) area of augmented reality applications is to support AR system engineers when they set up and maintain an AR system by providing visual guides and giving immediate feedback on the current quality of their calibration measurements. In this poster we present an approach to use Augmented Reality itself to support the user in calibrating an Augmented Reality system.},
	Author = {F. {Pankratz} and G. {Klinker}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.41},
	Keywords = {augmented reality;calibration;augmented reality systems setup;AR systems;visual guides;calibration measurements;Cameras;Calibration;Target tracking;Augmented reality;Three-dimensional displays;Current measurement;Visualization},
	Month = {Sep.},
	Pages = {140-143},
	Title = {[POSTER] AR4AR: Using Augmented Reality for guidance in Augmented Reality Systems Setup},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.41}}

@inproceedings{7328082,
	Abstract = {In this work, we encourage the idea of using Photogrammetric targets for object tracking in Industrial Augmented Reality (IAR). Photogrammetric targets, especially uncoded circular targets, are widely used in the industry to perform 3D surface measurements. Therefore, an AR solution based on the uncoded circular targets can improve the work flow integration by reusing existing targets and saving time. These circular targets do not have coded patterns to establish unique 2D-3D correspondences between the targets on the model and their image projections. We solve this particular problem of 2D-3D correspondence of non-coplanar circular targets from a single image. We introduce a Conic pair descriptor, which computes the Eucledian invariants from circular targets in the model space and in the image space. A three stage method is used to compare the descriptors and compute the correspondences with up to 100% precision and 89% recall rates. We are able to achieve tracking performance of 3 FPS (2560x1920 pix) to 8 FPS (640Ã—480 pix) depending on the camera resolution and the targets present in the scene.},
	Author = {H. {Naik} and Y. {Oyamada} and P. {Keitler} and N. {Navab}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.42},
	Keywords = {augmented reality;cameras;image resolution;object tracking;photogrammetry;production engineering computing;target tracking;photogrammetric targets;industrial AR;object tracking;industrial augmented reality;IAR;3D surface measurements;work flow integration;2D-3D correspondences;image projections;noncoplanar circular targets;conic pair descriptor;precision rates;recall rates;camera resolution;Three-dimensional displays;Cameras;Solid modeling;Computational modeling;Target tracking;Robustness;Augmented reality;Industrial AR;Correspondence Matching;Eucledian Invariants},
	Month = {Sep.},
	Pages = {144-147},
	Title = {[POSTER] Exploiting Photogrammetric Targets for Industrial AR},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.42}}

@inproceedings{7328083,
	Abstract = {Dynamic spatial augmented reality requires accurate real-time 3D pose information of the physical objects that are to be projected onto. Previous depth-based methods for tracking objects required strong features to enable recognition; making it difficult to estimate an accurate 6DOF pose for physical objects with a small set of recognizable features (such as a non-textured cube). We propose a more accurate method with fewer limitations for the pose estimation of a tangible object that has known planar faces and using depth data from an RGB-D camera only. In this paper, the physical object's shape is limited to cubes of different sizes. We apply this new tracking method to achieve dynamic projections onto these cubes. In our method, 3D points from an RGB-D camera are divided into a cluster of planar regions, and the point cloud inside each face of the object is fitted to an already-known geometric model of a cube. With the 6DOF pose of the physical object, SAR generated imagery is then projected correctly onto the physical object. The 6DOF tracking is designed to support tangible interactions with the physical object. We implemented example interactive applications with one or multiple cubes to show the capability of our method.},
	Author = {M. {Sano} and K. {Matsumoto} and B. H. {Thomas} and H. {Saito}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.43},
	Keywords = {augmented reality;cameras;feature extraction;object tracking;pose estimation;Rubix;plane region extraction;dynamic spatial augmented reality;tangible object pose estimation;RGB-D camera;tracking method;point cloud;SAR generated imagery;Three-dimensional displays;Cameras;Iterative closest point algorithm;Augmented reality;Real-time systems;Target tracking;Spatial Augmented Reality;Six Degree of Freedom Tracking;RGB-D Camera},
	Month = {Sep.},
	Pages = {148-151},
	Title = {[POSTER] Rubix: Dynamic Spatial Augmented Reality by Extraction of Plane Regions with a RGB-D Camera},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.43}}

@inproceedings{7328084,
	Abstract = {In this paper we propose an adaptive Augmented Reality interface for hand gestures based on a probabilistic model. The proposed method provides an in-situ interface and the corresponding functionalities by recognizing a context of hand shape and gesture which requires the accurate recognition of static and dynamic hand states. We present an appearance-based hand feature representation that yields robustness against hand shape variations, and a feature extraction method based on the fingertip likelihood from a GMM model. Experimental results show that both context-sensitivity and accurate hand gesture recognition are achieved throughout the quantitative evaluation and its implementation as a three-in-one virtual interface.},
	Author = {J. {Jung} and H. {Lee} and H. S. {Yang}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.44},
	Keywords = {augmented reality;feature extraction;Gaussian processes;gesture recognition;image representation;mixture models;shape recognition;adaptive augmented reality interface;probabilistic model;hand shape recognition;appearance-based hand feature representation;feature extraction method;fingertip likelihood;GMM model;context-sensitivity;hand gesture recognition;three-in-one virtual interface;Gaussian mixture model;Shape;Estimation;Gesture recognition;Accuracy;Robustness;Augmented reality;Adaptation models},
	Month = {Sep.},
	Pages = {152-155},
	Title = {[POSTER] An Adaptive Augmented Reality Interface for Hand Based on Probabilistic Approach},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.44}}

@inproceedings{7328085,
	Abstract = {A novel framework for image/video content completion comprising three stages is proposed. First, input images/videos are converted to a lower dimensional feature space, which is done to achieve effective restoration even in cases where a damaged region includes complex structures and changes in color. Second, a damaged region is restored in the converted feature space. Finally, an inverse conversion from the lower dimensional feature space to the original feature space is performed to generate the completed image in the original feature space. This three-step solution generates two advantages. First, it enhances the possibility of applying patches dissimilar to those in the original color space. Second, it enables the use of many existing restoration methods, each having various advantages, because the feature space for retrieving the similar patches is the only extension. Experiments verify the effectiveness of the proposed framework.},
	Author = {M. {Isogawa} and D. {Mikami} and K. {Takahashi} and A. {Kojima}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.45},
	Keywords = {image colour analysis;image restoration;video signal processing;lower dimensional feature space;feature reduction;feature compensation;video content completion;image content completion;inverse conversion;color space;image restoration methods;Image restoration;Image color analysis;Gray-scale;Yttrium;Streaming media;Integrated circuits;Media},
	Month = {Sep.},
	Pages = {156-159},
	Title = {[POSTER] Content Completion in Lower Dimensional Feature Space through Feature Reduction and Compensation},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.45}}

@inproceedings{7328086,
	Abstract = {The successful application of augmented reality as a guidance tool for procedural tasks like maintenance or repair requires an easily usable way of modeling support processes. Even though some suggestions have already been made to address this problem, they still have shortcomings and don't provide all the required features. Thus in a first step the requirements a possible solution has to meet are collected and presented. Based on these, the augmented reality process modeling language (ARPML) is developed, which consists of the four building blocks (i) templates, (ii) sensors, (iii) work steps and (iv) tasks. In contrast to existing approaches it facilitates the creation of multiple views on a single process. This makes it possible to specifically select instructions and information needed in targeted work contexts. It also allows to combine multiple variants of one process into one model with only a minimum of redundancy. The application of ARPML is shown with a practical example.},
	Author = {T. {M{\"u}ller} and T. {Rieger}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.46},
	Keywords = {augmented reality;redundancy;Unified Modeling Language;ARPML;augmented reality process modeling language;redundancy;Sensors;Solid modeling;Augmented reality;Maintenance engineering;Adaptation models;Plugs;Sockets;Augmented reality;Authoring},
	Month = {Sep.},
	Pages = {160-163},
	Title = {[POSTER] ARPML: The Augmented Reality Process Modeling Language},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.46}}

@inproceedings{7328087,
	Abstract = {Augmented Reality Authoring Tools are important instruments that can help a widespread use of AR. They can be classified as programming or content design tools in which the latter completely removes the necessity of programming skills to develop an AR solution. Several solutions have been developed in the past years, however there are few works aiming to identify patterns and general models for such tools. This work aims to perform a trend analysis on content design tools in order to identify their functionalities regarding AR, authoring paradigms, deployment strategies and general dataflow models. This work is aimed to assist developers willing to create authoring tools, therefore, it focus on the last three aspects. Thus, 19 tools were analyzed and through this evaluation it were identified two authoring paradigms and two deployment strategies. Moreover, from their combination it was possible to elaborate four generic dataflow models in which every tool could be fit into.},
	Author = {R. C. {Mota} and R. A. {Roberto} and V. {Teichrieb}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.47},
	Keywords = {augmented reality;authoring systems;data flow computing;augmented reality authoring tools;content design tools;programming tools;AR;general dataflow models;Design tools;Software;Augmented reality;Market research;Analytical models;Libraries;Augmented reality;authoring tools;content design tools},
	Month = {Sep.},
	Pages = {164-167},
	Title = {[POSTER] Authoring Tools in Augmented Reality: An Analysis and Classification of Content Design Tools},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.47}}

@inproceedings{7328088,
	Abstract = {In this paper, we propose a method for overlaying navigation signs on a road surface and displaying them on a head-up display (HUD). Accurate overlaying is realized by measuring 3D data of the surface in real time using a depth camera. In addition, the effect of head movement is reduced by performing face tracking with a camera that is placed in front of the HUD, and by performing distortion correction of projection images according to the driver's viewpoint position. Using an experimental system, we conducted an experiment to display a navigation sign and confirmed that the sign is overlaid on a surface. We also confirmed that the sign looks to be fixed on the surface in real space.},
	Author = {K. {Ueno} and T. {Komuro}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.48},
	Keywords = {cameras;driver information systems;image processing;overlaying navigation sign;road surface;head-up display;HUD;depth camera;projection image;Roads;Cameras;Navigation;Three-dimensional displays;Calibration;Vehicles;Face},
	Month = {Sep.},
	Pages = {168-169},
	Title = {[POSTER] Overlaying Navigation Signs on a Road Surface Using a Head-Up Display},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.48}}

@inproceedings{7328089,
	Abstract = {This study proposes a method to estimate elastic deformation using silhouettes obtained from multiple endoscopic images. Our method can estimate the intraoperative deformation of organs using a volumetric mesh model reconstructed from preoperative CT data. We use this elastic body silhouette information of elastic bodies not to model the shape but to estimate the local displacements. The model shape is updated to satisfy the silhouette constraint while preserving the shape as much as possible. The result of the experiments showed that the proposed methods could estimate the deformation with root mean square (RMS) errors of 5.0--10 mm.},
	Author = {A. {Saito} and M. {Nakao} and Y. {Uranishi} and T. {Matsuda}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.49},
	Keywords = {computerised tomography;endoscopes;image reconstruction;mean square error methods;medical image processing;elastic bodies deformation estimation;multiple silhouette images;endoscopic image augmentation;volumetric mesh model;preoperative CT data reconstruction;root mean square error;RMS errors;Shape;Estimation;Deformable models;Surgery;Computed tomography;Augmented reality;Computational modeling;Deformation estimation;Shape matching;Computer-assisted surgery},
	Month = {Sep.},
	Pages = {170-171},
	Title = {[POSTER] Deformation Estimation of Elastic Bodies Using Multiple Silhouette Images for Endoscopic Image Augmentation},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.49}}

@inproceedings{7328090,
	Abstract = {We present a hands-free AR work support system that provides work instructions to workers without interrupting normal work procedures. This system estimates the work progress by monitoring the status of work objects only on the basis of 3D data captured from a depth sensor mounted on a helmet, and it selects appropriate information to be displayed on a head-mounted display (HMD) on the basis of the estimated work progress. We describe a prototype of the proposed system and the results of primary experiments carried out to evaluate the accuracy and performance of the system.},
	Author = {H. {Sagawa} and H. {Nagayoshi} and H. {Kiyomizu} and T. {Kurihara}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.50},
	Keywords = {augmented reality;hands-free AR work support system;point-cloud data processing;work instructions;3D data;depth sensor;helmet;head-mounted display;HMD;augmented reality;Three-dimensional displays;Solid modeling;Augmented reality;Accuracy;Monitoring;Prototypes;Data models;augmented reality;work support;hands-free},
	Month = {Sep.},
	Pages = {172-173},
	Title = {[POSTER] Hands-Free AR Work Support System Monitoring Work Progress with Point-cloud Data Processing},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.50}}

@inproceedings{7328091,
	Abstract = {Projection-based Augmented Reality commonly projects on rigid objects, while only few systems project on deformable objects. In this paper, we present Pseudo Printed Fabrics (PPF), which enables the projection on a deforming piece of cloth. This can be applied to previewing a cloth design while manipulating its shape. We support challenging manipulations, including heavy occlusions and stretching the cloth. In previous work, we developed a similar system, based on a novel marker pattern; PPF extends it in two important aspects. First, we improved performance by two orders of magnitudes to achieve interactive performance. Second, we developed a new interpolation algorithm to keep registration during challenging manipulations. We believe that PPF can be applied to domains including virtual-try on and fashion design.},
	Author = {Y. {Fujimoto} and G. {Yamamoto} and T. {Taketomi} and C. {Sandor} and H. {Kato}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.51},
	Keywords = {augmented reality;fabrics;humanities;interpolation;pseudoprinted fabrics;projection mapping;projection-based augmented reality;cloth design;PPF system;interpolation algorithm;virtual-try on;fashion design;Cameras;Augmented reality;Fabrics;Shape;Real-time systems;Interpolation;Algorithm design and analysis},
	Month = {Sep.},
	Pages = {174-175},
	Title = {[POSTER] Pseudo Printed Fabrics through Projection Mapping},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.51}}

@inproceedings{7328092,
	Abstract = {This paper introduces a concept of endoscopic image augmentation that overlays shape changes to support cutting procedures. This framework handles the history of measured drill tip's location as a volume label, and visualizes the remains to be cut overlaid on the endoscopic image in real time. We performed a cutting experiment, and the efficacy of the cutting aid was evaluated among shape similarity, total moved distance of a cutting tool, and the required cutting time. The results of the experiments showed that cutting performance was significantly improved by the proposed framework.},
	Author = {M. {Nakao} and S. {Endo} and K. {Imanishi} and T. {Matsuda}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.52},
	Keywords = {endoscopes;medical image processing;shape recognition;endoscopic image augmentation;shape changes;cutting procedures;measured drill tip location;cutting aid;shape similarity;Shape;Surgery;Visualization;Cameras;Augmented reality;Three-dimensional displays;Computed tomography;Endoscopic image augmentation;volumetric cutting model;computer assisted surgery},
	Month = {Sep.},
	Pages = {176-177},
	Title = {[POSTER] Endoscopic Image Augmentation Reflecting Shape Changes in Cutting Procedures},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.52}}

@inproceedings{7328093,
	Abstract = {A method for blindly predicting inpainted image quality is proposed for enhancing the robustness of diminished reality (DR), which uses inpainting to remove unwanted objects by replacing them with background textures in real time. The method maps from inpainted image features to subjective image quality scores without the need for reference images. It enables more complex background textures to be applied to DR.},
	Author = {M. {Isogawa} and D. {Mikami} and K. {Takahashi} and A. {Kojima}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.53},
	Keywords = {image restoration;image texture;DR system;ranking model;background inpainting;inpainted image quality;diminished reality;background textures;inpainted image features;subjective image quality scores;image inpainting technique;Image quality;Image color analysis;Image edge detection;Training;Estimation;Electronic mail;Real-time systems},
	Month = {Sep.},
	Pages = {178-179},
	Title = {[POSTER] Toward Enhancing Robustness of DR System: Ranking Model for Background Inpainting},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.53}}

@inproceedings{7328094,
	Abstract = {Assembly or repair tasks often require objects to be held in specific orientations to view or fit together. Research has addressed the use of AR to assist in these tasks, delivered as registered overlaid graphics on stereoscopic head-worn displays. In contrast, we are interested in using monoscopic head-worn displays, such as Google Glass. To accommodate their small monoscopic field of view, off center from the user's line of sight, we are exploring alternatives to registered overlays. We describe four interactive rotation guidance visualizations for tracked objects intended for these displays.},
	Author = {C. {Elvezio} and M. {Sukan} and S. {Feiner} and B. {Tversky}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.54},
	Keywords = {augmented reality;data visualisation;helmet mounted displays;interactive visualizations;monoscopic eyewear;manual object orientation;3D;repair tasks;assembly tasks;AR;monoscopic head-worn displays;Google Glass;registered overlays;interactive rotation guidance visualizations;tracked objects;Visualization;Three-dimensional displays;Animation;Glass;Google;Color;Augmented reality},
	Month = {Sep.},
	Pages = {180-181},
	Title = {[POSTER] Interactive Visualizations for Monoscopic Eyewear to Assist in Manually Orienting Objects in 3D},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.54}}

@inproceedings{7328095,
	Abstract = {We present a movable spatial augmented reality (SAR) system that can be easily installed in a user workspace. The proposed system aims to dynamically cover a wider projection area using a portable projector attached to a simple robotic device. It has a clear advantage than a conventional SAR scenario where, for example, a projector should be installe1d with a fixed projection area in the workspace. In the previous research [1], we proposed a data-driven kinematic control method for a movable SAR system. This method targets a SAR system integrated with a user-created robotic (UCR) device where an explicit kinematic configuration such as CAD model is unavailable. Our contribution in this paper is to show the feasibility of the data-driven control method by developing a practical application where dynamic change of projection area matters. We outline the control method and demonstrate an assembly guide example using a casually installed movable SAR system.},
	Author = {A. {Lee} and J. {Lee} and J. {Kim}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.55},
	Keywords = {augmented reality;optical projectors;robots;movable spatial AR;augmented reality;SAR system;data-driven control method;spatial augmented reality;user created robot;UCR;projector-camera unit;PCU;Kinematics;Assembly;Robots;Joints;Splines (mathematics);Augmented reality;Design automation;Spatial augmented reality;kinematics;data-driven control;assembly guide},
	Month = {Sep.},
	Pages = {182-183},
	Title = {[POSTER] Movable Spatial AR On-The-Go},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.55}}

@inproceedings{7328096,
	Abstract = {In Augmented Reality (AR) based remote collaboration, a remote user can draw a 2D annotation that emphasizes an object of interest to guide a local user accomplishing a task. This annotation is typically performed only once and then sticks to the selected object in the local user's view, independent of his or her camera movement. In this paper, we present an algorithm to segment the selected object, including its occluded surfaces, such that the 2D selection can be appropriately interpreted in 3D and rendered as a useful AR annotation even when the local user moves and significantly changes the viewpoint.},
	Author = {K. {Lien} and B. {Nuernberger} and M. {Turk} and T. {H{\"o}llerer}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.56},
	Keywords = {augmented reality;groupware;image segmentation;image sensors;2D-3D cosegmentation;AR-based remote collaboration;augmented reality;2D annotation;local user view;camera movement;occluded surfaces;2D selection;AR annotation;Three-dimensional displays;Image segmentation;Object segmentation;Collaboration;Augmented reality;Cameras;Solid modeling},
	Month = {Sep.},
	Pages = {184-185},
	Title = {[POSTER] 2D-3D Co-segmentation for AR-based Remote Collaboration},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.56}}

@inproceedings{7328097,
	Abstract = {Securing one's personal space is quite important in leading a comfortable social life. However, it is difficult to maintain an appropriate interpersonal distance all the time. Therefore, we propose an interpersonal distance control system with a video see-through system, consisting of a head-mounted display (HMD), depth sensor, and RGB camera. The proposed system controls the interpersonal distance by changing the size of the person in the HMD view. In this paper, we describe the proposed system and conduct an experiment to confirm the capability of the proposed system. Finally, we show and discuss the results of the experiment.},
	Author = {M. {Maeda} and N. {Sakata}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.57},
	Keywords = {augmented reality;cameras;helmet mounted displays;interpersonal distance;virtual body size;head-mounted display;HMD;depth sensor;RGB camera;red-green-blue camera;Control systems;Cameras;Augmented reality;Electronic mail;Aerospace electronics;Indexes;Augmented reality;head-mounted display;interface},
	Month = {Sep.},
	Pages = {186-187},
	Title = {[POSTER] Maintaining Appropriate Interpersonal Distance Using Virtual Body Size},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.57}}

@inproceedings{7328098,
	Abstract = {The ideal AR x-ray vision should enable users to clearly observe and grasp not only occludees, but also occluders. We propose a novel selective visualization method of both occludee and oc-cluder layers with dynamic opacity depending on the user's gaze depth. Using the gaze depth as a trigger to select the layers has a essential advantage over using other gestures or spoken commands in the sense of avoiding collision between user's intentional commands and unintentional actions. Our experiment by a visual paired-comparison task shows that our method has achieved a 20% higher success rate, and significantly reduced 30% of the average task completion time than a non-selective method using a constant and half transparency.},
	Author = {Y. {Kitajima} and S. {Ikeda} and K. {Sato}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.58},
	Keywords = {augmented reality;opacity;X-ray imaging;vergence-based AR x-ray vision;selective visualization method;opacity;users gaze depth;AR ghosted views;augmented reality;Visualization;Three-dimensional displays;Augmented reality;X-ray imaging;Yttrium;Indexes;Brain-computer interfaces;x-ray vision;ghosted views;visibility;vergence;gaze-contingent display;augmented reality},
	Month = {Sep.},
	Pages = {188-189},
	Title = {[POSTER] Vergence-Based AR X-ray Vision},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.58}}

@inproceedings{7328099,
	Abstract = {Many researchers are trying to realize a pseudo-haptic system which can visually manipulate a user's haptic shape perception when touching a physical object. In this paper, we focus on manipulating the perceived surface shape of a curved object when touching it with an index finger, by visually deforming it's surface shape and displacing the visual representation of the user's index finger as like s/he is touching the deformed surface, using spatial augmented reality. Experiments were conducted with a projection system to confirm the effect of the visual feedback for manipulating the perceived shape of curved surface. The results supported the proposed concept.},
	Author = {T. {Kanamori} and D. {Iwai} and K. {Sato}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.59},
	Keywords = {augmented reality;haptic interfaces;human computer interaction;haptic shape perception;visual surface deformation;finger displacement;spatial augmented reality;visual feedback;Shape;Haptic interfaces;Visualization;Augmented reality;Indexes;Electronic mail;Cameras;Spatial augmented reality;pseudo-haptics;shape perception},
	Month = {Sep.},
	Pages = {190-191},
	Title = {[POSTER] Manipulating Haptic Shape Perception by Visual Surface Deformation and Finger Displacement in Spatial Augmented Reality},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.59}}

@inproceedings{7328100,
	Abstract = {This paper proposes a mixed-reality shopping system for users who do not own a PC but do own a tablet. In this system, while viewing panoramic images photographed along the aisles of a real store, the user can move freely around the store. Products can be selected and freely viewed from any angle. Furthermore, by utilizing a Photo-based augmented reality (Photo AR) technology the product can be displayed as if it were in the hands of the user. The results of a user evaluation showed that even though the proposed system uses a tablet with a smaller screen it was preferred over a conventional e-commerce site using a larger monitor.},
	Author = {M. {Ohta} and S. {Nagano} and H. {Niwa} and K. {Yamashita}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.60},
	Keywords = {augmented reality;electronic commerce;notebook computers;retail data processing;mixed-reality store;tablet;mixed-reality shopping system;panoramic image;photo AR technology;photo-based augmented reality technology;user evaluation;e-commerce site;Monitoring;Cameras;Augmented reality;Face;Browsers;Switches;Shopping system;EC sites;Panoramic photos;Store interior;Tablet;Photo AR},
	Month = {Sep.},
	Pages = {192-193},
	Title = {[POSTER] Mixed-Reality Store on the Other Side of a Tablet},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.60}}

@inproceedings{7328101,
	Abstract = {Social touch such as a handshake increases the sense of coexistence and closeness between remote users in a social telepresence environment, but creating such coordinated contact movements with a distant person is extremely difficult if given only visual feedback, without haptic feedback. This paper presents a method to enable hand-contact interaction between remote users in an avatar-mediated telepresence environment. The key approach is, while the avatar directly follows its owner's motion in normal conditions, it adjusts the pose to maintain contact with the other user when the two users attempt to make contact interaction. To this end, we develop classifiers to recognize the users' intention for the contact interaction. The contact classifier identifies whether the users try to initiate contact when they are not in contact, and the separation classifier identifies whether the two in contact attempt to break contact. The classifiers are trained based on a set of geometric distance features. During the contact phase, inverse kinematics is solved to determine the pose of the avatar's arm so as to initiate and maintain natural contact with the other user's hand. Our system is unique in that two remote users can perform real time hand contact interaction in a social telepresence environment.},
	Author = {J. {Oh} and Y. {Kim} and T. {Jin} and S. {Lee} and Y. {Lee} and S. {Lee}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.61},
	Keywords = {avatars;real-time systems;user interfaces;avatar-mediated contact interaction;remote users;social touch;social telepresence environment;coordinated contact movements;visual feedback;hand-contact interaction;contact classifier;geometric distance features;separation classifier;inverse kinematics;contact phase;real time interaction;Avatars;Kinematics;Real-time systems;Support vector machines;Joints;Augmented reality;Telepresence;Avatar interaction;Character animation},
	Month = {Sep.},
	Pages = {194-195},
	Title = {[POSTER] Avatar-Mediated Contact Interaction between Remote Users for Social Telepresence},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.61}}

@inproceedings{7328102,
	Abstract = {Usability evaluations are important to the development of augmented reality systems. However, conducting large-scale longitudinal studies remains challenging because of the lack of inexpensive but appropriate methods. In response, we propose a method for implicitly estimating usability ratings based on readily available sensor logs. To demonstrate our idea, we explored the use of features of accelerometer data in estimating usability ratings in an annotation task. Results show that our implicit method corresponds with explicit usability ratings at 79% and 84%. These results should be investigated further in other use cases, with other sensor logs.},
	Author = {M. {Ericson} and T. {Taketomi} and G. {Yamamoto} and G. {Klinker} and C. {Santos} and H. {Kato}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.62},
	Keywords = {augmented reality;user interfaces;usability ratings estimation;handheld augmented reality;accelerometer data;usability evaluation;augmented reality systems;sensor logs;annotation task;Usability;Augmented reality;Accelerometers;Standards;Handheld computers;Yttrium;Feature extraction},
	Month = {Sep.},
	Pages = {196-197},
	Title = {[POSTER] Towards Estimating Usability Ratings of Handheld Augmented Reality Using Accelerometer Data},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.62}}

@inproceedings{7328103,
	Abstract = {This paper introduces a toolkit with camera calibration, monocular visual Simultaneous Localization and Mapping (vSLAM) and registration with a calibration marker. With the toolkit, users can perform the whole procedure of the ISMAR on-site tracking competition in 2015. Since the source code is designed to be well-structured and highly-readable, users can easily install and modify the toolkit. By providing the toolkit, we encourage beginners to learn tracking techniques and to participate in the competition.},
	Author = {H. {Uchiyama} and T. {Taketomi} and S. {Ikeda} and J. P. {Silva Do Monte Lima}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.63},
	Keywords = {calibration;image sensors;object tracking;SLAM (robots);source code (software);abecedary tracking;abecedary mapping;competition tracking;camera calibration;monocular visual simultaneous localization and mapping;vSLAM;calibration marker;ISMAR on-site tracking competition;source code;tracking techniques;Cameras;Calibration;Simultaneous localization and mapping;Three-dimensional displays;Tracking;Electronic mail;Visualization},
	Month = {Sep.},
	Pages = {198-199},
	Title = {[POSTER] Abecedary Tracking and Mapping: A Toolkit for Tracking Competitions},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.63}}

@inproceedings{7328104,
	Abstract = {We are investigating methods for improving the robustness and consistency of the Single Point Active Alignment Method (SPAAM) optical see-through (OST) head-mounted display (HMD) calibration procedure. Our investigation focuses on two variants of SPAAM. The first utilizes a standard monocular alignment strategy to calibrate the left and right eye separately, while the second leverages stereoscopic cues available from binocular HMDs to calibrate both eyes simultaneously. We compare results from repeated calibrations between methods using eye location estimates and inter pupillary distance (IPD) measures. Our findings indicate that the stereo SPAAM method produces more accurate and consistent results during calibration compared to the monocular variant.},
	Author = {K. R. {Moser} and J. E. {Swan}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.64},
	Keywords = {calibration;helmet mounted displays;stereo image processing;SPAAM robustness;stereo calibration;single point active alignment method;optical see-through head-mounted display;OST HMD calibration procedure;monocular alignment strategy;binocular HMD;eye location estimation;inter pupillary distance measure;IPD measure;Calibration;Augmented reality;Accuracy;Robustness;Standards;Cameras;Hardware;Augmented reality;calibration;OST HMD;SPAAM},
	Month = {Sep.},
	Pages = {200-201},
	Title = {[POSTER] Improved SPAAM Robustness through Stereo Calibration},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.64}}

@inproceedings{7328105,
	Abstract = {We present a novel method to retrieve multiple positions of point lights in real indoor scenes based on a 3D reconstruction. This method takes advantage of illumination over planes detected using a segmentation of the reconstructed mesh of the scene. We can also provide an estimation without suffering from the presence of specular highlights but rather use this component to refine the final estimation. This allows consistent relighting throughout the entire scene for aumented reality purposes.},
	Author = {P. {Buteau} and H. {Saito}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.65},
	Keywords = {augmented reality;image reconstruction;lighting;light position retrieval;plane segmentation;diffuse illumination;specular component;indoor scenes;3D reconstruction;scene mesh reconstruction;Light sources;Estimation;Lighting;Cameras;Three-dimensional displays;Indoor environments;Approximation methods},
	Month = {Sep.},
	Pages = {202-203},
	Title = {[POSTER] Retrieving Lights Positions Using Plane Segmentation with Diffuse Illumination Reinforced with Specular Component},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.65}}

@inproceedings{7328106,
	Abstract = {We have been developing a mixed reality system to support road maintenance using overlaid visual aids. Such a system requires a positioning method that can provide sub-meter accuracy and function even if the appearance of the road surface changes significantly caused by many factors such as construction phase, time and weather. Therefore, we are developing a real-time worker positioning method that can be applied to these situation by integrating laser range finder (LRF) and pedestrian dead-reckoning (PDR) data. In the field, multiple workers move around the workspace. Therefore, it is necessary to determine corresponding pairs of PDR-based and LRF-based trajectories by identifying similar trajectories. In this study, we propose a method to calculate the similarity between trajectories and a procedure to integrate corresponding pairs of trajectories to acquire the position and movement direction of a worker.},
	Author = {C. {Chang} and R. {Ichikari} and K. {Makita} and T. {Okuma} and T. {Kurata}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.66},
	Keywords = {augmented reality;civil engineering computing;laser ranging;maintenance engineering;road building;road maintenance MR system;LRF;PDR;mixed reality system;overlaid visual aids;real-time worker positioning method;laser range finder;pedestrian dead-reckoning;Trajectory;Roads;Maintenance engineering;Virtual reality;Cameras;Real-time systems;Visualization;Mixed reality;geographic information systems},
	Month = {Sep.},
	Pages = {204-205},
	Title = {[POSTER] Road Maintenance MR System Using LRF and PDR},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.66}}

@inproceedings{7328107,
	Abstract = {The SAR technique using a projector-camera system allows us to make various effect on a real scene without physical reconstitution. In order to project contents on a textured scene without color imperfections, geometric and radiometric compensation of a projection image should be conducted as preprocessing. In this paper, we present a new geometric mapping method for color compensation in the projector-camera system. We capture the scene and segment it into adaptive patch according to the scene structure using the SLIC segmentation. The piece-wise polynomial function is evaluated for each patch to find pixel-to-pixel correspondences between the measured and projection images. Finally, color compensation is performed by using a color mixing matrix. Experimental results show that our geometric mapping method establishes accurate correspondences and color compensation alleviates the color imperfections which is caused by texture of a general scene.},
	Author = {J. H. {Lee} and Y. H. {Kim} and Y. Y. {Lee} and K. H. {Lee}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.67},
	Keywords = {augmented reality;image colour analysis;image segmentation;image texture;scene adaptive patches;geometric mapping method;projector-camera system;SLIC segmentation;piece-wise polynomial function;pixel-to-pixel correspondences;color compensation;color mixing matrix;general scene texture;SAR technique;spatial augmented reality;Image color analysis;Cameras;Augmented reality;Image segmentation;Polynomials;Calibration;Distortion},
	Month = {Sep.},
	Pages = {206-207},
	Title = {[POSTER] Geometric Mapping for Color Compensation Using Scene Adaptive Patches},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.67}}

@inproceedings{7328108,
	Abstract = {This paper proposes an AR (augmented reality) interface for on-site use in an archaeological project. We have already been developing a web-based 3D archiving system for supporting the diverse specialties and nationalities needed for carrying out the survey and restoration work of the archaeological project. Our 3D archiving system is designed for the spontaneous updating and sharing of information on findings in order to better enable frequent discussions, through a 3D virtual copy of the field site that a user can visit, explore, and embed information into, over the Internet. Here we present an AR user interface to enhance access from mobile devices at the actual site to the archiving system. Using SFM (structure from motion) and solving the Perspective-n-Point (PnP) problem, a photo taken at the site can be stably matched to the pre-registered photo sets in the archiving system and the archived information is automatically overlaid on the photo, just in the same manner as exploring the virtual version of the site on desktop PCs. Our implementation effectively works on an on-going project in Saqqara, Egypt.},
	Author = {R. {Matsushita} and T. {Higo} and H. {Suita} and Y. {Yasumuro}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.68},
	Keywords = {archaeology;augmented reality;image motion analysis;Internet;user interfaces;augmented reality;Web-based 3D archiving system;archaeological project survey;archaeological project restoration;3D virtual copy;AR user interface;SFM;structure from motion;PnP problem;perspective-n-point problem;Saqqara;Egypt;Three-dimensional displays;Databases;Browsers;Portals;Servers;Feature extraction;Mobile handsets;Egyptology;Digital archive;Database;HTML5;We-bGL;AR},
	Month = {Sep.},
	Pages = {208-209},
	Title = {[POSTER] On-site AR Interface with Web-Based 3D Archiving System for Archaeological Project},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.68}}

@inproceedings{7328109,
	Abstract = {This paper describes a mobile pedestrian navigation system that provides users with clues that help understanding spatial relationship between mobile camera views and a 2D map. The proposed method draws on the map upright billboards that correspond to the basal planes of past and current viewing frustums of the camera. The user can take photographs of arbitrary landmarks on the way to build billboards with photographs corresponding to them on the map. Subjective evaluation by eight participants showed that the proposed method offers improved experiences over navigation using a standard 2D map.},
	Author = {J. {Watanabe} and S. {Kagami} and K. {Hashimoto}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.69},
	Keywords = {cameras;mobile computing;pedestrians;user interfaces;photo billboarding;mobile pedestrian navigation system;mobile camera;photograph;user interface;Cameras;Navigation;Mobile communication;Switches;Three-dimensional displays;Cities and towns},
	Month = {Sep.},
	Pages = {210-211},
	Title = {[POSTER] Photo Billboarding: A Simple Method to Provide Clues that Relate Camera Views and a 2D Map for Mobile Pedestrian Navigation},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.69}}

@inproceedings{7328110,
	Abstract = {A system providing visual feedback of a trainee's motions for effectively enhancing motor learning is presented. It provides feedback in synchronization with a reference motion from multiple view angles automatically with only a few seconds delay. Because the feedback is provided automatically, a trainee can obtain it without performing any operations while the memory of the motion is still clear. By employing features with low computational cost, the system achieves synchronized video feedback with four cameras connected to a consumer tablet PC.},
	Author = {D. {Mikami} and M. {Isogawa} and K. {Takahashi} and A. {Kojima}},
	Booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
	Doi = {10.1109/ISMAR.2015.70},
	Keywords = {augmented reality;computer aided instruction;image motion analysis;automatic visual feedback;motor learning;synchronized video feedback;consumer tablet PC;tablet personal computer;Synchronization;Cameras;Visualization;Training;Feature extraction;Streaming media},
	Month = {Sep.},
	Pages = {212-213},
	Title = {[POSTER] Automatic Visual Feedback from Multiple Views for Motor Learning},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2015.70}}
