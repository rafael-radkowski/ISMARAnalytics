@INPROCEEDINGS{1383027,
author={},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={[Cover page]},
year={2004},
volume={},
number={},
pages={c1-c1},
abstract={Presents the front cover or splash screen of the proceedings.},
keywords={},
doi={10.1109/ISMAR.2004.25},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383028,
author={},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={[Title page]},
year={2004},
volume={},
number={},
pages={i-iv},
abstract={Conference proceedings title page.},
keywords={},
doi={10.1109/ISMAR.2004.61},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383029,
author={},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Table of contents},
year={2004},
volume={},
number={},
pages={v-viii},
abstract={Presents the table of contents of the proceedings.},
keywords={},
doi={10.1109/ISMAR.2004.57},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383030,
author={},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Message from the General Chairs},
year={2004},
volume={},
number={},
pages={ix-ix},
abstract={Presents the welcome message from the conference proceedings.},
keywords={},
doi={10.1109/ISMAR.2004.40},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383031,
author={},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Message from the Program Chairs},
year={2004},
volume={},
number={},
pages={x-x},
abstract={Presents the welcome message from the conference proceedings.},
keywords={},
doi={10.1109/ISMAR.2004.41},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383032,
author={},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Organizing Committee},
year={2004},
volume={},
number={},
pages={xi-xi},
abstract={Provides a listing of current committee members.},
keywords={},
doi={10.1109/ISMAR.2004.43},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383033,
author={},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Program Committee},
year={2004},
volume={},
number={},
pages={xii-xiii},
abstract={Provides a listing of current committee members.},
keywords={},
doi={10.1109/ISMAR.2004.46},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383034,
author={},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Additional reviewers},
year={2004},
volume={},
number={},
pages={xiv-xv},
abstract={The conference offers a note of thanks and lists its reviewers.},
keywords={},
doi={10.1109/ISMAR.2004.9},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383036,
author={A. {Pentland}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={The transcendent Greek [keynote speech abstract]},
year={2004},
volume={},
number={},
pages={1-1},
abstract={Summary form only given, as follows. Ever wish you were better at getting a date? Or just remembering names? Have trouble getting a fair shake at your annual job review? Are you the last one to hear about the corporate reorg? Computers are now becoming socially aware, and that means we can begin to augment our social reality. I will describe a series of machine perception tools that sense social signals and map social networks, and then use AR interfaces that may someday help you get a date, get a job, and get a raise.},
keywords={},
doi={10.1109/ISMAR.2004.60},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383037,
author={D. {Schmorrow} and A. {Kruse}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Augmenting This ... Augmented That: Maximizing Human Performance},
year={2004},
volume={},
number={},
pages={1-1},
abstract={},
keywords={Humans},
doi={10.1109/ISMAR.2004.19},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383038,
author={E. M. {Coelho} and S. J. {Julier} and B. {Maclntyre}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={OSGAR: a scene graph with uncertain transformations},
year={2004},
volume={},
number={},
pages={6-15},
abstract={An important problem for augmented reality is registration error. No system can be perfectly tracked, calibrated or modeled. As a result, the overlaid graphics are not aligned perfectly with objects in the physical world. This can be distracting, annoying or confusing. In this paper, we propose a method for mitigating the effects of registration errors that enables application developers to build dynamically adaptive AR displays. Our solution is implemented in a programming toolkit called OSGAR. Built upon OpenSceneGraph (OSG), OSGAR statistically characterizes registration errors, monitors those errors and, when a set of criteria are met, dynamically adapts the display to mitigate the effects of the errors. Because the architecture is based on a scene graph, it provides a simple, familiar and intuitive environment for application developers. We describe the components of OSGAR, discuss how several proposed methods for error registration can be implemented, and illustrate its use through a set of examples.},
keywords={augmented reality;computer graphics;software tools;OSGAR;scene graph;uncertain transformations;augmented reality;overlaid graphics;adaptive AR display;programming toolkit;OpenSceneGraph;error registration;Layout;Augmented reality;Graphics;Displays;Switches;Computer errors;Laboratories;Calibration;Educational institutions;Virtual reality},
doi={10.1109/ISMAR.2004.44},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383039,
author={O. {Cakmakci} and {Yonggang Ha} and J. P. {Rolland}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={A compact optical see-through head-worn display with occlusion support},
year={2004},
volume={},
number={},
pages={16-25},
abstract={We are proposing an optical see-through head-worn display that is capable of mutual occlusions. Mutual occlusion is an attribute of an augmented reality display where real objects can occlude virtual objects and virtual objects can occlude real objects. For a user to achieve the perception of indifference between the real and the virtual images superimposed on the real environment, mutual occlusion is a strongly desired attribute for certain applications. This paper presents a breakthrough in display hardware from a mobility (i.e. compactness), resolution, and a switching speed based criteria. Specifically, we focus on the research that is related to virtual objects being able to occlude real objects. The core of the system is a spatial light modulator (SLM) and polarization-based optics which allow us to block or pass certain parts of a scene which is viewed through the head-worn display. An objective lens images the scene onto the SLM and the modulated image is mapped back to the original scene via an eyepiece. We are combining computer generated imagery with the modulated version of the scene to form the final image a user would see.},
keywords={helmet mounted displays;augmented reality;computer graphics;spatial light modulators;optics;image processing;compact optical see-through head-worn display;occlusion support;mutual occlusions;augmented reality display;virtual objects;real objects;virtual images;display hardware;spatial light modulator;polarization-based optics;computer generated imagery;optical system design;head mounted display;Optical sensors;Layout;Computer displays;Augmented reality;Optical modulation;Humans;Hardware;Image generation;Educational institutions;Spatial resolution;display hardware;occlusion;augmented reality;optical system design;head mounted display;spatial light modulator},
doi={10.1109/ISMAR.2004.2},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383040,
author={J. {Ehnes} and K. {Hirota} and M. {Hirose}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Projected augmentation - augmented reality using rotatable video projectors},
year={2004},
volume={},
number={},
pages={26-35},
abstract={In this paper, we propose a new way of augmenting our environment with information without making the user carry any devices. We propose the use of video projection to display the augmentation on the objects directly. We use a projector that can be rotated and in other ways controlled remotely by a computer, to follow objects carrying a marker. The main contribution of this paper is a system that keeps the augmentation displayed in the correct place while the object or the projector moves. We describe the hardware and software design of our system, the way certain functions such as following the marker or keeping it in focus are implemented and how to calibrate the multitude of parameters of all the subsystems.},
keywords={augmented reality;image processing equipment;video signal processing;calibration;projected augmentation;augmented reality;rotatable video projectors;video projection;calibration;hardware-software design;Augmented reality;Computer displays;Application software;Video sharing;Cameras;Books;Hardware;Software design;Cables;Position measurement},
doi={10.1109/ISMAR.2004.47},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383041,
author={G. {Klein} and T. {Drummond}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Sensor fusion and occlusion refinement for tablet-based AR},
year={2004},
volume={},
number={},
pages={38-47},
abstract={This paper presents a set of technologies which enable robust, accurate, high resolution augmentation of live video, delivered via a tablet PC to which a video camera has been attached. By combining several technologies, this is achieved without the use of contrived markers in the environment: An outside-in tracker observes the tablet to generate robust, low-accuracy pose estimates. An inside-out tracker running on the tablet observes the video feed from the tablet-mounted camera and provides high-accuracy pose estimates by tracking natural features in the environment. Information from both of these trackers is combined in an extended Kalman filter. Finally, to maximise the quality of the augmented imagery, boundaries where the real world occludes the virtual imagery are identified and another tracker is used to refine the boundaries between real and virtual imagery so that their synthesis is as convincing as possible.},
keywords={augmented reality;Kalman filters;sensor fusion;tracking;cameras;hidden feature removal;video signal processing;sensor fusion;occlusion refinement;tablet-based AR;live video;tablet PC;video camera;outside-in tracker;inside-out tracker;tablet-mounted camera;extended Kalman filter;augmented imagery;virtual imagery;Sensor fusion;Robustness;Cameras;Delay;Feeds;Augmented reality;Jitter;Displays;Calibration;Personal digital assistants},
doi={10.1109/ISMAR.2004.54},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383042,
author={L. {Vacchetti} and V. {Lepetit} and P. {Fua}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Combining edge and texture information for real-time accurate 3D camera tracking},
year={2004},
volume={},
number={},
pages={48-56},
abstract={We present an effective way to combine the information provided by edges and by feature points for the purpose of robust real-time 3-D tracking. This lets our tracker handle both textured and untextured objects. As it can exploit more of the image information, it is more stable and less prone to drift that purely edge or feature-based ones. We start with a feature-point based tracker we developed in earlier work and integrate the ability to take edge-information into account. Achieving optimal performance in the presence of cluttered or textured backgrounds, however, is far from trivial because of the many spurious edges that bedevil typical edge-detectors. We overcome this difficulty by proposing a method for handling multiple hypotheses for potential edge-locations that is similar in speed to approaches that consider only single hypotheses and therefore much faster than conventional multiple-hypothesis ones. This results in a real-time 3-D tracking algorithm that exploits both texture and edge information without being sensitive to misleading background information and that does not drift over time.},
keywords={edge detection;image texture;tracking;real-time systems;edge information;texture information;3D camera tracking;real-time 3D tracking;image information;feature-point based tracker;cluttered background;textured background;edge-detectors;edge-locations;Cameras;Robustness;Layout;Degradation;Data mining;Feature extraction;Information resources;Augmented reality;Video sequences;Jitter},
doi={10.1109/ISMAR.2004.24},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383043,
author={M. {Aron} and G. {Simon} and M. -. {Berger}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Handling uncertain sensor data in vision-based camera tracking},
year={2004},
volume={},
number={},
pages={58-67},
abstract={A hybrid approach for real-time markerless tracking is presented. Robust and accurate tracking is obtained from the coupling of camera and inertial sensor data. Unlike previous approaches, we use sensor information only when the image-based system fails to track the camera. In addition, sensor errors are measured and taken into account at each step of our algorithm. Finally, we address the camera/sensor synchronization problem and propose a method to resynchronize these two devices online. We demonstrate our method in two example sequences that illustrate the behavior and benefits of the new tracking method.},
keywords={computer vision;optical tracking;real-time systems;cameras;synchronisation;sensor fusion;data handling;uncertain sensor data;vision-based camera tracking;real-time markerless tracking;inertial sensor data;sensor information;image-based system;sensor errors;camera synchronization;sensor synchronization;Cameras;Magnetic sensors;Robustness;Image sensors;Layout;Sensor systems;Augmented reality;Global Positioning System;Computational efficiency;Computer vision},
doi={10.1109/ISMAR.2004.33},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383044,
author={C. B. {Owen} and {Ji Zhou} and A. {Tang} and {Fan Xiao}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Display-relative calibration for optical see-through head-mounted displays},
year={2004},
volume={},
number={},
pages={70-78},
abstract={Optical see-through head-mounted displays (OSTHMDs) have many advantages in augmented reality application, but their utility in practical applications has been limited by the complexity of calibration. Because the human subject is an inseparable part of the eye-display system, previous methods for OSTHMD calibration have required extensive manual data collection using either instrumentation or manual point correspondences and are highly dependent on operator skill. This paper describes display-relative calibration (DRC) for OSTHMDs, a new two phase calibration method that minimizes the human element in the calibration process and ensures reliable calibration. Phase I of the calibration captures the parameters of the display system relative to a normalized reference frame and is performed in a jig with no human factors issues. The second phase optimizes the display for a specific user and the placement of the display on the head. Several phase II alternatives provide flexibility in a variety of applications including applications involving untrained users.},
keywords={helmet mounted displays;calibration;augmented reality;display-relative calibration;optical see-through head-mounted displays;OSTHMD;augmented reality;eye-display system;data collection;phase calibration;display system;Calibration;Computer displays;Augmented reality;Head;Application software;Laboratories;Instruments;Human factors;Computer graphics;Glass},
doi={10.1109/ISMAR.2004.28},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383045,
author={H. {Najafi} and N. {Navab} and G. {Klinker}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Automated initialization for marker-less tracking: a sensor fusion approach},
year={2004},
volume={},
number={},
pages={79-88},
abstract={We introduce a sensor fusion approach for automated initialization of marker-less tracking systems. It is not limited in tracking range and working environment, given a 3D model of the objects or the real scene. This is achieved based on a statistical analysis and probabilistic estimation of the uncertainties of the tracking sensors. The explicit representation of the error distribution allows the fusion of different sensor data. This methodology was applied to an augmented reality system, using a mobile camera and several stationary tracking sensors, and can be easily extended to the case of any additional sensor. In order to solve the initialization problem, we adapt, modify and integrate advanced techniques such as plenoptic viewing, intensity-based registration, and ICP. Thereby, the registration error is minimized in 3D object space rather than in 2D image. Experimental results show how complex objects can be registered efficiently and accurately to a single image.},
keywords={augmented reality;cameras;target tracking;sensor fusion;statistical analysis;probability;image registration;sensor fusion;automated initialization;marker-less tracking system;3D object model;real scene;statistical analysis;probabilistic estimation;error distribution;augmented reality system;mobile camera;stationary tracking sensors;plenoptic viewing;intensity-based registration;ICP;registration error;3D object space;Sensor fusion;Cameras;Augmented reality;Layout;Sensor systems;Space technology;Global Positioning System;Target tracking;Statistical analysis;Uncertainty},
doi={10.1109/ISMAR.2004.21},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383046,
author={D. {Kotake} and S. {Uchiyama} and H. {Yamamoto}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={A marker calibration method utilizing a priori knowledge on marker arrangement},
year={2004},
volume={},
number={},
pages={89-98},
abstract={This paper describes a calibration method of markers which are used for registration in MR applications. There have been many vision-based approaches proposed as registration methods in MR. When multiple markers are utilized in a vision-based method, it is necessary that the geometric information of the marker arrangement such as their positions and orientations be known in advance. In this paper, we propose a hybrid method combining the "bundle adjustment method," which is a photogrammetric technique that calculates the geometric information from a set of images, with some constraints on the marker arrangement which are obtained a priori (e.g. the multiple markers are located on a single plane). After considering marker arrangements seen in many MR systems, we summarize some constraints seen in these arrangements. Then, we explain the basic framework of this method as well as a solution method under some practical constraints. We further describe several experiments and their results in order to show the effectiveness of this method.},
keywords={image registration;augmented reality;calibration;photogrammetry;computer vision;marker calibration;a priori knowledge;marker arrangement;registration methods;vision-based method;geometric information;bundle adjustment;photogrammetric technique;MR systems;mixed reality;Calibration;Virtual reality;Layout;Laboratories;Fuses;Target tracking;Magnetic sensors;Computer errors;Computer vision;Concrete},
doi={10.1109/ISMAR.2004.4},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383047,
author={D. {Cotting} and M. {Naef} and M. {Gross} and H. {Fuchs}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Embedding imperceptible patterns into projected images for simultaneous acquisition and display},
year={2004},
volume={},
number={},
pages={100-109},
abstract={We introduce a method to imperceptibly embed arbitrary binary patterns into ordinary color images displayed by unmodified off-the-shelf digital light processing (DLP) projectors. The encoded images are visible only to cameras synchronized with the projectors and exposed for a short interval, while the original images appear only minimally degraded to the human eye. To achieve this goal, we analyze and exploit the micro-mirror modulation pattern used by the projection technology to generate intensity levels for each pixel and color channel. Our real-time embedding process maps the user's original color image values to the nearest values whose camera-perceived intensities are the ones desired by the binary image to be embedded. The color differences caused by this mapping process are compensated by error-diffusion dithering. The non-intrusive nature of our approach allows simultaneous (immersive) display and acquisition under controlled lighting conditions, as defined on a pixel level by the binary patterns. We therefore introduce structured light techniques into human-inhabited mixed and augmented reality environments, where they previously often were too intrusive.},
keywords={image colour analysis;computer displays;cameras;lighting;augmented reality;image coding;imperceptible patterns;projected images;arbitrary binary patterns;color images;digital light processing;DLP projector;encoded images;micro-mirror modulation pattern;projection technology;color channel;real-time embedding process;binary image;error-diffusion dithering;controlled lighting;structured light techniques;mixed reality;augmented reality;Displays;Cameras;Color;Lighting control;Augmented reality;Optical modulation;Virtual reality;Hardware;Degradation;Humans},
doi={10.1109/ISMAR.2004.30},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383048,
author={I. {Skrypnyk} and D. G. {Lowe}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Scene modelling, recognition and tracking with invariant image features},
year={2004},
volume={},
number={},
pages={110-119},
abstract={We present a complete system architecture for fully automated markerless augmented reality (AR). The system constructs a sparse metric model of the real-world environment, provides interactive means for specifying the pose of a virtual object, and performs model-based camera tracking with visually pleasing augmentation results. Our approach does not require camera pre-calibration, prior knowledge of scene geometry, manual initialization of the tracker or placement of special markers. Robust tracking in the presence of occlusions and scene changes is achieved by using highly distinctive natural features to establish image correspondences.},
keywords={image recognition;augmented reality;cameras;optical tracking;solid modelling;scene modelling;scene recognition;scene tracking;invariant image features;system architecture;automated markerless augmented reality;sparse metric model;real-world environment;virtual object;model-based camera tracking;robust tracking;occlusions;scene changes;natural features;image correspondences;Layout;Image recognition;Cameras;Power system modeling;Augmented reality;Robustness;Calibration;Computer science;Computer architecture;Geometry},
doi={10.1109/ISMAR.2004.53},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383049,
author={L. {Davis} and F. G. {Hamza-Lup} and J. P. {Rolland}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={A method for designing marker-based tracking probes},
year={2004},
volume={},
number={},
pages={120-129},
abstract={Many tracking systems utilize collections of fiducial markers arranged in rigid configurations, called tracking probes, to determine the pose of objects within an environment. In this paper, we present a technique for designing tracking probes called the viewpoints algorithm. The algorithm is generally applicable to tracking systems that use at least three fiduciary marks to determine the pose of an object. The algorithm is used to create an integrated, head-mounted display tracking probe. The predicted accuracy of this probe was 0.032 /spl plusmn/ 0.02 degrees in orientation and 0.09 /spl plusmn/ 0.07 mm in position. The measured accuracy of the probe was 0.028 /spl plusmn/ 0.01 degrees in orientation and 0.11 /spl plusmn/ 0.01 mm in position. These results translate to a predicted, static positional overlay error of a virtual object presented at 1m of less than 0.5 mm. The algorithm is part of a larger framework for designing tracking probes based upon performance goals and environmental constraints.},
keywords={virtual reality;target tracking;object detection;helmet mounted displays;marker-based tracking probes;tracking system;fiducial markers;viewpoint algorithm;integrated head-mounted display;static positional overlay error;virtual object;environmental constraint;Design methodology;Probes;Tracking;Algorithm design and analysis;Augmented reality;Computer science;Educational institutions;Photonics;Displays;Accuracy},
doi={10.1109/ISMAR.2004.5},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383050,
author={H. {Benko} and E. W. {Ishak} and S. {Feiner}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Collaborative mixed reality visualization of an archaeological excavation},
year={2004},
volume={},
number={},
pages={132-140},
abstract={We present VITA (visual interaction tool for archaeology), an experimental collaborative mixed reality system for offsite visualization of an archaeological dig. Our system allows multiple users to visualize the dig site in a mixed reality environment in which tracked, see-through, head-worn displays are combined with a multi-user, multi-touch, projected table surface, a large screen display, and tracked hand-held displays. We focus on augmenting existing archaeological analysis methods with new ways to organize, visualize, and combine the standard 2D information available from an excavation (drawings, pictures, and notes) with textured, laser range-scanned 3D models of objects and the site itself. Users can combine speech, touch, and 3D hand gestures to interact multimodally with the environment. Preliminary user tests were conducted with archaeology researchers and students, and their feedback is presented here.},
keywords={archaeology;helmet mounted displays;data visualisation;augmented reality;large screen displays;collaborative mixed reality visualization;archaeological excavation;VITA;visual interaction tool for archaeology;experimental collaborative mixed reality system;archaeological dig;see-through display;head-worn display;projected table surface;large screen display;tracked hand-held display;archaeological analysis;object texture;laser range-scanned 3D model;3D hand gestures;Collaboration;Virtual reality;Visualization;Large screen displays;Laser feedback;Collaborative tools;Information analysis;Laser modes;Speech;Testing},
doi={10.1109/ISMAR.2004.23},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383051,
author={I. {Barakonyi} and T. {Psik} and D. {Schmalstieg}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Agents that talk and hit back: animated agents in augmented reality},
year={2004},
volume={},
number={},
pages={141-150},
abstract={AR puppet is a hierarchical animation framework for augmented reality agents, which is a research area combining augmented reality (AR), sentient computing and autonomous animated agents into a single coherent human-computer interface paradigm. While sentient computing systems use the physical environment as an input channel, AR outputs virtual information superimposed on real world objects. To enhance man-machine communication with more natural and efficient information presentation, this framework adds animated agents to AR applications that make autonomous decisions based on their perception of the real environment. These agents are able to turn physical objects into interactive, responsive entities collaborating with both anthropomorphic and non-anthropomorphic virtual characters, extending AR with a previously unexplored output modality. AR puppet explores the requirements for context-aware animated agents concerning visualization, appearance, behavior, in addition to associated technologies and application areas. A demo application with a virtual repairman collaborating with an augmented LEGO/spl reg/ robot illustrates our concepts.},
keywords={augmented reality;computer animation;software agents;human computer interaction;robots;interactive systems;augmented reality;AR puppet;animation framework;sentient computing;autonomous animated agents;human-computer interface;physical environment;virtual information;real world object;man-machine communication;anthropomorphic virtual character;nonanthropomorphic virtual character;context-aware animated agents;virtual repairman;augmented LEGO/spl reg/ robot;Animation;Augmented reality;Computer interfaces;Collaboration;Anthropomorphism;Humans;Autonomous agents;Virtual reality;Man machine systems;Application software},
doi={10.1109/ISMAR.2004.11},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383052,
author={Y. {Kameda} and T. {Takemasa} and Y. {Ohta}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Outdoor see-through vision utilizing surveillance cameras},
year={2004},
volume={},
number={},
pages={151-160},
abstract={This paper presents a new outdoor mixed-reality system designed for people who carry a camera-attached small handy device in an outdoor scene where a number of surveillance cameras are embedded. We propose a new functionality in outdoor mixed reality that the handy device can display live status of invisible areas hidden by some structures such as buildings, walls, etc. The function is implemented on a camera-attached, small handy subnotebook PC (HPC). The videos of the invisible areas are taken by surveillance cameras and they are precisely overlapped on the video of HPC camera, hence a user can notice objects in the invisible areas and see directly what the objects do. We utilize surveillance cameras for two purposes. (1) They obtain videos of invisible areas. The videos are trimmed and warped so as to impose them into the video of the HPC camera. (2) They are also used for updating textures of calibration markers in order to handle possible texture changes in real outdoor world. We have implemented a preliminary system with four surveillance cameras and proved that our system can visualize invisible areas in real time.},
keywords={augmented reality;notebook computers;computer displays;video cameras;video signal processing;calibration;surveillance;computer vision;outdoor see-through vision;surveillance cameras;mixed-reality system;outdoor scene;outdoor mixed reality;handy device;subnotebook PC;HPC camera;calibration markers;Surveillance;Cameras;Videos;Virtual reality;Layout;Displays;Buildings;Calibration;Visualization;Real time systems},
doi={10.1109/ISMAR.2004.45},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383053,
author={W. {Piekarski} and B. H. {Thomas}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Augmented reality working planes: a foundation for action and construction at a distance},
year={2004},
volume={},
number={},
pages={162-171},
abstract={This paper introduces the concept of augmented reality working planes to action and construction at a distance for mobile outdoor augmented reality systems. We have based our new AR working planes technique on CAD working planes, but by using coordinate systems relative to the body they can be specified and used much more intuitively than on a desktop system. We demonstrate in this paper how AR working planes can be used for the display of information, the manipulation of existing 3D objects, and the creation of new geometry. AR working planes are particularly well suited to supporting 3D modelling in mobile outdoor AR systems. This modelling task is a particularly difficult problem, because the environment is typically at a scale much larger than the user, and direct manipulation techniques can not be used.},
keywords={augmented reality;CAD;solid modelling;computational geometry;augmented reality working planes;augmented reality system;CAD working planes;coordinate system;desktop system;information display;3D objects;3D modelling;mobile outdoor AR system;Augmented reality;Humans;Wearable computers;Virtual reality;Geometry;Laboratories;Information science;Lakes;Australia;Mobile computing},
doi={10.1109/ISMAR.2004.17},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383054,
author={G. A. {Lee} and C. {Nelles} and M. {Billinghurst} and G. J. {Kim}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Immersive authoring of tangible augmented reality applications},
year={2004},
volume={},
number={},
pages={172-181},
abstract={In this paper, we suggest a new approach for authoring tangible augmented reality applications, called 'immersive authoring.' The approach allows the user to carry out the authoring tasks within the AR application being built, so that the development and testing of the application can be done concurrently throughout the development process. We describe the functionalities and the interaction design for the proposed authoring system that are specifically targeted for intuitive specification of scenes and various object behaviors. Several cases of applications developed using the authoring system are presented. A small pilot user study was conducted to compare the proposed method to a non-immersive approach, and the results have shown that the users generally found it easier and faster to carry out authoring tasks in the immersive environment.},
keywords={augmented reality;authoring systems;immersive authoring;tangible augmented reality;authoring tasks;AR application;interaction design;authoring system;immersive environment;Augmented reality;Authoring systems;Programming profession;Virtual reality;Testing;Layout;Application software;Software libraries;Humans;Educational programs},
doi={10.1109/ISMAR.2004.34},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383055,
author={ {Jia Li} and R. {Laganiere} and G. {Roth}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Online estimation of trifocal tensors for augmenting live video},
year={2004},
volume={},
number={},
pages={182-190},
abstract={We propose a method to augment live video based on the tracking of natural features, and the online estimation of the trinocular geometry. Previous without-marker approaches require the computation of camera pose to render virtual objects. The strength of our proposed method is that it doesn 7 require tracking of camera pose, and exploits the usual advantages of marker-based approaches for a fast implementation. A 3-view AR system is used to demonstrate our approach. It consists of an uncalibrated camera that moves freely inside the scene of interest, and of three reference frames taken at the time of system initialization. As the camera is moving, image features taken from an initial triplet set are tracked throughout the video sequence. And the trifocal tensor associated with each frame is estimated online. With this tensor, the square pattern that was visible in the reference frames is transferred to the video. This invisible pattern is then used by the ARToolkit to embed virtual objects.},
keywords={video signal processing;augmented reality;image sequences;cameras;motion estimation;tracking;online estimation;trifocal tensors;live video augmentation;trinocular geometry;marker-based approach;3-view AR system;uncalibrated camera;video sequence;virtual objects;ARToolkit;Tensile stress;Cameras;Layout;Augmented reality;Information technology;Calibration;Councils;Information geometry;Video sequences;Streaming media},
doi={10.1109/ISMAR.2004.42},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383056,
author={J. {Newman} and M. {Wagner} and M. {Bauer} and A. {MacWilliams} and T. {Pintaric} and D. {Beyer} and D. {Pustka} and F. {Strasser} and D. {Schmalstieg} and G. {Klinker}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Ubiquitous tracking for augmented reality},
year={2004},
volume={},
number={},
pages={192-201},
abstract={Augmented reality (AR) provides a natural interface to the "calm" pervasive technology anticipated in large-scale ubiquitous computing environments. However, the range of classic AR applications has been limited by the scope, range and cost of sensors used for tracking. Hybrid tracking approaches can go some way to extending this range. We propose an approach, called ubiquitous tracking, in which data from widespread and diverse heterogeneous tracking sensors is automatically and dynamically fused, and then transparently provided to applications. A formal model represents spatial relationships between objects as a graph attributed with quality-of-service parameters. This paper presents a software implementation, in which a dynamic data flow network of distributed software components is thereby constructed in response to queries and optimisation criteria specified by applications. This implementation is demonstrated using a small laboratory example, and larger setups modelled in a simulation environment.},
keywords={augmented reality;ubiquitous computing;object-oriented programming;quality of service;sensor fusion;tracking;ubiquitous tracking;augmented reality;pervasive technology;large-scale ubiquitous computing;heterogeneous tracking sensors;quality-of-service;data flow network;distributed software component;Augmented reality;Target tracking;Subspace constraints;Sensor fusion;Application software;Space technology;Cameras;Large-scale systems;Ubiquitous computing;Middleware},
doi={10.1109/ISMAR.2004.62},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383057,
author={K. {Satoh} and S. {Uchiyama} and H. {Yamamoto}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={A head tracking method using bird's-eye view camera and gyroscope},
year={2004},
volume={},
number={},
pages={202-211},
abstract={This paper describes a new head tracking method utilizing a gyroscope mounted on a head-mounted display (HMD) and a bird's-eye view camera that observes the HMD from a fixed third-person viewpoint. Furthermore, we propose an extension of this method to hybrid registration, combining it with user's view cameras. The HMD is equipped with a gyroscope and a marker. The gyroscope measures the orientation of the user s view camera so that the number of pose parameters to be solved can be reduced. The other parameters are to be estimated by the bird's-eye view camera that observes the marker. This method is an improvement over the conventional outside-in-style vision-based tracker method, which only uses visual information. Hence, it can be thought of as an alternative to a physical head tracker such as a magnetic sensor and to an inside-out-style vision-based tracker. In addition to theoretical discussions, this paper demonstrates the effectiveness of our methods by experiments. We also propose methods for calibrating the gyroscope and the marker on HMD, which are essential in implementing the tracking method.},
keywords={helmet mounted displays;image registration;computer vision;cameras;gyroscopes;tracking;head tracking method;bird eye view camera;gyroscope;head-mounted display;fixed third-person viewpoint;hybrid registration;vision-based tracker;magnetic sensor;Cameras;Gyroscopes;Layout;Magnetic sensors;Magnetic heads;Target tracking;Space technology;Augmented reality;Laboratories;Displays},
doi={10.1109/ISMAR.2004.3},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383058,
author={E. {Foxlin} and Y. {Altshuler} and L. {Naimark} and M. {Harrington}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={FlightTracker: a novel optical/inertial tracker for cockpit enhanced vision},
year={2004},
volume={},
number={},
pages={212-221},
abstract={One of the earliest fielded augmented reality applications was enhanced vision for pilots, in which a display projected on the pilot's visor provides geo-spatially registered information to help the pilot navigate, avoid obstacles, maintain situational awareness in reduced visibility, and interact with avionics instruments without looking down. This requires exceptionally robust and accurate head-tracking, for which there is not a sufficient solution yet available. In this paper, we apply miniature MEMS sensors to cockpit helmet-tracking for enhanced/synthetic vision by implementing algorithms for differential inertial tracking between helmet-mounted and aircraft-mounted inertial sensors, and novel optical drift correction techniques. By fusing low-rate inside-out and outside-in optical measurements with high-rate inertial data, we achieve millimeter position accuracy and milliradian angular accuracy, low-latency and high robustness using small and inexpensive sensors.},
keywords={aircraft displays;augmented reality;avionics;sensors;helmet mounted displays;computer vision;aerospace computing;optical tracking;FlightTracker;optical-inertial tracker;cockpit enhanced vision;augmented reality;geo-spatially registered information;avionics instruments;miniature MEMS sensors;cockpit helmet-tracking;synthetic vision;helmet-mounted inertial sensors;aircraft-mounted inertial sensors;optical drift correction;Target tracking;Optical sensors;Radar tracking;High temperature superconductors;Computer displays;Weapons;Aircraft;Robustness;Aerospace electronics;Two dimensional displays},
doi={10.1109/ISMAR.2004.32},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383059,
author={A. {Leykin} and M. {Tuceryan}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Automatic determination of text readability over textured backgrounds for augmented reality systems},
year={2004},
volume={},
number={},
pages={224-230},
abstract={This paper describes a pattern recognition approach to determine readability of text labels in augmented reality systems. In many augmented reality applications, one of the ways in which information is presented to the user is to place a text label over the area of interest. However, if this information is placed over very busy and textured backgrounds, this can affect the readability of the text. The goal of this work was to identify methods of quantitatively describing conditions under which such text would be readable or unreadable. We used texture properties and other visual features to determine if a text placed on a particular background would be readable or not. Based on these features, a supervised classifier was built that was trained using data collected front human subjects' judgment of text readability. Using a rather small training set of about 400 human evaluations over 50 heterogeneous textures the system is able to achieve a correct classification rate of over 85%.},
keywords={augmented reality;text analysis;pattern recognition;automatic text readability determination;textured background;augmented reality system;pattern recognition;supervised classifier;Augmented reality;Layout;Pattern recognition;Humans;Computer science;Information science;Application software;Graphics;Interference;Labeling},
doi={10.1109/ISMAR.2004.22},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383060,
author={R. {Bane} and T. {Hollerer}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Interactive tools for virtual x-ray vision in mobile augmented reality},
year={2004},
volume={},
number={},
pages={231-239},
abstract={This paper presents a set of interactive tools designed to give users virtual x-ray vision. These tools address a common problem in depicting occluded infrastructure: either too much information is displayed, confusing users, or too little information is displayed, depriving users of important depth cues. Four tools are presented: the tunnel tool and room selector tool directly augment the user's view of the environment, allowing them to explore the scene in direct, first person view. The room in miniature tool allows the user to select and interact with a room from a third person perspective, allowing users to view the contents of the room from points of view that would normally be difficult or impossible to achieve. The room slicer tool aids users in exploring volumetric data displayed within the room in miniature tool. Used together, the tools presented in this paper can be used to achieve the virtual x-ray vision effect. We test our prototype system in a far-field mobile augmented reality setup, visualizing the interiors of a small set of buildings on the UCSB campus.},
keywords={augmented reality;computer vision;X-ray imaging;interactive tools;virtual x-ray vision;mobile augmented reality;tunnel tool;room selector tool;room slicer tool;room in miniature tool;Augmented reality;Data visualization;Buildings;Data security;Layout;System testing;Prototypes;Solids;Wireless networks;Floors},
doi={10.1109/ISMAR.2004.36},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383061,
author={ {Zhiyun Li} and R. {Duraiswami} and L. S. {Davis}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Recording and reproducing high order surround auditory scenes for mixed and augmented reality},
year={2004},
volume={},
number={},
pages={240-249},
abstract={Virtual reality systems are largely based on computer graphics and vision technologies. However, sound also plays an important role in human's interaction with the surrounding environment, especially for the visually impaired people. In this paper, we develop the theory of recording and reproducing real-world surround auditory scenes in high orders using specially designed microphone and loudspeaker arrays. It is complementary to vision-based technologies in creating mixed and augmented realities. Design examples and simulations are presented.},
keywords={augmented reality;computer vision;audio signal processing;computer graphics;high order surround auditory scene;mixed reality;augmented reality;virtual reality system;computer graphics;vision technology;microphone array;loudspeaker array;Layout;Augmented reality;Microphone arrays;Loudspeakers;Virtual reality;Computer graphics;Computational modeling;Computer displays;Robustness;Laboratories},
doi={10.1109/ISMAR.2004.51},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383062,
author={M. {Mohring} and C. {Lessig} and O. {Bimber}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Video see-through AR on consumer cell-phones},
year={2004},
volume={},
number={},
pages={252-253},
abstract={We present a first running video see-through augmented reality system on a consumer cell-phone. It supports the detection and differentiation of different markers, and correct integration of rendered 3D graphics into the live video stream via a weak perspective projection camera model and an OpenGL rendering pipeline.},
keywords={cellular radio;augmented reality;rendering (computer graphics);video signal processing;video see-through AR;consumer cell phones;augmented reality system;rendered 3D graphics;live video stream;weak perspective projection camera model;OpenGL rendering pipeline;Image edge detection;Augmented reality;Rendering (computer graphics);Cameras;Optical distortion;Graphics;Personal digital assistants;Distortion measurement;Streaming media;Pipelines},
doi={10.1109/ISMAR.2004.63},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383063,
author={C. {Matysczok} and A. {Wojdala}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Rendering of highly polygonal augmented reality applications on a scalable PC-cluster architecture},
year={2004},
volume={},
number={},
pages={254-255},
abstract={In the automobile industry virtual prototypes are often used within the product development process. Here computer models of cars which are still under development are generated and analyzed to reduce the time and costs for building and testing real prototypes. In this context the technology of augmented reality offers a new perspective. In this paper we describe a system architecture for a PC-cluster consisting of AR nodes for the image processing and VR nodes for the parallel rendering. The AR nodes analyze the live video coming from two cameras and calculate the tracking data for the VR nodes. According to the tracking data the VR nodes render the 3D objects in parallel. The rendered images are sent back to the AR nodes where they are mixed with the life video. The proposed scalable architecture provides a very high image quality and rendering performance. This allows the superimposing of highly polygonal 3D models at high frame rates and excellent image quality.},
keywords={mechanical engineering computing;virtual prototyping;rendering (computer graphics);augmented reality;workstation clusters;automobile industry;video signal processing;highly polygonal augmented reality;scalable PC-cluster architecture;automobile industry virtual prototypes;system architecture;image processing;parallel rendering;Augmented reality;Rendering (computer graphics);Virtual reality;Computer architecture;Image quality;Application software;Automobiles;Virtual prototyping;Product development;Costs},
doi={10.1109/ISMAR.2004.52},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383064,
author={M. C. {Juan} and C. {Botella} and M. {Alcaniz} and R. {Banos} and C. {Carrion} and M. {Melero} and J. A. {Lozano}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={An augmented reality system for treating psychological disorders: application to phobia to cockroaches},
year={2004},
volume={},
number={},
pages={256-257},
abstract={Augmented reality has been used in many fields, but it has not been used to treat psychological disorders. Augmented reality presents several advantages respect to: the traditional treatment of psychological disorders and virtual reality treatments. In this paper we present the first augmented reality system for the treatment of phobia to cockroaches. Our system has been developed using ARToolkit software. It has been tested with one patient and the results have been very satisfactory. At first of the exposure session the patient was not able to approach to a real cockroach and after the exposure session using our augmented reality system, the patient was able to approach to a real cockroach, to interact with it and to kill it by herself. This first result is very encouraging and it demonstrates that augmented reality exposure is effective for the treatment of this kind of phobias.},
keywords={augmented reality;psychology;patient diagnosis;augmented reality system;psychological disorders;cockroach phobia;virtual reality;ARToolkit software;Augmented reality;Psychology;Virtual reality;Medical treatment;Animals;Cameras;Testing;Streaming media;Universal Serial Bus;Instruments},
doi={10.1109/ISMAR.2004.14},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383065,
author={ {Xiang Zhang}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Projection matrix decomposition in AR - a study with Access3D},
year={2004},
volume={},
number={},
pages={258-259},
abstract={Access3D is an AR application developed for industrial maintenance. An important feature of Access3D is that, in addition to visualization, it allows the user to inquire further information, directly from the AR view through the hyperlinks. In Access3D, we use VRML for visualization, the advantage is the direct database inquiry through the hyperlinks. On the other hand, VRML requires explicit camera parameters for the virtual camera modeling. Explicit camera calibration is often obtained from projection matrix computation (PMC) followed by projection matrix decomposition (PMD). The PMD is known to be numerically bistable, this is a problem of great importance and research interest in computer vision. Different methods, including data normalization have been introduced for finding numerically stable algorithms. Our analysis shows that even if data normalization is applied to the PMC step, a non-favorite distribution of the 3D points may still cause error in the results and numerical instability. This paper provides a look at the numerical instability of PMD from a different point of view.},
keywords={augmented reality;matrix decomposition;data visualisation;virtual reality languages;numerical stability;projection matrix decomposition;Access3D;AR application;industrial maintenance;VRML;data visualization;virtual camera modeling;camera calibration;projection matrix computation;computer vision;data normalization;numerical instability;Matrix decomposition;Cameras;Transmission line matrix methods;Calibration;Matrix converters;Computer errors;Computer vision;High definition video;Data visualization;Spatial databases},
doi={10.1109/ISMAR.2004.48},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383066,
author={D. F. {Abawi} and J. {Bienwald} and R. {Dorner}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Accuracy in optical tracking with fiducial markers: an accuracy function for ARToolKit},
year={2004},
volume={},
number={},
pages={260-261},
abstract={Optical tracking with fiducial markers is commonly used in augmented reality (AR) systems - AR systems that rely on the ARToolKit (Kato and Billinghurst, 1999) are prominent examples. The information obtained by the tracking subsystem are widely used in AR, e.g. in order to calculate how virtual objects should be located and oriented. The results of extensive accuracy experiments with single markers are reported and made operational by the definition of an accuracy function. The results show a specific distribution of tracking accuracy dependent on distance as well as angle between camera and marker. This insight is applicable for designing the set-up of AR applications in general that rely on optical tracking.},
keywords={augmented reality;optical tracking;optical tracking;fiducial markers;ARToolKit;augmented reality systems;virtual objects;Cameras;Testing;Augmented reality;Optical design;Lenses;Pattern matching},
doi={10.1109/ISMAR.2004.8},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383067,
author={J. Y. {Didier} and D. {Roussel} and M. {Mallem}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={A texture based time delay compensation method for augmented reality},
year={2004},
volume={},
number={},
pages={262-263},
abstract={One of the key problems in augmented reality systems is registration, that is to say the synchronization of real and virtual world. Augmented reality uses a lot of different sensors in order to estimate camera or operator's point of view. These sensors could provide samples faster than mixing of virtual and real information could be displayed. We expose here a way to fake into account samples that are generated during the mixing process. This method is using a post-rendering technique involving a texture to perform this task. We expose the errors reduction obtained by performing such technique with a simulation test-bench implementing our proposal.},
keywords={rendering (computer graphics);augmented reality;image registration;cameras;texture based time delay compensation;augmented reality systems;postrendering technique;error reduction;sensor data;registration problem;Delay effects;Augmented reality;Chromium;Rendering (computer graphics);Cameras;Testing;Displays;Layout;Switches;Performance evaluation;pseudo-correction;post-rendering techniques;sensors data;augmented reality},
doi={10.1109/ISMAR.2004.7},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383068,
author={T. {Asai} and M. {Kanbara} and N. {Yokoya}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={3D modeling of wide area outdoor environments by integrating omnidirectional range and color images},
year={2004},
volume={},
number={},
pages={264-265},
abstract={This paper describes a method for modeling wide area outdoor environments by integrating omnidirectional range and color images. The proposed method effectively reconstructs the 3D models of outdoor environments by using omnidirectional laser rangefinder and omnidirectional multi-camera system (OMS). In this paper, we also give experimental results of 3D wide area reconstruction using the data acquired at 50 points in our campus.},
keywords={solid modelling;image reconstruction;cameras;laser ranging;image colour analysis;wide area outdoor environments;omnidirectional range;color images;3D model reconstruction;omnidirectional laser rangefinder;omnidirectional multicamera system;3D wide area reconstruction;Color;Sensor systems;Laser modes;Image reconstruction;Density measurement;Rotation measurement;Image resolution;Shape measurement;Cameras;Position measurement},
doi={10.1109/ISMAR.2004.1},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383069,
author={F. {Caillette} and T. {Howard}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Real-time markerless human body tracking using colored voxels and 3D blobs},
year={2004},
volume={},
number={},
pages={266-267},
abstract={This paper presents a robust method for real-time visual human body tracking. We perform a hierarchical 3D reconstruction from multiple camera views as a basis for tracking. Blobs attached to a kinematic model are then used to reliably track individual body parts with both volume and color information. We describe how the blob-model is dynamically adjusted to accommodate different body configurations. In tests, our system has proved robust in presence of noisy data and self-occlusions.},
keywords={image reconstruction;cameras;real-time systems;optical tracking;image colour analysis;real-time markerless human body tracking;colored voxels;3D blobs;real-time visual human body tracking;hierarchical 3D reconstruction;kinematic model;blob model;multiple camera view;Humans;Image reconstruction;Robustness;Cameras;Kinematics;Three dimensional displays;Augmented reality;Layout;Costs;Real time systems},
doi={10.1109/ISMAR.2004.50},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383070,
author={R. {Dorner} and L. {Oppermann} and C. {Geiger}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Implementing MR-based interaction techniques for manipulating 2D visualizations in 3D information space},
year={2004},
volume={},
number={},
pages={268-269},
abstract={A 3D information space is a three-dimensional visualization that contains 2D visualizations and puts them in a semantic context. In this paper, we sketch how mixed reality (MR) can be exploited as a technology in order to realize better interaction in a 3D information space and, as a result, to develop new interactive visualization techniques. Here, the real space is used as a metaphor for interacting with the 3D information space.},
keywords={virtual reality;data visualisation;MR-based interaction technique;mixed reality;2D visualizations;3D information space;3D visualization;interactive visualization;Data visualization;Virtual reality;Space technology;Mice;Liver;Lenses;Navigation;Image restoration;Filters;Lamps},
doi={10.1109/ISMAR.2004.35},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383071,
author={F. D. {McKenzie} and H. M. {Garcia} and R. J. {Castelino} and T. W. {Hubbard} and J. A. {Ullian} and G. A. {Gliva}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Augmented standardized patients now virtually a reality},
year={2004},
volume={},
number={},
pages={270-271},
abstract={Standardized patients (SPs), individuals who realistically portray patients, are widely used in medical education to teach and assess communication skills, eliciting a history, performing a physical exam, and other important clinical skills. One limitation is that each SP can only portray a limited set of physical symptoms. Finding SPs with the abnormalities students need to encounter is typically not feasible. This project augments the SP by permitting the learner to hear abnormal heart and lung sounds in a normal SP.},
keywords={medical diagnostic computing;computer aided instruction;augmented reality;patient care;biomedical education;augmented standardized patients;medical education;communication skills;physical exam;clinical skills;abnormal heart;lung sounds;Lungs;Computational modeling;Medical simulation;Heart;Medical diagnostic imaging;History;Stethoscope;Education;Computer simulation;Augmented reality},
doi={10.1109/ISMAR.2004.18},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383072,
author={ {Hanhoon Park} and {Jong-Il Park}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Invisible marker tracking for AR},
year={2004},
volume={},
number={},
pages={272-273},
abstract={We introduce a novel tracking system based on invisible markers which are created/drawn with an IR fluorescent pen. The tracking system consists of one scene camera, one IR camera, and one half mirror. The two cameras are positioned in each side of half mirror so that their optical centers coincide with each other. We track the invisible markers using the IR camera and visualize AR in the view of the scene camera. Thus, it works as a robust marker-less tracking system. Experimental results are given to demonstrate the viability of the proposed system.},
keywords={augmented reality;cameras;infrared imaging;optical tracking;invisible marker tracking;augmented reality;IR fluorescent pen;markerless tracking system;Cameras;Layout;Filters;Mirrors;Visualization;Robustness;Augmented reality;Computer vision;Geometry;Ink},
doi={10.1109/ISMAR.2004.37},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383073,
author={R. {Lapeer} and M. S. {Chen} and J. {Villagrana}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={An augmented reality based simulation of obstetric forceps delivery},
year={2004},
volume={},
number={},
pages={274-275},
abstract={During the process of human childbirth, obstetric forceps delivery is a justified alternative to Caesarean section when vaginal delivery proves difficult or impossible. Currently, training of forceps interventions is done on a real case due to the lack of realistic dummy models. This paper presents a basic augmented reality implementation of a forceps delivery which provides a platform for both training of forceps placement and manipulation for junior obstetricians as well as the assessment of any mechanical effects these actions may have on the fetus, and the fetal head and skull in particular.},
keywords={medical computing;computer based training;augmented reality;digital simulation;obstetrics;augmented reality based simulation;human childbirth;obstetric forceps delivery;Caesarean section;vaginal delivery;realistic dummy model;forceps placement training;junior obstetricians;Augmented reality;Fetus;Image segmentation;Blades;Skull;Optical polarization;Optical devices;Magnetic heads;Face detection;Humans},
doi={10.1109/ISMAR.2004.13},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383074,
author={M. {Wagner} and S. {Hennauer} and G. {Klinker}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Easing the transition between multiple trackers},
year={2004},
volume={},
number={},
pages={276-277},
abstract={Augmented reality research currently aims at extending the working range of applications by combining multiple trackers in adjacent areas. The transition between two such devices leads to discontinuities in the trajectory of a tracked object. The result are "jumps" in visual augmentations shown to the user. We present a three step unsupervised learning algorithm that determines the working areas of involved trackers and the area they overlap, and permanently observes the tracked object's position with regard to these areas in order to enable a smooth interpolation within the overlapping area from one tracker's readings to another's. We have tested the algorithm's performance in an experimental setup. The results show that the method is feasible and only adds a negligible overhead to AR systems.},
keywords={augmented reality;unsupervised learning;interpolation;tracking;multiple trackers;augmented reality;visual augmentation;unsupervised learning;smooth interpolation;Augmented reality;Interpolation;Area measurement;Neural networks;Trajectory;Unsupervised learning;Testing;Machine learning algorithms;Machine learning;Calibration},
doi={10.1109/ISMAR.2004.29},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383075,
author={L. {Soler} and S. {Nicolau} and J. {Schmid} and C. {Koehl} and J. {Marescaux} and X. {Pennec} and N. {Ayache}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Virtual reality and augmented reality in digestive surgery},
year={2004},
volume={},
number={},
pages={278-279},
abstract={Medical image processing led to a major improvement of patient care: the 3D modeling of patients from their CT-scan or MRI provides an improved surgical planning and simulation allows to train the surgical gesture before carrying it out. These two preoperative steps can be used intra-operatively with the development of augmented reality (AR). In this paper, we present the tools we developed to provide our first prototypal AR guiding system for abdominal surgery.},
keywords={augmented reality;surgery;medical image processing;patient care;virtual reality;augmented reality;digestive surgery;medical image processing;patient care;3D modeling;surgical planning;surgical simulation;AR guiding system;Virtual reality;Augmented reality;Surgery;Abdomen;Pathology;Biomedical imaging;Magnetic resonance imaging;Liver neoplasms;Visualization;Medical simulation},
doi={10.1109/ISMAR.2004.64},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383076,
author={W. {Piekarski} and R. {Smith} and B. H. {Thomas}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Designing backpacks for high fidelity mobile outdoor augmented reality},
year={2004},
volume={},
number={},
pages={280-281},
abstract={This paper presents the design for our latest backpack to support mobile outdoor augmented reality, and how it evolved from lessons learned with our previous designs. We present a number of novel features which help to reduce size and weight, improve reliability and ease of configuration, and reduce CPU usage on laptop computers.},
keywords={augmented reality;mobile computing;laptop computers;backpack design;high fidelity mobile outdoor augmented reality;laptop computer;Augmented reality;Portable computers;Wearable computers;Australia;Global Positioning System;Optical design;Cables;Design engineering;Fasteners;Joining processes},
doi={10.1109/ISMAR.2004.26},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383077,
author={M. {Gandy} and V. {MacIntyre} and S. {Dow}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Making tracking technology accessible in a rapid prototyping environment},
year={2004},
volume={},
number={},
pages={282-283},
abstract={In this paper we present an approach for exposing tracking technology in an accessible and flexible way to users of a rapid prototyping system for mixed (MR) and augmented reality (AR). Our system provides a tracking framework that alleviates the need for a high level of expertise while also presenting a model of the technology that allows for flexible modification of tracking configurations, the ability to quickly change an application from one type of tracking technology to another, and the creation of synthetic trackers for playback of prerecorded data, data fusion from multiple trackers, and wizard-of-oz applications.},
keywords={software prototyping;augmented reality;tracking;sensor fusion;tracking technology;rapid prototyping environment;rapid prototyping system;mixed reality;augmented reality;tracking configuration;data fusion;multiple trackers;wizard-of-oz application;Prototypes;Augmented reality;Application software;Hardware;Object oriented modeling;Switches;Educational institutions;System testing;Computer languages;Cameras},
doi={10.1109/ISMAR.2004.39},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383078,
author={V. {Novak} and C. {Sandor} and G. {Klinker}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={An AR workbench for experimenting with attentive user interfaces},
year={2004},
volume={},
number={},
pages={284-285},
abstract={We present a workbench to build, evaluate and iteratively develop user interfaces using augmented reality, eye tracking, and visual programming. To test our system, we have developed an attentive user interface (AUI) for automotive environments, which coordinates its activities based on the context and visual attention of the user. Development of AUI requires interdisciplinary teams like psychologists, human-factor engineers, designers and computer scientists to work together. The main problem of interdisciplinary communication is a lack of common language and different notion of the system. We have developed a workbench, which facilitates the communication between the team members and enhances the comprehension of the system by visualizing users' attention and system reactions.},
keywords={augmented reality;tracking;visual programming;team working;user interfaces;data visualisation;attentive user interfaces;augmented reality;eye tracking;visual programming;automotive environments;interdisciplinary communication;team working;User interfaces;Visualization;Augmented reality;Automotive engineering;Prototypes;System testing;Context;Psychology;Design engineering;Computer interfaces},
doi={10.1109/ISMAR.2004.12},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383079,
author={K. {Rehman}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Augmented reality in support of interaction for location-aware applications},
year={2004},
volume={},
number={},
pages={286-287},
abstract={There has been an increased interest in both the augmented reality (AR) and ubiquitous computing (Ubicomp) research communities to integrate these two technologies. In an attempt to introduce visual interaction into location-aware applications we have developed a prototype that lets users experience a Ubicomp environment visually. Some system issues we came across in accomplishing this task are described.},
keywords={augmented reality;mobile computing;augmented reality;location-aware applications;ubiquitous computing;visual interaction;Augmented reality;Pervasive computing;Prototypes;Yarn;Laboratories;Design engineering;Ubiquitous computing;Physics computing;Visualization;Programming profession},
doi={10.1109/ISMAR.2004.16},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383080,
author={M. {Koeda} and Y. {Matsumoto} and T. {Ogasawara}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Annotation-based assistance system for unmanned helicopter with wearable augmented reality environment},
year={2004},
volume={},
number={},
pages={288-289},
abstract={In this paper, we introduce an annotation-based assistance system for an unmanned helicopter with a wearable augmented reality environment. In this system, an operator controls the helicopter remotely while watching an annotated view from the helicopter through a head mounted display (HMD) with a laptop PC in a backpack. Annotations assist the operation indicating some conditions of the helicopter and a name of buildings nearby. The position and the attitude of the helicopter is measured by GPS and a gyroscope, and sent to the operator's PC via a wireless LAN.},
keywords={augmented reality;wearable computers;helmet mounted displays;telecontrol;aircraft control;wireless LAN;Global Positioning System;laptop computers;remotely operated vehicles;aerospace computing;helicopters;annotation-based assistance system;unmanned helicopter;wearable augmented reality environment;remote control;head mounted display;laptop PC;GPS;gyroscope;wireless LAN;Helicopters;Augmented reality;Wireless LAN;Gyroscopes;Control systems;Global Positioning System;Cameras;Head;Payloads;Image generation},
doi={10.1109/ISMAR.2004.15},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383081,
author={T. {Sielhorst} and J. {Traub} and N. {Navab}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={The AR apprenticeship: replication and omnidirectional viewing of subtle movements},
year={2004},
volume={},
number={},
pages={290-291},
abstract={We propose an AR system that learns from the expert by tracking his movements while he/she uses a simulator or performs a real (often complicated) task. This information is reproduced for demonstration to students in an enhanced simulator. By simultaneous visualization and comparison of the experts and students performance, direct feedback is provided. We propose real-time recording of instruments' tracking data of expert's actions for latter replication in an AR system for teaching purposes. The movements are recorded relative to an existing physical simulator. By this means the student can view the subtle movements from any direction repeatedly without time limits of the expert in order to learn and improve his own performance.},
keywords={tracking;digital simulation;real-time systems;augmented reality;teaching;computer aided instruction;expert systems;omnidirectional viewing;movement tracking;real-time recording;AR system;teaching;Target tracking;Feedback;Instruments;Biological system modeling;Computational modeling;Imaging phantoms;Visualization;Cameras;Head;Medical simulation},
doi={10.1109/ISMAR.2004.58},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383082,
author={C. {Geiger} and T. {Schmidt} and J. {Stocklein}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Rapid development of expressive AR applications},
year={2004},
volume={},
number={},
pages={292-293},
abstract={We describe our approach of developing AR applications with expressive visual representations and realistic object behavior. We combine concepts from high-level scene graphs, physical simulation libraries and a Java binding for ARToolkit with the ideas of iterative prototyping and show that rapid development of expressive AR applications using Java is possible.},
keywords={software prototyping;augmented reality;graph theory;software libraries;Java;data visualisation;realistic object behavior;high-level scene graphs;physical simulation libraries;Java language;ARToolkit;iterative prototyping;expressive AR applications;expressive visual representations;Java;Layout;Rendering (computer graphics);Libraries;Application software;Augmented reality;Graphics;Testing;Physics;Prototypes},
doi={10.1109/ISMAR.2004.49},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383083,
author={E. {Bennett} and B. {Stevens}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={The effect that haptically perceiving a projection augmented model has on the perception of size},
year={2004},
volume={},
number={},
pages={294-295},
abstract={This paper reports on a study that investigated the effect touching a projection augmented model, and interacting with it using a spatially-coincident device, has on the perception of size. It was found that touching increased the accuracy of size estimates, however interaction using a spatially-coincident device did not.},
keywords={visual perception;augmented reality;haptic interfaces;haptic perception;projection augmented model;size perception;spatially-coincident device;Mice;Augmented reality;Haptic interfaces;Feedback;Information systems;Solid modeling;Computer applications;Computer displays;Physics computing;Shape},
doi={10.1109/ISMAR.2004.59},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383084,
author={S. {Tariq} and F. {Dellaert}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={A multi-camera 6-DOF pose tracker},
year={2004},
volume={},
number={},
pages={296-297},
abstract={Most of the work in head-pose tracking has concentrated on single-camera systems with a relatively small field of view which have limited accuracy because features are only observed in a single viewing direction. We present a multicamera pose tracker that handles an arbitrary configuration of cameras rigidly fixed to the observer's head. By using multiple cameras, we increase the robustness and accuracy by which a 6-DOF pose is tracked. However, in a multicamera rig setting, earlier methods for determining the unknown pose from three world-to-camera correspondences are no longer applicable. We present a RANSAC (M. Fischler and R. Bolles, 1981) based method that handles multicamera rigs by using a fast nonlinear minimization step in each RANSAC round.},
keywords={cameras;tracking;computer vision;minimisation;motion estimation;multicamera 6-DOF pose tracker;head-pose tracking;world-to-camera correspondences;RANSAC;multicamera rigs;fast nonlinear minimization;Cameras;Robustness;Noise measurement;Educational institutions;Minimization methods;Calibration;Equations;Gaussian noise;Machinery;Algorithms},
doi={10.1109/ISMAR.2004.6},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383085,
author={K. {Fast} and T. {Gifford} and R. {Yancey}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Virtual training for welding},
year={2004},
volume={},
number={},
pages={298-299},
abstract={A mixed reality system has been created for simulating gas metal arc welding (GMAW) welding. This simulation system is intended for use in training human welders. The system is comprised of a real welding torch attached to a force feedback device, a head-mounted display, a 6 DOF tracking system for both the torch and the user's head, and external audio speakers. The welding simulation is based on empirical results from detailed analysis of a series of test welds. The simulation runs in real-time, using a neural network to determine the quality and shape of the created weld based on the orientation and speed of the welding torch. The welding process and resulting weld bead are displayed in a virtual environment. Weld quality and recorded process values can be displayed after welding for review.},
keywords={computer based training;production engineering computing;arc welding;augmented reality;force feedback;helmet mounted displays;real-time systems;neural nets;tracking;digital simulation;virtual training;welding training;mixed reality system;gas metal arc welding simulation;welding torch;force feedback device;head-mounted display;6-DOF tracking system;external audio speakers;real-time simulation;neural network;Welding;Virtual reality;Humans;Force feedback;Auditory displays;Analytical models;Testing;Neural networks;Shape;Virtual environment},
doi={10.1109/ISMAR.2004.65},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383086,
author={S. {DiVerdi} and T. {Hollerer} and R. {Schreyer}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Level of detail interfaces},
year={2004},
volume={},
number={},
pages={300-301},
abstract={We present the level of detail interface based on the marriage of level of detail geometry and an adaptable user interface. Level of detail interfaces allow applications to paramaterize their display of data and interface widgets with respect to distance from the camera, to best take advantage of diminished screen space in a 3D environment.},
keywords={graphical user interfaces;computational geometry;level of detail interfaces;level of detail geometry;adaptable user interface;User interfaces;Lenses;Bars;Graphics;Navigation;Computer science;Computational geometry;Application software;Three dimensional displays;Computer displays},
doi={10.1109/ISMAR.2004.38},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383087,
author={J. {Molineros} and R. {Behringer} and C. {Tam}},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Vision-based augmented reality for pilot guidance in airport runways and taxiways},
year={2004},
volume={},
number={},
pages={302-303},
abstract={This paper describes our on-going efforts to develop an augmented reality system for enhanced pilot situational awareness in airport runways and taxiways. The system consists of a sensing component based on computer vision and an information component based on high-fidelity graphic model databases. Vision algorithms are used for a variety of guidance and warning tasks. A necessary requirement is for vision algorithms to have a real-time response.},
keywords={computer vision;augmented reality;visual databases;real-time systems;airports;aircraft landing guidance;vision-based augmented reality;pilot guidance;airport runways;taxiways;pilot situational awareness;computer vision;high-fidelity graphic model databases;real-time response;Augmented reality;Airports;Image databases;Spatial databases;Visual databases;Graphics;Data mining;Feature extraction;Character recognition;Optical character recognition software},
doi={10.1109/ISMAR.2004.66},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383088,
author={},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Developing AR Applications with ARToolKit},
year={2004},
volume={},
number={},
pages={305-305},
abstract={},
keywords={},
doi={10.1109/ISMAR.2004.27},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383089,
author={},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Advanced Visual Tracking},
year={2004},
volume={},
number={},
pages={305-305},
abstract={},
keywords={},
doi={10.1109/ISMAR.2004.10},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383090,
author={},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Spatial Augmented Reality},
year={2004},
volume={},
number={},
pages={306-306},
abstract={},
keywords={Augmented reality},
doi={10.1109/ISMAR.2004.55},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383091,
author={},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Extreme MR: Going Beyond Reality to Create Extreme Multi-Modal Mixed Reality for Entertainment, Training and Education},
year={2004},
volume={},
number={},
pages={306-306},
abstract={},
keywords={Virtual reality},
doi={10.1109/ISMAR.2004.31},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1383092,
author={},
booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Author index},
year={2004},
volume={},
number={},
pages={307-308},
abstract={The author index contains an entry for each author and coauthor included in the proceedings record.},
keywords={},
doi={10.1109/ISMAR.2004.20},
ISSN={},
month={Nov},}