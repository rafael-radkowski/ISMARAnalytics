@INPROCEEDINGS{6162844,
author={},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Art exhibition committee},
year={2011},
volume={},
number={},
pages={1-2},
abstract={Provides a listing of current committee members.},
keywords={},
doi={10.1109/ISMAR.2011.6092343},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162845,
author={},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Conference committee},
year={2011},
volume={},
number={},
pages={1-1},
abstract={Provides a listing of current committee members.},
keywords={},
doi={10.1109/ISMAR.2011.6092352},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162846,
author={A. {Simon} and D. {Wagner} and S. {White}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={ISMAR demo chairs},
year={2011},
volume={},
number={},
pages={1-1},
abstract={We are very happy to present the ISMAR 2011 Demonstrations Program. Demos embody "hello world" - the slogan of this year's ISMAR conference - and have a special place in the consciousness of the community. They provide us with our first glimpse of new ideas, a venue for directly interacting with implemented versions of novel concepts, and an opportunity to engage with the people of this community in the context of their work.},
keywords={},
doi={10.1109/ISMAR.2011.6092345},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162847,
author={},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Symposium general chairs},
year={2011},
volume={},
number={},
pages={1-2},
abstract={Welcome to the Tenth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR 2011).},
keywords={},
doi={10.1109/ISMAR.2011.6092346},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162848,
author={O. {Grau}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Will we ever become used to immersion? Art history and image science},
year={2011},
volume={},
number={},
pages={1-1},
abstract={3D Television and Immersive Cinema, Virtual and Augmented Reality, do we enter soon a total space of polysensual illusion? The aim of this contribution is to create an understanding that the present image revolution using indeed new technologies has also developed a large number of so far unknown visual expressions that cannot be conceived without our image history. Art History and Image Science help in understanding the leading and forming functions of today´s image worlds in our society. With the history of illusion and immersion, the history of artificial life or the tradition of telepresence, Image Science offers sub-histories of the present image revolutions. We know that a central problem of current cultural policy stems from serious lack of knowledge about the origins of the audiovisual media and this stands in complete contradistinction to current demands for more media and image competence. Social media competence, which goes beyond mere technical skills, is difficult to acquire if the area of historic media experience is excluded. Although many people view the concept of presence, virtual or mixed realities as a totally new phenomenon, it has its foundations in an unrecognized history of immersive images. Immersion is undoubtedly a key to any understanding of the development of media in general. Overseeing 2000 years of immersive images and by bringing them in a relativity with the image competence of their users, this talk aims to gain distance and with that a reflective thinking space towards the desire to create ever new immersive image experiences.},
keywords={},
doi={10.1109/ISMAR.2011.6092347},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162849,
author={M. {Bolas}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Surface/space},
year={2011},
volume={},
number={},
pages={1-1},
abstract={Mixed and augmented reality is crossing the chasm from research to widespread adoption. This requires us to think beyond registered planes of pixels in space, and confront the unregistered mess of users, culture, and application.},
keywords={},
doi={10.1109/ISMAR.2011.6092348},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162850,
author={A. D. {Cheok}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Multi modal sensory human communication in the Internet society},
year={2011},
volume={},
number={},
pages={1-1},
abstract={This talk outlines new facilities within human media spaces supporting embodied interaction between humans, animals, and computation both socially and physically, with the aim of novel interactive communication and entertainment. We aim to develop new types of human communications and entertainment environments using all the senses, including touch, taste, and smell, which can increase support for multi-person multi-modal interaction and remote presence. In this talk, we present an alternative ubiquitous computing environment based on an integrated design of real and virtual worlds. We discuss some different research prototype systems for interactive communication, culture, and play.},
keywords={},
doi={10.1109/ISMAR.2011.6092349},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162851,
author={},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Science technology program chairs},
year={2011},
volume={},
number={},
pages={1-4},
abstract={The S&T Program Chairs are delighted to welcome you to ISMAR 2011, the 10th symposium on Mixed and Augmented Reality! This year's symposium continues a long tradition of ISMAR meetings, a series that itself followed a related series of IWAR, ISMR, and ISAR meetings. This year's Science and Technology (S&T) track comprises a mix of highly selective mixed and augmented reality research and related work. Specifically, in the S&T program this year you will find 26 papers, 27 posters, as well as a stimulating mixture of keynote talks, demonstrations, tutorials, workshops and the tracking competition. All of these elements of the program are the result of dedicated hard work by members of various conference committees and additional volunteers, and we would like to thank all of them for their generous efforts.},
keywords={},
doi={10.1109/ISMAR.2011.6092350},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162852,
author={},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={S T program committee},
year={2011},
volume={},
number={},
pages={1-1},
abstract={Provides a listing of current committee members.},
keywords={},
doi={10.1109/ISMAR.2011.6162852},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162854,
author={},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={S T reviewers},
year={2011},
volume={},
number={},
pages={1-3},
abstract={The publication offers a note of thanks and lists its reviewers.},
keywords={},
doi={10.1109/ISMAR.2011.6092351},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162853,
author={},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Steering committee},
year={2011},
volume={},
number={},
pages={1-1},
abstract={Provides a listing of current committee members.},
keywords={},
doi={10.1109/ISMAR.2011.6092353},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162855,
author={},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Sponsored by},
year={2011},
volume={},
number={},
pages={1-1},
abstract={Sponsored by: Qualcomm; Nokia; A.R.T.; metaio; Volkswagen; EST; TRIVISIO; TRAKMARK; AIST; ARToolworks.},
keywords={},
doi={10.1109/ISMAR.2011.6092354},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162856,
author={D. {Wagner} and I. {Barakonyi} and I. {Siklossy} and J. {Wright} and R. {Ashok} and S. {Diaz} and B. {MacIntyre} and D. {Schmalstieg}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Building your vision with Qualcomm's Mobile Augmented Reality (AR) platform: AR on mobile devices},
year={2011},
volume={},
number={},
pages={1-1},
abstract={Academic/Research Overview of Mobile Augmented Reality Introduction to the Qualcomm AR Platform — Features and Usage Scenarios Developing Mobile AR Applications using Qualcomm's Unity Extension Introduction to Qualcomm's QCAR SDK Native API Cross Platform Development with the Native QCAR SDK},
keywords={},
doi={10.1109/ISMAR.2011.6092355},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162857,
author={U. {Bockholt} and U. {Vogel} and R. {Herold} and P. {Schreiber} and S. {Voth}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Bi-directional OLED microdisplay for see-through HMD},
year={2011},
volume={},
number={},
pages={1-1},
abstract={Within the research project “iStar — Interactive See-Through Augmented-Reality Display” the Fraunhofer Gesellschaft developed Augmented Reality goggles comprising a VGA OLED microdisplay with embedded image sensor aimed on gaze-control and see-through head-mounted optics. The active area of the bi-directional microdisplay consists of nested display and image sensor (embedded camera) pixels surrounded by a second image sensor (frame camera) as well as driving and control circuitry (c.f. Table). The display and image sensor systems are electrically independent of one another, simply interacting via synchronization signals. iStar also includes a developer kit integrating Eye-Tracking software, AR system and application demonstrators. The topic of high-contrast See-Through HMDs still forms an important research topic within the AR community, thereby iStar not only offers a light-weight display solution but it also integrates camera sensors into the display to support Eye-Tracking. With the presented demonstrators the tutorial attendees can evaluate the possibilities and the maturity of the developed technologies. The attendees will get an overview to requirements, solution possibilities and open research topics in the field of microdisplays, optics and software development for the realization of interactive see-Through HMDs. Feedback to learned objectives will be evaluated in questioners.},
keywords={},
doi={10.1109/ISMAR.2011.6092356},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162858,
author={M. {Neal} and J. {Cabiria} and J. L. {Hogg}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Psychological keys to success in MAR systems},
year={2011},
volume={},
number={},
pages={1-1},
abstract={Imagine a significant improvement in understanding user response to your application. This tutorial examines psychological considerations in developing and deploying MAR applications to help improve your understanding of key psychological factors in technological development. Successful MAR applications will be those that take advantage of the inherent way our brains process information in this new environment. This panel provides a one half-day session where media-focused psychology is explored at a basic level with numerous illustrations, examples, and techniques to ensure that the MAR application has a strong technology-mind interface. The purpose of this tutorial is to help define the role psychological research and theories play in the successful development and deployment of commercial MAR applications. It contains four major sections of study for session participants. The first three are: 1) cognitive science, 2) psychological design, and 3) narrative transportation theory for applications. There is considerable research in user experience (UX), and there are many lessons to be learned as MAR technology moves into the mainstream. Each section deconstructs core foundational components in the understanding of user-experience (UX) with regard to new interactive technologies. The final section ties the first three sections together and provides practical tips and techniques to help MAR researchers and practitioners take the psychological sciences into their labs, design, and development shops. Throughout this tutorial, attendees will view various examples of each of the topics, as well as be exposed to a variety of similarities and differences in cognitive interpretation of object and environment design. By the end of this session, attendees will have a better understanding of how to design for diverse end-users, as well as how to capture and hold end-user attention for the ultimate purpose of engagement and immersion.},
keywords={},
doi={10.1109/ISMAR.2011.6092357},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162864,
author={},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Workshops tutorials committee},
year={2011},
volume={},
number={},
pages={1-1},
abstract={Provides a listing of current committee members.},
keywords={},
doi={10.1109/ISMAR.2011.6092358},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162859,
author={S. {Feiner} and T. {Korah} and D. {Murphy} and V. {Parameswaran} and M. {Stroila}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Enabling large-scale outdoor mixed reality and augmented reality},
year={2011},
volume={},
number={},
pages={1-1},
abstract={While there is significant recent progress in technologies supporting augmented reality for small indoor environments, there is still much work to be done for large outdoor environments. This workshop focuses primarily on research that enables high-quality outdoor Mixed Reality (MR) and Augmented Reality (AR) applications. These research topics include, but are not restricted to: — 3D geo-referenced data (images, point clouds, and models) — Algorithms for object recognition from large databases of geo-referenced data — Algorithms for object tracking in outdoor environment — Multi-cue fusion to achieve improved performance of object detection and tracking — Novel representation schemes to facilitate large-scale content distribution — 3D reasoning to support intelligent augmentation — Novel and improved mobile capabilities for data capture (device sensors), processing, and display — Applications, experiences, and user interface techniques. The workshop will also showcase existing prototypes of applications enabled by these technologies: mirror worlds, high-fidelity virtual environments, applications of panoramic imagery, and user studies relating to these media types. This workshop aims to bring together academic and industrial researchers and to foster discussion amongst participants on the current state of the art and future directions for technologies that enable large-scale outdoor MR and AR applications. The workshop will start with a session in which position statements and overviews of the state of the art are presented. In the afternoon, we will follow up with discussion sessions and a short closing session.},
keywords={},
doi={10.1109/ISMAR.2011.6092359},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162860,
author={H. {Kato} and T. {Höllerer} and S. {Benhimane} and W. {Chinthammit}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={International workshop on AR/MR registration, tracking and benchmarking (TrakMark2011)},
year={2011},
volume={},
number={},
pages={1-1},
abstract={In the research fields of Augmented Reality (AR) and Mixed Reality (MR), tracking and registration methods are still one of the most important topics. The tracking research field is highly active, and numerous methods appear on a regular basis. The TrakMark working group (WG) was established 2009 to create a benchmark test that permits objective and accurate evaluation of the tracking methods. This year, the workshop will cover a wide range of topics concerning AR/MR registration, tracking and benchmarking. Key areas include, but are not limited to: — Vision-based registration, camera localization — Visual SLAM, structure from motion, camera calibration, sensor fusion — Natural feature tracking, object tracking, feature detection, feature description — Comparison of methods, evaluation of methods, suggestion of new benchmarking scheme — Survey of tracking papers.},
keywords={},
doi={10.1109/ISMAR.2011.6092360},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162861,
author={S. {White} and D. {Kalkofen} and C. {Sandor}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Visualization in mixed reality environments},
year={2011},
volume={},
number={},
pages={1-1},
abstract={Mixed and Augmented Reality displays extend the user's perception with computer generated information. This information is typically registered in three-dimensional space, and related to objects and places in the physical world. While individual annotation of objects has historically been a topic of MR research, visualization incorporating multiple related data points or models provides a variety of new research challenges in systems and techniques. For example, photorealistic augmented reality visualization presents data by adapting additionally presented imagery to the real world condition while illustrative visualization techniques aim at enhancing the understanding of augmented scenarios by carefully combining and mediating real and virtual data. Situated visualization techniques present virtual representations of data in relevant locations in the physical scene. A challenge in many of these techniques is the need to correctly communicate the relationships between physical imagery and virtual data.},
keywords={},
doi={10.1109/ISMAR.2011.6092361},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162862,
author={C. {Perey}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Indoor positioning and navigation for mobile AR},
year={2011},
volume={},
number={},
pages={1-1},
abstract={The researchers and developers of mobile AR platforms need to use a common platform for developing experiences regardless of the surroundings of the user. In order to expand the use of AR both indoor and outdoor with and without computer vision techniques, the breadth of options available for positioning users and points of interest needs to expand. Separately, the experts in indoor positioning and navigation are generally not as familiar with AR use scenarios as they are with other domains. Together, positioning and navigation experts, and mobile AR experts, will discuss: — What are the indoor positioning and navigation systems best suited for mobile AR? — What studies are underway or need to be conducted in order to advance this field?},
keywords={},
doi={10.1109/ISMAR.2011.6092362},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162863,
author={M. {Billinghurst} and T. {Langlotz} and B. {MacIntyre} and H. {Seichter}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Authoring solutions for Augmented Reality},
year={2011},
volume={},
number={},
pages={1-1},
abstract={The motivation of this workshop is to discuss future directions of content authoring in the field of Augmented Reality, as well as to discuss the current state of art on content creation and asset assembly. The workshop will comprise of a paper session where papers, late-breaking results and overviews over state-of-the-art in content authoring for AR are presented. In the afternoon, we will follow up with a discussion on different topics ranging from AR asset creation to content distribution and a closing session. Our goal is to collect ideas and thoughts of research about desired approaches for authoring content for AR, as well as review current and future needs to achieve a high quality content and at the same time scalable approaches for AR.},
keywords={},
doi={10.1109/ISMAR.2011.6092363},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162865,
author={E. {Ito} and T. {Okatani} and K. {Deguchi}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Accurate and robust planar tracking based on a model of image sampling and reconstruction process},
year={2011},
volume={},
number={},
pages={1-8},
abstract={It is one of the central issues in augmented reality and computer vision to track a planar object moving relatively to a camera in an accurate and robust manner. In previous studies, it was pointed out that there are several factors making the tracking difficult, such as illumination change and motion blur, and effective solutions were proposed for them. In this paper, we point out that degradation in effective image resolution can also deteriorate tracking performance, which typically occurs when the plane being tracked has an oblique pose with respect to the viewing direction, or when it moves to a distant location from the camera. The deterioration tends to become significantly large for extreme configurations, e.g., when the planar object has nearly a right angle with the viewing direction. Such configurations can frequently occur in AR applications targeted at ordinary users. To cope with this problem, we model the sampling and reconstruction process of images, and present a tracking algorithm that incorporates the model to correctly handle these configurations. We show through several experiments that the proposed method shows better performance than conventional methods.},
keywords={Target tracking;Cameras;Image resolution;Degradation;Image reconstruction;Accuracy;Visual tracking;Planar tracking;Image formation process;Image sampling and reconstruction},
doi={10.1109/ISMAR.2011.6092364},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162866,
author={M. {Donoser} and P. {Kontschieder} and H. {Bischof}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Robust planar target tracking and pose estimation from a single concavity},
year={2011},
volume={},
number={},
pages={9-15},
abstract={In this paper we introduce a novel real-time method to track weakly textured planar objects and to simultaneously estimate their 3D pose. The basic idea is to adapt the classic tracking-by-detection approach, which seeks for the object to be tracked independently in each frame, for tracking non-textured objects. In order to robustly estimate the 3D pose of such objects in each frame, we have to tackle three demanding problems. First, we need to find a stable representation of the object which is discriminable against the background and highly repetitive. Second, we have to robustly relocate this representation in every frame, also during considerable viewpoint changes. Finally, we have to estimate the pose from a single, closed object contour. Of course, all demands shall be accommodated at low computational costs and in real-time. To attack the above mentioned problems, we propose to exploit the properties of Maximally Stable Extremal Regions (MSERs) for detecting the required contours in an efficient manner and to apply random ferns as efficient and robust classifier for tracking. To estimate the 3D pose, we construct a perspectively invariant frame on the closed contour which is intrinsically provided by the extracted MSER. In our experiments we obtain robust tracking results with accurate poses on various challenging image sequences at a single requirement: One MSER used for tracking has to have at least one concavity that sufficiently deviates from its convex hull.},
keywords={Estimation;Robustness;Training;Three dimensional displays;Detectors;Image edge detection;Shape},
doi={10.1109/ISMAR.2011.6092365},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162867,
author={H. {Uchiyama} and E. {Marchand}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Toward augmenting everything: Detecting and tracking geometrical features on planar objects},
year={2011},
volume={},
number={},
pages={17-25},
abstract={This paper presents an approach for detecting and tracking various types of planar objects with geometrical features. We combine traditional keypoint detectors with Locally Likely Arrangement Hashing (LLAH) [21] for geometrical feature based keypoint matching. Because the stability of keypoint extraction affects the accuracy of the keypoint matching, we set the criteria of keypoint selection on keypoint response and the distance between keypoints. In order to produce robustness to scale changes, we build a non-uniform image pyramid according to keypoint distribution at each scale. In the experiments, we evaluate the applicability of traditional keypoint detectors with LLAH for the detection. We also compare our approach with SURF and finally demonstrate that it is possible to detect and track different types of textures including colorful pictures, binary fiducial markers and handwritings.},
keywords={Detectors;Feature extraction;Cameras;Buildings;Robustness;Robot vision systems;Databases},
doi={10.1109/ISMAR.2011.6092366},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162868,
author={C. {Pirchheim} and G. {Reitmayr}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Homography-based planar mapping and tracking for mobile phones},
year={2011},
volume={},
number={},
pages={27-36},
abstract={We present a real-time camera pose tracking and mapping system which uses the assumption of a planar scene to implement a highly efficient mapping algorithm. Our light-weight mapping approach is based on keyframes and plane-induced homographies between them. We solve the planar reconstruction problem of estimating the keyframe poses with an efficient image rectification algorithm. Camera pose tracking uses continuously extended and refined planar point maps and delivers robustly estimated 6DOF poses. We compare system and method with bundle adjustment and monocular SLAM on synthetic and indoor image sequences. We demonstrate large savings in computational effort compared to the monocular SLAM system while the reduction in accuracy remains acceptable.},
keywords={Cameras;Cost function;Three dimensional displays;Mobile handsets;Estimation;Real time systems;monocular SLAM;mapping;tracking;plane estimation;mobile phone},
doi={10.1109/ISMAR.2011.6092367},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162870,
author={C. {Arth} and M. {Klopschitz} and G. {Reitmayr} and D. {Schmalstieg}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Real-time self-localization from panoramic images on mobile devices},
year={2011},
volume={},
number={},
pages={37-46},
abstract={Self-localization in large environments is a vital task for accurately registered information visualization in outdoor Augmented Reality (AR) applications. In this work, we present a system for self-localization on mobile phones using a GPS prior and an online-generated panoramic view of the user's environment. The approach is suitable for executing entirely on current generation mobile devices, such as smartphones. Parallel execution of online incremental panorama generation and accurate 6DOF pose estimation using 3D point reconstructions allows for real-time self-localization and registration in large-scale environments. The power of our approach is demonstrated in several experimental evaluations.},
keywords={Cameras;Image reconstruction;Three dimensional displays;Mobile handsets;Databases;Global Positioning System;Accuracy},
doi={10.1109/ISMAR.2011.6092368},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162871,
author={P. K. {Baheti} and A. {Swaminathan} and M. {Chari} and S. {Diaz} and S. {Grzechnik}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Information-theoretic database building and querying for mobile augmented reality applications},
year={2011},
volume={},
number={},
pages={47-53},
abstract={Recently, there has been tremendous interest in the area of mobile Augmented Reality (AR) with applications including navigation, social networking, gaming and education. Current generation mobile phones are equipped with camera, GPS and other sensors, e.g., magnetic compass, accelerometer, gyro in addition to having ever increasing computing/graphics capabilities and memory storage. Mobile AR applications process the output of one or more sensors to augment the real world view with useful information. This paper's focus is on the camera sensor output, and describes the building blocks for a vision-based AR system. We present information-theoretic techniques to build and maintain an image (feature) database based on reference images, and for querying the captured input images against this database. Performance results using standard image sets are provided demonstrating superior recognition performance even with dramatic reductions in feature database size.},
keywords={Databases;Feature extraction;Accuracy;Entropy;Object detection;Mobile communication;Robustness;Mobile Augmented Reality;Object detection;database pruning},
doi={10.1109/ISMAR.2011.6092369},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162872,
author={Q. {Pan} and C. {Arth} and G. {Reitmayr} and E. {Rosten} and T. {Drummond}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Rapid scene reconstruction on mobile phones from panoramic images},
year={2011},
volume={},
number={},
pages={55-64},
abstract={Rapid 3D reconstruction of environments has become an active research topic due to the importance of 3D models in a huge number of applications, be it in Augmented Reality (AR), architecture or other commercial areas. In this paper we present a novel system that allows for the generation of a coarse 3D model of the environment within several seconds on mobile smartphones. By using a very fast and flexible algorithm a set of panoramic images is captured to form the basis of wide field-of-view images required for reliable and robust reconstruction. A cheap on-line space carving approach based on Delaunay triangulation is employed to obtain dense, polygonal, textured representations. The use of an intuitive method to capture these images, as well as the efficiency of the reconstruction approach allows for an application on recent mobile phone hardware, giving visually pleasing results almost instantly.},
keywords={Image reconstruction;Three dimensional displays;Cameras;Mobile handsets;Solid modeling;Computational modeling;Feature extraction},
doi={10.1109/ISMAR.2011.6092370},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162873,
author={B. {MacIntyre} and A. {Hill} and H. {Rouzati} and M. {Gandy} and B. {Davidson}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={The Argon AR Web Browser and standards-based AR application environment},
year={2011},
volume={},
number={},
pages={65-74},
abstract={A common vision of Augmented Reality (AR) is that of a person immersed in a diverse collection of virtual information, superimposed on their view of the world around them. If such a vision is to become reality, an ecosystem for AR must be created that satisfies at least these properties: multiple sources (or channels of interactive information) must be able to be simultaneously displayed and interacted with, channels must be isolated from each other (for security and stability), channel authors must have the flexibility to design the content and interactivity of their channel, and the application must fluidly integrate with the ever-growing cloud of systems and services that define our digital lives. In this paper, we present the design and implementation of the Argon AR Web Browser and describe our vision of an AR application environment that leverages the WWW ecosystem. We also describe KARML, our extension to KML (the spatial markup language for Google Earth and Maps), that supports the functionality required for mobile AR. We combine KARML with the full range of standard web technologies to create a standards-based web browser for mobile AR. KARML lets users develop 2D and 3D content using existing web technologies and facilitates easy deployment from standard web servers. We highlight a number of projects that have used Argon and point out the ways in which our web-based architecture has made previously impractical AR concepts possible.},
keywords={Argon;Mobile communication;Browsers;Three dimensional displays;Computer architecture;HTML;Service oriented architecture;augmented reality;web-based architecture},
doi={10.1109/ISMAR.2011.6092371},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162874,
author={T. {Olsson} and M. {Salo}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Online user survey on current mobile augmented reality applications},
year={2011},
volume={},
number={},
pages={75-84},
abstract={Augmented reality (AR) as an emerging technology in the mobile computing domain is becoming mature enough to engender publicly available applications for end users. Various commercial applications have recently been emerging in the mobile consumer domain at an increasing pace — Layar, Junaio, Google Goggles, and Wikitude are perhaps the most prominent ones. However, the research community lacks an understanding of how well such timely applications have been accepted, what kind of user experiences they have evoked, and what the users perceive as the weaknesses of the various applications overall. During the spring of 2011 we conducted an online survey to study the overall acceptance and user experience of the mobile AR-like consumer applications currently existing on the market. This paper reports the first analyses of the qualitative and quantitative survey data of 90 respondents. We highlight an extensive set of user-oriented issues to be considered in developing the applications further, as well as in directing future user research in AR. The results indicate that the experiences have been inconsistent: generally positive evaluations are overshadowed by mentions of applications' pragmatic uselessness in everyday life and technical unreliability, as well as excessive or limited and irrelevant content.},
keywords={Mobile communication;Context;Augmented reality;Image recognition;Visualization;Browsers;Cameras;augmented reality;user experience;user acceptance;online survey;end user application;evaluation},
doi={10.1109/ISMAR.2011.6092372},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162875,
author={G. {Simon}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Tracking-by-synthesis using point features and pyramidal blurring},
year={2011},
volume={},
number={},
pages={85-92},
abstract={Tracking-by-synthesis is a promising method for markerless vision-based camera tracking, particularly suitable for Augmented Reality applications. In particular, it is drift-free, viewpoint invariant and easy-to-combine with physical sensors such as GPS and inertial sensors. While edge features have been used succesfully within the tracking-by-synthesis framework, point features have, to our knowledge, still never been used. We believe that this is due to the fact that real-time corner detectors are generally weakly repeatable between a camera image and a rendered texture. In this paper, we compare the repeatability of commonly used FAST, Harris and SURF interest point detectors across view synthesis. We show that adding depth blur to the rendered texture can drastically improve the repeatability of FAST and Harris corner detectors (up to 100% in our experiments), which can be very helpful, e.g., to make tracking-by-synthesis running on mobile phones. We propose a method for simulating depth blur on the rendered images using a pre-calibrated depth response curve. In order to fulfil the performance requirements, a pyramidal approach is used based on the well-known MIP mapping technique. We also propose an original method for calibrating the depth response curve, which is suitable for any kind of focus lenses and comes for free in terms of programming effort, once the tracking-by-synthesis algorithm has been implemented.},
keywords={Cameras;Equations;Lenses;Solid modeling;Detectors;Calibration},
doi={10.1109/ISMAR.2011.6092373},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162876,
author={C. {Coffin} and C. {Lee} and T. {Höllerer}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Evaluating the impact of recovery density on augmented reality tracking},
year={2011},
volume={},
number={},
pages={93-101},
abstract={Natural feature tracking systems for augmented reality are highly accurate, but can suffer from lost tracking. When registration is lost, the system must be able to re-localize and recover tracking. Likewise, when a camera is new to a scene, it must be able to perform the related task of localization. Localization and re-localization can only be performed at certain points or when viewing particular objects or parts of the scene with a sufficient number and quality of recognizable features to allow for tracking recovery. We explore how the density of such recovery locations/poses influences the time it takes users to resume tracking. We focus our evaluation on two generalized techniques for localization: keyframe-based and model-based. For the keyframe-based approach we assume a constant collection rate for keyframes. We find that at practical collection rates, the task of localization to a previously acquired keyframe that is shown to the user does not become more time-consuming as the interval between keyframes increases. For a localization approach using model data, we consider a grid of points around the model at which localization is guaranteed to succeed. We find that the user interface is crucial to successful localization. Localization can occur quickly if users do not need to orient themselves to marked localization points. When users are forced to mentally register themselves with a map of the scene, localization quickly becomes impractical as the distance to the next localization point increases. We contend that our results will help future designers of localization techniques to better plan for the effects of their proposed solutions.},
keywords={Time frequency analysis;Cameras;Tracking;Data models;Cities and towns;Buildings;Augmented reality},
doi={10.1109/ISMAR.2011.6092374},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162877,
author={G. {Bleser} and G. {Hendeby} and M. {Miezal}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Using egocentric vision to achieve robust inertial body tracking under magnetic disturbances},
year={2011},
volume={},
number={},
pages={103-109},
abstract={In the context of a smart user assistance system for industrial manipulation tasks it is necessary to capture motions of the upper body and limbs of the worker in order to derive his or her interactions with the task space. While such capturing technology already exists, the novelty of the proposed work results from the strong requirements of the application context: The method should be flexible and use only on-body sensors, work accurately in industrial environments that suffer from severe magnetic disturbances, and enable consistent registration between the user body frame and the task space. Currently available systems cannot provide this. This paper suggests a novel egocentric solution for visual-inertial upper-body motion tracking based on recursive filtering and model-based sensor fusion. Visual detections of the wrists in the images of a chest-mounted camera are used as substitute for the commonly used magnetometer measurements. The on-body sensor network, the motion capturing system, and the required calibration procedure are described and successful operation is shown in a real industrial environment.},
keywords={Cameras;Magnetometers;Wrist;Magnetic resonance imaging;Sensors;Joints;Tracking},
doi={10.1109/ISMAR.2011.6092528},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162878,
author={D. {Kurz} and S. {Benhimane}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Gravity-aware handheld Augmented Reality},
year={2011},
volume={},
number={},
pages={111-120},
abstract={This paper investigates how different stages in handheld Augmented Reality (AR) applications can benefit from knowing the direction of the gravity measured with inertial sensors. It presents approaches to improve the description and matching of feature points, detection and tracking of planar templates, and the visual quality of the rendering of virtual 3D objects by incorporating the gravity vector. In handheld AR, both the camera and the display are located in the user's hand and therefore can be freely moved. The pose of the camera is generally determined with respect to piecewise planar objects that have a known static orientation with respect to gravity. In the presence of (close to) vertical surfaces, we show how gravity-aligned feature descriptors (GAFD) improve the initialization of tracking algorithms relying on feature point descriptor-based approaches in terms of quality and performance. For (close to) horizontal surfaces, we propose to use the gravity vector to rectify the camera image and detect and describe features in the rectified image. The resulting gravity-rectified feature descriptors (GREFD) provide an improved precision-recall characteristic and enable faster initialization, in particular under steep viewing angles. Gravity-rectified camera images also allow for real-time 6 DoF pose estimation using an edge-based object detection algorithm handling only 4 DoF similarity transforms. Finally, the rendering of virtual 3D objects can be made more realistic and plausible by taking into account the orientation of the gravitational force in addition to the relative pose between the handheld device and a real object.},
keywords={Cameras;Gravity;Vectors;Feature extraction;Sensors;Three dimensional displays;Mobile handsets},
doi={10.1109/ISMAR.2011.6092376},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162879,
author={Y. {Park} and V. {Lepetit} and W. {Woo}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Texture-less object tracking with online training using an RGB-D camera},
year={2011},
volume={},
number={},
pages={121-126},
abstract={We propose a texture-less object detection and 3D tracking method which automatically extracts on the fly the information it needs from color images and the corresponding depth maps. While texture-less 3D tracking is not new, it requires a prior CAD model, and real-time methods for detection still have to be developed for robust tracking. To detect the target, we propose to rely on a fast template-based method, which provides an initial estimate of its 3D pose, and we refine this estimate using the depth and image contours information. We automatically extract a 3D model for the target from the depth information. To this end, we developed methods to enhance the depth map and to stabilize the 3D pose estimation. We demonstrate our method on challenging sequences exhibiting partial occlusions and fast motions.},
keywords={Three dimensional displays;Cameras;Training;US Department of Transportation;Solid modeling;Tracking;Iterative closest point algorithm},
doi={10.1109/ISMAR.2011.6092377},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162880,
author={R. A. {Newcombe} and S. {Izadi} and O. {Hilliges} and D. {Molyneaux} and D. {Kim} and A. J. {Davison} and P. {Kohi} and J. {Shotton} and S. {Hodges} and A. {Fitzgibbon}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={KinectFusion: Real-time dense surface mapping and tracking},
year={2011},
volume={},
number={},
pages={127-136},
abstract={We present a system for accurate real-time mapping of complex and arbitrary indoor scenes in variable lighting conditions, using only a moving low-cost depth camera and commodity graphics hardware. We fuse all of the depth data streamed from a Kinect sensor into a single global implicit surface model of the observed scene in real-time. The current sensor pose is simultaneously obtained by tracking the live depth frame relative to the global model using a coarse-to-fine iterative closest point (ICP) algorithm, which uses all of the observed depth data available. We demonstrate the advantages of tracking against the growing full surface model compared with frame-to-frame tracking, obtaining tracking and mapping results in constant time within room sized scenes with limited drift and high accuracy. We also show both qualitative and quantitative results relating to various aspects of our tracking and mapping system. Modelling of natural scenes, in real-time with only commodity sensor and GPU hardware, promises an exciting step forward in augmented reality (AR), in particular, it allows dense surfaces to be reconstructed in real-time, with a level of detail and robustness beyond any solution yet presented using passive computer vision.},
keywords={Surface reconstruction;Cameras;Image reconstruction;Real time systems;Simultaneous localization and mapping;Iterative closest point algorithm;Three dimensional displays;Real-Time;Dense Reconstruction;Tracking;GPU;SLAM;Depth Cameras;Volumetric Representation;AR},
doi={10.1109/ISMAR.2011.6092378},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162881,
author={A. {Maimone} and H. {Fuchs}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Encumbrance-free telepresence system with real-time 3D capture and display using commodity depth cameras},
year={2011},
volume={},
number={},
pages={137-146},
abstract={This paper introduces a proof-of-concept telepresence system that offers fully dynamic, real-time 3D scene capture and continuous-viewpoint, head-tracked stereo 3D display without requiring the user to wear any tracking or viewing apparatus. We present a complete software and hardware framework for implementing the system, which is based on an array of commodity Microsoft Kinect™color-plus-depth cameras. Novel contributions include an algorithm for merging data between multiple depth cameras and techniques for automatic color calibration and preserving stereo quality even with low rendering rates. Also presented is a solution to the problem of interference that occurs between Kinect cameras with overlapping views. Emphasis is placed on a fully GPU-accelerated data processing and rendering pipeline that can apply hole filling, smoothing, data merger, surface generation, and color correction at rates of up to 100 million triangles/sec on a single PC and graphics board. Also presented is a Kinect-based marker-less tracking system that combines 2D eye recognition with depth information to allow head-tracked stereo views to be rendered for a parallax barrier autostereoscopic display. Our system is affordable and reproducible, offering the opportunity to easily deliver 3D telepresence beyond the researcher's lab.},
keywords={Cameras;Three dimensional displays;Image color analysis;Graphics processing unit;Interference;Rendering (computer graphics);Smoothing methods;teleconferencing;virtual reality;sensor fusion;camera calibration;color calibration;surface fitting;filtering;parallel processing;computer vision;tracking;object recognition;three-dimensional displays},
doi={10.1109/ISMAR.2011.6092379},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162882,
author={S. {Lieberknecht} and A. {Huber} and S. {Ilic} and S. {Benhimane}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={RGB-D camera-based parallel tracking and meshing},
year={2011},
volume={},
number={},
pages={147-155},
abstract={Compared to standard color cameras, RGB-D cameras are designed to additionally provide the depth of imaged pixels which in turn results in a dense colored 3D point cloud representing the environment from a certain viewpoint. We present a real-time tracking method that performs motion estimation of a consumer RGB-D camera with respect to an unknown environment while at the same time reconstructing this environment as a dense textured mesh. Unlike parallel tracking and mapping performed with a standard color or grey scale camera, tracking with an RGB-D camera allows a correctly scaled camera motion estimation. Therefore, there is no need for measuring the environment by any additional tool or equipping the environment by placing objects in it with known sizes. The tracking can be directly started and does not require any preliminary known and/or constrained camera motion. The colored point clouds obtained from every RGB-D image are used to create textured meshes representing the environment from a certain camera view and the real-time estimated camera motion is used to correctly align these meshes over time in order to combine them into a dense reconstruction of the environment. We quantitatively evaluated the proposed method using real image sequences of a challenging scenario and their corresponding ground truth motion obtained with a mechanical measurement arm. We also compared it to a commonly used state-of-the-art method where only the color information is used. We show the superiority of the proposed tracking in terms of accuracy, robustness and usability. We also demonstrate its usage in several Augmented Reality scenarios where the tracking allows a reliable camera motion estimation and the meshing increases the realism of the augmentations by correctly handling their occlusions.},
keywords={Cameras;Tracking;Three dimensional displays;Image reconstruction;Real time systems;Feature extraction;Image color analysis},
doi={10.1109/ISMAR.2011.6092380},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162883,
author={C. {Menk} and R. {Koch}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Interactive visualization technique for truthful color reproduction in spatial augmented reality applications},
year={2011},
volume={},
number={},
pages={157-164},
abstract={Spatial augmented reality is especially interesting for the design process of a car, because a lot of virtual content and corresponding real objects are used. One important issue in such a process is that the designer can trust the visualized colors on the real object, because design decisions are made on basis of the projection. In this article, we present an interactive visualization technique which is able to exactly compute the RGB values for the projected image, so that the resulting colors on the real object are equally perceived as the real desired colors. Our approach computes the influences of the ambient light, the material, the pose and the color model of the projector to the resulting colors of the projected RGB values by using a physically-based computation. This information allows us to compute the adjustment for the RGB values for varying projector positions at interactive rates. Since the amount of projectable colors does not only depend on the material and the ambient light, but also on the pose of the projector, our method can be used to interactively adjust the range of projectable colors by moving the projector to arbitrary positions around the real object. The proposed method is evaluated in a number of experiments.},
keywords={Image color analysis;Three dimensional displays;Table lookup;Materials;Equations;Visualization;Dynamic range},
doi={10.1109/ISMAR.2011.6092381},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162884,
author={M. {Knecht} and C. {Traxler} and W. {Purgathofer} and M. {Wimmer}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Adaptive camera-based color mapping for mixed-reality applications},
year={2011},
volume={},
number={},
pages={165-168},
abstract={We present a novel adaptive color mapping method for virtual objects in mixed-reality environments. In several mixed-reality applications, added virtual objects should be visually indistinguishable from real objects. Recent mixed-reality methods use global-illumination algorithms to approach this goal. However, simulating the light distribution is not enough for visually plausible images. Since the observing camera has its very own transfer function from real-world radiance values to RGB colors, virtual objects look artificial just because their rendered colors do not match with those of the camera. Our approach combines an on-line camera characterization method with a heuristic to map colors of virtual objects to colors as they would be seen by the observing camera. Previous tone-mapping functions were not designed for use in mixed-reality systems and thus did not take the camera-specific behavior into account. In contrast, our method takes the camera into account and thus can also handle changes of its parameters during runtime. The results show that virtual objects look visually more plausible than by just applying tone-mapping operators.},
keywords={Image color analysis;Cameras;Virtual reality;Color;Lighting;Rendering (computer graphics);Visualization;Tone Mapping;Color Matching;Differential Rendering;Mixed Reality},
doi={10.1109/ISMAR.2011.6092382},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162885,
author={S. {Hauswiesner} and M. {Straka} and G. {Reitmayr}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Image-based clothes transfer},
year={2011},
volume={},
number={},
pages={169-172},
abstract={Virtual dressing rooms for the fashion industry and digital entertainment applications aim at creating an image or a video of a user in which he or she wears different garments than in the real world. Such images can be displayed, for example, in a magic mirror shopping application or in games and movies. Current solutions involve the error-prone task of body pose tracking. We suggest an approach that allows users who are captured by a set of cameras to be virtually dressed with previously recorded garments in 3D. By using image-based algorithms, we can bypass critical components of other systems, especially tracking based on skeleton models. We rather transfer the appearance of a garment from one user to another by image processing and image-based rendering. Using images of real garments allows for photo-realistic rendering quality with high performance.},
keywords={Clothing;Databases;Three dimensional displays;Cameras;Rendering (computer graphics);Runtime;Feature extraction;augmented reality;image-based rendering;visual hull;virtual dressing room;CUDA},
doi={10.1109/ISMAR.2011.6092383},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162886,
author={D. {Nowrouzezahrai} and S. {Geiger} and K. {Mitchell} and R. {Sumner} and W. {Jarosz} and M. {Gross}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Light factorization for mixed-frequency shadows in augmented reality},
year={2011},
volume={},
number={},
pages={173-179},
abstract={Integrating animated virtual objects with their surroundings for high-quality augmented reality requires both geometric and radio-metric consistency. We focus on the latter of these problems and present an approach that captures and factorizes external lighting in a manner that allows for realistic relighting of both animated and static virtual objects. Our factorization facilitates a combination of hard and soft shadows, with high-performance, in a manner that is consistent with the surrounding scene lighting.},
keywords={Lighting;Mathematical model;Equations;Geometry;Computational modeling;Shadow mapping;Vectors},
doi={10.1109/ISMAR.2011.6092384},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162887,
author={H. {Álvarez} and I. {Aguinaga} and D. {Borro}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Providing guidance for maintenance operations using automatic markerless Augmented Reality system},
year={2011},
volume={},
number={},
pages={181-190},
abstract={This paper proposes a new real-time Augmented Reality based tool to help in disassembly for maintenance operations. This tool provides workers with augmented instructions to perform maintenance tasks more efficiently. Our prototype is a complete framework characterized by its capability to automatically generate all the necessary data from input based on untextured 3D triangle meshes, without requiring additional user intervention. An automatic offline stage extracts the basic geometric features. These are used during the online stage to compute the camera pose from a monocular image. Thus, we can handle the usual textureless 3D models used in industrial applications. A self-supplied and robust markerless tracking system that combines an edge tracker, a point based tracker and a 3D particle filter has also been designed to continuously update the camera pose. Our framework incorporates an automatic path-planning module. During the offline stage, the assembly/disassembly sequence is automatically deduced from the 3D model geometry. This information is used to generate the disassembly instructions for workers.},
keywords={Three dimensional displays;Junctions;Cameras;Image edge detection;Solid modeling;Feature extraction;Assembly},
doi={10.1109/ISMAR.2011.6092385},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162888,
author={S. J. {Henderson} and S. K. {Feiner}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Augmented reality in the psychomotor phase of a procedural task},
year={2011},
volume={},
number={},
pages={191-200},
abstract={Procedural tasks are common to many domains, ranging from maintenance and repair, to medicine, to the arts. We describe and evaluate a prototype augmented reality (AR) user interface designed to assist users in the relatively under-explored psychomotor phase of procedural tasks. In this phase, the user begins physical manipulations, and thus alters aspects of the underlying task environment. Our prototype tracks the user and multiple components in a typical maintenance assembly task, and provides dynamic, prescriptive, overlaid instructions on a see-through head-worn display in response to the user's ongoing activity. A user study shows participants were able to complete psychomotor aspects of the assembly task significantly faster and with significantly greater accuracy than when using 3D-graphics-based assistance presented on a stationary LCD. Qualitative questionnaire results indicate that participants overwhelmingly preferred the AR condition, and ranked it as more intuitive than the LCD condition.},
keywords={Three dimensional displays;Assembly;Prototypes;Maintenance engineering;Documentation;Combustion;Engines;augmented reality;maintenance;repair;workpiece},
doi={10.1109/ISMAR.2011.6092386},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162889,
author={P. {Fite-Georgel}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Is there a reality in Industrial Augmented Reality?},
year={2011},
volume={},
number={},
pages={201-210},
abstract={In the spirit of the seminal article by Brooks [12] that surveys the field of Virtual Reality to evaluate its level of applicability, we study the readiness of Industrial Augmented Reality (IAR). We have been hearing about IAR since Mizell and Caudell [14] first gave a name to AR, but how many applications broke out of the lab to be used by non-developers? In reviewing the literature, we note the amazing progress made in display technology, rendering and tracking. Given these improvements, one might expect AR-based industrial products to flourish. Unfortunately, this is still not the case. In this paper, we provide a comprehensive and up-to-date survey of industrial AR applications. We organize the different applications of IAR over the life-cycle of products, in order to draw some parallels between the different proposed concepts and offer a clear taxonomy for future applications. We also propose and apply a rubric to evaluate existing IAR systems in order to highlight reasons for success and offer guidelines in the hope that it will help IAR become “really real”.},
keywords={Solid modeling;Welding;Design automation;Maintenance engineering;Production facilities;Assembly;Three dimensional displays},
doi={10.1109/ISMAR.2011.6092387},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162890,
author={A. {Okur} and S. {Ahmadi} and A. {Bigdelou} and T. {Wendler} and N. {Navab}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={MR in OR: First analysis of AR/VR visualization in 100 intra-operative Freehand SPECT acquisitions},
year={2011},
volume={},
number={},
pages={211-218},
abstract={For the past two decades, medical Augmented Reality visualization has been researched and prototype systems have been tested in laboratory setups and limited clinical trials. Up to our knowledge, until now, no commercial system incorporating Augmented Reality visualization has been developed and used routinely within the real-life surgical environment. In this paper, we are reporting on observations and analysis concerning the usage of a commercially developed and clinically approved Freehand SPECT system, which incorporates monitor-based Mixed Reality visualization, during real-life surgeries. The workflow-based analysis we present is focused on an atomic sub-task of sentinel lymph node biopsy. We analyzed the usage of the Augmented and Virtual Reality visualization modes by the surgical team, while leaving the staff completely uninfluenced and unbiased in order to capture the natural interaction with the system. We report on our observations in over 100 Freehand SPECT acquisitions within different phases of 52 surgeries.},
keywords={Surgery;Single photon emission computed tomography;Probes;Three dimensional displays;Image reconstruction;Visualization;Target tracking},
doi={10.1109/ISMAR.2011.6092388},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162891,
author={H. {Regenbrecht} and G. {McGregor} and C. {Ott} and S. {Hoermann} and T. {Schubert} and L. {Hale} and J. {Hoermann} and B. {Dixon} and E. {Franz}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Out of reach? — A novel AR interface approach for motor rehabilitation},
year={2011},
volume={},
number={},
pages={219-228},
abstract={Mixed reality rehabilitation systems and games are demonstrating potential as innovative adjunctive therapies for health professionals in their treatment of various hand and upper limb motor impairments. Unilateral motor deficits of the arm, for example, are commonly experienced post stroke. Our TheraMem system provides an augmented reality game environment that contributes to this increasingly rich area of research. We present a prototype system which “fools the brain” by visually amplifying users' hand movements — small actual hand movements lead to perceived larger movements. We validate the usability of our system in an empirical study with forty-five non-clinical participants. In addition, we present early qualitative evidence for the utility of our approach and system for stroke recovery and motor rehabilitation. Future uses of the system are considered by way of conclusion.},
keywords={Games;Tiles;Thumb;Usability;Three dimensional displays;Cameras;Augmented Reality;Therapy;Physical and Motor Rehabilitation},
doi={10.1109/ISMAR.2011.6092389},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162892,
author={A. {Mulloni} and H. {Seichter} and D. {Schmalstieg}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={User experiences with augmented reality aided navigation on phones},
year={2011},
volume={},
number={},
pages={229-230},
abstract={We investigate user experiences when using augmented reality (AR) as a new aid to navigation. We integrate AR with other more common interfaces into a handheld navigation system, and we conduct an exploratory study to see where and how people exploit AR. Based on previous work on augmented photographs, we hypothesize that AR is used more to support wayfinding at static locations when users approach a road intersection. In partial contrast to this hypothesis, our results from a user evaluation hint that users will expect to use the system while walking. Further, our results also show that AR is usually exploited shortly before and after road intersections, suggesting that tracking support will be mostly needed in proximity of road intersections.},
keywords={Navigation;Legged locomotion;Roads;Augmented reality;Mobile communication;Software;Cameras},
doi={10.1109/ISMAR.2011.6092390},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162893,
author={N. J. {Dedual} and O. {Oda} and S. K. {Feiner}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Creating hybrid user interfaces with a 2D multi-touch tabletop and a 3D see-through head-worn display},
year={2011},
volume={},
number={},
pages={231-232},
abstract={How can multiple different display and interaction devices be used together to create an effective augmented reality environment? We explore the design of several prototype hybrid user interfaces that combine a 2D multi-touch tabletop display with a 3D head-tracked video-see-through display. We describe a simple modeling application and an urban visualization tool in which the information presented on the head-worn display supplements the information displayed on the tabletop, using a variety of approaches to track the head-worn display relative to the tabletop. In all cases, our goal is to allow users who can see only the tabletop to interact effectively with users wearing head-worn displays.},
keywords={Three dimensional displays;Buildings;User interfaces;Solid modeling;Prototypes;Two dimensional displays;Visualization;Augmented reality;hybrid user interface;tabletop interaction;multi-modal interaction;urban visualization},
doi={10.1109/ISMAR.2011.6092391},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162894,
author={J. {Choi} and Y. {Kim} and G. J. {Kim}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Usability of one handed interaction methods for hand-held projection-based augmented reality},
year={2011},
volume={},
number={},
pages={233-234},
abstract={With the advent of portable projectors (also embedded in a smart phone), projection based augmented reality (AR) will be an attractive form of AR as the augmentation is made directly in real space (instead of on the video screen). Several interaction methods for “Procam” systems, also applicable to projection based AR, have been developed, but their comparative usability has not been studied in depth. In this paper, we compare the usability of four representative interaction methods, applied to the menu selection task, for the hand-held projection based AR. The four menu selection methods studied are formed by combinations of two types of cursor control (projector cursor vs. on-device touch screen), and two types of item selection (explicit click vs. crossing). Experimental results have shown that the menu selection task was most efficient, usable and preferred when the projector cursor with the crossing widget was used. Furthermore, the task performance was not statistically different between using the dominant, non-dominant hand and even both hands.},
keywords={Smart phones;Usability;Augmented reality;Educational institutions;Thumb;Prototypes;Augmented Reality;Projection based;Interaction;Menu Selection},
doi={10.1109/ISMAR.2011.6092392},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162895,
author={S. {Shimazu} and D. {Iwai} and K. {Sato}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={3D high dynamic range display system},
year={2011},
volume={},
number={},
pages={235-236},
abstract={This paper introduces a new high dynamic range (HDR) display system that generates a physical 3D HDR image without using stereoscopic methods. To boost contrast beyond that obtained using either a hardcopy or a projector, we employ a multiprojection system to superimpose images onto a textured solid hardcopy that is output by a 3D printer or a rapid prototyping machine. We introduce two basic techniques for our 3D HDR display. The first technique computes an optimal placement of projectors so that projected images cover the hardcopy's entire surface while maximizing image quality. The second technique allows a user to place the projectors near the computed optimal position by projecting from each projector images that act as visual guides. Through proof-of-concept experiments, we were able to modulate luminance and chrominance with a registration error of less than 3 mm. The physical contrast ratio obtained using our method was approximately 5,000:1, while it was 5:1 in the case of viewing the 3D printout under environmental light and 1,000:1 in the case of using the projectors to project the image on regular screens.},
keywords={Three dimensional displays;Printers;Dynamic range;Image color analysis;Image quality;Visualization;Stereo image processing},
doi={10.1109/ISMAR.2011.6092393},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162896,
author={H. {Uchiyama} and E. {Marchand}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Deformable random dot markers},
year={2011},
volume={},
number={},
pages={237-238},
abstract={We extend planar fiducial markers using random dots [8] to nonrigidly deformable markers. Because the recognition and tracking of random dot markers are based on keypoint matching, we can estimate the deformation of the markers with nonrigid surface detection from keypoint correspondences. First, the initial pose of the markers is computed from a homography with RANSAC as a planar detection. Second, deformations are estimated from the minimization of a cost function for deformable surface fitting. We show augmentation results of 2D surface deformation recovery with several markers.},
keywords={Shape;Databases;Augmented reality;Robustness;Surface treatment;Computational efficiency},
doi={10.1109/ISMAR.2011.6092394},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162897,
author={A. {Hill} and J. {Schiefer} and J. {Wilson} and B. {Davidson} and M. {Gandy} and B. {MacIntyre}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Virtual transparency: Introducing parallax view into video see-through AR},
year={2011},
volume={},
number={},
pages={239-240},
abstract={In this poster, we present the idea of “virtual transparency” for video see-through AR. In fully synthetic 3D graphics, head-tracked motion parallax has been shown to be a powerful depth cue for understanding the structure of the virtual world. To leverage head-tracked motion parallax in video see-through AR, the view of the virtual and physical world must change together in response to head motion. We present a system for accomplishing this, and discuss the benefits and limitations of our approach.},
keywords={Cameras;Head;Three dimensional displays;Lenses;Tracking;Augmented reality;augmented reality;motion parallax;head tracking},
doi={10.1109/ISMAR.2011.6092395},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162869,
author={Y. {Wu} and M. E. {Choubassi} and I. {Kozintsev}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Augmenting 3D urban environment using mobile devices},
year={2011},
volume={},
number={},
pages={241-242},
abstract={We describe an augmented reality prototype for exploring a 3D urban environment on mobile devices. Our system utilizes the location and orientation sensors on the mobile platform as well as computer vision techniques to register the live view of the device with the 3D urban data. In particular, the system recognizes the buildings in the live video, tracks the camera pose, and augments the video with relevant information about the buildings in the correct perspective. The 3D urban data consist of 3D point clouds and corresponding geo-tagged RGB images of the urban environment. We also discuss the processing steps to make such 3D data scalable and usable by our system.},
keywords={Three dimensional displays;Buildings;Sensors;Mobile communication;Feature extraction;Cameras;Servers},
doi={10.1109/ISMAR.2011.6092396},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162898,
author={I. {Han} and H. {Kim} and J. {Park}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Graph-cut-based 3D model segmentation for articulated object reconstruction},
year={2011},
volume={},
number={},
pages={243-244},
abstract={The three-dimensional (3D) reconstruction of objects has been well studied in the literature of augmented reality (AR) [1, 2]. Most existing studies have assumed that the to-be-constructed target object is rigid, whereas objects in the real world can be dynamic or deformable. Therefore, AR systems are required to deal with non-rigid objects to be adaptive to environmental changes. In this paper, we address the problem of reconstructing articulated objects as a starting point for modeling deformable objects. An articulated object is composed of partially rigid components linked with joints. After building a mesh model of the object, the model is segmented into the components along their boundaries by a graph-cut-based approach that we propose.},
keywords={Joints;Feature extraction;Three dimensional displays;Image reconstruction;Motion segmentation;Cameras;Tracking},
doi={10.1109/ISMAR.2011.6092397},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162899,
author={G. A. {Lee} and M. {Billinghurst}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={A user study on the Snap-To-Feature interaction method},
year={2011},
volume={},
number={},
pages={245-246},
abstract={Recent advances in mobile computing and augmented reality (AR) technology have lead to popularization of mobile AR applications. Touch screen input is common in mobile devices, and also widely used in mobile AR applications. However, due to unsteady camera view movement, it can be hard to carry out precise interactions in handheld AR environments, for tasks such as tracing physical objects. In this research, we investigate a Snap-To-Feature interaction method that helps users to perform more accurate touch screen interactions by attracting user input points to image features in the AR scene. A user experiment is performed using the method to trace a physical object, which is typical for modeling real objects within the AR scene. The results shows that the Snap-To-Feature method makes a significant difference in the accuracy of touch screen based AR interaction.},
keywords={Augmented reality;Mobile communication;Cameras;Feature extraction;Image edge detection;Accuracy;Augmented reality;touch screen interface;annotation},
doi={10.1109/ISMAR.2011.6092398},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162900,
author={J. {Ventura} and T. {Höllerer}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Outdoor mobile localization from panoramic imagery},
year={2011},
volume={},
number={},
pages={247-248},
abstract={We describe an end-to-end system for mobile, vision-based localization and tracking in urban environments. Our system uses panoramic imagery which is processed and indexed to provide localization coverage over a large area using few capture points. We utilize a client-server model which allows for remote computation and data storage while maintaining real-time tracking performance. Previous search results are cached and re-used by the mobile client to minimize communication overhead. We evaluate the use of the system for flexible real-time camera tracking in large outdoor spaces.},
keywords={Cameras;Servers;Tracking;Mobile communication;Augmented reality;Real time systems;Urban areas},
doi={10.1109/ISMAR.2011.6092399},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162901,
author={P. {Debenham} and G. {Thomas} and J. {Trout}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Evolutionary augmented reality at the Natural History Museum},
year={2011},
volume={},
number={},
pages={249-250},
abstract={In this paper we describe the development of an augmented reality system designed to provide an exciting new way for the Natural History Museum in London to present evolutionary history to their visitors. The system uses a through-the-lens tracker and infrared LED markers to provide an unobtrusive and robust system that can operate for multiple users across a wide area.},
keywords={Cameras;History;Films;Augmented reality;Light emitting diodes;Multimedia communication;Optical sensors;Augmented Reality application;camera tracking},
doi={10.1109/ISMAR.2011.6092400},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162902,
author={M. {Broecker} and R. T. {Smith} and B. H. {Thomas}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Adaptive substrate for enhanced spatial augmented reality contrast and resolution},
year={2011},
volume={},
number={},
pages={251-252},
abstract={This poster presents the concept of combining two display technologies to enhance graphics effects in spatial augmented reality (SAR) environments. This is achieved by using an ePaper surface as an adaptive substrate instead of a white painted surface allowing the development of novel image techniques to improve image quality and object appearance in projector-based SAR environments.},
keywords={Substrates;Surface treatment;Augmented reality;Consumer electronics;Image resolution;Image color analysis;Concrete},
doi={10.1109/ISMAR.2011.6092401},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162903,
author={W. L. D. {Lui} and D. {Browne} and L. {Kleeman} and T. {Drummond} and W. H. {Li}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Transformative reality: Augmented reality for visual prostheses},
year={2011},
volume={},
number={},
pages={253-254},
abstract={Visual prostheses such as retinal implants provide bionic vision that is limited in spatial and intensity resolution. This limitation is a fundamental challenge of bionic vision as it severely truncates salient visual information. We propose to address this challenge by performing real time transformations of visual and non-visual sensor data into symbolic representations that are then rendered as low resolution vision; a concept we call Transformative Reality. For example, a depth camera allows the detection of empty ground in cluttered environments that is then visually rendered as bionic vision to enable indoor navigation. Such symbolic representations are similar to virtual content overlays used in Augmented Reality but are registered to the 3D world via the user's sense of touch. Preliminary user trials, where a head mounted display artificially constrains vision to a 25×25 grid of binary dots, suggest that Transformative Reality provides practical and significant improvements over traditional bionic vision in tasks such as indoor navigation, object localisation and people detection.},
keywords={Visualization;Image edge detection;Biomedical imaging;Rendering (computer graphics);Real time systems;Spatial resolution;Cameras},
doi={10.1109/ISMAR.2011.6092402},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162904,
author={T. {Oskiper} and S. {Samarasekera} and R. {Kumar}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Tightly-coupled robust vision aided inertial navigation algorithm for augmented reality using monocular camera and IMU},
year={2011},
volume={},
number={},
pages={255-256},
abstract={Odometry component of a camera tracking system for augmented reality applications is described. The system uses a MEMS-type inertial measurement unit (IMU) with 3-axis gyroscopes and accelerometers and a monocular camera to accurately and robustly track the camera motion in 6 degrees of freedom (with correct scale) in arbitrary indoor or outdoor scenes. Tight coupling of IMU and camera is achieved by an error-state extended Kalman filter (EKF) which performs sensor fusion for inertial navigation at a deep level such that each visually tracked feature contributes as an individual measurement as opposed to the more traditional approaches where camera pose estimates are first extracted by means of feature tracking and then used as measurement updates in a filter framework. Robustness, on the other hand, is achieved by using a geometric hypothesize-and-test architecture based on the five-point relative pose estimation method, rather than a Mahalanobis distance type gating mechanism derived from the Kalman filter state prediction, to select the inlier tracks and remove outliers from the raw feature point matches which would otherwise corrupt the filter since tracks are directly used as measurements.},
keywords={Cameras;Current measurement;Cloning;Feature extraction;Tracking;Kalman filters;Vectors;MEMS IMU;monocular camera;inertial navigation;sensor fusion;EKF},
doi={10.1109/ISMAR.2011.6143485},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162905,
author={T. {Lee} and S. {Soatto}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Edgel templates for fast planar object detection and pose estimation},
year={2011},
volume={},
number={},
pages={257-258},
abstract={We describe a method to select edgels and to calculate gradient orientation-based template descriptors for edgel features. An edgel is selected within a grid block based on gradient magnitude; its position and orientation are used to determine a canonical frame where the descriptor is computed based on quantized orientation. The resulting descriptor is efficiently matched using logical operations. We demonstrate the use of the resulting edgel detection and description method for planar object detection and pose estimation.},
keywords={Image edge detection;Estimation;Object detection;Augmented reality;Real time systems;Shape;Mobile handsets},
doi={10.1109/ISMAR.2011.6143486},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162906,
author={A. {Clark} and A. {Dünser} and R. {Grasset}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={An interactive augmented reality coloring book},
year={2011},
volume={},
number={},
pages={259-260},
abstract={Creating entertaining and educational books not only requires providing visually stimulating content but also means for students to interact, create, and express themselves. In this paper we present a new type of mixed-reality book experience, which augments an educational coloring book with user-generated three dimensional content. We explore a “pop-up book” metaphor and describe a process by which children's drawing and coloring is used as input to generate and change the appearance of the book content. Our system is based on natural feature tracking and image processing techniques that can be easily exploited for other AR publishing applications.},
keywords={Image color analysis;Books;Augmented reality;Solid modeling;Three dimensional displays;Education;Real time systems;Natural Feature Tracking;Mixed Reality book;interactive AR;edutainment;education;3D texturing},
doi={10.1109/ISMAR.2011.6143487},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162907,
author={P. {Lensing} and W. {Broll}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Fusing the real and the virtual: A depth-camera based approach to Mixed Reality},
year={2011},
volume={},
number={},
pages={261-262},
abstract={The seamless integration of the real and the virtual content is the ultimate yet unreached goal of Mixed Reality applications. Among others it requires mutual blocking and lighting between real and virtual objects. In this paper we present our approach of applying a low-cost depth camera, such as Kinect, allowing for an easy acquisition of depth images. However, as the quality of the raw input data is insufficient for this purpose, we apply a series of filter and optimization operations. This allows us to realize mutual real-time lighting and rigid interaction in a dynamic environment. Our approach produces an acceptable quality of images of low-frequency scenes at interactive frame rates on an off-the-shelf desktop computer.},
keywords={Image edge detection;Image color analysis;Smoothing methods;Lighting;Cameras;Virtual reality;Real time systems;Mixed Reality;Augmented Reality;image based lighting;occlusions;depth camera;Kinect},
doi={10.1109/ISMAR.2011.6143892},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162908,
author={F. {Mannuß} and J. {Rubel} and C. {Wagner} and F. {Bingel} and A. {Hinkenjann}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Augmenting magnetic field lines for school experiments},
year={2011},
volume={},
number={},
pages={263-264},
abstract={We present a system for interactive magnetic field simulation in an AR-setup. The aim of this work is to investigate how AR technology can help to develop a better understanding of the concept of fields and field lines and their relationship to the magnetic forces in typical school experiments. The haptic feedback is provided by real magnets that are optically tracked. In a stereo video see-through head-mounted display, the magnets are augmented with the dynamically computed field lines.},
keywords={Magnetic separation;Magnetic resonance imaging;Magnetomechanical effects;Visualization;Magnetic devices;Educational institutions;Augmented reality},
doi={10.1109/ISMAR.2011.6143893},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162909,
author={H. {Kim} and G. {Reitmayr} and W. {Woo}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Interactive annotation on mobile phones for real and virtual space registration},
year={2011},
volume={},
number={},
pages={265-266},
abstract={Registration of real space and virtual information is a fundamental requirement for any augmented reality system. This paper presents an interactive method to quickly create a 3D room model and annotate locations within the room to provide registration anchors for virtual information. The method operates on a mobile phone and uses a visual rotation tracker to obtain orientation tracking for in-situ applications. The simple interaction allows non-expert users to create models of their environment and thus contribute marked-up representations to an online AR platform.},
keywords={Three dimensional displays;Solid modeling;Mobile handsets;Cameras;Computational modeling;Visualization;Target tracking;Annotation;mobile AR;interactive modeling;visual rotation tracking},
doi={10.1109/ISMAR.2011.6143894},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162910,
author={P. {Maier} and A. {Dey} and C. A. L. {Waechter} and C. {Sandor} and M. {Tönnis} and G. {Klinker}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={An empiric evaluation of confirmation methods for optical see-through head-mounted display calibration},
year={2011},
volume={},
number={},
pages={267-268},
abstract={The calibration of optical see-through head-mounted displays is an important fundament for correct object alignment in augmented reality. Any calibration process for OSTHMDs requires users to align 2D points in screen space with 3D points in the real world and to confirm each alignment. In this poster, we present the results of our empiric evaluation where we compared four confirmation methods: Keyboard, Hand-held, Voice, and Waiting. The Waiting method, designed to reduce head motion during confirmation, showed a significantly higher accuracy than all other methods. Averaging over a time frame for sampling user input before the time of confirmation improved the accuracy of all methods in addition. We conducted a further expert study proving that the results achieved with a video see-through head-mounted display showed valid for optical see-through head-mounted display calibration, too.},
keywords={Calibration;Keyboards;Three dimensional displays;Accuracy;Augmented reality;Electronic mail;Adaptive optics},
doi={10.1109/ISMAR.2011.6143895},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162911,
author={M. {Olbrich} and H. {Wuest} and P. {Riess} and U. {Bockholt}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Augmented reality pipe layout planning in the shipbuilding industry},
year={2011},
volume={},
number={},
pages={269-270},
abstract={As large ships are never produced in masses, it often occurs that the construction process and production process overlap in time. Many shipbuilding companies have problems with discrepancies between the construction data and the real built ship. The assembly department often has to modify CAD data for a successful installation. We present an augmented reality system, where a user can visualize the construction data of pipes and modify these in the case of misalignment, collisions or any other conflicts. The modified pipe geometry can be stored and further used as input for CNC pipe bending machines. To guarantee an exactly orthogonal passage of the pipes through aligning bolt holes, we integrated an optical measurement tool into the pipe alignment process.},
keywords={Three dimensional displays;Augmented reality;Calibration;Image reconstruction;Solid modeling;Design automation;Planning},
doi={10.1109/ISMAR.2011.6143896},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162912,
author={T. {Korah} and Y. {Tsai}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Urban canvas: Unfreezing street-view imagery with semantically compressed LIDAR pointclouds},
year={2011},
volume={},
number={},
pages={271-272},
abstract={Detailed 3D scans of urban environments are increasingly being collected with the goal of bringing more location-aware content to mobile users. This work converts large collections of LIDAR scans and street-view panoramas into a representation that extracts semantically meaningful components of the scene. Compressing this data by an order of magnitude or more enables rich user interactions with mobile applications that have a very good knowledge of the scene around them. These representations are suitable for integrating into physics engines and transmission over mobile networks — key components of modern AR entertainment solutions.},
keywords={Three dimensional displays;Laser radar;Physics;Lighting;Geometry;Mobile communication;Strips},
doi={10.1109/ISMAR.2011.6143897},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162913,
author={J. {Wither} and S. {White} and R. {Azuma}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Comparing spatial understanding between touch-based and AR-style interaction},
year={2011},
volume={},
number={},
pages={273-274},
abstract={There are currently two primary ways of viewing location specific information in-situ on hand-held mobile device screens: using a see-through augmented reality interface and using a touch-based interface with panoramas. The two approaches use fundamentally different interaction metaphors: an AR-style of interacting where the user holds up the device and physically moves it to change views of the world, and a touch-based technique where panorama navigation is independent of the physical world. We have investigated how this difference in interaction technique impacts a user's spatial understanding of the mixed reality world. Our study found that AR-style interaction provided better spatial understanding overall, while touch-based interaction changed the experience to have more similar characteristics to interaction in a separate virtual environment.},
keywords={Visualization;Virtual environments;Analysis of variance;Green products;Cameras;Augmented reality;Estimation},
doi={10.1109/ISMAR.2011.6143898},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162914,
author={J. {Choi} and H. {Park} and J. {Park} and J. {Park}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Bare-hand-based augmented reality interface on mobile phone},
year={2011},
volume={},
number={},
pages={275-276},
abstract={This paper proposes an augmented reality interface that provides natural hand-based interaction with virtual objects on mobile phones. Assume that one holds a mobile phone in a hand and sees the other hand through mobile phone's camera. Then, a virtual object is rendered on his/her palm and reacts to hand and finger movements. Since the proposed interface does not require any additional sensors or markers, one freely interacts with the virtual object anytime and anywhere. The proposed interface worked at 5 fps on a mobile phone (Galaxy S2 having a dual-core processor).},
keywords={Thumb;Mobile handsets;Cameras;Mobile communication;Augmented reality;Shape;Interaction techniques for MR/AR;MR/AR for entertainment;vision-based registration and tracking},
doi={10.1109/ISMAR.2011.6143899},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162915,
author={G. {Yamamoto} and I. {Kanaya} and K. {Yamamoto} and Y. {Uranishi} and H. {Kato}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Visualization of geometric properties of flexible objects for form designing},
year={2011},
volume={},
number={},
pages={277-278},
abstract={Computer-aided design (CAD) system conventionally have been widely used to support designers for creating, modifying, adding something to or removing something from objects by showing simulated objects on computer screen. These virtual, non-physical, objects have been, however, known as imperfect imitation of reality. The impression of shape is highly related to the second order derivative of geometric feature of the shape. Conventional CAD systems, including AutoCAD, usually have visualization feature of the first derivative (normal) and the second derivative (curvature) of given surfaces. There, however, still have been problems in curvature visualization on the screen. First, it lacks true feeling of physical objects. Second, even if designers were given a physical mock-up object in hand, they wouldn't precisely recognize minute change of curvatures — few designers can sense small differences of curvature and most others need a special device to check the curvature. For solving these problem, the authors propose a novel curvature visualization system based on mixed reality technology. The color mapping according to the Gaussian curvature calculated via a time-of-flight camera provides the observers with intuitively understanding the object's curvature information.},
keywords={Cameras;Visualization;Design automation;Shape;Noise;Computers;Observers},
doi={10.1109/ISMAR.2011.6143900},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162916,
author={D. {Weng} and W. {Xu} and D. {Li} and Y. {Wang} and Y. {Liu}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={“Soul Hunter”: A novel augmented reality application in theme parks},
year={2011},
volume={},
number={},
pages={279-280},
abstract={This paper introduces a novel augmented reality shooting game named “Soul Hunter”, which has been successfully operating in a theme park in China. Soul Hunter adopts an innovative infrared marker scheme to build a mobile augmented reality application in a wide area. It is an extension of the traditional first person game, in which a player is able to fight with virtual ghost through a gunlike device in real environment. This paper describes the challenges of applying augmented reality in theme parks and shares some experiences in solving the problems encountered in practical applications.},
keywords={Games;Augmented reality;Rendering (computer graphics);Painting;Lighting;Cameras;Radiofrequency identification;Augment reality;Infrared markers;Shooting game},
doi={10.1109/ISMAR.2011.6143901},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6162917,
author={D. {Matsuda} and K. {Uemura} and N. {Sakata} and S. {Nishida}},
booktitle={2011 10th IEEE International Symposium on Mixed and Augmented Reality}, title={Toe input with mobile projector and depth camera},
year={2011},
volume={},
number={},
pages={281-282},
abstract={With the miniaturization of mobile projectors, we can provide a larger projection surface for information browsing, despite the small size of the projection devices. Moreover, the opportunities for using location-based information services both indoors and outdoors increase. We can obtain the necessary information via the small LCDs of handheld devices, thanks to the prevalence of cell phones and GPS technology. However, these mobile terminal devices restrict the use of one hand and demand that the user keeps a close watch on the small display. It is necessary to take the devices out of a pocket or bag. To solve these problems, many researchers have focused on wearable projection systems. These systems allow for the provision of hands-free viewing via large projected screens and eliminate the need to take out and hold devices. In this paper, we propose a “Toe Input” method, which can realize haptic interaction, direct manipulation, and floor projection in wearable projection systems with a slightly larger projection surface. We evaluate the system in terms of accuracy, required time, comfort, and area suitable for input.},
keywords={Floors;Mobile communication;Educational institutions;Electronic mail;Information services;Accuracy;Foot;wearable;projection;input interface;depth camera},
doi={10.1109/ISMAR.2011.6143902},
ISSN={},
month={Oct},}