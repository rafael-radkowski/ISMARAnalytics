@INPROCEEDINGS{6948386,
author={},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Title page]},
year={2014},
volume={},
number={},
pages={1-1},
abstract={Presents the title page of the proceedings record.},
keywords={},
doi={10.1109/ISMAR.2014.6948386},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948387,
author={},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Copyright notice]},
year={2014},
volume={},
number={},
pages={1-1},
abstract={Presents the copyright information for the conference. May include reprint permission information.},
keywords={},
doi={10.1109/ISMAR.2014.6948387},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948388,
author={},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Table of contents},
year={2014},
volume={},
number={},
pages={1-7},
abstract={The following topics are dealt with: mixed reality; augmented reality; rendering; user interfaces; image processing; video signal processing; and helmet mounted displays.},
keywords={augmented reality;helmet mounted displays;image processing;rendering (computer graphics);user interfaces;mixed reality;augmented reality;rendering;user interfaces;image processing;video signal processing;helmet mounted displays},
doi={10.1109/ISMAR.2014.6948388},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948389,
author={},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Message from the general chairs},
year={2014},
volume={},
number={},
pages={x-x},
abstract={Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.},
keywords={},
doi={10.1109/ISMAR.2014.6948389},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948390,
author={},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Message from the science technology paper chairs},
year={2014},
volume={},
number={},
pages={xi-xii},
abstract={Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.},
keywords={},
doi={10.1109/ISMAR.2014.6948390},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948391,
author={},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Message from the poster chairs},
year={2014},
volume={},
number={},
pages={xiii-xiii},
abstract={Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.},
keywords={},
doi={10.1109/ISMAR.2014.6948391},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948392,
author={},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Message from the demo chairs},
year={2014},
volume={},
number={},
pages={xiv-xiv},
abstract={Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.},
keywords={},
doi={10.1109/ISMAR.2014.6948392},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948393,
author={},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Message from the doctoral consortium chairs},
year={2014},
volume={},
number={},
pages={xv-xv},
abstract={Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.},
keywords={},
doi={10.1109/ISMAR.2014.6948393},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948394,
author={},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={IEEE Visualization and Graphics Technical Committee (VGTC)},
year={2014},
volume={},
number={},
pages={xvi-xvi},
abstract={Provides a listing of current committee members and society officers.},
keywords={},
doi={10.1109/ISMAR.2014.6948394},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948395,
author={},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Symposium committee},
year={2014},
volume={},
number={},
pages={xvii-xvii},
abstract={Provides a listing of current committee members and society officers.},
keywords={},
doi={10.1109/ISMAR.2014.6948395},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948396,
author={},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Steering committee},
year={2014},
volume={},
number={},
pages={xviii-xviii},
abstract={Provides a listing of current committee members and society officers.},
keywords={},
doi={10.1109/ISMAR.2014.6948396},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948397,
author={},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Reviewers},
year={2014},
volume={},
number={},
pages={xix-xix},
abstract={The conference offers a note of thanks and lists its reviewers.},
keywords={},
doi={10.1109/ISMAR.2014.6948397},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948398,
author={T. {Kanade}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Keynote address: Smart Headlight: A new active augmented reality that improves how the reality appears to a human},
year={2014},
volume={},
number={},
pages={xx-xx},
abstract={Summary form only given. A combination of computer vision and projector-based illumination opens the possibility for a new type of augmented reality: selectively illuminating the scene to improve or manipulate how the reality itself, rather than its display, appears to a human. One such example is the Smart Headlight being developed at Carnegie Mellon University's Robotics Institute. The project team has been working on a new set of capabilities for the headlight, such as making rain drops and snowfakes disappear, allowing for the high beams to always be on without glare, and enhancing the appearance of objects of interest. This talk will present the idea, approach, and current status of the Smart Headlight Project.},
keywords={augmented reality;computer vision;lighting;snowfakes;rain drops;projector-based illumination;computer vision;active augmented reality;smart headlight},
doi={10.1109/ISMAR.2014.6948398},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948399,
author={T. {Peters}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Keynote address: The role of augmented reality displays for guiding intra-cardiac interventions},
year={2014},
volume={},
number={},
pages={xxi-xxi},
abstract={Summary form only given. Many inter-cardiac interventions are performed either via open-heart surgery, or using minimally invasive approaches, where instrumentation is introduced into the cardiac chambers via the vascular system or heart wall. While many of the latter procedures are often employed under x-ray guidance, for some of these xray imaging is not appropriate, and ultrasound is the preferred intra-operative imaging modality. Two such procedures involves the repair of a mitral-valve leafet, and the replacement of aortic valves. Both employ instruments introduced into the heart via the apex. For the mitral procedure, the standard of care for this procedure employs a 3D Trans-esophageal echo (TEE) probe as guidance, but using primarily its bi-plane mode, with full 3D only being used sporadically. In spite of the clinical success of this procedure, many problems are encountered during the navigation of the instrument to the site of the therapy. To overcome these difficulties, we have developed a guidance platform that tracks the US probe and instrument, and augments the US mages with virtual elements representing the instrument and target, to optimise the navigation process. Results of using this approach on animal studies have demonstrated increased performance in multiple metrics, including total tool distance from ideal pathway, total navigation time, and total tool path lengths, by factors of 3,4, and 5 respectively, as well as a 40 fold reduction in the number of times an instrument intruded into potentially unsafe zones in the heart.},
keywords={augmented reality;biomedical ultrasonics;cardiology;medical image processing;surgery;bi-plane mode;mitral procedure;TEE probe;3D trans-esophageal echo;mitral-valve leafet;imaging modality;ultrasound;X-ray guidance;minimally invasive approach;open-heart surgery;intra-cardiac intervention guidance;augmented reality display},
doi={10.1109/ISMAR.2014.6948399},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948400,
author={T. {Furness}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Keynote Address: Seeing Anew: Paradigm shifting across the virtuality continuum},
year={2014},
volume={},
number={},
pages={xxii-xxii},
abstract={As an original pioneer of Virtual Reality, Tom Furness, founder of the international network of HIT Labs, presents both the legacy and future of Mixed and Augmented Reality through innovations that unlock and link minds. Inventing some of the original inspirations that led to the Google Glass and Oculus Rift, his keynote will inspire making creative leaps to realize the promise of research. Since the 1960's, his career has been about how to achieve radical innovation through helping others to “see anew.” In inspiring a paradigm shift in thinking about the virtuality continuum, his work and global network of labs has shifted the thinking from being about function to purpose; perception to performance; from capabilities to possibilities. “The virtuality continuum for me has been really about really bridging between the wonders of the real world and the infnite possibilities of the human imagination.” As Mixed and Augmented Reality innovation begins to transform everyday life, Dr. Furness presents the future role of trans disciplinary research and curriculum in melting the boundaries between Science, Technology, Media, Art, Humanities and Design to teach the next generation of innovators. He shares his future venture of the Virtual World Society that is creating a collaborative initiative of experiential learning, using virtual, mixed and augmented reality as a platform to empower youth to envision and create a better world for tomorrow.},
keywords={},
doi={10.1109/ISMAR.2014.6948400},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948401,
author={},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Supporting organizations},
year={2014},
volume={},
number={},
pages={xxiii-xxiii},
abstract={The conference organizers greatly appreciate the support of the various corporate sponsors listed.},
keywords={},
doi={10.1109/ISMAR.2014.6948401},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948402,
author={Q. {Rao} and T. {Tropper} and C. {Grünler} and M. {Hammori} and S. {Chakraborty}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={AR-IVI — Implementation of In-Vehicle Augmented Reality},
year={2014},
volume={},
number={},
pages={3-8},
abstract={In the last three years, a number of automotive Augmented Reality (AR) concepts and demonstrators have been presented, all looking for an interpretation of what AR in a car may look like. In October 2013, Mercedes-Benz exhibited to a public audience the AR In-Vehicle Infotainment (AR-IVI) system aimed at defining an overall in-vehicle electric/electronic (E/E) architecture for augmented reality rather than showing specific use cases. In this paper, we explain the requirements and design decisions that lead to the system-design, and we share the challenges and experiences in developing the AR-IVI system in the prototype vehicle. Based on our experiences, we give an outlook on future software and E/E architectural challenges of in-vehicle augmented reality.},
keywords={augmented reality;automotive electronics;electronic engineering computing;in-vehicle augmented reality;AR-IVI system;AR in-vehicle infotainment system;Mercedes-Benz;in-vehicle electric-electronic architecture;Vehicles;Augmented reality;Sensors;Computer architecture;Automotive engineering;Global Positioning System;Roads;H.5.1 [Information Interfaces and Presentation];Multimedia Information Systems — Artificial, augmented, virtual realities},
doi={10.1109/ISMAR.2014.6948402},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948403,
author={D. {Kurz}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Thermal touch: Thermography-enabled everywhere touch interfaces for mobile augmented reality applications},
year={2014},
volume={},
number={},
pages={9-16},
abstract={We present an approach that makes any real object a true touch interface for mobile Augmented Reality applications. Using infrared thermography, we detect residual heat resulting from a warm fingertip touching the colder surface of an object. This approach can clearly distinguish if a surface has actually been touched, or if a finger only approached it without any physical contact, and hence significantly less heat transfer. Once a touch has been detected in the thermal image, we determine the corresponding 3D position on the touched object based on visual object tracking using a visible light camera. Finally the 3D position of the touch is used by human machine interfaces for Augmented Reality providing natural means to interact with real and virtual objects. The emergence of wearable computers and head-mounted displays desires for alternatives to a touch screen, which is the primary user interface in handheld Augmented Reality applications. Voice control and touchpads provide a useful alternative to interact with wearables for certain tasks, but particularly common interaction tasks in Augmented Reality require to accurately select or define 3D points on real surfaces. We propose to enable this kind of interaction by simply touching the respective surface with a fingertip. Based on tests with a variety of different materials and different users, we show that our method enables intuitive interaction for mobile Augmented Reality with most common objects.},
keywords={augmented reality;mobile computing;object tracking;user interfaces;thermal touch;thermography-enabled everywhere touch interfaces;mobile augmented reality application;infrared thermography;residual heat detection;warm fingertip;heat transfer;visual object tracking;visible light camera;wearable computers;head-mounted displays;voice control;touchpads;Cameras;Three-dimensional displays;Temperature measurement;Materials;Augmented reality;User interfaces;Heating;H.5.2 [User Interfaces]: Input devices and strategies — Graphical user interfaces;H.5.1 [Multimedia Information Systems];Artificial, augmented, virtual realities — Evaluation/methodology},
doi={10.1109/ISMAR.2014.6948403},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948404,
author={Z. {Zhu} and V. {Branzoi} and M. {Wolverton} and G. {Murray} and N. {Vitovitch} and L. {Yarnall} and G. {Acharya} and S. {Samarasekera} and R. {Kumar}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={AR-mentor: Augmented reality based mentoring system},
year={2014},
volume={},
number={},
pages={17-22},
abstract={AR-Mentor is a wearable real time Augmented Reality (AR) mentoring system that is configured to assist in maintenance and repair tasks of complex machinery, such as vehicles, appliances, and industrial machinery. The system combines a wearable Optical-See-Through (OST) display device with high precision 6-Degree-Of-Freedom (DOF) pose tracking and a virtual personal assistant (VPA) with natural language, verbal conversational interaction, providing guidance to the user in the form of visual, audio and locational cues. The system is designed to be heads-up and hands-free allowing the user to freely move about the maintenance or training environment and receive globally aligned and context aware visual and audio instructions (animations, symbolic icons, text, multimedia content, speech). The user can interact with the system, ask questions and get clarifications and specific guidance for the task at hand. A pilot application with AR-Mentor was successfully built to instruct a novice to perform an advanced 33-step maintenance task on a training vehicle. The initial live training tests demonstrate that AR-Mentor is able to help and serve as an assistant to an instructor, freeing him/her to cover more students and to focus on higher-order teaching.},
keywords={augmented reality;computer aided instruction;computer displays;AR-mentor system;augmented reality based mentoring system;wearable realtime augmented reality;machinery maintenance task;machinery repair task;wearable optical-see-through display device;OST display device;pose tracking;virtual personal assistant;VPA;natural language;verbal conversational interaction;user guidance;visual cue;audio cue;locational cue;user interaction;higher-order teaching;Vehicles;Training;Maintenance engineering;Visualization;Three-dimensional displays;Databases;Speech;Mentoring System;Wearable Technology;Virtual Personal Assistant;Optical See Through Glasses},
doi={10.1109/ISMAR.2014.6948404},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948405,
author={M. {Krichenbauer} and G. {Yamamoto} and T. {Taketomi} and C. {Sandor} and H. {Kato}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Towards Augmented Reality user interfaces in 3D media production},
year={2014},
volume={},
number={},
pages={23-28},
abstract={The idea of using Augmented Reality (AR) user interfaces (UIs) to create 3D media content, such as 3D models for movies and games has been repeatedly suggested over the last decade. Even though the concept is intuitively compelling and recent technological advances have made such an application increasingly feasible, very little progress has been made towards an actual real-world application of AR in professional media production. To this day, no immersive 3D UI has been commonly used by professionals for 3D computer graphics (CG) content creation. In this paper, we are first to publish a requirements analysis for our target application in the professional domain. Based on a survey that we conducted with media professionals, the analysis of professional 3D CG software, and professional training tutorials, we identify these requirements and put them into the context of AR UIs. From these findings, we derive several interaction design principles that aim to address the challenges of real-world application of AR to the production pipeline. We implemented these in our own prototype system while receiving feedback from media professionals. The insights gained in the survey, requirements analysis, and user interface design are relevant for research and development aimed at creating production methods for 3D media production.},
keywords={augmented reality;solid modelling;user interfaces;augmented reality user interfaces;3D media production;UI;3D media content;3D models;professional media production;3D computer graphics;CG content creation;requirements analysis;professional 3D CG software;professional training tutorials;Three-dimensional displays;Collaboration;Software;Media;Production;Solid modeling;Navigation;Augmented Reality;Immersive Authoring},
doi={10.1109/ISMAR.2014.6948405},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948406,
author={K. {Rohmer} and W. {Büschel} and R. {Dachselt} and T. {Grosch}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Interactive near-field illumination for photorealistic augmented reality on mobile devices},
year={2014},
volume={},
number={},
pages={29-38},
abstract={Mobile devices become more and more important today, especially for augmented reality (AR) applications in which the camera of the mobile device acts like a window into the mixed reality world. Up to now, no photorealistic augmentation is possible since the computational power of the mobile devices is still too weak. Even a streaming solution from a stationary PC would cause a latency that affects user interactions considerably. Therefore, we introduce a differential illumination method that allows for a consistent illumination of the inserted virtual objects on mobile devices, avoiding a delay. The necessary computation effort is shared between a stationary PC and the mobile devices to make use of the capacities available on both sides. The method is designed such that only a minimum amount of data has to be transferred asynchronously between the stationary PC and one or multiple mobile devices. This allows for an interactive illumination of virtual objects with a consistent appearance under both temporally and spatially varying real illumination conditions. To describe the complex near-field illumination in an indoor scenario, multiple HDR video cameras are used to capture the illumination from multiple directions. In this way, sources of illumination can be considered that are not directly visible to the mobile device because of occlusions and the limited field of view of built-in cameras.},
keywords={augmented reality;lighting;mobile computing;interactive near-field illumination;photorealistic augmented reality;mobile device;AR application;mixed reality;differential illumination method;virtual objects;necessary computation effort;illumination condition;illumination source;Lighting;Cameras;Mobile handsets;Rendering (computer graphics);Image reconstruction;Light sources;Streaming media;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism H.5.1 [Information Interfaces and Representation];Artificial;Augmented and Virtual Realities},
doi={10.1109/ISMAR.2014.6948406},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948407,
author={T. A. {Franke}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Delta Voxel Cone Tracing},
year={2014},
volume={},
number={},
pages={39-44},
abstract={Mixed reality applications which must provide visual coherence between synthetic and real objects need relighting solutions for both: synthetic objects have to match lighting conditions of their real counterparts, while real surfaces need to account for the change in illumination introduced by the presence of an additional synthetic object. In this paper we present a novel relighting solution called Delta Voxel Cone Tracing to compute both direct shadows and first bounce mutual indirect illumination. We introduce a voxelized, pre-filtered representation of the combined real and synthetic surfaces together with the extracted illumination difference due to the augmentation. In a final gathering step this representation is cone-traced and superimposed onto both types of surfaces, adding additional light from indirect bounces and synthetic shadows from anti-radiance present in the volume. The algorithm computes results at interactive rates, is temporally coherent and to our knowledge provides the first real-time rasterizer solution for mutual diffuse, glossy and perfect specular indirect reflections between synthetic and real surfaces in mixed reality.},
keywords={augmented reality;lighting;delta voxel cone tracing;mixed reality application;visual coherence;relighting solutions;lighting conditions;direct shadows;first bounce mutual indirect illumination;indirect reflections;mutual diffuse reflections;glossy reflections;Lighting;Image reconstruction;Light sources;Geometry;Rough surfaces;Surface roughness;Virtual reality;Mixed Reality;Real-time Global Illumination;Relighting;Voxel Cone Tracing;Delta Radiance Fields},
doi={10.1109/ISMAR.2014.6948407},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948408,
author={C. {McCarthy} and N. {Barnes}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Importance weighted image enhancement for prosthetic vision: An augmentation framework},
year={2014},
volume={},
number={},
pages={45-51},
abstract={Augmentations to enhance perception in prosthetic vision (also known as bionic eyes) have the potential to improve functional outcomes significantly for implantees. In current (and near-term) im-plantable electrode arrays resolution and dynamic range are highly constrained in comparison to images from modern cameras that can be head mounted. In this paper, we propose a novel, generally applicable adaptive contrast augmentation framework for prosthetic vision that addresses the specific perceptual needs of low resolution and low dynamic range displays. The scheme accepts an externally defined pixel-wise weighting of importance describing features of the image to enhance in the output dynamic range. Our approach explicitly incorporates the logarithmic scaling of enhancement required in human visual perception to ensure perceivability of all contrast augmentations. It requires no pre-existing contrast, and thus extends previous work in local contrast enhancement to a formulation for general image augmentation. We demonstrate the generality of our augmentation scheme for scene structure and looming object enhancement using simulated prosthetic vision.},
keywords={computer vision;image enhancement;medical image processing;prosthetics;importance weighted image enhancement;prosthetic vision;augmentation framework;bionic eyes;implantable electrode arrays resolution;pixel-wise weighting;enhancement scaling;scene structure;looming object enhancement;Dynamic range;Prosthetics;Image resolution;Visualization;Brightness;Histograms;Equations},
doi={10.1109/ISMAR.2014.6948408},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948409,
author={A. {Meshram} and R. {Mehra} and H. {Yang} and E. {Dunn} and J. {Franm} and D. {Manocha}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={P-HRTF: Efficient personalized HRTF computation for high-fidelity spatial sound},
year={2014},
volume={},
number={},
pages={53-61},
abstract={Accurate rendering of 3D spatial audio for interactive virtual auditory displays requires the use of personalized head-related transfer functions (HRTFs). We present a new approach to compute personalized HRTFs for any individual using a method that combines state-of-the-art image-based 3D modeling with an efficient numerical simulation pipeline. Our 3D modeling framework enables capture of the listener's head and torso using consumer-grade digital cameras to estimate a high-resolution non-parametric surface representation of the head, including the extended vicinity of the listener's ear. We leverage sparse structure from motion and dense surface reconstruction techniques to generate a 3D mesh. This mesh is used as input to a numeric sound propagation solver, which uses acoustic reciprocity and Kirchhoff surface integral representation to efficiently compute an individual's personalized HRTF. The overall computation takes tens of minutes on multi-core desktop machine. We have used our approach to compute the personalized HRTFs of few individuals, and we present our preliminary evaluation here. To the best of our knowledge, this is the first commodity technique that can be used to compute personalized HRTFs in a lab or home setting.},
keywords={image motion analysis;image reconstruction;image representation;multiprocessing systems;numerical analysis;rendering (computer graphics);solid modelling;P-HRTF;personalized head-related transfer functions;high-fidelity spatial sound;3D spatial audio rendering;interactive virtual auditory displays;image-based 3D modeling;numerical simulation pipeline;nonparametric surface representation;sparse structure from motion technique;dense surface reconstruction technique;3D mesh generation;numeric sound propagation solver;acoustic reciprocity;Kirchhoff surface integral representation;multicore desktop machine;commodity technique;Three-dimensional displays;Computational modeling;Ear;Numerical models;Pipelines;Solid modeling},
doi={10.1109/ISMAR.2014.6948409},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948410,
author={T. {Fukiage} and T. {Oishi} and K. {Ikeuchi}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Visibility-based blending for real-time applications},
year={2014},
volume={},
number={},
pages={63-72},
abstract={There are many situations in which virtual objects are presented half-transparently on a background in real time applications. In such cases, we often want to show the object with constant visibility. However, using the conventional alpha blending, visibility of a blended object substantially varies depending on colors, textures, and structures of the background scene. To overcome this problem, we present a framework for blending images based on a subjective metric of visibility. In our method, a blending parameter is locally and adaptively optimized so that visibility of each location achieves the targeted level. To predict visibility of an object blended by an arbitrary parameter, we utilize one of the error visibility metrics that have been developed for image quality assessment. In this study, we demonstrated that the metric we used can linearly predict visibility of a blended pattern on various texture images, and showed that the proposed blending methods can work in practical situations assuming augmented reality.},
keywords={augmented reality;image processing;rendering (computer graphics);visibility-based blending;virtual objects;alpha blending;blended object visibility;blending parameter;image blending;error visibility metrics;image quality assessment;augmented reality;blending;Image color analysis;Computational modeling;Predictive models;Sensitivity;Visualization;Transforms;Visibility;Human visual system model;Blending},
doi={10.1109/ISMAR.2014.6948410},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948411,
author={T. {Piumsomboon} and D. {Altimira} and H. {Kim} and A. {Clark} and G. {Lee} and M. {Billinghurst}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Grasp-Shell vs gesture-speech: A comparison of direct and indirect natural interaction techniques in augmented reality},
year={2014},
volume={},
number={},
pages={73-82},
abstract={In order for natural interaction in Augmented Reality (AR) to become widely adopted, the techniques used need to be shown to support precise interaction, and the gestures used proven to be easy to understand and perform. Recent research has explored free-hand gesture interaction with AR interfaces, but there have been few formal evaluations conducted with such systems. In this paper we introduce and evaluate two natural interaction techniques: the free-hand gesture based Grasp-Shell, which provides direct physical manipulation of virtual content; and the multi-modal Gesture-Speech, which combines speech and gesture for indirect natural interaction. These techniques support object selection, 6 degree of freedom movement, uniform scaling, as well as physics-based interaction such as pushing and flinging. We conducted a study evaluating and comparing Grasp-Shell and Gesture-Speech for fundamental manipulation tasks. The results show that Grasp-Shell outperforms Gesture-Speech in both efficiency and user preference for translation and rotation tasks, while Gesture-Speech is better for uniform scaling. They could be good complementary interaction methods in a physics-enabled AR environment, as this combination potentially provides both control and interactivity in one interface. We conclude by discussing implications and future directions of this research.},
keywords={augmented reality;user interfaces;grasp-shell technique;gesture-speech technique;natural interaction techniques;augmented reality;AR;free-hand gesture interaction;virtual content manipulation;object selection;physics-based interaction;pushing interaction;flinging interaction;user preference;translation task;rotation task;Speech;Cameras;Thumb;Kinematics;Grasping;Three-dimensional displays;Shape;Augmented reality;natural interaction;multi-modal interface},
doi={10.1109/ISMAR.2014.6948411},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948412,
author={S. {Kim} and G. {Lee} and N. {Sakata} and M. {Billinghurst}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Improving co-presence with augmented visual communication cues for sharing experience through video conference},
year={2014},
volume={},
number={},
pages={83-92},
abstract={Video conferencing is becoming more widely used in areas other than face-to-face conversation, such as sharing real world experience with remote friends or family. In this paper we explore how adding augmented visual communication cues can improve the experience of sharing remote task space and collaborating together. We developed a prototype system that allows users to share live video view of their task space taken on a Head Mounted Display (HMD) or Handheld Display (HHD), and communicate through not only voice but also using augmented pointer or annotations drawn on the shared view. To explore the effect of having such an interface for remote collaboration, we conducted a user study comparing three video-conferencing conditions with different combination of communication cues: (1) voice only, (2) voice + pointer, and (3) voice + annotation. The participants used our remote collaboration system to share a parallel experience of puzzle solving in the user study, and we found that adding augmented visual cues significantly improved the sense of being together. The pointer was the most preferred additional cue by users for parallel experience, and there were different states of the users' behavior found in remote collaboration.},
keywords={augmented reality;helmet mounted displays;telecommunication computing;teleconferencing;video communication;augmented visual communication cue;video conference;face-to-face conversation;remote task space sharing;live video sharing;head mounted display;HMD;handheld display;HHD;voice only cue;voice plus pointer cue;voice plus annotation cue;parallel experience;user behavior;Prototypes;Collaboration;Visual communication;Streaming media;Cameras;Visualization;Tracking;Video Conferencing;Augmented Reality},
doi={10.1109/ISMAR.2014.6948412},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948413,
author={M. {Berning} and D. {Kleinert} and T. {Riedel} and M. {Beigl}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={A study of depth perception in hand-held augmented reality using autostereoscopic displays},
year={2014},
volume={},
number={},
pages={93-98},
abstract={Displaying three-dimensional content on a flat display is bound to reduce the impression of depth, particularly for mobile video see-trough augmented reality. Several applications in this domain can benefit from accurate depth perception, especially if there are contradictory depth cues, like occlusion in a x-ray visualization. The use of stereoscopy for this effect is already prevalent in head-mounted displays, but there is little research on the applicability for hand-held augmented reality. We have implemented such a prototype using an off-the-shelf smartphone equipped with a stereo camera and an autostereoscopic display. We designed and conducted an extensive user study to explore the effects of stereoscopic hand-held augmented reality on depth perception. The results show that in this scenario depth judgment is mostly influenced by monoscopic depth cues, but our system can improve positioning accuracy in challenging scenes.},
keywords={augmented reality;computer displays;depth perception;handheld augmented reality;autostereoscopic displays;three-dimensional content;mobile video see-trough augmented reality;depth cue;occlusion;X-ray visualization;stereo camera;Stereo image processing;Cameras;Augmented reality;Accuracy;Visualization;Hardware;Three-dimensional displays;Autostereoscopy;mobile devices;depth perception;augmented reality;user study},
doi={10.1109/ISMAR.2014.6948413},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948414,
author={G. {Hough} and I. {Williams} and C. {Athwal}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Measurements of live actor motion in mixed reality interaction},
year={2014},
volume={},
number={},
pages={99-104},
abstract={This paper presents a method for measuring the magnitude and impact of errors in mixed reality interactions. We define the errors as measurements of hand placement accuracy and consistency within bimanual movement of an interactive virtual object. First, a study is presented which illustrates the amount of variability between the hands and the mean distance of the hands from the surfaces of a common virtual object. The results allow a discussion of the most significant factors which should be considered in the frame of developing realistic mixed reality interaction systems. The degree of error was found to be independent of interaction speed, whilst the size of virtual object and the position of the hands are significant. Second, a further study illustrates how perceptible these errors are to a third person viewer of the interaction (e.g. an audience member). We found that interaction errors arising from the overestimation of an object surface affected the visual credibility for the viewer considerably more than an underestimation of the object. This work is presented within the application of a real-time Interactive Virtual Television Studio, which offers convincing realtime interaction for live TV production. We believe the results and methodology presented here could also be applied for designing, implementing and assessing interaction quality in many other Mixed Reality applications.},
keywords={augmented reality;live actor motion measurement;mixed reality interaction system;hand placement accuracy;bimanual movement;interactive virtual object;third person interaction viewer;interactive virtual television studio;Videos;Observers;Virtual reality;Standards;Visualization;Measurement uncertainty;Interactive Virtual Studios;Performance Measurement;Interaction Framework;Mixed Reality},
doi={10.1109/ISMAR.2014.6948414},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948415,
author={E. {Foxlin} and T. {Calloway} and H. {Zhang}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Improved registration for vehicular AR using auto-harmonization},
year={2014},
volume={},
number={},
pages={105-112},
abstract={This paper describes the design, development and testing of an AR system that was developed for aerospace and ground vehicles to meet stringent accuracy and robustness requirements. The system uses an optical see-through HMD, and thus requires extremely low latency, high tracking accuracy and precision alignment and calibration of all subsystems in order to avoid mis-registration and “swim”. The paper focuses on the optical/inertial hybrid tracking system and describes novel solutions to the challenges with the optics, algorithms, synchronization, and alignment with the vehicle and HMD systems. A system accuracy analysis is presented with simulation results to predict the registration accuracy. Finally, a car test is used to create a through-the-eyepiece video demonstrating well-registered augmentations of the road and nearby structures while driving.},
keywords={augmented reality;helmet mounted displays;image registration;object tracking;video signal processing;vehicular AR registration improvement;autoharmonization;AR system design;AR system development;AR system testing;optical see-through HMD;optical-inertial hybrid tracking system;registration accuracy prediction;car test;through-the-eyepiece video;augmented reality;subsystem latency;subsystem tracking accuracy;subsystem precision alignment;optical see-through HMDsubsystems;subsystem calibration;Aircraft;Vehicles;Accuracy;Optical sensors;Cameras;Optical imaging;Calibration;Augmented reality;registration;calibration;hybrid tracking;inertial;see-through HMD},
doi={10.1109/ISMAR.2014.6948415},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948416,
author={S. B. {Knorr} and D. {Kurz}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Real-time illumination estimation from faces for coherent rendering},
year={2014},
volume={},
number={},
pages={113-122},
abstract={We present a method for estimating the real-world lighting conditions within a scene in real-time. The estimation is based on the visual appearance of a human face in the real scene captured in a single image of a monocular camera. In hardware setups featuring a user-facing camera, an image of the user's face can be acquired at any time. The limited range in variations between different human faces makes it possible to analyze their appearance offline, and to apply the results to new faces. Our approach uses radiance transfer functions - learned offline from a dataset of images of faces under different known illuminations - for particular points on the human face. Based on these functions, we recover the most plausible real-world lighting conditions for measured reflections in a face, represented by a function depending on incident light angle using Spherical Harmonics. The pose of the camera relative to the face is determined by means of optical tracking, and virtual 3D content is rendered and overlaid onto the real scene with a fixed spatial relationship to the face. By applying the estimated lighting conditions to the rendering of the virtual content, the augmented scene is shaded coherently with regard to the real and virtual parts of the scene. We show with different examples under a variety of lighting conditions, that our approach provides plausible results, which considerably enhance the visual realism in real-time Augmented Reality applications.},
keywords={augmented reality;lighting;rendering (computer graphics);realtime illumination estimation;coherent rendering;lighting condition;visual appearance;monocular camera;user-facing camera;radiance transfer functions;incident light angle;spherical harmonics;optical tracking;virtual content;visual realism;augmented reality applications;Lighting;Cameras;Geometry;Three-dimensional displays;Rendering (computer graphics);Solid modeling;Estimation},
doi={10.1109/ISMAR.2014.6948416},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948417,
author={U. {Eck} and F. {Pankratz} and C. {Sandor} and G. {Klinker} and H. {Laga}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Comprehensive workspace calibration for visuo-haptic augmented reality},
year={2014},
volume={},
number={},
pages={123-128},
abstract={Visuo-haptic augmented reality systems enable users to see and touch digital information that is embedded in the real world. Precise co-location of computer graphics and the haptic stylus is necessary to provide a realistic user experience. PHANToM haptic devices are often used in such systems to provide haptic feedback. They consist of two interlinked joints, whose angles define the position of the haptic stylus and three sensors at the gimbal to sense its orientation. Previous work has focused on calibration procedures that align the haptic workspace within a global reference coordinate system and developing algorithms that compensate the non-linear position error, caused by inaccuracies in the joint angle sensors. In this paper, we present an improved workspace calibration that additionally compensates for errors in the gimbal sensors. This enables us to also align the orientation of the haptic stylus with high precision. To reduce the required time for calibration and to increase the sampling coverage, we utilize time-delay estimation to temporally align external sensor readings. This enables users to continuously move the haptic stylus during the calibration process, as opposed to commonly used point and hold processes. We conducted an evaluation of the calibration procedure for visuo-haptic augmented reality setups with two different PHANToMs and two different optical trackers. Our results show a significant improvement of orientation alignment for both setups over the previous state of the art calibration procedure. Improved position and orientation accuracy results in higher fidelity visual and haptic augmentations, which is crucial for fine-motor tasks in areas including medical training simulators, assembly planning tools, or rapid prototyping applications. A user friendly calibration procedure is essential for real-world applications of VHAR.},
keywords={augmented reality;calibration;error compensation;haptic interfaces;optical tracking;sampling methods;comprehensive workspace calibration;visuo-haptic augmented reality systems;touch digital information;computer graphics;haptic stylus;realistic user experience;PHANToM haptic devices;haptic feedback;calibration procedures;haptic workspace;global reference coordinate system;nonlinear position error;joint angle sensors;improved workspace calibration;gimbal sensors;time-delay estimation;calibration process;point and hold processes;optical trackers;orientation accuracy;position accuracy;visual augmentations;haptic augmentations;sampling coverage;medical training simulators;assembly planning tools;rapid prototyping applications;user friendly calibration procedure;VHAR;Calibration;Haptic interfaces;Sensors;Joints;Phantoms;Target tracking;Visualization;H.5.1. [Information Interfaces and Presentation]: Multimedia Information Systems — [Artificial;augmented and virtual realities];H.5.2. [Information Interfaces and Presentation];User Interfaces — [Haptic I/O]},
doi={10.1109/ISMAR.2014.6948417},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948418,
author={A. {Torres-Gómez} and W. {Mayol-Cuevas}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Recognition and reconstruction of transparent objects for augmented reality},
year={2014},
volume={},
number={},
pages={129-134},
abstract={Dealing with real transparent objects for AR is challenging due to their lack of texture and visual features as well as the drastic changes in appearance as the background, illumination and camera pose change. The few existing methods for glass object detection usually require a carefully controlled environment, specialized illumination hardware or ignore information from different viewpoints. In this work, we explore the use of a learning approach for classifying transparent objects from multiple images with the aim of both discovering such objects and building a 3D reconstruction to support convincing augmentations. We extract, classify and group small image patches using a fast graph-based segmentation and employ a probabilistic formulation for aggregating spatially consistent glass regions. We demonstrate our approach via analysis of the performance of glass region detection and example 3D reconstructions that allow virtual objects to interact with them.},
keywords={augmented reality;edge detection;graph theory;image classification;image reconstruction;image segmentation;image texture;learning (artificial intelligence);object recognition;transparent object reconstruction;transparent object recognition;augmented reality;texture features;visual features;object appearance;glass object detection;learning approach;transparent object classification;multiple images;object discovery;3D reconstruction;image patch extraction;image patch classification;image patch grouping;graph-based segmentation;probabilistic formulation;spatially-consistent glass region aggregation;glass region detection;virtual objects;Image segmentation;Glass;Image edge detection;Image reconstruction;Three-dimensional displays;Cameras;Distortion measurement;Glass detection;3D Reconstruction;Computer Vision;Augmented Reality},
doi={10.1109/ISMAR.2014.6948418},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948419,
author={F. {Zheng} and D. {Schmalstieg} and G. {Welch}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Pixel-wise closed-loop registration in video-based augmented reality},
year={2014},
volume={},
number={},
pages={135-143},
abstract={In Augmented Reality (AR), visible misregistration can be caused by many inherent error sources, such as errors in tracking, calibration, and modeling. In this paper we present a novel pixel-wise closed-loop registration framework that can automatically detect and correct registration errors using a reference model comprised of the real scene model and the desired virtual augmentations. Registration errors are corrected in both global world space via camera pose refinement, and local screen space via pixel-wise corrections, resulting in spatially accurate and visually coherent registration. Specifically we present a registration-enforcing model-based tracking approach that weights important image regions while refining the camera pose estimates (from any conventional tracking method) to achieve better registration, even in the case of modeling errors. To deal with remaining errors, which can be rigid or non-rigid, we compute the optical flow between the camera image and the real model image rendered with the refined pose, enabling direct screen-space pixel-wise corrections to misregistration. The estimated flow field can be applied to improve registration in two distinct ways: (1) forward warping of modeled on-real-object-surface augmentations (e.g., object re-texturing) into the camera image, leading to surface details that are not present in the virtual object; and (2) backward warping of the camera image into the real scene model, preserving the full use of the dense geometry buffer (depth in particular) provided by the combined real-virtual model for registration, leading to pixel accurate real-virtual occlusion. We discuss the trade-offs between, and different use cases of, forward and backward warping with model-based tracking in terms of specific properties for registration. We demonstrate the efficacy of our approach with both simulated and real data.},
keywords={augmented reality;image registration;object tracking;pose estimation;rendering (computer graphics);video signal processing;pixel-wise closed-loop registration framework;video-based augmented reality;AR;tracking error;calibration error;modeling error;virtual augmentation;registration-enforcing model-based tracking approach;camera pose estimation;rendering;direct screen-space pixel-wise corrections;real-virtual occlusion;dense geometry buffer;backward warping;forward warping;Cameras;Computational modeling;Solid modeling;Mathematical model;Integrated circuit modeling;Three-dimensional displays;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial;Augmented;Virtual Realities;I.4.8 [Image Processing and Computer Vision]: Scene Analysis — Tracking;I.4.9 [Image Processing and Computer Vision];Miscellaneous — Optical Flow},
doi={10.1109/ISMAR.2014.6948419},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948420,
author={T. {Schöps} and J. {Engel} and D. {Cremers}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Semi-dense visual odometry for AR on a smartphone},
year={2014},
volume={},
number={},
pages={145-150},
abstract={We present a direct monocular visual odometry system which runs in real-time on a smartphone. Being a direct method, it tracks and maps on the images themselves instead of extracted features such as keypoints. New images are tracked using direct image alignment, while geometry is represented in the form of a semi-dense depth map. Depth is estimated by filtering over many small-baseline, pixel-wise stereo comparisons. This leads to significantly less outliers and allows to map and use all image regions with sufficient gradient, including edges. We show how a simple world model for AR applications can be derived from semi-dense depth maps, and demonstrate the practical applicability in the context of an AR application in which simulated objects can collide with real geometry.},
keywords={augmented reality;filtering theory;object tracking;smart phones;stereo image processing;AR applications;pixel-wise stereo comparison;depth estimation;semidense depth map;direct image alignment;image tracking;feature extraction;direct monocular visual odometry system;smart phone;augmented reality;AR;semidense visual odometry;Tracking;Real-time systems;Cameras;Accuracy;Image resolution;Simultaneous localization and mapping;Feature extraction;Semi-Dense;Direct Visual Odometry;Tracking;Mapping;AR;Mobile Devices;3D Reconstruction;NEON},
doi={10.1109/ISMAR.2014.6948420},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948421,
author={C. {Resch} and P. {Keitler} and G. {Klinker}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Sticky projections — A new approach to interactive shader lamp tracking},
year={2014},
volume={},
number={},
pages={151-156},
abstract={Shader lamps can augment physical objects with projected virtual replications using a camera-projector system, provided that the physical and virtual object are well registered. Precise registration and tracking has been a cumbersome and intrusive process in the past. In this paper, we present a new method for tracking arbitrarily shaped physical objects interactively. In contrast to previous approaches our system is mobile and makes solely use of the projection of the virtual replication to track the physical object and “stick” the projection to it. Our method consists of two stages, a fast pose initialization based on structured light patterns and a non-intrusive frame-by-frame tracking based on features detected in the projection. In the initialization phase a dense point cloud of the physical object is reconstructed and precisely matched to the virtual model to perfectly overlay the projection. During the tracking phase, a radiometrically corrected virtual camera view based on the current pose prediction is rendered and compared to the captured image. Matched features are triangulated providing a sparse set of surface points that is robustly aligned to the virtual model. The alignment transformation serves as an input for the new pose prediction. Quantitative experiments show that our approach can robustly track complex objects at interactive rates.},
keywords={object tracking;pose estimation;rendering (computer graphics);virtual reality;sticky projections approach;interactive shader lamp tracking;virtual replication;camera-projector system;virtual object;object tracking;fast pose initialization;nonintrusive frame-by-frame tracking;initialization phase;dense point cloud;virtual model;radiometrically corrected virtual camera view;pose prediction;rendering;alignment transformation;Three-dimensional displays;Cameras;Tracking;Iterative closest point algorithm;Feature extraction;Graphics processing units;Mobile communication},
doi={10.1109/ISMAR.2014.6948421},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948422,
author={R. F. {Salas-Moreno} and B. {Glocken} and P. H. J. {Kelly} and A. J. {Davison}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Dense planar SLAM},
year={2014},
volume={},
number={},
pages={157-164},
abstract={Using higher-level entities during mapping has the potential to improve camera localisation performance and give substantial perception capabilities to real-time 3D SLAM systems. We present an efficient new real-time approach which densely maps an environment using bounded planes and surfels extracted from depth images (like those produced by RGB-D sensors or dense multi-view stereo reconstruction). Our method offers the every-pixel descriptive power of the latest dense SLAM approaches, but takes advantage directly of the planarity of many parts of real-world scenes via a data-driven process to directly regularize planar regions and represent their accurate extent efficiently using an occupancy approach with on-line compression. Large areas can be mapped efficiently and with useful semantic planar structure which enables intuitive and useful AR applications such as using any wall or other planar surface in a scene to display a user's content.},
keywords={augmented reality;feature extraction;image reconstruction;SLAM (robots);stereo image processing;dense planar SLAM;simultaneous localization and planning;3D SLAM system;bounded planes;surfel extraction;RGB-D sensors;dense multiview stereo reconstruction;red-green-blue-depth sensor;AR applications;augmented reality;Simultaneous localization and mapping;Cameras;Real-time systems;Three-dimensional displays;Noise;Indexes;Computing methodologies [Scene understanding];Computing methodologies [Reconstruction]. Computing methodologies [Image Processing and Computer Vision]: Segmentation. Information Systems [Information Interfaces and Presentation];Artificial;augmented;virtual realities},
doi={10.1109/ISMAR.2014.6948422},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948423,
author={I. {Leizea} and H. {Álvarez} and I. {Aguinaga} and D. {Borro}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Real-time deformation, registration and tracking of solids based on physical simulation},
year={2014},
volume={},
number={},
pages={165-170},
abstract={This paper proposes a novel approach to registering deformations of 3D non-rigid objects for Augmented Reality applications. Our prototype is able to handle different types of objects in real-time regardless of their geometry and appearance (with and without texture) with the support of an RGB-D camera. During an automatic offline stage, the model is processed in order to extract the data that serves as input for a physics-based simulation. Using its output, the deformations of the model are estimated by considering the simulated behaviour as a constraint. Furthermore, our framework incorporates a tracking method based on templates in order to detect the object in the scene and continuously update the camera pose without any user intervention. Therefore, it is a complete solution that extends from tracking to deformation formulation for either textured or untextured objects regardless of their geometrical shape. Our proposal focuses on providing a correct visual with a low computational cost. Experiments with real and synthetic data demonstrate the visual accuracy and the performance of our approach.},
keywords={augmented reality;cameras;solid modelling;solid deformation;solid registration;solid tracking;3D nonrigid objects;augmented reality application;RGB-D camera;red-green-blue-depth camera;physics-based simulation;camera pose;visual accuracy;Three-dimensional displays;Shape;Cameras;Computational modeling;Deformable models;Visualization;Real-time systems;I.4.8 [IMAGE PROCESSING AND COMPUTER VISION]: Scene Analysis;I.6.8 [SIMULATION AND MODELING]: Types of Simulation — [I.2.10]: ARTIFICIAL INTELLIGENCE — Vision and Scene Understanding},
doi={10.1109/ISMAR.2014.6948423},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948424,
author={Y. {Itoh} and G. {Klinker}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Performance and sensitivity analysis of INDICA: INteraction-Free DIsplay CAlibration for Optical See-Through Head-Mounted Displays},
year={2014},
volume={},
number={},
pages={171-176},
abstract={An issue in AR applications with Optical See-Through Head-Mounted Display (OST-HMD) is to correctly project 3D information to the current viewpoint of the user. Manual calibration methods give the projection as a black box which explains observed 2D-3D relationships well (Fig. 1). Recently, we have proposed an INteraction-free DIsplay CAlibration method (INDICA) for OST-HMD, utilizing camera-based eye tracking [7]. It reformulates the projection in two ways: a black box with an actual eye model (Recycle Setup), and a combination of an explicit display model and an eye model (Full Setup). Although we have shown the former performs more stably than a repeated SPAAM calibration, we could not yet prove whether the same holds for the Full Setup. More importantly, it is still unclear how the error in the calibration parameters affects the final results. Thus, the users can not know how accurately they need to estimate each parameter in practice. We provide: (1) the fact that the Full Setup performs as accurately as the Recycle Setup under a marker-based display calibration, (2) an error sensitivity analysis for both SPAAM and INDICA over the on-/offline parameters, and (3) an investigation of the theoretical sensitivity on an OST-HMD justified by the real measurements.},
keywords={augmented reality;calibration;helmet mounted displays;INDICA method;interaction-free display calibration;optical see-through head-mounted displays;AR applications;augmented reality;OST-HMD;3D information;user viewpoint;2D-3D relationship;camera-based eye tracking;recycle setup;black box model;eye model;explicit display model;repeated SPAAM calibration;Calibration;Three-dimensional displays;Cameras;Recycling;Sensitivity;Correlation;Accuracy;H.5.1 [Information Interfaces and Presentation];Multimedia Information Systems — Artificial;augmented;virtual realities},
doi={10.1109/ISMAR.2014.6948424},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948425,
author={N. {Kishishita} and K. {Kiyokawa} and J. {Orlosky} and T. {Mashita} and H. {Takemura} and E. {Kruijff}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Analysing the effects of a wide field of view augmented reality display on search performance in divided attention tasks},
year={2014},
volume={},
number={},
pages={177-186},
abstract={A wide field of view augmented reality display is a special type of head-worn device that enables users to view augmentations in the peripheral visual field. However, the actual effects of a wide field of view display on the perception of augmentations have not been widely studied. To improve our understanding of this type of display when conducting divided attention search tasks, we conducted an in depth experiment testing two view management methods, in-view and in-situ labelling. With in-view labelling, search target annotations appear on the display border with a corresponding leader line, whereas in-situ annotations appear without a leader line, as if they are affixed to the referenced objects in the environment. Results show that target discovery rates consistently drop with in-view labelling and increase with in-situ labelling as display angle approaches 100 degrees of field of view. Past this point, the performances of the two view management methods begin to converge, suggesting equivalent discovery rates at approximately 130 degrees of field of view. Results also indicate that users exhibited lower discovery rates for targets appearing in peripheral vision, and that there is little impact of field of view on response time and mental workload.},
keywords={augmented reality;helmet mounted displays;user interfaces;wide field-of-view augmented reality display;search performance;head-worn device;peripheral visual field;divided attention search tasks;in-view method;in-situ labelling method;peripheral vision;mental workload;Visualization;Labeling;Search problems;Augmented reality;Shape;Sensitivity;Green products;Augmented reality;see-through head-mounted displays;peripheral visual field;information display methods},
doi={10.1109/ISMAR.2014.6948425},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948426,
author={J. {David Hincapié-Ramos} and L. {Ivanchuk} and S. K. {Sridharan} and P. {Irani}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={SmartColor: Real-time color correction and contrast for optical see-through head-mounted displays},
year={2014},
volume={},
number={},
pages={187-194},
abstract={Users of optical see-through head-mounted displays (OHMD) perceive color as a blend of the display color and the background. Color-blending is a major usability challenge as it leads to loss of color encodings and poor text legibility. Color correction aims at mitigating color blending by producing an alternative color which, when blended with the background, more closely approaches the color originally intended. To date, approaches to color correction do not yield optimal results or do not work in real-time. This paper makes two contributions. First, we present QuickCorrection, a realtime color correction algorithm based on display profiles. We describe the algorithm, measure its accuracy and analyze two implementations for the OpenGL graphics pipeline. Second, we present SmartColor, a middleware for color management of userinterface components in OHMD. SmartColor uses color correction to provide three management strategies: correction, contrast, and show-up-on-contrast. Correction determines the alternate color which best preserves the original color. Contrast determines the color which best warranties text legibility while preserving as much of the original hue. Show-up-on-contrast makes a component visible when a related component does not have enough contrast to be legible. We describe the SmartColor's architecture and illustrate the color strategies for various types of display content.},
keywords={helmet mounted displays;image colour analysis;middleware;SmartColor algorithm;realtime color correction;realtime color contrast;optical see-through head-mounted displays;OHMD;color perception;color encodings;text legibility;color blending;alternative color production;SmartColor middleware;OpenGL graphics pipeline;QuickCorrection algorithm;user interface components;correction management strategy;contrast management strategy;show-up-on-contrast management strategy;display content;Color;Image color analysis;Real-time systems;Algorithm design and analysis;Graphics;Adaptive optics;Accuracy;Head-Mounted Displays;See-through Displays;Transparency;Color Blending;Correction;Contrast},
doi={10.1109/ISMAR.2014.6948426},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948427,
author={F. {Zheng} and T. {Whitted} and A. {Lastra} and P. {Lincoln} and A. {State} and A. {Maimone} and H. {Fuchs}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Minimizing latency for augmented reality displays: Frames considered harmful},
year={2014},
volume={},
number={},
pages={195-200},
abstract={We present initial results from a new image generation approach for low-latency displays such as those needed in head-worn AR devices. Avoiding the usual video interfaces, such as HDMI, we favor direct control of the internal display technology. We illustrate our new approach with a bench-top optical see-through AR proof-of-concept prototype that uses a Digital Light Processing (DLPTM) projector whose Digital Micromirror Device (DMD) imaging chip is directly controlled by a computer, similar to the way random access memory is controlled. We show that a perceptually-continuous-tone dynamic gray-scale image can be efficiently composed from a very rapid succession of binary (partial) images, each calculated from the continuous-tone image generated with the most recent tracking data. As the DMD projects only a binary image at any moment, it cannot instantly display this latest continuous-tone image, and conventional decomposition of a continuous-tone image into binary time-division-multiplexed values would induce just the latency we seek to avoid. Instead, our approach maintains an estimate of the image the user currently perceives, and at every opportunity allowed by the control circuitry, sets each binary DMD pixel to the value that will reduce the difference between that user-perceived image and the newly generated image from the latest tracking data. The resulting displayed binary image is “neither here nor there,” but always approaches the moving target that is the constantly changing desired image, even when that image changes every 50μs. We compare our experimental results with imagery from a conventional DLP projector with similar internal speed, and demonstrate that AR overlays on a moving object are more effective with this kind of low-latency display device than with displays of similar speed that use a conventional video interface.},
keywords={augmented reality;helmet mounted displays;augmented reality displays;low-latency displays;head-worn AR devices;internal display technology;video interfaces;bench-top optical see-through AR;digital light processing projector;DLP projector;digital micromirror device;DMD imaging chip;random access memory;perceptually-continuous-tone dynamic gray-scale image;continuous-tone image;binary time-division-multiplexed value;Mirrors;Image color analysis;Optical imaging;Streaming media;Rendering (computer graphics);Image generation;Gray-scale;H.5.1 [Information Interfaces and Presentation];Multimedia Information Systems — Artificial, Augmented, Virtual Realities},
doi={10.1109/ISMAR.2014.6948427},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948428,
author={N. H. {Lehment} and D. {Merget} and G. {Rigoll}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Creating automatically aligned consensus realities for AR videoconferencing},
year={2014},
volume={},
number={},
pages={201-206},
abstract={This paper presents an AR videoconferencing approach merging two remote rooms into a shared workspace. Such bilateral AR telepresence inherently suffers from breaks in immersion stemming from the different physical layouts of participating spaces. As a remedy, we develop an automatic alignment scheme which ensures that participants share a maximum of common features in their physical surroundings. The system optimizes alignment with regard to initial user position, free shared floor space, camera positioning and other factors. Thus we can reduce discrepancies between different room and furniture layouts without actually modifying the rooms themselves. A description and discussion of our alignment scheme is given along with an exemplary implementation on real-world datasets.},
keywords={augmented reality;teleconferencing;AR videoconferencing;augmented reality;bilateral AR telepresence;immersion stemming;automatic alignment scheme;user position;free shared floor space;camera positioning;Cameras;Three-dimensional displays;Avatars;Observability;Optimization;Teleconferencing;Computational modeling;H.4.3 [Information Systems Applications];Communications Applications — Computer Conferencing;Teleconferencing;Videoconferencing},
doi={10.1109/ISMAR.2014.6948428},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948429,
author={R. {Gal} and L. {Shapira} and E. {Ofek} and P. {Kohli}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={FLARE: Fast layout for augmented reality applications},
year={2014},
volume={},
number={},
pages={207-212},
abstract={Creating a layout for an augmented reality (AR) application which embeds virtual objects in a physical environment is difficult as it must adapt to any physical space. We propose a rule-based framework for generating object layouts for AR applications. Under our framework, the developer of an AR application specifies a set of rules (constraints) which enforce self-consistency (rules regarding the inter-relationships of application components) and scene-consistency (application components are consistent with the physical environment they are placed in). When a user enters a new environment, we create, in real-time, a layout for the application, which is consistent with the defined constraints (as much as possible). We find the optimal configurations for each object by solving a constraint-satisfaction problem. Our stochastic move making algorithm is domain-aware, and allows us to efficiently converge to a solution for most rule-sets. In the paper we demonstrate several augmented reality applications that automatically adapt to different rooms and changing circumstances in each room.},
keywords={augmented reality;knowledge based systems;FLARE;fast layout for augmented reality applications;virtual objects;rule-based framework;object layout generation;AR applications;self-consistency rule;scene-consistency rule;constraint-satisfaction problem;Layout;Proposals;Algorithm design and analysis;Cameras;Augmented reality;Cost function;Games;F.4.1 [Mathematical Logic]: Logic and Constraint Programming;G.3 [Probability and Statistics];Markov Processes},
doi={10.1109/ISMAR.2014.6948429},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948430,
author={W. {Steptoe} and S. {Julier} and A. {Steed}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Presence and discernability in conventional and non-photorealistic immersive augmented reality},
year={2014},
volume={},
number={},
pages={213-218},
abstract={Non-photorealistic rendering (NPR) has been shown as a powerful way to enhance both visual coherence and immersion in augmented reality (AR). However, it has only been evaluated in idealized pre-rendered scenarios with handheld AR devices. In this paper we investigate the use of NPR in an immersive, stereoscopic, wide field-of-view head-mounted video see-through AR display. This is a demanding scenario, which introduces many real-world effects including latency, tracking failures, optical artifacts and mismatches in lighting. We present the AR-Rift, a low-cost video see-through AR system using an Oculus Rift and consumer webcams. We investigate the themes of consistency and immersion as measures of psychophysical non-mediation. An experiment measures discernability and presence in three visual modes: conventional (unprocessed video and graphics), stylized (edge-enhancement) and virtualized (edge-enhancement and color extraction). The stylized mode results in chance-level discernability judgments, indicating successful integration of virtual content to form a visually coherent scene. Conventional and virutalized rendering bias judgments towards correct or incorrect respectively. Presence as it may apply to immersive AR, and which, measured both behaviorally and subjectively, is seen to be similarly high over all three conditions.},
keywords={augmented reality;feature extraction;image colour analysis;image enhancement;rendering (computer graphics);immersive augmented reality;nonphotorealistic rendering;NPR;visual coherence;AR immersion;field-of-view head-mounted video see-through AR display;AR-Rift system;Oculus Rift;consumer Web camera;edge enhancement;color extraction;chance-level discernability judgment;visually coherent scene;Rendering (computer graphics);Accuracy;Visualization;Image edge detection;Cameras;Tracking;Real-time systems;H.5.1 [Information Interfaces and PresentationMultimedia Information Systems]: Artificial, augmented, virtual realities;[I.3.7] Computer Graphics — Three-Dimensional Graphics and RealismColor, shading, shadowing, texture},
doi={10.1109/ISMAR.2014.6948430},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948431,
author={T. {Ha} and S. {Feiner} and W. {Woo}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={WeARHand: Head-worn, RGB-D camera-based, bare-hand user interface with visually enhanced depth perception},
year={2014},
volume={},
number={},
pages={219-228},
abstract={We introduce WeARHand, which allows a user to manipulate virtual 3D objects with a bare hand in a wearable augmented reality (AR) environment. Our method uses no environmentally tethered tracking devices and localizes a pair of near-range and far-range RGB-D cameras mounted on a head-worn display and a moving bare hand in 3D space by exploiting depth input data. Depth perception is enhanced through egocentric visual feedback, including a semi-transparent proxy hand. We implement a virtual hand interaction technique and feedback approaches, and evaluate their performance and usability. The proposed method can apply to many 3D interaction scenarios using hands in a wearable AR environment, such as AR information browsing, maintenance, design, and games.},
keywords={augmented reality;cameras;helmet mounted displays;user interfaces;WeARHand;bare-hand user interface;RGB-D camera;red-green-blue-depth camera;wearable augmented reality;AR environment;tethered tracking devices;head-worn display;depth perception;egocentric visual feedback;semitransparent proxy hand;AR information browsing;AR maintenance;AR design;AR games;Cameras;Three-dimensional displays;Visualization;Image color analysis;Rendering (computer graphics);Image resolution;Equations;Wearable Computing;Augmented Reality;3D User Interfaces;Hand Interaction;Virtual 3D Object Manipulation},
doi={10.1109/ISMAR.2014.6948431},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948432,
author={N. {Haouchine} and J. {Dequidt} and M. {Berger} and S. {Cotin}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Single view augmentation of 3D elastic objects},
year={2014},
volume={},
number={},
pages={229-236},
abstract={This paper proposes an efficient method to capture and augment highly elastic objects from a single view. 3D shape recovery from a monocular video sequence is an underconstrained problem and many approaches have been proposed to enforce constraints and resolve the ambiguities. State-of-the art solutions enforce smoothness or geometric constraints, consider specific deformation properties such as inextensibility or ressort to shading constraints. However, few of them can handle properly large elastic deformations. We propose in this paper a real-time method which makes use of a mechanical model and is able to handle highly elastic objects. Our method is formulated as a energy minimization problem accounting for a non-linear elastic model constrained by external image points acquired from a monocular camera. This method prevents us from formulating restrictive assumptions and specific constraint terms in the minimization. The only parameter involved in the method is the Young's modulus where we show in experiments that a rough estimate of its value is sufficient to obtain a good reconstruction. Our method is compared to existing techniques with experiments conducted on computer-generated and real data that show the effectiveness of our approach. Experiments in the context of minimally invasive liver surgery are also provided.},
keywords={elastic deformation;image reconstruction;image sequences;minimisation;video signal processing;single view augmentation;3D elastic object;3D shape recovery;monocular video sequence;deformation property;shading constraint;elastic deformation;energy minimization problem;Young modulus;minimally invasive liver surgery;Three-dimensional displays;Deformable models;Young's modulus;Computational modeling;Shape;Strain;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial;augmented;virtual realities;I.3.5 [Computer Graphics];Computational Geometry and Object Modeling — Physically based modeling},
doi={10.1109/ISMAR.2014.6948432},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948433,
author={X. {Wang} and C. {Schulte zu Berge} and S. {Demirci} and P. {Fallavollita} and N. {Navab}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Improved interventional X-ray appearance},
year={2014},
volume={},
number={},
pages={237-242},
abstract={Depth cues are an essential part of navigation and device positioning tasks during clinical interventions. Yet, many minimally-invasive procedures, such as catheterizations, are usually performed under X-ray guidance only depicting a 2D projection of the anatomy, which lacks depth information. Previous attempts to integrate pre-operative 3D data of the patient by registering these to intra-operative data have led to virtual 3D renderings independent of the original X-ray appearance and planar 2D color overlays (e.g. roadmaps). A major drawback associated to these solutions is the trade-off between X-ray attenuation values that is completely neglected during 3D renderings, and depth perception not being incorporated into the 2D roadmaps. This paper presents a novel technique for enhancing depth perception of interventional X-ray images preserving the original attenuation appearance. Starting from patient-specific pre-operative 3D data, our method relies on GPU ray casting to compute a colored depth map, which assigns a predefined color to the first incidence of gradient magnitude value above a predefined threshold along the ray. The colored depth map values are carefully integrated into the X-Ray image while maintaining its original grey-scale intensities. The presented method was tested and analysed for three relevant clinical scenarios covering different anatomical aspects and targeting different levels of interventional expertise. Results demonstrate that improving depth perception of X-ray images has the potential to lead to safer and more efficient clinical interventions.},
keywords={graphics processing units;image representation;medical image processing;rendering (computer graphics);interventional x-ray appearance;depth cue;clinical intervention;minimally-invasive procedure;catheterization;X-ray guidance;depth information;virtual 3D renderings;planar 2D color overlays;X-ray appearance;X-ray attenuation value;attenuation appearance;GPU ray casting;graphics processing unit;colored depth map;grey-scale intensity;Image color analysis;X-ray imaging;Three-dimensional displays;Rendering (computer graphics);Medical diagnostic imaging;Encoding;Medical image visualization;depth perception;image-guided interventions;GPU ray casting;transfer function},
doi={10.1109/ISMAR.2014.6948433},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948434,
author={T. {Collins} and D. {Pizarro} and A. {Bartoli} and M. {Canis} and N. {Bourdel}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Computer-Assisted Laparoscopic myomectomy by augmenting the uterus with pre-operative MRI data},
year={2014},
volume={},
number={},
pages={243-248},
abstract={An active research objective in Computer Assisted Intervention (CAI) is to develop guidance systems to aid surgical teams in laparoscopic Minimal Invasive Surgery (MIS) using Augmented Reality (AR). This involves registering and fusing additional data from other modalities and overlaying it onto the laparoscopic video in realtime. We present the first AR-based image guidance system for assisted myoma localisation in uterine laparosurgery. This involves a framework for semi-automatically registering a pre-operative Magnetic Resonance Image (MRI) to the laparoscopic video with a deformable model. Although there has been several previous works involving other organs, this is the first to tackle the uterus. Furthermore, whereas previous works perform registration between one or two laparoscopic images (which come from a stereo laparoscope) we show how to solve the problem using many images (e.g. 20 or more), and show that this can dramatically improve registration. Also unlike previous works, we show how to integrate occluding contours as registration cues. These cues provide powerful registration constraints and should be used wherever possible. We present retrospective qualitative results on a patient with two myomas and quantitative semi-synthetic results. Our multi-image framework is quite general and could be adapted to improve registration in other organs with other modalities such as CT.},
keywords={augmented reality;biomedical MRI;image registration;medical image processing;surgery;video signal processing;computer-assisted laparoscopic myomectomy;preoperative MRI data;magnetic resonance imaging;computer assisted intervention;CAI;laparoscopic minimal invasive surgery;MIS;augmented reality;AR;laparoscopic video;AR-based image guidance system;deformable model;image registration;Laparoscopes;Three-dimensional displays;Transforms;Magnetic resonance imaging;Surgery;Solid modeling;Deformable models},
doi={10.1109/ISMAR.2014.6948434},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948435,
author={A. {Bellarbi} and S. {Otmane} and N. {Zenati} and S. {Benbelkacem}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Poster] MOBIL: A moments based local binary descriptor},
year={2014},
volume={},
number={},
pages={251-252},
abstract={In this paper, we propose an efficient, and fast binary descriptor, called MOBIL (MOments based BInary differences for Local description), which compares not just the intensity, but also sub-regions geometric proprieties by employing moments. This approach offers high distinctiveness against affine transformations and appearance changes. The experimental evaluation shows that MOBIL achieves a quite good performance in term of low computation complexity and high recognition rate compared to state-of-the-art real-time local descriptors.},
keywords={computer vision;feature extraction;object recognition;MOBIL descriptor;moments based binary differences for local description;geometric propriety;computation complexity;recognition rate;Computer vision;Robustness;Image recognition;Augmented reality;Computers;Approximation algorithms;Algorithm design and analysis;Computer vision;local binary descriptors;moments;Augmented reality},
doi={10.1109/ISMAR.2014.6948435},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948436,
author={L. F. {Bertuccelli} and T. {Khawaja} and B. N. {Walker} and P. {O'Neill}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Poster] Ongoing development of a user-centered, AR testbed in industry},
year={2014},
volume={},
number={},
pages={253-254},
abstract={Augmented reality (AR) is a live, direct or indirect, view of a physical, real-world environment whose elements are augmented by computer-generated sensory input such as sound, video, graphics or GPS data [1]. AR technology has seen the advent of smaller form factors, more powerful processors, higher resolution cameras, and distributed computation; the advent of technology like Google Glass and Epson Moverio BT-200 have generated renewed interest in the commercial domains. This new eyewear also extends the earlier capabilities of optical-/video-see through glasses with Bluetooth, Wifi, and 3G connectivity to remote databases. While there are as yet many significant technical hurdles for industry-specific AR systems that must be surmounted to ensure efficient operation, there are still numerous user-centric issues that still need to be addressed to enable the desired safety and efficiency potential of the technology itself. The issues themselves are plentiful, including ergonomic issues (size, weight of the augmented reality hardware, when dealing with glasses), user interface requirements (font sizes, lighting conditions impacting the legibility of text or the rendering of the digital content, interaction with tablets or glasses), and physiological issues (eye fatigue, user perception due to latency of content rendering, increased user workload). Our contribution in this paper is the presentation of an ongoing development of a user-centric, industrial testbed devoted to the requirements gathering, development, and assessment of AR technologies and presentation of two sample use cases. Our testbed is designed to investigate usability research questions related to AR, and this contribution constitutes some of our work in progress in developing this vision.},
keywords={augmented reality;interactive devices;user-centered AR testbed;augmented reality;AR technology;computer-generated sensory input;Google Glass;Epson Moverio BT-200;optical-video-see through glasses;Bluetooth;Wi-Fi;Wireless Fidelity;3G connectivity;ergonomic issue;user interface requirements;physiological issues;Inspection;Buildings;Maintenance engineering;Glass;Engines;Augmented reality;Usability},
doi={10.1109/ISMAR.2014.6948436},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948437,
author={M. {Beatriz Carmo} and A. P. {Cláudio} and A. {Ferreira} and A. P. {Afonso} and P. {Redweik} and C. {Catita} and M. C. {Brito} and J. N. {Pedrosa}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Poster] Visualization of solar radiation data in augmented reality},
year={2014},
volume={},
number={},
pages={255-256},
abstract={We present an AR application for visualizing solar radiation data on facades of buildings, generated from LiDAR data and climatic observations. Data can be visualized using colored surfaces and glyphs. A user study revealed the proposed AR visualizations were easy to use, which can lead to leverage the potential benefits of AR visualizations: to detect errors in the simulated data, to give support to the installation of photovoltaic equipment and to raise public awareness of the use of facades for power production.},
keywords={augmented reality;building integrated photovoltaics;data visualisation;power engineering computing;solar power;solar radiation;solar radiation data visualization;augmented reality;AR application;LiDAR data;climatic observation;light detection and ranging;colored surface;glyph;user study;photovoltaic equipment;power production;building facade;Data visualization;Solar radiation;Augmented reality;Buildings;Image color analysis;Three-dimensional displays;augmented reality;scientific data visualization;solar radiation;photovoltaic potential},
doi={10.1109/ISMAR.2014.6948437},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948438,
author={L. {Carozza} and F. {Bosché} and M. {Abdel-Wahab}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Poster] Visual-lnertial 6-DOF localization for a wearable immersive VR/AR system},
year={2014},
volume={},
number={},
pages={257-258},
abstract={We present a real-time visual-inertial localization approach directly integrable in a wearable immersive system for simulation and training. In this context, while CAVE systems typically require complex and expensive set-up, our approach relies on visual and inertial information provided by consumer monocular camera and Inertial Measurement Unit, embedded in a wearable stereoscopic HMD. 6-DOF localization is achieved through image registration with respect to a 3D map of descriptors of the training room and robust tracking of visual features. We propose a novel efficient and robust pipeline based on state-of-the-art image-based localization and sensor fusion approaches, which makes use of robust orientation information from FMU, to cope with camera fast motion and limit motion jitters. The proposed system runs at 30 fps on a standard PC and requires very limited set-up for its intended application.},
keywords={augmented reality;feature extraction;image fusion;image registration;visual-lnertial 6-DOF localization;degrees-of-freedom;wearable immersive VR-AR system;virtual reality;augmented reality;CAVE systems;visual information;inertial information;consumer monocular camera;inertial measurement unit;image registration;visual feature tracking;image-based localization;sensor fusion approach;Augmented reality},
doi={10.1109/ISMAR.2014.6948438},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948439,
author={S. {Côté} and I. {Létourneau} and J. {Marcoux-Ouellet}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Poster] Augmentation of live excavation work for subsurface utilities engineering},
year={2014},
volume={},
number={},
pages={259-260},
abstract={The virtual excavation is a well-known augmentation technique that was proposed for city road environments. It can be used for planning excavation work by augmenting the road surface with a virtual excavation revealing subsurface utility pipes. In this paper, we proposed an extension of the virtual excavation technique for live augmentation of excavation work sites. Our miniaturized setup, consisting of a sandbox and a Kinect device, was used to simulate dynamic terrain topography capture. We hypothesized that the virtual excavation could be used live on the ground being excavated, which could facilitate the excavator operator's work. Our results show that the technique can indeed be adapted to dynamic terrain topography, but turns out to occlude terrain in a potentially hazardous way. Potential solutions include the use of virtual paint markings instead of a virtual excavation.},
keywords={civil engineering computing;public utilities;virtual reality;live excavation work;subsurface utilities engineering;virtual excavation;city road environment;subsurface utility pipes;dynamic terrain topography capture;virtual paint markings;Surface topography;Augmented reality;Surface treatment;Roads;Real-time systems;Data visualization;Augmented reality;subsurface utilities;infrastructure;virtual excavation},
doi={10.1109/ISMAR.2014.6948439},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948440,
author={I. {Damian} and C. S. S. {Tan} and T. {Baur} and J. {Schöning} and K. {Luyten} and E. {André}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Poster] Exploring social augmentation concepts for public speaking using peripheral feedback and real-time behavior analysis},
year={2014},
volume={},
number={},
pages={261-262},
abstract={Non-verbal and unconscious behavior plays an important role for efficient human-to-human communication but are often undervalued when training people to become better communicators. This is particularly true for public speakers who need not only behave according to a social etiquette but do so while generating enthusiasm and interest for dozens if not hundreds of other persons. In this paper we propose the concept of social augmentation using wearable computing with the goal of giving users the ability to continuously monitor their performance as a communicator. To this end we explore interaction modalities and feedback mechanisms which would lend themselves to this task.},
keywords={social sciences computing;wearable computers;social augmentation concept;public speaking;peripheral feedback;realtime behavior analysis;human-to-human communication;wearable computing;Public speaking;Real-time systems;Biomedical monitoring;Prototypes;Sensors;Signal processing;Cameras},
doi={10.1109/ISMAR.2014.6948440},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948441,
author={D. {Datcu} and M. {Cidota} and H. {Lukosch} and S. {Lukosch}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Poster] Using augmented reality to support information exchange of teams in the security domain},
year={2014},
volume={},
number={},
pages={263-264},
abstract={For operational units in the security domain that work together in teams it is important to quickly and adequately exchange context-related information. This extended abstract investigates the potential of augmented reality (AR) techniques to facilitate information exchange and situational awareness of teams from the security domain. First, different scenarios from the security domain that have been elicited using an end-user oriented design approach are described. Second, a usability study is briefly presented based on an experiment with experts from operational security units. The results of the study show that the scenarios are well-defined and the AR environment can successfully support information exchange in teams operating in the security domain.},
keywords={augmented reality;security of data;team working;augmented reality;security domain;context-related information exchange;AR techniques;team situational awareness;end-user oriented design approach;Security;Augmented reality;Usability;Information exchange;Abstracts;Forensics;Three-dimensional displays;Augmented reality;information exchange;usability},
doi={10.1109/ISMAR.2014.6948441},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948442,
author={V. {Ferrari} and F. {Cutolo} and E. M. {Calabrò} and M. {Ferrari}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Poster] HMD Video see though AR with unfixed cameras vergence},
year={2014},
volume={},
number={},
pages={265-266},
abstract={Stereoscopic video see though AR systems permit accurate marker video based registration. To guarantee accurate registration, cameras are normally rigidly blocked while the user could require changing their vergence. We propose a solution working with lightweight hardware that, without the need for a new calibration of the cameras relative pose after each vergence adjustment, guarantees registration accuracy using pre-determined calibration data.},
keywords={augmented reality;cameras;helmet mounted displays;image registration;stereo image processing;video signal processing;HMD video see though AR;helmet mounted display;augmented reality;unfixed cameras vergence;stereoscopic video;marker video based registration;registration accuracy;pose calibration;vergence adjustment;Cameras;Calibration;Stereo image processing;Fasteners;Surgery;Augmented reality;Hardware;HMD;Video See Though;Vergence},
doi={10.1109/ISMAR.2014.6948442},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948443,
author={J. {Grubert} and H. {Seichter} and D. {Schmalstieg}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Poster] Towards user perspective augmented reality for public displays},
year={2014},
volume={},
number={},
pages={267-268},
abstract={We work towards ad-hoc augmentation of public displays on handheld devices, supporting user perspective rendering of display content. Our prototype system only requires access to a screencast of the public display, which can be easily provided through common streaming platforms and is otherwise self-contained. Hence, it easily scales to multiple users.},
keywords={augmented reality;rendering (computer graphics);user perspective augmented reality;public displays;handheld devices;user perspective rendering;screencast;streaming platform;Cameras;Three-dimensional displays;Rendering (computer graphics);Face;Augmented reality;Handheld computers;Visualization;user perspective rendering;public displays;augmented reality},
doi={10.1109/ISMAR.2014.6948443},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948444,
author={A. {Hill} and H. {Leach}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Poster] Contextually panned and zoomed augmented reality interactions using COTS heads up displays},
year={2014},
volume={},
number={},
pages={269-270},
abstract={Consumer of the shelf heads up displays with onboard cameras and processing power have recently become available. Evaluations of a naive implementation of video-see-through augmented reality suggest that their small display and off-axis camera presents usability problems. We panned and zoomed a composited video feed on the Google Glass device to center the augmented reality context within the display and to give the appearance of a fixed distance to the content. We pilot tested both the panned and zoomed display against a naive implementation and found that users preferred the view-stabilized version.},
keywords={augmented reality;computer displays;contextually panned and zoomed augmented reality interaction;COTS heads up displays;commecial-off-the-shelf;video-see-through augmented reality;composite video feed;Google Glass device;panned and zoomed display;view-stabilized display;Cameras;Glass;Augmented reality;Google;Feeds;Image resolution;Head;Mixed/Augmented Reality;Displays and Imagers;Computer Vision;Interaction Techniques},
doi={10.1109/ISMAR.2014.6948444},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948445,
author={J. {Kim} and J. {Lee} and B. {Yoo} and S. {Ahn} and H. {Ko}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Poster] View management for webized mobile AR contents},
year={2014},
volume={},
number={},
pages={271-272},
abstract={Information presentation techniques are important in augmented reality applications as they are in the traditional desktop user interface (WIMP) and web user interface [6]. This paper introduces view management for a web-based augmented reality mobile platform. We use a webized mobile AR browser called Insight that provides separation of the application logic, including the tracking engine and AR content, so that the view management logic and contents are easy to reuse. In addition, the view management is able to accommodate in-situ context of an AR application.},
keywords={augmented reality;Internet;mobile computing;user interfaces;view management;webized mobile AR contents;augmented reality;information presentation techniques;desktop user interface;WIMP;Web user interface;Insight;application logic;tracking engine;AR content;Layout;Mobile communication;Augmented reality;Browsers;Semantics;Real-time systems;User interfaces;Content;layout management;mobile AR;view management;web architecture},
doi={10.1109/ISMAR.2014.6948445},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948446,
author={M. {Klemm} and H. {Hoppe} and F. {Seebacher}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Poster] Non-parametric camera-based calibration of optical see-through glasses for augmented reality applications},
year={2014},
volume={},
number={},
pages={273-274},
abstract={This work describes a camera-based method for the calibration of optical See-Through Glasses (STGs). A new calibration technique is introduced for calibrating every single display pixel of the STGs in order to overcome the disadvantages of a parametric model. A non-parametric model compared to the parametric one has the advantage that it can also map arbitrary distortions. The new generation of STGs using waveguide-based displays [5] will have higher arbitrary distortions due to the characteristics of their optics. First tests show better accuracies than in previous works. By using cameras which are placed behind the displays of the STGs, no error prone user interaction is necessary. It is shown that a high accuracy tracking device is not necessary for a good calibration. A camera mounted rigidly on the STGs is used to find the relations between the system components. Furthermore, this work elaborates on the necessity of a second subsequent calibration step which adapts the STGs to a specific user. First tests prove the theory that this subsequent step is necessary.},
keywords={augmented reality;computer displays;optical glass;user interfaces;nonparametric camera-based calibration;optical see-through glass;augmented reality;STG;camera-based method;calibration technique;nonparametric model;waveguide-based display;user interaction;Calibration;Cameras;Optical distortion;Accuracy;Adaptive optics;Three-dimensional displays;Augmented reality;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems —Artificial;augmented;virtual realities;I.4.9 [Image Processing and Computer Vision]: Applications;I.4.m [Image Processing and Computer Vision];Miscellaneous},
doi={10.1109/ISMAR.2014.6948446},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948447,
author={D. {Kurz} and A. {Fedosov} and S. {Diewald} and J. {Güttier} and B. {Geilhof} and M. {Heuberger}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Poster] Towards mobile augmented reality for the elderly},
year={2014},
volume={},
number={},
pages={275-276},
abstract={Mobility and independence are key aspects for self-determined living in today's world and demographic change presents the challenge to retain these aspects for the aging population. Augmented Reality (AR) user interfaces might support the elderly, for example, when navigating as pedestrians or by explaining how devices and mobility aids work and how they are maintained. This poster reports on the results of practical field tests with elderly subjects testing handheld AR applications. The main finding is that common handheld AR user interfaces are not suited for the elderly because they require the user to hold up the device so the back-facing camera captures the object or environment related to which digital information shall be presented. Tablet computers are too heavy and they do not provide sufficient grip to hold them over a long period of time. One possible alternative is using head-mounted displays (HMD). We present the promising results of a user test evaluating whether elderly people can deal with AR interfaces on a lightweight HMD. We conclude with an outlook to improved handheld AR user interfaces that do not require continuously holding up the device, which we hope are better suited for the elderly.alternative},
keywords={augmented reality;helmet mounted displays;mobile computing;user interfaces;mobile augmented reality;elderly;AR user interfaces;handheld AR applications;tablet computers;head-mounted displays;HMD;Senior citizens;Tablet computers;Augmented reality;User interfaces;Navigation;Cameras;Manuals},
doi={10.1109/ISMAR.2014.6948447},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948448,
author={N. S. {Lakshmiprabha} and A. {Santos} and D. {Mladenov} and O. {Beltramello}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Poster] An augmented and virtual reality system for training autistic children},
year={2014},
volume={},
number={},
pages={277-278},
abstract={Autism or Autism Spectrum disorder (ASD) is a pervasive development disorder causing impairment in thinking, feeling, hearing, speaking and social interaction. For this reason, children suffering from autism need to follow special training in order to increase their ability to learn new skills and knowledge. These children have propensity to be attracted by the technology devices especially virtual animations. The interest of this research work is to explore and study the use of Augmented and Virtual Reality (AR/VR) system for training the children with ASD based on Applied Behavior Analysis (ABA) techniques. This system assists in teaching children about new pictures or objects along with the associated keyword or matching sentence in an immersive way with fast interaction. The preliminary prototype demonstrates satisfactory performance of the proposed AR/VR system working in laboratory conditions.},
keywords={augmented reality;computer animation;computer based training;augmented reality system;virtual reality system;autistic children training;autism spectrum disorder;virtual animation;AR-VR system;applied behavior analysis;ABA technique;Training;Variable speed drives;Psychology;Autism;Cameras;Monitoring;Virtual reality;H.5.1 [Information Interfaces and Presentation];Multimedia Information Systems — Artificial, Augmented, Virtual Realities},
doi={10.1109/ISMAR.2014.6948448},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948449,
author={J. P. {Lima} and R. {Roberto} and J. M. {Teixeira} and V. {Teichrieb}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Poster] Device vs. user perspective rendering in google glass AR applications},
year={2014},
volume={},
number={},
pages={279-280},
abstract={According to Gartner's 2013 Hype Cycle for Emerging Technologies, Augmented Reality will reach its Plateu of Productivity before Wearable User Interfaces. In this work, device and user-perspective rendering are compared regarding their applicability to AR-based solutions for Google Glass. The conducted experiment measured and evaluated the advantages and drawbacks on each method and also got positive and negative feedbacks given by users. The tests showed that users preferred the device-perspective approach.},
keywords={augmented reality;interactive devices;rendering (computer graphics);device rendering;user perspective rendering;Google Glass AR application;augmented reality;Hype Cycle for Emerging Technologies;wearable user interfaces;device-perspective approach;Glass;Calibration;Google;Rendering (computer graphics);Cameras;Augmented reality;Optical sensors;H.5.1 [Information Interfaces and Presentation];Multimedia Information Systems — Artificial, Augmented, Virtual Realities},
doi={10.1109/ISMAR.2014.6948449},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948450,
author={A. {Marzo} and B. {Bossavit} and M. {Hachet}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Poster] Combining multi-touch and device movement in mobile augmented reality manipulations},
year={2014},
volume={},
number={},
pages={281-282},
abstract={Three input modalities for manipulation techniques in Mobile Augmented Reality have been compared. The first one employs only multi-touch input. The second modality uses the movements of the device. Finally, the third one is a hybrid approach based on a combination of the two previous modalities. A user evaluation (N=12) on a 6 DOF docking task suggests that combining multi-touch input and device movement offers the best results in terms of task completion time and efficiency. Nonetheless, using solely the device is more intuitive and performs worse only in large rotations. Given that mobile devices are increasingly supporting movement tracking, the presented results encourage the addition of device movement as an input modality.},
keywords={augmented reality;mobile computing;user interfaces;multitouch input;device movement input;mobile augmented reality;manipulation techniques;user evaluation;task completion time;mobile devices;movement tracking;input modality;Performance evaluation;Thumb;Augmented reality;Mobile communication;Three-dimensional displays;Grasping;Mobile Augmented Reality;Manipulation;Multi-touch},
doi={10.1109/ISMAR.2014.6948450},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948451,
author={C. {Morales} and T. {Oishi} and K. {Ikeuchi}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Poster] Turbidity-based aerial perspective rendering for mixed reality},
year={2014},
volume={},
number={},
pages={283-284},
abstract={In outdoor Mixed Reality (MR), objects distant from the observer suffer from an effect called aerial perspective that fades the color of the objects and blends it to the environmental light color. The aerial perspective can be modeled using a physics-based approach; however, handling the changing and unpredictable environmental illumination is demanding. We present a turbidity-based method for rendering a virtual object with aerial perspective effect in a MR application. The proposed method first estimates the turbidity by matching luminance distributions of sky models and a captured omnidirectional sky image. Then the obtained turbidity is used to render the virtual object with aerial perspective.},
keywords={augmented reality;rendering (computer graphics);turbidity-based aerial perspective;mixed reality;MR;environmental light color;physics-based approach;environmental illumination;virtual object;luminance distribution;omnidirectional sky image;Atmospheric modeling;Rendering (computer graphics);Scattering;Virtual reality;Solid modeling;Mathematical model;Observers;Photorealistic rendering;MR/AR for art;cultural heritage;or education and training},
doi={10.1109/ISMAR.2014.6948451},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948452,
author={T. {Ogawa} and Y. {Manabe} and N. {Yata}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Poster] Representing degradation of real objects using augmented reality},
year={2014},
volume={},
number={},
pages={285-286},
abstract={Much research in augmented reality (AR) technology attempts to match the textures of virtual objects with real world. However, the textures of real objects must also be rendered consistent with virtual information. This paper proposes a method for representing the degradation of real objects in virtual time. Real-world depth information, used to build three-dimensional models of real objects, is captured by a RGB-D camera. The degradation of real objects is then represented by superimposing the degradation texture onto the real object.},
keywords={augmented reality;image representation;real object degradation representation;augmented reality;AR technology;virtual objects;real-world depth information;three-dimensional model;RGB-D camera;red-green-blue-depth camera;Degradation;Cameras;Solid modeling;Visualization;Real-time systems;Shape;Augmented reality;Augmented Reality;RGB-D Camera;Degradation},
doi={10.1109/ISMAR.2014.6948452},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948453,
author={F. {Okura} and T. {Akaguma} and T. {Sato} and N. {Yokoya}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Poster] Indirect augmented reality considering real-world illumination change},
year={2014},
volume={},
number={},
pages={287-288},
abstract={Indirect augmented reality (IAR) utilizes pre-captured omnidirectional images and offline superimposition of virtual objects for achieving high-quality geometric and photometric registration. Meanwhile, IAR causes inconsistency between the real world and the pre-captured image. This paper describes the first-ever study focusing on the temporal inconsistency issue in IAR. We propose a novel IAR system which reflects real-world illumination change by selecting an appropriate image from a set of images pre-captured under various illumination. Results of a public experiment show that the proposed system can improve the realism in IAR.},
keywords={augmented reality;image registration;lighting;indirect augmented reality;real-world illumination change;pre-captured omnidirectional image;virtual objects superimposition;geometric registration;photometric registration;IAR system;Lighting;Histograms;Mobile communication;Mobile handsets;Clouds;Augmented reality;Databases;H.5.1 [Information Interfaces and Presentation];Multimedia Information Systems — Artificial, augmented, virtual realities},
doi={10.1109/ISMAR.2014.6948453},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948454,
author={T. {Oskiper} and M. {Sizintsev} and V. {Branzoi} and S. {Samarasekera} and R. {Kumar}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Poster] Augmented reality binoculars on the move},
year={2014},
volume={},
number={},
pages={289-290},
abstract={In this paper, we expand our previous work on augmented reality (AR) binoculars to support wider range of user motion - up to thousand square meters compared to only a few square meters as before. We present our latest improvements and additions to our pose estimation pipeline and demonstrate stable registration of objects on the real world scenery while the binoculars are undergoing significant amount of parallax-inducing translation.},
keywords={augmented reality;image registration;interactive devices;pose estimation;augmented reality binoculars;AR binoculars;user motion;pose estimation;object registration;parallax-inducing translation;Augmented reality;Global Positioning System;Materials;Training;Accuracy;Google;Earth;inertial navigation;sensor fusion;EKF},
doi={10.1109/ISMAR.2014.6948454},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948455,
author={A. {Padilha} and V. {Teichrieb}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Poster] Motion detection based ghosted views for occlusion handling in augmented reality},
year={2014},
volume={},
number={},
pages={291-292},
abstract={This work presents an improvement to the scene analysis pipeline of a visualization technique called Ghosting. Computer vision and image processing techniques are used to extract natural features, from each video frame. These features will guide the assignment of transparency to pixels, in order to give the ghosting effect, while blending the virtual object into the real scene. Video sequences were obtained from traditional RGB cameras. The main contribution of this work is the inclusion of a motion detection technique to the scene feature analysis step. This procedure leads to a better perception of the augmented scene because the proper ghosting effect is achieved when a moving natural salient object, that catches users attention, passes in front of an augmented one.},
keywords={augmented reality;computer vision;feature extraction;image motion analysis;image sequences;object detection;video signal processing;motion detection based ghosted view;occlusion handling;augmented reality;scene analysis pipeline;visualization technique;ghosting technique;computer vision;image processing techniques;feature extraction;video sequences;scene feature analysis;moving natural salient object;Motion detection;Augmented reality;Visualization;Pipelines;Image analysis;Computer vision;Feature extraction;H.5.1 [Information Interfaces and Presentation Systems]: Multimedia Information Systems — Artificial, augmented, virtual realities;I.4.8 [Image Processing and Computer Vision]: Scene Analysis — Depth cues},
doi={10.1109/ISMAR.2014.6948455},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948456,
author={H. {Park} and T. {Kim} and J. {Park}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Poster] QR code alteration for augmented reality interactions},
year={2014},
volume={},
number={},
pages={293-294},
abstract={QR code, for its recognition robustness and data capacity, has been often used for Augmented Reality applications as well as for other commercial applications. However, it is difficult to enable tangible interactions through which users may change 3D models or animations. It is because QR codes are automatically generated by the rules, and are not easily modifiable. Our goal was to enable QR code based Augmented Reality interactions. By analysis and through experiments, we discovered that some parts of a QR code can be altered to change the text string that the QR code represents. In this paper, we introduced a prototype for QR code based Augmented Reality interactions, which allows for Rubik's cube style rolling interactions.},
keywords={augmented reality;bar codes;QR code alteration;augmented reality interactions;recognition robustness;data capacity;tangible interactions;3D models;animations;cube style rolling interactions;Augmented reality;Prototypes;Dinosaurs;Three-dimensional displays;Solid modeling;Media;Laboratories;QR code;Interaction;Augmented Reality},
doi={10.1109/ISMAR.2014.6948456},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948457,
author={J. {Park} and B. {Seo} and J. {Park}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Poster] Interactive deformation of real objects},
year={2014},
volume={},
number={},
pages={295-296},
abstract={This paper presents a method for interactive deformation of a real object. Our method uses a predefined 3D model of a target object for tracking and deformation. A camera pose relative to the target object is estimated using 3D model-based tracking. Object region in camera image is obtained by projecting the 3D model onto image plane with the estimated camera pose, and a texture map is extracted from the object region and mapped to the 3D model. Then a texture-mapped model is rendered based on a mesh deformed by user via Laplacian operation. Experimental results demonstrate that our method provides user interactions with 3D real objects on real scenes, not augmented virtual contents.},
keywords={image texture;object tracking;pose estimation;solid modelling;interactive deformation;object tracking;object deformation;3D model-based tracking;camera pose estimation;image plane;texture map;Laplacian operation;user interaction;Solid modeling;Three-dimensional displays;Cameras;Deformable models;Target tracking;Computational modeling;Real-time systems;I.3.6 [Computer Graphics]: Methodology and Techniques—Interaction techniques;H.5.1 [Information Interface and Presentation];Multimedia Information Systems — Artificial, augmented, virtual realities},
doi={10.1109/ISMAR.2014.6948457},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948458,
author={K. Č. {Pucihar} and P. {Coulton}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Poster] Contact-view: A magic-lens paradigm designed to solve the dual-view problem},
year={2014},
volume={},
number={},
pages={297-298},
abstract={Typically handheld AR systems utilize a single back-facing camera and the screen in order to implement device transparency. This creates the dual-view problem a consequence of virtual transparency which does not match true transparency - what the user would see looking through a transparent glass pane. The dual-view problem affects usability of handheld AR systems and is commonly addressed though user-perspective rendering solutions. Whilst such approach produces promising results, the complexity of implementing user-perspective rendering and the fact it does not solve all sources that produce the dual-view problem, means it only ever addresses part of the problem. This paper seeks to create a more complete solution for the dual-view problem that will be applicable to readily available handheld-device. We pursue this goal by designing, implementing and evaluating a novel interaction paradigm we call `contact-view'. By utilizing the back and front-facing camera and the environment base-plane texture - predefined or incrementally created on the fly, we enable placing the device directly on top of the base-plane. As long as the position of the phone in relation to the base-plane is known, appropriate segment of the occluded base-plane can be rendered on the device screen, result of which is transparency in which dual-view problem is eliminated.},
keywords={augmented reality;mobile handsets;rendering (computer graphics);user interfaces;contact-view paradigm;magic-lens paradigm;dual-view problem;handheld AR systems;augmented reality systems;device transparency;virtual transparency;transparent glass pane;user-perspective rendering;interaction paradigm;occluded base-plane;Cameras;Rendering (computer graphics);Observers;Context;Prototypes;Switches;Glass;AR;magic-lens;dual-view problem;tabletop},
doi={10.1109/ISMAR.2014.6948458},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948459,
author={K. Č. {Pucihar} and P. {Coulton}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Poster] Utilizing contact-view as an augmented reality authoring method for printed document annotation},
year={2014},
volume={},
number={},
pages={299-300},
abstract={In Augmented Reality (AR) the real world is enhanced by superimposed digital information commonly visualized through augmented annotations. The visualized data comes from many different data sources. One increasingly important source of data is user generated content. Unfortunately, AR tools that support user generated content are not common hence the majority of augmented data within AR applications is not generated utilizing AR technology. In this paper we discuss the main reasons for this and evaluate how the contact-view paradigm could enhance annotation authoring process within the class of tabletop size AR workspaces. This evaluation is based on a prototype that allows musicians to annotate a music score manuscript utilizing freehand drawing on top of device screen. Experimentation showed the potential of contact-view paradigm as an annotation authoring method that performs well in single and collaborative multi-user situations.},
keywords={augmented reality;document handling;contact-view method;augmented reality authoring method;printed document annotation;AR;augmented annotation;AR technology;contact-view paradigm;annotation authoring process;freehand drawing;multiuser situation;Augmented reality;Context;Prototypes;Collaboration;Rendering (computer graphics);User-generated content;Cameras;AR;magic-lens;authoring;annotation;collaboration;contact-view;mobile;handheld;tbaletop},
doi={10.1109/ISMAR.2014.6948459},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948460,
author={P. {Punpongsanon} and D. {Iwai} and K. {Sato}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Poster] A preliminary study on altering surface softness perception using augmented color and deformation},
year={2014},
volume={},
number={},
pages={301-302},
abstract={Choosing the appropriate soft/hard material is important for designing a product such as sofa or bed, but typically limited by the number of physical materials that the designer owns. Pseudo-haptic feedback is an alternative way that enables designer to roughly simulate material properties (e.g., softness, hardness) by only generating the visual illusion. However, the current technique is limited within video see-through augmented reality, in which the user interact in a real space while looking at a virtual space. This paper explores the possibility to realize pseudo-haptic feedback for touching objects in spatial augmented reality. We investigate and compare effects of visually superimposing a projection graphics onto the surface of a touched object and the fingernail/finger for changing the surface tactile perception. The potential of our method is discussed through a preliminary user study.},
keywords={augmented reality;haptic interfaces;surface softness perception;augmented color;augmented deformation;product design;pseudohaptic feedback;material softness property;material hardness property;projection graphics;surface tactile perception;Image color analysis;Materials;Visualization;Force;Color;Augmented reality;Force measurement;Spatial augmented reality;pseudo-haptics;perception},
doi={10.1109/ISMAR.2014.6948460},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948461,
author={C. {Reichherzer} and A. {Nassani} and M. {Billinghurst}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Poster] Social panoramas using wearable computers},
year={2014},
volume={},
number={},
pages={303-304},
abstract={In this paper we describe the concept of Social Panoramas that combine panorama images, Mixed Reality, and wearable computers to support remote collaboration. We have developed a prototype that allows panorama images to be explored in real time between a Google Glass user and a remote tablet user. This uses a variety of cues for supporting awareness, and enabling pointing and drawing. We conducted a study to explore if these cues can increase Social Presence. The results suggest that increased interaction does not increase Social Presence, but tools with a higher perceived usability show an improved sense of presence.},
keywords={augmented reality;image processing;mobile computing;user interfaces;wearable computers;social panorama concept;wearable computers;panorama images;mixed reality;remote collaboration;Google Glass user;remote tablet user;social presence;perceived usability;Glass;Usability;Collaboration;Cameras;Head;Prototypes;Wearable computers;Head-mounted Display;Panorama;Remote Collaboration;Mixed Reality;Mobile Computing},
doi={10.1109/ISMAR.2014.6948461},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948462,
author={D. {Stanimirovic} and N. {Damasky} and S. {Webel} and D. {Koriath} and A. {Spillner} and D. {Kurz}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Poster] A Mobile Augmented reality system to assist auto mechanics},
year={2014},
volume={},
number={},
pages={305-306},
abstract={Ground-breaking technologies and innovative design of upcoming vehicles introduce complex maintenance procedures for auto mechanics. In order to present these procedures in an intuitive manner, the Mobile Augmented Reality Technical Assistance (MARTA) project was initiated. The goal was to create an Augmented Reality-aided application running on a tablet computer, which shows maintenance instructions superimposed on a live video feed of the car. Robust image-based tracking of specular surfaces using both edge and texture features as well as the software framework are the most important aspects of the project, which are presented here. The resulting application is deployed and used productively to support maintenance of the Volkswagen XL1 vehicle across the world.},
keywords={augmented reality;automobile industry;feature extraction;image texture;maintenance engineering;mobile computing;object tracking;production engineering computing;mobile augmented reality system;auto mechanics;MARTA;mobile augmented reality technical assistance;augmented reality-aided application;tablet computer;maintenance instructions;robust image-based tracking;specular surface tracking;edge feature;texture feature;Volkswagen XL1 vehicle;Maintenance engineering;Solid modeling;Three-dimensional displays;Tablet computers;Image edge detection;Cameras;Computational modeling},
doi={10.1109/ISMAR.2014.6948462},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948463,
author={D. {Stanimirovic} and D. {Kurz}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Poster] Smartwatch-aided handheld augmented reality},
year={2014},
volume={},
number={},
pages={307-308},
abstract={We propose a novel method for interaction of humans with real objects in their surrounding combining Visual Search and Augmented Reality (AR). This method is based on utilizing a smartwatch tethered to a smartphone, and it is designed to provide a more user-friendly experience compared to approaches based only on a handheld device, such as a smartphone or a tablet computer. The smart-watch has a built-in camera, which enables scanning objects without the need to take the smartphone out of the pocket. An image captured by the watch is sent wirelessly to the phone that performs Visual Search and subsequently informs the smartwatch whether digital information related to the object is available or not. We thereby distinguish between three cases. If no information is available or the object recognition failed, the user is notified accordingly. If there is digital information available that can be presented using the smartwatch display and/or audio output, it is presented there. The third case is that the recognized object has digital information related to it, which would be beneficial to see in an Augmented Reality view spatially registered with the object in realtime. Then the smartwatch informs the user that this option exists and encourages using the smartphone to experience the Augmented Reality view. Thereby, the user only needs to take the phone out of the pocket in case Augmented Reality content is available, and when the content is of interest for the user.},
keywords={augmented reality;mobile computing;object recognition;user interfaces;smartwatch-aided handheld augmented reality;human interaction;visual search;AR;smart phone;user-friendly experience;tablet computer;digital information;object recognition;Visualization;Augmented reality;Search problems;Cameras;Three-dimensional displays;Watches;Prototypes},
doi={10.1109/ISMAR.2014.6948463},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948464,
author={M. {Tait} and M. {Billinghurst}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Poster] View independence in remote collaboration using AR},
year={2014},
volume={},
number={},
pages={309-310},
abstract={This poster presents an Augmented Reality (AR) system for remote collaboration that allows a remote user to navigate a local user's scene, independently from their viewpoint. This is achieved by using a 3D scan and reconstruction of the user's environment. The remote user can place virtual objects in the scene that the local user views through a head mounted display (HMD), helping them place real objects. A user study tested how the amount of remote view independence affected collaboration. We found that increased view independence led to faster task completion, more user confiden), helping them place real objects. A user study tested how the amount of remote view independence affected collaboration. We found that increased view independence led to faster task completion, more user confidence, and a decrease in verbal communication, but object placement accuracy remained unchanged.},
keywords={augmented reality;helmet mounted displays;user interfaces;view independence;remote collaboration;AR;augmented reality;user navigation;3D scan;virtual object;head mounted display;HMD;task completion;user confidence;verbal communication;object placement accuracy;Collaboration;Three-dimensional displays;Accuracy;Solid modeling;Cameras;Streaming media;Usability;Remote Collaboration;Augmented Reality},
doi={10.1109/ISMAR.2014.6948464},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948465,
author={E. {Tappeiner} and D. {Schmalstieg} and T. {Langlotz}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Poster] Local optimization for natural feature tracking targets},
year={2014},
volume={},
number={},
pages={311-312},
abstract={In this work, we present an approach for optimizing targets for natural feature-based pose tracking such as used in Augmented Reality applications. Our contribution is an approach for locally optimizing a given tracking target instead of applying global optimizations, such as proposed in the literature. The local optimization together with visualized trackability rating leads to a tool to create high quality tracking targets.},
keywords={augmented reality;object tracking;pose estimation;target tracking;local optimization;natural feature tracking target;augmented reality application;visualized trackability rating;natural feature-based pose tracking;Target tracking;Image segmentation;Optimization;Histograms;Cameras;Pipelines;Image edge detection;Pose tracking;Natural features},
doi={10.1109/ISMAR.2014.6948465},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948466,
author={F. {Tecchia} and G. {Avveduto} and M. {Carrozzino} and R. {Brondi} and M. {Bergamasco} and L. {Alem}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Poster] Interacting with your own hands in a fully immersive MR system},
year={2014},
volume={},
number={},
pages={313-314},
abstract={This poster introduces a fully immersive Mixed Reality system we have recently developed, where the user is free to walk inside a virtual scenario while wearing a HMD. The novelty of the system lies in the fact that users can see and use their real hands - by means of a Kinect-like camera mounted on the HMD - in order to naturally interact with the virtual objects. Our working hypothesis are that the introduction of the photorealistic capture of users' hands in a coherently rendered virtual scenario induces in them a strong feeling of presence and embodiment without the need of using a synthetic 3D modelled avatar as a representation of the self. We also argue that the users' ability of grasping and manipulating virtual objects using their own hands not only provides an intuitive interaction experience, but also improves self-perception as well as the perception of the environment.},
keywords={augmented reality;avatars;user interfaces;user interaction;fully immersive MR system;fully immersive mixed reality system;virtual scenario;HMD;helmet mounted device;Kinect-like camera;virtual objects;photorealistic capture;synthetic 3D modelled avatar;intuitive interaction experience;Training;Three-dimensional displays;Virtual environments;Cameras;Avatars;Industries;Mixed Reality;hand gestures;natural interaction},
doi={10.1109/ISMAR.2014.6948466},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948467,
author={P. {Tiefenbacher} and A. {Pflaum} and G. {Rigoll}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Poster] Touch gestures for improved 3D object manipulation in mobile augmented reality},
year={2014},
volume={},
number={},
pages={315-316},
abstract={This work presents three techniques for 3D manipulation on mobile touch devices, taking the specifics of mobile AR scenes into account. We compare the common direct manipulation technique with two indirect techniques, which utilize only the thumbs to perform the transformations. The evaluation of the manipulation variants is conducted in a mixed reality (MR) environment which takes advantage of the controlled conditions of a full virtual reality (VR) system. A study with 18 participants shows that the two-thumb method tops the other techniques. It performs better with respect to the total manipulation time and total number of gestures.},
keywords={augmented reality;gesture recognition;mobile computing;touch gesture;3D object manipulation;mobile augmented reality;mobile touch devices;direct manipulation technique;mixed reality environment;MR environment;full virtual reality system;VR system;two-thumb method;Three-dimensional displays;Thumb;Mobile communication;Mobile handsets;Tracking;Virtual reality;Performance evaluation;H.5.2 [Information Interfaces and Presentation];User Interfaces — Interaction styles},
doi={10.1109/ISMAR.2014.6948467},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948468,
author={M. {Tennis} and S. {Weber} and G. {Klinker}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Poster] The posture angle threshold between airplane and window frame metaphors},
year={2014},
volume={},
number={},
pages={317-318},
abstract={Integrating different mental models and the corresponding interaction techniques for navigation and object manipulation tasks is a topic of concern for providing user interfaces for the wide variety of potential users. With a user controlled, egocentric hand-held device we aim at integrating the so far identified three major interaction techniques based on the metaphors of a steering wheel, a toy airplane and a window frame. Here, specifically the toy airplane and the window frame contradict each other, leaving the issue at which postural angle of the hand-held device the mental model of the users changes.},
keywords={mobile computing;user interfaces;posture angle threshold;airplane metaphor;window frame metaphor;navigation task;object manipulation task;user interface;egocentric hand-held device;steering wheel;toy airplane;postural angle;user mental model;Airplanes;Wheels;Navigation;Augmented reality;Cognitive science;Cameras;Virtual environments;H.5.m [Information interfaces and presentation];Miscellaneous},
doi={10.1109/ISMAR.2014.6948468},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948469,
author={K. {Waegel}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Poster] A reconstructive see-through display},
year={2014},
volume={},
number={},
pages={319-320},
abstract={The two most common display technologies used in augmented reality head-mounted displays are optical see-through and video see-through. In this paper I demonstrate a third alternative: reconstructive see-through. By using a commodity depth camera to construct a dense 3D model of the world and rendering this to the user, distracting latency and position discrepancies between real and virtual objects can be reduced.},
keywords={augmented reality;helmet mounted displays;solid modelling;reconstructive see-through display;augmented reality;head-mounted display;optical see-through display;video see-through display;commodity depth camera;dense 3D model;virtual objects;Cameras;Three-dimensional displays;Solid modeling;Rendering (computer graphics);Computational modeling;Jitter;Image color analysis;H.5.1 [Information Systems]: Multimedia Information Systems — Artificial, augmented, virtual realities;I.4.8 [Image Processing and Computer Vision]: Scene Analysis — Tracking},
doi={10.1109/ISMAR.2014.6948469},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948470,
author={T. {Adamek} and L. {Martinell} and M. {Ferrarons} and A. {Torrents} and D. {Marimon}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[DEMO] High volume offline image recognition},
year={2014},
volume={},
number={},
pages={323-324},
abstract={We show a prototype of an offline image recognition engine, running on a tablet with Intel®Atom™ processor, searching within less than 250ms through thousands (5000+) of images. Moreover, the prototype still offers the advanced capabilities of recognising real world 3D objects, until now reserved only for cloud solutions. Until now image search within large collections of images could be performed only in the cloud, requiring mobile devices to have Internet connectivity. However, for many use cases the connectivity requirement is impractical, e.g. many museums have no network coverage, or do not want their visitors incurring expensive roaming charges. Existing commercial solutions are very limited in terms of searched collections sizes, often imposing a maximum limit of <100 reference images. Moreover, adding images typically affects the recognition speed and increases RAM requirements.},
keywords={},
doi={10.1109/ISMAR.2014.6948470},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948471,
author={S. {Ahn} and J. {Lee} and J. {Kim} and S. {Chun} and J. {Kim} and I. {Kim} and J. {Shim} and B. {Yoo} and H. {Ko}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[DEMO] Insight: Webized mobile AR and real-life use cases},
year={2014},
volume={},
number={},
pages={325-326},
abstract={This demonstration shows a novel approach for Webizing mobile augmented reality, which uses HTML as its content structure, and its real-life use cases. Insight is a mobile AR Web browser that executes HTML5-based AR applications. By extending physical objects and places with a uniform resource identifier (URI), we could build objects of interest for mobile AR application as document object model (DOM) elements and control their behavior and user interactions through DOM events in standard HTML documents. A new CSS media type is defined to augment virtual objects to the physical objects. In this model, we introduce the concept of PLACE, which is the model of a physical space in which the user can be located. With this approach, mobile AR applications can be seamlessly developed as common HTML documents under the current Web eco-system. The advantages of the webized mobile AR, which is able to utilize all kind of Web resources without reworking, and its high productivity due to the seamless development of AR contents in HTML documents are shown with real-life use cases in various domains, such as shopping, entertainment, education, and manufacturing.},
keywords={authoring environment;content structure;mobile AR;real life;Web architecture;World Wide Web},
doi={10.1109/ISMAR.2014.6948471},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948472,
author={A. {Behmel} and W. {Höhl} and T. {Kienzl}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[DEMO] MRI design review system: A mixed reality interactive design review system for architecture, serious games and engineering using game engines, standard software, a tablet computer and natural interfaces},
year={2014},
volume={},
number={},
pages={327-328},
abstract={Experience and control your design using natural interfaces! Most of todays conventional design review systems require special programming skills for preparation and high-capacity hard- and software for demonstration. Interacting with 3D data sometimes can be complicated. Today we face five major problem fields using design review systems: Interaction with 3D data, navigation in 3D space, controlling design alternatives, design presentation using less extensive hardware, content development without special software and programming skills. Developments also targeting these issues by using different methods are presented e.g. by LANCELLE, SETTGAST and FELLNER (2008). They developed DAVE — Definitely Affordable Virtual Environment at Graz University of Technology. This immersive cage-based system today is used in evaluating the design of the new main railway station in Vienna, Austria. Also SHIRATUDDIN and THABET (2011) utilized the Torque 3D game engine to develop a Virtual Design Review System. Finally DUNSTON et. al. (2011) designed an Immersive Virtual Reality Mock-Up for Design Review of Hospital Patient Rooms. These and other research work was based on standard 3D game engines by using a conventional cave or power wall for presentation and physical immersion. The edddison MRI Design Review System is an easy to use mixed reality interface for design evaluation and presentation. It integrates a wide range of hardware input systems including a special 3D-printed tangible user interface, desktop computers, tablets and touch screens. On the software side it offers plug-ins for standard 3D software including Autodesk Navisworks and Showcase, Unity3D, Trimbles, SketchUp, Web GL and others. The edddison MRI Design Review System enables laymen to create their own interactive 3D content. It is a solution which makes the creation and presentation of interactive 3D applications as simple as preparing a powerpoint presentation. Without any programming skills you can easily manipulate 3D models within standard software applications. Control, change or adapt your design easily and interact with 3D models by natural interfaces and standard handheld devices. Navigate in 3D space using only your tablet computer. Complex buildings can be experienced by means of 2D floor plans and a touchscreen. System requirements are reduced by using standard software applications such as SketchUp or Unity3D. The edddison MRI Design Review System also makes it easy to present different design stages without extensive hard- and software on all common mobile platforms. Actual application areas are Architectural Design, Digital Prototyping, Industrial Simulation, Serious Games and Product Presentation. Currently, the system has two major use-cases: one setup will show the WebGL demo running on an iPad or an Android tablet computer. Using a WebGL/HTML5 cloud solution MRI Design Review System is able to reach the masses. The second demo is a SketchUp file controlled by optical tracking and 3D printed tangible objects also using a touchscreen or a handheld device. The edddison MRI Design Review System extends the range of existing design review systems with an easy-to-use hard- and software. Herein it simplifies the whole design process by an evolutionary, iterative approach, combined with a bunch of user-friendly intuitive interfaces.},
keywords={Easy to use Mixed-Reality Interface (MRI);User-Friendly Virtual Construction Kit;Building Information Modeling (BIM);Digital Prototyping;3D-Visualization;Serious Games;Unity3D;SketchUp;Autodesk Navisworks;Autodesk Showcase;Unity3D;Trimble;SketchUp;Web GL},
doi={10.1109/ISMAR.2014.6948472},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948473,
author={D. {Caetano} and F. {Mattioli} and E. {Lamounier} and A. {Cardoso}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[DEMO] On the use of augmented reality techniques in a telerehabilitation environment for wheelchair users' training},
year={2014},
volume={},
number={},
pages={329-330},
abstract={This work's purpose is to investigate the use of Augmented Reality techniques on telerehabilitation, applied to wheelchair users training. In this scenario, using a computer with unconventional devices, the user will be connected to a remote training space and will be able to issue commands, in order to accomplish the execution of training exercises. The telerehabilitation environment should reproduce the main challenges faced by wheelchair users in their daily activities.},
keywords={Augmented Reality;Telerehabilitation;Wheelchair Training},
doi={10.1109/ISMAR.2014.6948473},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948474,
author={A. {Crivellaro} and Y. {Verdie} and K. M. {Yi} and P. {Fua} and V. {Lepetit}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[DEMO] Tracking texture-less, shiny objects with descriptor fields},
year={2014},
volume={},
number={},
pages={331-332},
abstract={Our demo demonstrates the method we published at CVPR this year for tracking specular and poorly textured objects, and lets the visitors experiment with it and with their own patterns. Our approach only requires a standard monocular camera (no need for a depth sensor), and can be easily integrated within existing systems to improve their robustness and accuracy.},
keywords={Robust tracking;Dense Descriptors;Specular objects},
doi={10.1109/ISMAR.2014.6948474},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948475,
author={U. {Eck} and F. {Pankratz} and C. {Sandor} and G. {Klinker} and H. {Laga}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[DEMO] Comprehensive workspace calibration for visuo-haptic augmented reality},
year={2014},
volume={},
number={},
pages={333-334},
abstract={Visuo-haptic augmented reality systems enable users to see and touch digital information that is embedded in the real world. Precise colocation of computer graphics and the haptic stylus is necessary to provide a realistic user experience. PHANToM haptic devices are often used in such systems to provide haptic feedback. They consist of two interlinked joints, whose angles define the position of the haptic stylus and three sensors at the gimbal to sense its orientation. Previous work has focused on a calibration procedures that align the haptic workspace within a global reference coordinate system and an algorithms that compensate the non-linear position error, which is caused by inaccuracies in the joint angle sensors. In our science and technology paper “Comprehensive Workspace Calibration for Visuo-Haptic Augmented Reality” [1], we present an improved workspace calibration that additionally compensates for errors in the gimbal sensors. This enables us to also align the orientation of the haptic stylus with high precision. To reduce the required time for calibration and to increase the sampling coverage, we utilize time-delay estimation to temporally align external sensor readings. This enables users to continuously move the haptic stylus during the calibration process, as opposed to commonly used point and hold processes. This demonstration showcases the complete workspace calibration procedure as described in our paper including a mixed reality demo scenario, that allows users to experience the calibrated workspace. Additionally, we demonstrate an early stage of our proposed future work in improved user guidance during the calibration procedure using visual guides.},
keywords={H.5.1. [Information Interfaces and Presentation]: Multimedia Information Systems — [Artificial, augmented and virtual realities];H.5.2. [Information Interfaces and Presentation];User Interfaces — [Haptic I/O]},
doi={10.1109/ISMAR.2014.6948475},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948476,
author={M. {Fischbach} and C. {Zimmerer} and A. {Giebler-Schubert} and M. E. {Latoschik}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[DEMO] Exploring multimodal interaction techniques for a mixed reality digital surface},
year={2014},
volume={},
number={},
pages={335-336},
abstract={Quest — XRoads is a multimodal and multimedia mixed reality version of the traditional role-play tabletop game Quest: Zeit der Helden. The original game concept is augmented with virtual content to permit a novel gaming experience. The demonstration consists of a turn-based skirmish, where players have to collaborate, control heroes or villains and use their abilities via speech, gesture, touch as well as tangible interactions.},
keywords={Multimodal;Mixed Reality;Tangible;Speech;Gesture;User Experience;Tabletop},
doi={10.1109/ISMAR.2014.6948476},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948477,
author={T. {Goldschwendt} and C. {Anthes} and G. {Schubert} and D. {Kranzlmüller} and F. {Petzold}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[DEMO] The collaborative design platform — A protocol for a mixed reality installation for improved incorporation of laypeople in architecture},
year={2014},
volume={},
number={},
pages={337-338},
abstract={Presentations and discussions between architects and clients during the early stages of design usually involve sketches, paper and models, with digital information in the form of simulations and analyses used to assess variants and underpin arguments. Laypeople, however, are not used to reading plans or models and find it difficult to relate digital representations to the real world. Immersive environments represent an alternative approach but are laborious and costly to produce, particularly in the early design phases where information and ideas are still vague. Our project shows, how linking analogue design tools and digital VR representation has given rise to a new interactive presentation platform that bridges the gap between analogue design methods and digital architectural presentation. The prototypical platform creates a direct connection between a physical volumetric model and interactive digital content using a large-format multi-touch table as a work surface combined with real-time 3D scanning. Coupling the 3D data from the scanned model with the 3D digital environment model makes it possible to compute design relevant simulations and analyses. These are displayed in real-time on the working model to help architects assess and substantiate their design decisions. Combining this with a 5-sided projection installation based on the concepts Carolina Cruz Neiras CAVE Automatic Virtual Environment (CAVE)1 offers an entirely new means of presentation and interaction. The design (physical working model), the surroundings (GIS data) and the simulations and analyses are presented stereoscopically in real-time in the virtual environment. While the architect can work as usual, the observer is presented with an entirely new mode of viewing. Different ideas and scenarios can be tried out spontaneously and new ideas can be developed and viewed directly in three dimensions. The client is involved more directly in the process and can contribute own ideas and changes, and then see these in user-centred stereoscopic 3D. By varying system parameters, the model can be walked through at life size.},
keywords={C.2.2 [Computer Systems Organization]: Computer-Communication Networks — Network Protocols;H.5.1 [Information Systems]: Information Interfaces and Presentation — Multimedia Information Systems;J.5 [Computer Applications]: Arts and Humanities — Architecture},
doi={10.1109/ISMAR.2014.6948477},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948478,
author={J. {Grubert} and H. {Seichter} and D. {Schmalstieg}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[DEMO] Towards user perspective augmented reality for public displays},
year={2014},
volume={},
number={},
pages={339-340},
abstract={We demonstrate ad-hoc augmentation of public displays on handheld devices, supporting user perspective rendering of display content. Our prototype system only requires access to a screencast of the public display, which can be easily provided through common streaming platforms and is otherwise self-contained. Hence, it easily scales to multiple users.},
keywords={user perspective rendering;public displays;augmented reality},
doi={10.1109/ISMAR.2014.6948478},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948479,
author={Y. {Hashemian} and M. {Gotsis} and D. {Baron}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[DEMO] Adventurous Dreaming Highflying Dragon: A full body game for children with Attention Deficit Hyperactivity Disorder (ADHD)},
year={2014},
volume={},
number={},
pages={341-342},
abstract={Adventurous Dreaming Highflying Dragon is a full body-driven, game prototype for children ages 6–8 with a diagnosis of Attention Deficit Hyperactivity Disorder (ADHD). The current prototype incorporates research evidence showing that physical activity can help improve ADHD-related symptoms. Physical activity is integrated with cognitively challenging tasks that may improve brain activity by encouraging goal planning and dedication. The current prototype is includes three mini-games, each of which teaches skills with real-life use potential. Players role-play as a young dragon in the story and virtual world as they repeat virtual tasks several times to gain mastery over real-life skills. Each activity is focused on a specific strength/weakness reported in children with ADHD, with game mechanics targeting ADHD diagnosis categories: specific hyperactivity, impulsivity and inattention. This demonstration is based on the findings published in Hashemian, Y., & Gotsis, M. 2013, November. In Proceedings of the 4th Conference on Wireless Health (p. 12). ACM. http://dl.acm.org/citation.cfm?id=2534101},
keywords={Games;ADHD/ADD;Kinect;Attention;Executive Function;Motor skills;Prototyping},
doi={10.1109/ISMAR.2014.6948479},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948480,
author={G. {Hough} and I. {Williams} and C. {Athwal}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Demo] Measurement of perceptual tolerance for inconsistencies within mixed reality scenes},
year={2014},
volume={},
number={},
pages={343-344},
abstract={This demonstration is a live example of the experiment presented in [1], namely a method of assessing the visual credibility of a scene where a real person interacts with a virtual object in realtime. Inconsistencies created by actor's incorrect estimation of the virtual object are measured through a series of videos, each containing a defined visual error and rated against interaction credibility on a scale of 1–5 by conference delegates.},
keywords={Virtual Studios;Performance Measurement;Interaction Framework;Mixed Reality},
doi={10.1109/ISMAR.2014.6948480},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948481,
author={Y. {Itoh} and G. {Klinker}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[DEMO] INDICA : Interaction-free display calibration for optical see-through head-mounted displays based on 3D eye localization},
year={2014},
volume={},
number={},
pages={345-346},
abstract={A correct spatial registration of Optical See-Through Head-Mounted Displays (OST-HMD) w.r.t. a user's eye(s) is an essential problem for any AR application using the such HMDs (Fig. 1). Maintaining the correct registration demands frequent (re)calibrations for the end-users whenever they move the HMD on their head. Thus, a calibration technique should be simple and accurate for the universal, long-run use of the displays. This demonstration showcases INDICA, an automatic OST-HMD calibration approach presented in our previous work[1] and ISMAR 2014 paper [2]. The method calibrates the display to the user's current eyeball position by combining online eye-position tracking with offline parameters. Visitors of our demonstration can try our both manual calibration and our interaction-free calibration on a customized OST-HMD.},
keywords={Optical see-though head-mounted display;HMD;calibration;eye tracking},
doi={10.1109/ISMAR.2014.6948481},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948482,
author={F. L. {Keppmann} and T. {Käfer} and S. {Stadtmüller} and R. {Schubotz} and A. {Harth}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[DEMO] Integrating highly dynamic RESTful linked data APIs in a virtual reality environment},
year={2014},
volume={},
number={},
pages={347-348},
abstract={We demonstrate a Virtual Reality information system that shows the applicability of REST in highly dynamic environments as well as the advantages of Linked Data for on-the-fly data integration. We integrate a motion detection sensor application to remote control an avatar in the Virtual Reality. In the Virtual Reality, information about the user is integrated and visualised. Moreover, the user can interact with the visualised information.},
keywords={},
doi={10.1109/ISMAR.2014.6948482},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948483,
author={S. B. {Knorr} and D. {Kurz}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Demo] Real-time illumination estimation from faces for coherent rendering},
year={2014},
volume={},
number={},
pages={349-350},
abstract={We showcase a method for estimating the real-world lighting conditions within a scene based on the visual appearance of the user's face captured in a single image of a monocular user-facing RGB camera. The implementation is based on our ISMAR 2014 paper [8]. The light reflected from the face towards the camera is measured and the most plausible real-world lighting condition explaining the measurement is estimated in real-time based on knowledge acquired in a one-time pre-processing of a set of images of different faces under known illumination. The estimated illumination is instantly used for the rendering of the virtual objects.},
keywords={ISMAR 2014;Demo;Illumination Estimation;Coherent Rendering;Real-Time;Monocular;Augmented Reality;Inverse Lighting;Precomputed Radiance Transfer;Spherical Harmonics;Computer Graphics;Computer Vision;Machine Learning},
doi={10.1109/ISMAR.2014.6948483},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948484,
author={M. {Krichenbauer} and G. {Yamamoto} and T. {Taketomi} and C. {Sandor} and H. {Kato}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[DEMO] Towards augmented reality user interfaces in 3D media production},
year={2014},
volume={},
number={},
pages={351-351},
abstract={For this demo, we present an Augmented Reality (AR) User Interface (UI) for the 3D design software Autodesk Maya, aimed at professional media creation. A user wears a head-mounted display (HMD) and thin cotton gloves which allow him to interact with virtual 3D models in the work area. Additional viewers can see the video stream on a projector and thus share the users view. Both head and hand positions are tracked from the HMD video stream, and an inertial measurement unit (IMU) and conductive materials on the gloves allow interaction with virtual objects. This system is built using Autodesk Maya — a professional 3D software package commonly used in the media industry — and aims to fulfill the requirements of professional 3D design work which we identified in our paper of the same title. While still an early prototype, it was already tested with media professionals to evaluate our approach.},
keywords={},
doi={10.1109/ISMAR.2014.6948484},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948485,
author={D. {Kurz}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Demo] Thermal touch: Thermography-enabled everywhere touch interfaces for mobile augmented reality applications},
year={2014},
volume={},
number={},
pages={353-354},
abstract={We present an approach that makes any real object a touch interface for mobile Augmented Reality (AR) applications. Using infrared thermography, we detect residual heat resulting from a warm fingertip touching the colder surface of an object. This approach can clearly distinguish if a surface has actually been touched, or if a finger only approached it without any physical contact, and hence significantly less heat transfer. Once a touch has been detected in the thermal image, we determine the corresponding 3D position on the touched object based on object tracking using a visible light camera. This 3D position then enables user interfaces to naturally interact with real objects and associated digital information. The emergence of wearable computers and head-mounted displays desires for alternatives to a touch screen, which is the primary user interface in handheld Augmented Reality applications. Voice control and touchpads provide a useful alternative to interact with wearables for certain tasks, but particularly common interaction tasks in Augmented Reality require to accurately select or define 3D points on real surfaces. We propose to enable this kind of interaction by simply touching the respective surface with a fingertip. In this demonstration, which is based on our ISMAR 2014 paper [2], we show that our method enables intuitive interaction for mobile Augmented Reality with common objects.},
keywords={},
doi={10.1109/ISMAR.2014.6948485},
ISSN={},
month={Sep.},}