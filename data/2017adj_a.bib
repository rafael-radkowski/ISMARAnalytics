@INPROCEEDINGS{8088418,
author={},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[Title page i]},
year={2017},
volume={},
number={},
pages={i-i},
abstract={Presents the title page of the proceedings record.},
keywords={},
doi={10.1109/ISMAR-Adjunct.2017.1},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088419,
author={},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[Title page iii]},
year={2017},
volume={},
number={},
pages={iii-iii},
abstract={Presents the title page of the proceedings record.},
keywords={},
doi={10.1109/ISMAR-Adjunct.2017.2},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088420,
author={},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[Copyright notice]},
year={2017},
volume={},
number={},
pages={iv-iv},
abstract={Presents the copyright information for the conference. May include reprint permission information.},
keywords={},
doi={10.1109/ISMAR-Adjunct.2017.3},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088421,
author={},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Table of contents},
year={2017},
volume={},
number={},
pages={v-xii},
abstract={The following topics are dealt with: mixed reality; augmented reality; and interactive systems.},
keywords={augmented reality;interactive systems;mixed reality;augmented reality;interactive systems},
doi={10.1109/ISMAR-Adjunct.2017.4},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088422,
author={},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Message from the ISMAR 2017 General Chair and Deputy General Chairs},
year={2017},
volume={},
number={},
pages={xiii-xiii},
abstract={Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.},
keywords={},
doi={10.1109/ISMAR-Adjunct.2017.5},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088423,
author={},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Message from the ISMAR 2017 Science and Technology Program Chairs},
year={2017},
volume={},
number={},
pages={xiv-xvi},
abstract={Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.},
keywords={},
doi={10.1109/ISMAR-Adjunct.2017.6},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088424,
author={},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Message from the ISMAR 2017 Science and Technology Poster Chairs},
year={2017},
volume={},
number={},
pages={xvii-xviii},
abstract={Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.},
keywords={},
doi={10.1109/ISMAR-Adjunct.2017.7},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088425,
author={},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Message from the Workshop and Tutorial Chairs},
year={2017},
volume={},
number={},
pages={xix-xix},
abstract={Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.},
keywords={},
doi={10.1109/ISMAR-Adjunct.2017.8},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088426,
author={},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Message from the Demonstration Chairs},
year={2017},
volume={},
number={},
pages={xx-xx},
abstract={Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.},
keywords={},
doi={10.1109/ISMAR-Adjunct.2017.9},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088427,
author={},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={ISMAR 2017 Conference Committee Members},
year={2017},
volume={},
number={},
pages={xxi-xxi},
abstract={Provides a listing of current committee members and society officers.},
keywords={},
doi={10.1109/ISMAR-Adjunct.2017.10},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088428,
author={},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={ISMAR 2017 Science and Technology Program Committee Members},
year={2017},
volume={},
number={},
pages={xxii-xxii},
abstract={Provides a listing of current committee members and society officers.},
keywords={},
doi={10.1109/ISMAR-Adjunct.2017.11},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088429,
author={},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={ISMAR 2017 Steering Committee Members},
year={2017},
volume={},
number={},
pages={xxiii-xxiii},
abstract={Provides a listing of current committee members and society officers.},
keywords={},
doi={10.1109/ISMAR-Adjunct.2017.12},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088430,
author={},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Reviewers},
year={2017},
volume={},
number={},
pages={xxiv-xxiv},
abstract={The conference offers a note of thanks and lists its reviewers.},
keywords={},
doi={10.1109/ISMAR-Adjunct.2017.13},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088431,
author={},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Keynotes},
year={2017},
volume={},
number={},
pages={xxv-xxvi},
abstract={Provides an abstract for each of the keynote presentations and may include a brief professional biography of each},
keywords={},
doi={10.1109/ISMAR-Adjunct.2017.14},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088432,
author={},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Sponsors and Supporters},
year={2017},
volume={},
number={},
pages={xxvii-xxix},
abstract={The conference organizers greatly appreciate the support of the various corporate sponsors listed.},
keywords={},
doi={10.1109/ISMAR-Adjunct.2017.15},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088433,
author={},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Tutorial 1: Future-TV Content and Aesthetics},
year={2017},
volume={},
number={},
pages={xxx-xxx},
abstract={Summary form only given, as follows. A complete record of the tutorial was not made available for publication as part of the conference proceedings. The first part of this tutorial gives a picture of the broadcast market and summarizes the key features of the new current standard UHDTV: 4K spatial resolution, High Dynamic Range, High Frame Rate and Wide Color Gamut. According to the current trend, the next step in the broadcast market should be to address more immersive and interactive contents such as 360 videos or even more free view-point contents. Next, the second part of the tutorial presents recent advances in the capture of animated 3D point clouds. 3D points clouds seem to be the most promising format that can propose many new user experiences. This tutorial part explains how to combine in a single acquisition system: 3d reconstruction, high resolution and high dynamic range. The last part of the tutorial takes a step back and open the scope to aesthetic intents, more precisely what are the challenges related to aesthetics intent when considering these new kinds of content. Using previous works on video tone mapping and video tone expansion, we focus on how to ensure aesthetic fidelity to an original content when retargeting this original content to a specific display.},
keywords={Three-dimensional displays;Videos;Dynamic range;Spatial resolution;HDTV},
doi={10.1109/ISMAR-Adjunct.2017.16},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088434,
author={L. {Gandel} and J. {Jomier}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Tutorial 2: Developing Virtual Reality applications with the Visualization Toolkit (VTK)},
year={2017},
volume={},
number={},
pages={xxxi-xxxi},
abstract={The aim of this tutorial is to provide an introduction to the recent features added to the Visualization Toolkit (VTK) that now allows for rendering in external immersive environment. An introduction to VTK will be given in order to explain the basics and how to create visualization applications, followed by a description of modules allowing you to take your application into virtual reality (VR). The last part of the tutorial will focus on interactions by presenting different ways to interact in a VR environment using the VTK pipeline. The aim is for attendees to be able to build their first application in VR using VTK. They will get strong knowledge on a cross-platform, open-source and freely available system software. By taking more people into the VTK-VR loop, we hope to make mixed and augmented reality benefit more from VTK's advanced visualization, processing and modeling techniques, such as volume rendering or point cloud visualization.},
keywords={augmented reality;data visualisation;public domain software;rendering (computer graphics);VR environment;VTK-VR loop;volume rendering;virtual reality applications;visualization toolkit;mixed reality;augmented reality;cross-platform software;open-source software;Virtual reality},
doi={10.1109/ISMAR-Adjunct.2017.17},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088435,
author={},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Tutorial 3: SOFA, an Open-Source Framework for Physics Simulation in Augmented Reality},
year={2017},
volume={},
number={},
pages={xxxii-xxxii},
abstract={This tutorial will shortly review the background in physics simulation and introduce the main principles of the SOFA framework. Examples of SOFA simulations will be presented. The afternoon will be more "hands-on" oriented starting with an interactive user tutorial, followed by a developer tutorial. Further to this tutorial, you should have all the basis to build your own physics simulation. The SOFA tutorial at ISMAR is the opportunity to discover an open-source physics engine and include physics in your AR applications. From a starting up to a developer level, this tutorial focus on the wide topic of physics simulation. Not only will the physics of SOFA add realism into your AR application, but it might allow you to address new research and industrial challenges. Moreover, the flexible architecture of the software and the large international open-source community will make your start with SOFA easier. Attend the tutorial and join the community! This tutorial is done once a year. A global publication about SOFA has been published in 2012: Multi-Model Framework for Interactive Physical Simulation. All publications based on SOFA can be found here: https://www.sofa-framework.org/applications/publications/.},
keywords={augmented reality;physics computing;public domain software;SOFA;open-source framework;physics simulation;augmented reality;Augmented reality},
doi={10.1109/ISMAR-Adjunct.2017.18},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088436,
author={A. {Tewari} and B. {Taetz} and F. {Grandidier} and D. {Stricker}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] A Probabilistic Combination of CNN and RNN Estimates for Hand Gesture Based Interaction in Car},
year={2017},
volume={},
number={},
pages={1-6},
abstract={Hand Gesture Recognition is completed on top-view hand images observed by a Time of Flight(ToF) camera in a car. The work attempts to solve two important problems of touchless interactions inside a car. First, low latency identification of the gestures which are unobtrusive for the driver. Second, reducing the labelled data required to train learning based solutions, this is particularly important because labelling of gesture sequences is expensive and exigent. This work attempts to improve the fast detection of hand-gestures by correcting the probability estimate of a Long Short Term Memory (LSTM) network by pose prediction made by a Convolutional Neural Network(CNN). Weak models for hand gesture classes based on five hand poses are designed to assist in the prediction-correction scheme. A training procedure to reduce the labelled data required for hand pose classification is also introduced. This method tries to utilise the statistical property of the dataset to identify a good initialization of weights for the CNN, here we demonstrate this using the Principal Component Analysis(PCA) embedding of non-labelled hand pose sequences. While solving a nine class hand gesture problem we demonstrate an accuracy of 89.50% which is better than existing systems. We also show that a PCA embedding based initialization improves the classification performance of the CNN based pose classifier.},
keywords={gesture recognition;image classification;learning (artificial intelligence);pose estimation;principal component analysis;probability;recurrent neural nets;RNN estimates;touchless interactions;low latency identification;labelled data;gesture sequences;hand-gestures;hand gesture classes;hand poses;prediction-correction scheme;PCA embedding based initialization;hand gesture recognition;convolutional neural network;CNN estimates;hand gesture based interaction;long short term memory network;principal component analysis;Hidden Markov models;Automobiles;Cameras;Training;Shape;Gesture recognition;Estimation},
doi={10.1109/ISMAR-Adjunct.2017.19},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088437,
author={A. {Nassani} and G. {Lee} and M. {Billinghurst} and T. {Langlotz} and S. {Hoermann} and R. W. {Lindeman}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] The Social AR Continuum: Concept and User Study},
year={2017},
volume={},
number={},
pages={7-8},
abstract={In this poster, we describe The Social AR Continuum, a space that encompasses different dimensions of Augmented Reality (AR) for sharing social experiences. We explore various dimensions, discuss options for each dimension, and brainstorm possible scenarios where these options might be useful. We describe a prototype interface using the contact placement dimension, and report on feedback from potential users which supports its usefulness for visualising social contacts. Based on this concept work, we suggest user studies in the social AR space, and give insights into future directions.},
keywords={augmented reality;data visualisation;social networking (online);user interfaces;social AR continuum;user study;social experiences;contact placement dimension;augmented reality;prototype interface;social contacts visualization;social AR space;Avatars;Three-dimensional displays;Visualization;Augmented reality;Prototypes;Collaboration;Face},
doi={10.1109/ISMAR-Adjunct.2017.20},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088438,
author={A. {Dolhasz} and M. {Frutos-Pascual} and I. {Williams}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] Composite Realism: Effects of Object Knowledge and Mismatched Feature Type on Observer Gaze and Subjective Quality},
year={2017},
volume={},
number={},
pages={9-14},
abstract={We report on the results of the first visual search and rating study (N60) evaluating human gaze when assessing the realism of image composites. The effects of object identity knowledge and mismatched feature type on observers' gaze and subjective realism scores are studied. Gaze metrics used include: fixation count, fixation duration, time and duration of first fixation on target object, as well as area of interest similarity and inter-observer consistency. Monte-Carlo-based techniques are used for analysis of the data obtained. Results indicate that while knowledge of object identity impacts gaze allocation and response times, it leaves subjective realism ratings unchanged. We show that the type of mismatched feature (correlated colour temperature vs exposure) has a significant impact on fixation counts and durations. This study provides a first step to utilising objective gaze metrics to better understand subjective assessment processes and leads towards the development of gaze-inspired compositing methods.},
keywords={gaze tracking;image processing;Monte Carlo methods;fixation count;fixation duration;target object;inter-observer consistency;Monte-Carlo-based techniques;objective gaze metrics;observer gaze;subjective quality;visual search;human gaze;image composites;object identity knowledge;composite realism;subjective realism;gaze allocation;Observers;Visualization;Measurement;Image segmentation;Image color analysis;Semantics;compositing;eye tracking;subjective quality;realism},
doi={10.1109/ISMAR-Adjunct.2017.21},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088439,
author={A. A. {Cervera-Uribe}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] The Augmented Library: An Approach for Improving Users Awareness in a Campus Library},
year={2017},
volume={},
number={},
pages={15-19},
abstract={Most existing libraries use floor plans and call numbers in order to locate books rather than providing truly user's awareness of the extensive collections and services for students in the library context. With a collection of more than 48,568 volumes at the Engineering and Exact Sciences library of the Universidad Autónoma de Yucatán (UADY), the task of looking for a book can be really a time-consuming and frustrating task, specially for newcomers. This paper proposes an Augmented Reality (AR) based application which aims to solve users spatial unawareness at library by providing an AR shelf searching system, and to assist students and newcomers by providing a mobile guide with library information services into library. In this work, the results of a pilot study are presented demonstrating that user's experiences and performance at library were enhanced through the use of augmented reality technology.},
keywords={academic libraries;augmented reality;educational institutions;information services;library automation;mobile computing;campus library;floor plans;book;extensive collections;library context;users spatial unawareness;library information services;augmented reality technology;augmented library;call numbers;Engineering and Exact Sciences library;Universidad Autónoma de Yucatán;UADY;augmented reality based application;AR shelf searching system;mobile guide;Libraries;Augmented reality;Cameras;Mobile communication;User interfaces;Mobile handsets;library;augmented reality;mobile guide;localization},
doi={10.1109/ISMAR-Adjunct.2017.22},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088440,
author={A. {Kacete} and T. {Wentz} and J. {Royan}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] Decision Forest For Efficient and Robust Camera Relocalization},
year={2017},
volume={},
number={},
pages={20-24},
abstract={To robustly estimate the pose, classical methods assume some geometrical and temporal assumptions (SfM: Structure from Motion, SLAM: Simultaneous Localization and mapping). These approaches take a pair of images as input and establish correspondences based on global strategy (using the whole image information) or sparse strategy (using key-points features). These correspondences allow solving a set of linear equations related to the 3D information and camera pose in that environment. These past years, machine learning has been considered as an efficient way to tackle different problems in image processing and computer vision fields. To handle the task in hand, we propose to learn directly the mapping function between the acquired information from the camera and its pose using sparse decision forest. We achieved state-of the-art results on public and on our databases.},
keywords={cameras;computer vision;feature extraction;image processing;learning (artificial intelligence);pose estimation;robot vision;SLAM (robots);machine learning;image processing;sparse decision forest;robust camera relocalization;image information;linear equations;computer vision;Cameras;Vegetation;Training;Robustness;Pose estimation;Testing;Camera relocalization;pose estimation;Random Forest;SLAM},
doi={10.1109/ISMAR-Adjunct.2017.23},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088441,
author={A. {Schankin} and D. {Reichert} and M. {Berning} and M. {Beigl}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] The Impact of the Frame of Reference on Attention Shifts Between Augmented Reality and Real-World Environment},
year={2017},
volume={},
number={},
pages={25-30},
abstract={Augmented reality (AR) systems allow enriching the environment by supportive and useful virtual data. However, the human information processing capacity is limited. Thus, additional information may distract attention from the environment to the virtual data, resulting in a lower perception of relevant real-world information. In this paper we evaluate whether this effect depends on the frame of reference used to overlay the real world with virtual data. In a user study with 20 participants, we measured the reaction time to simple color stimuli presented either in AR or the environment while participants focused their attention either in AR or the environment. Stimuli in AR were presented in two different frames of reference - screen-stabilized or world-fixed. Responses to signals in AR were significantly faster than to signals in the environment, suggesting a dominance of virtual information over real ones. This also resulted in a significantly prolonged reaction time when participants needed to shift their attention from AR to the environment (but not vice versa). This negative effect disappeared when virtual data was presented world-fixed. Obviously, a world-fixed frame of reference allows a better blending of AR and real world information, and thus avoiding a dominant perception of virtual information.},
keywords={augmented reality;cognition;psychology;visual perception;attention shifts;augmented reality systems;human information processing capacity;psychological studies;color stimuli;virtual data;Augmented reality;Tunneling;Electronic mail;Automobiles;Image color analysis;See-through augmented reality system;world-fixed frame of reference;selective attention;attention tunneling},
doi={10.1109/ISMAR-Adjunct.2017.24},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088442,
author={A. {Blaga} and M. {Frutos-Pascual} and M. {Al-Kalbani} and I. {Williams}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] Usability Analysis of an Off-the-Shelf Hand Posture Estimation Sensor for Freehand Physical Interaction in Egocentric Mixed Reality},
year={2017},
volume={},
number={},
pages={31-34},
abstract={This paper explores freehand physical interaction in egocentric Mixed Reality by performing a usability study on the use of hand posture estimation sensors. We report on precision, interactivity and usability metrics in a task-based user study, exploring the importance of additional visual cues when interacting. A total of 750 interactions were recorded from 30 participants performing 5 different interaction tasks (Move, Rotate: Pitch (Y axis) and Yaw (Z axis), Uniform scale: enlarge and shrink). Additional visual cues resulted in an average shorter time to interact, however, no consistent statistical differences were found in between groups for performance and precision results. The group with additional visual cues gave the system and average System Usability Scale (SUS) score of 72.33 (SD 16.24) while the other scored a 68.0 (SD 18.68). Overall, additional visual cues made the system being perceived as more usable, despite the fact that the use of these two different conditions had limited effect on precision and interactivity metrics.},
keywords={human computer interaction;image sensors;pose estimation;user interfaces;virtual reality;freehand physical interaction;usability metrics;interactivity metrics;off-the-shelf hand posture estimation sensor;visual cues;usability analysis;egocentric mixed reality;interaction tasks;task-based user study;average system usability scale score;SUS;Three-dimensional displays;Usability;Visualization;Measurement;Virtual reality;Estimation},
doi={10.1109/ISMAR-Adjunct.2017.25},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088443,
author={C. {Heinrich} and T. {Langlotz} and R. {O'Keefe}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] Volume Lens: Exploring Medical Volume Datasets Using Mobile Devices},
year={2017},
volume={},
number={},
pages={35-38},
abstract={Volume data representations are widely used in many different domains including medical imaging. Here, they are primarily accessed using WIMP interfaces on desktop PCs, while the use of handheld mobile devices to browse volume data is widely ignored. In this work, we present Volume Lens, an approach for displaying volume datasets on mobile hardware by implementing a novel interface that turns the mobile device into an exploration tool. The user can physically interact with the mobile device to interactively slice through the displayed virtual volume. The displayed slice is based on the position of the mobile device within the environment. We present our approach together with a explorative study gaining feedback from experts in the medical domain.},
keywords={data visualisation;medical computing;mobile computing;solid modelling;user interfaces;virtual reality;medical volume datasets;mobile device;Volume data representations;medical imaging;handheld mobile devices;mobile hardware;displayed virtual volume;medical domain;volume lens;WIMP interfaces;desktop PC;Lenses;Mobile handsets;Three-dimensional displays;Prototypes;Biomedical imaging;Rendering (computer graphics);Cameras;Volume Lens;Volume rendering;Mobile;User study;Magic Lens},
doi={10.1109/ISMAR-Adjunct.2017.26},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088444,
author={C. {Kollatsch} and M. {Schumann} and P. {Klimant} and M. {Lorenz}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] Industrial Augmented Reality: Transferring a Numerical Control Connected Augmented Realty System from Marketing to Maintenance},
year={2017},
volume={},
number={},
pages={39-41},
abstract={Connecting third party systems like machine controls and product data management systems with Augmented Reality (AR) support applications is crucial for their industrial success. We present a marketing use case of a Virtual Press demonstrator where the connection to a machine control was successfully implemented. The realization of this application utilized the AR framework ARViewer that is introduced in this paper. Subsequent, an outlook is given how this framework can be extended using three examples of AR maintenance applications.},
keywords={augmented reality;maintenance engineering;marketing data processing;numerical control;production engineering computing;numerical control connected augmented reality system;industrial augmented reality;AR framework ARViewer;AR maintenance applications;Virtual Press demonstrator;marketing;machine control;Augmented reality;Augmented Reality;Numerical Control;Machine Control;Marketing;Maintenance},
doi={10.1109/ISMAR-Adjunct.2017.27},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088445,
author={D. {Roth} and C. {Kleinbeck} and T. {Feigl} and C. {Mutschler} and M. E. {Latoschik}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] Social Augmentations in Multi-User Virtual Reality: A Virtual Museum Experience},
year={2017},
volume={},
number={},
pages={42-43},
abstract={This work in progress report demonstrates a novel approach for behavioral augmentations in Virtual Reality (VR). Using a large scale tracking system, groups of five users explored a virtual museum. We investigated how augmenting social interactions impacts this experience, by designing behavioral transformations for behavioral phenomena in social interactions. Preliminary data indicate a reduction of perceived isolation, and a more thought-provoking experience with active behavioral augmentations.},
keywords={augmented reality;groupware;human computer interaction;human factors;museums;virtual museum experience;multiuser virtual reality;social interactions;social augmentations;behavioral augmentations;Electronic mail;Tracking;Solid modeling;Visualization;Dinosaurs;Virtual environments},
doi={10.1109/ISMAR-Adjunct.2017.28},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088446,
author={D. {Schneider} and J. {Grubert}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] Feasibility of Corneal Imaging for Handheld Augmented Reality},
year={2017},
volume={},
number={},
pages={44-45},
abstract={Smartphones are a popular device class for mobile Augmented Reality but suffer from a limited input space. Around-device interaction techniques aim at extending this input space using various sensing modalities. In this paper we present our work towards extending the input area of mobile devices using front-facing device-centered cameras that capture reflections in the cornea. As current generation mobile devices lack high resolution front-facing cameras, we study the feasibility of around-device interaction using corneal reflective imaging based on a high resolution camera. We present a workflow, a technical prototype and a feasibility evaluation.},
keywords={augmented reality;cameras;eye;mobile computing;smart phones;corneal imaging;smartphones;high resolution front-facing cameras;corneal reflective imaging;high resolution camera;mobile augmented reality;mobile devices;handheld augmented reality;Cameras;Mobile handsets;Image resolution;Mobile communication;Cornea;Pipelines},
doi={10.1109/ISMAR-Adjunct.2017.29},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088447,
author={E. {Özgür} and A. {Lafont} and A. {Bartoli}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] Visualizing In-Organ Tumors in Augmented Monocular Laparoscopy},
year={2017},
volume={},
number={},
pages={46-51},
abstract={One of the important goals of medical augmented reality is to reveal the hidden anatomy, such as a tumor in an organ. However, conveying a hidden tumor's depth to the user effortlessly and precisely is still an unsolved problem. This is especially difficult in monocular laparoscopy. First, the number of available depth cues is in practice limited to only two: occlusion and relative size. Second, exploiting these cues is not an easy task either. We propose a specific visualization consisting of auxiliary orthographic tumor silhouettes on the front and back surfaces of the organ and a semi-transparent tumor in between. This creates two depth planes forming a perceivable ratio-scaled metric space for the tumor. We conducted a user study to evaluate the proposed visualization. The results show that subsurface tumor depth perception is improved dramatically compared to the conventional transparent overlay.},
keywords={augmented reality;data visualisation;medical computing;tumours;auxiliary orthographic tumor silhouettes;semitransparent tumor;depth planes;subsurface tumor depth perception;In-Organ Tumors;augmented monocular laparoscopy;medical augmented reality;hidden anatomy;hidden tumor;occlusion;relative size;perceivable ratio-scaled metric space;specific visualization;depth cues;Tumors;Laparoscopes;Visualization;Surgery;Augmented reality;Image color analysis;erol.ozgur@uca.fr;alexis.lafont@uca.fr;adrien.bartoli@gmail.com},
doi={10.1109/ISMAR-Adjunct.2017.30},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088448,
author={F. {Cutolo} and U. {Fontana} and M. {Carbone} and R. {D'Amato} and V. {Ferrari}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] Hybrid Video/Optical See-Through HMD},
year={2017},
volume={},
number={},
pages={52-57},
abstract={An old but still ongoing subject of debate among augmented reality (AR) experts is about which see-through paradigm is best in wearable AR displays. Video see-through (VST) and optical see-through (OST) paradigms have both their own strengths and shortcomings with respect to technological and human-factor aspects. The major difference between these see-through paradigms is in providing an aided (VST) or unaided (OST) view of the real world. In this work, we present a novel approach for the development of AR stereoscopic head-mounted displays (HMDs) that can provide both the see-through mechanisms. Our idea is to dynamically modify the transparency of the display through a liquid crystal (LC)-based electro-optical shutter applied on the top of a standard OST device opportunely modified for housing a pair of external cameras. A plane-induced homography transformation is used for consistently warping the video images, hence reducing the parallax between cameras and displays. An externally applied drive voltage is used for smoothly controlling the light transmittance of the LC shutters so as to allow an easy transition between the unaided and the camera-mediated view of the real scene. Our tests have proven the efficacy of the proposed solution under worst-case lighting conditions.},
keywords={augmented reality;cameras;helmet mounted displays;human factors;stereo image processing;video signal processing;augmented reality;VST;human-factor aspects;standard OST device;external cameras;externally applied drive voltage;LC shutters;camera-mediated view;wearable AR displays;hybrid video/optical see-through HMD;OST paradigms;technological aspect;AR stereoscopic head-mounted displays;see-through mechanisms;liquid crystal-based electro-optical shutter;LC;plane-induced homography transformation;video image warping;light transmittance control;worst-case lighting conditions;Cameras;Resists;Optical imaging;Three-dimensional displays;Electrooptical waveguides;Standards;Integrated optics;HMD;Video see-through;Optical see-through},
doi={10.1109/ISMAR-Adjunct.2017.31},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088449,
author={F. {Han} and J. {Liu} and W. {Hoff} and H. {Zhang}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] Planning-Based Workflow Modeling for AR-enabled Automated Task Guidance},
year={2017},
volume={},
number={},
pages={58-62},
abstract={In this paper, we implemented and validated a workflow modeling approach that is able to model a sequence of procedures to achieve a complex task to enable an AR-based automated task guidance system. We formulated automated task guidance as a decision making problem, based upon the general Partially Observable Markov Decision Processes (POMDP) paradigm as the foundation. Our approach is able to provide actionable information to actively instruct users through a complex multi-step task. Our method can also plan ahead an action sequence that is optimal in the long term, while maintaining flexibility to deal with changes in an uncertain environment. We validated our approach in the applications of copy machine inspection and compressor startup guidance. The experimental results have shown the effectiveness of our planning-based workflow models in real-world applications.},
keywords={augmented reality;compressors;decision making;inspection;Markov processes;mechanical engineering computing;planning (artificial intelligence);workflow management software;workflow modeling approach;complex task;decision making problem;general Partially Observable Markov Decision Processes paradigm;actionable information;complex multistep task;action sequence;compressor startup guidance;planning-based workflow models;AR-enabled automated task guidance system;copy machine inspection;Inspection;Pulse width modulation;Hidden Markov models;Maintenance engineering;Computational modeling;Training data;Workflow modeling;augmented reality;task guidance},
doi={10.1109/ISMAR-Adjunct.2017.32},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088450,
author={G. S. S. S. {Rao} and N. {Thakur} and V. {Namboodiri}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] Reactive Displays for Virtual Reality},
year={2017},
volume={},
number={},
pages={63-68},
abstract={The feeling of presence in virtual reality has enabled a large number of applications. These applications typically deal with 360° content. However, a large amount of existing content is available in terms of images and videos i.e 2D content. Unfortunately, these do not react to the viewer's position or motion when viewed through a VR HMD. Thus in this work, we propose reactive displays for VR which instigate a feeling of discovery while exploring 2D content. We create this by taking into account user's position and motion to compute homography based mappings that adapt the 2D content and re-project it onto the display. This allows the viewer to obtain a more richer experience of interacting with 2D content similar to the effect of viewing through the window at a scene. We also provide a VR interface that uses a constrained set of reactive displays to easily browse through 360° content. The proposed interface tackles the problem of nausea caused by existing interfaces like photospheres by providing a natural room-like intermediate interface before changing 360° content. We perform user studies to evaluate both of our interfaces. The results show that the proposed reactive display interfaces are indeed beneficial.},
keywords={graphical user interfaces;helmet mounted displays;virtual reality;reactive displays;helmet mounted displays;VR HMD;virtual reality;reactive display interfaces;natural room-like intermediate interface;VR interface;homography based mappings;Cameras;Two dimensional displays;Videos;Virtual reality;Three-dimensional displays;Transmission line matrix methods;Resists;Virtual Reality;Reactive Displays},
doi={10.1109/ISMAR-Adjunct.2017.33},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088451,
author={G. {Hiranandani} and K. {Ayush} and C. {Varsha} and A. {Sinha} and P. {Maneriker} and S. V. R. {Maram}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] Enhanced Personalized Targeting Using Augmented Reality},
year={2017},
volume={},
number={},
pages={69-74},
abstract={Augmented Reality (AR) based applications have existed for some time; however, their true potential in digital marketing remains unexploited. To bridge this gap we create a novel consumer targeting system. First, we analyze consumer interactions on AR-based retail apps to identify her preferred purchase viewpoint during the session. We then target the consumer through a personalized catalog, created by embedding recommended products in her viewpoint visual. The color and style of the embedded product are matched with the viewpoint to create recommendations, and personalized text content is created using visual cues from the AR data. Evaluation with user studies show that our system is able to identify the viewpoint, our recommendations are better than tag-based recommendations, and targeting using the viewpoint is better than that of usual product catalogs.},
keywords={augmented reality;consumer products;mobile computing;recommender systems;retail data processing;user interfaces;digital marketing;consumer interactions;AR-based retail apps;preferred purchase viewpoint;personalized catalog;viewpoint visual;embedded product;personalized text content;visual cues;tag-based recommendations;augmented reality;enhanced personalized targeting;AR based applications;consumer targeting system;recommended products embedding;Visualization;Solid modeling;Image color analysis;Three-dimensional displays;Augmented reality;Data models;Business;Augmented reality;viewpoint selection;recommendation;targeting;v-Commerce},
doi={10.1109/ISMAR-Adjunct.2017.34},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088452,
author={G. {Nie} and Y. {Liu} and Y. {Wang}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] Prevention of Visually Induced Motion Sickness Based on Dynamic Real-Time Content-Aware Non-salient Area Blurring},
year={2017},
volume={},
number={},
pages={75-78},
abstract={This paper proposes an innovative method for reducing the visually induced motion sickness (MS) occurred in a 3D immersive virtual environment (VE) by utilizing a flexible dynamic scene smoothing approach based on saliency analysis. A saliency model based on fully convolutional network (FCN) is first trained to establish the saliency map, then the probability maps representing the salient information and the non-salient information are combined to alter the field of view (FOV) by smoothing the non-salient area. An experiment is conducted to evaluate the performance of the proposed approach. The experimental data demonstrate that participants experiencing dynamic blurring VE reports a 50% reduction of the severity of MS symptom on average during the VR experience than the participants experiencing the control condition, which show that the proposed approach can be used to effectively prevent the visually induced MS in VR and support longer duration of users in VE.},
keywords={image motion analysis;image restoration;probability;virtual reality;flexible dynamic scene smoothing approach;saliency map;probability maps;salient information;nonsalient information;visually induced MS;dynamic real-time content-aware nonsalient area;3D immersive virtual environment;fully-convolutional network;dynamic blurring VE;POSTER;prevention-of-visually induced motion sickness;Dynamics;Visualization;Automobiles;Real-time systems;Smoothing methods;Three-dimensional displays;Object detection},
doi={10.1109/ISMAR-Adjunct.2017.35},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088453,
author={G. {Lee} and S. {Kim} and Y. {Lee} and A. {Dey} and T. {Piumsomboon} and M. {Norman} and M. {Billinghurst}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] Mutually Shared Gaze in Augmented Video Conference},
year={2017},
volume={},
number={},
pages={79-80},
abstract={Augmenting video conference with additional visual cues has been studied to improve remote collaboration. A common setup is a person wearing a head-mounted display (HMD) and camera sharing her view of the workspace with a remote collaborator and getting assistance on a real-world task. While this configuration has been extensively studied, there has been little research on how sharing gaze cues might affect the collaboration. This research investigates how sharing gaze in both directions between a local worker and remote helper affects the collaboration and communication. We developed a prototype system that shares the eye gaze of both users, and conducted a user study. Preliminary results showed that sharing gaze significantly improves the awareness of each other's focus, hence improving collaboration.},
keywords={augmented reality;cameras;groupware;helmet mounted displays;video communication;augmented video conference;remote collaboration;eye gaze;visual cues;head-mounted display;camera sharing;Collaboration;Prototypes;Visualization;Resists;Mice;Cameras;Remote collaboration;augmented video conference;eye gaze tracking},
doi={10.1109/ISMAR-Adjunct.2017.36},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088454,
author={G. {Alce} and M. {Roszko} and H. {Edlund} and S. {Olsson} and J. {Svedberg} and M. {Wallergård}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] AR as a User Interface for The Internet of Things—Comparing Three Interaction Models},
year={2017},
volume={},
number={},
pages={81-86},
abstract={It is estimated that the number of devices connected to the Internet will be 50 billion by 2020. How should a not-so-tech-savvy end-user be able to discover, configure, and directly interact with a myriad of connected things in an intuitive and comfortable manner? Even if smartphones and wearables have shown potential for managing IoT environments, we cannot rely on that they can be used for all future IoT interaction. Many observers believe that augmented reality (AR) technology may constitute a suitable solution for IoT interaction. The purpose of this paper was to compare three basic AR interaction models, with a focus on the aspects of discovering and selecting devices, implemented for Microsoft HoloLens. An experimental study with 20 participants was conducted. They were split into two groups; one with low device density and one with high device density. Each group had to solve the same task using each of the three interaction models. The results showed that with few devices to handle, the participants' interactions did not differ significantly. However, with many devices to engage with, the so-called world in miniature model stood out as especially demanding and time-consuming. There was also high variability in which model that was preferred by the participants, possibly implying that a combination of the three proposed models is desired in a fully developed AR system for managing IoT devices.},
keywords={augmented reality;Internet of Things;smart phones;user interfaces;interaction models;user interface;not-so-tech-savvy end-user;IoT environments;Internet of Things;smartphones;augmented reality;Solid modeling;Cameras;Color;Augmented reality;Smart phones;Computational modeling},
doi={10.1109/ISMAR-Adjunct.2017.37},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088455,
author={H. {Nguyen} and S. {Ketchell} and U. {Engelke} and B. {Thomas} and P. d. {Souza}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] HoloBee: Augmented Reality Based Bee Drift Analysis},
year={2017},
volume={},
number={},
pages={87-92},
abstract={In this paper, we introduce HoloBee, a visual analytics system that enables end users to analyse bee drift data using HoloLens as a head mounted augmented reality interface. HoloBee was designed to allow for interactive exploration of bee behaviour and specifically bee drift: the phenomenon of a bee leaving a hive and returning to another hive. For exploration in the bees' natural environment, bee drift is visualised as arcs connecting bee hives positioned in 3D geospatial maps. We describe in detail the design aspects of HoloBee, including command language, system feedback, and information display. We discuss the unique capabilities and limitations of the HoloLens as a guide to other researchers who intend to use it for visual analytics of spatio-temporal scientific data.},
keywords={augmented reality;biology computing;data analysis;data visualisation;zoology;bee behaviour;visual analytics system;HoloBee;augmented reality based bee drift analysis;bee drift data analysis;HoloLens;head mounted augmented reality interface;3D geospatial maps;spatio-temporal scientific data;Data visualization;Visual analytics;Three-dimensional displays;Australia;Electronic mail;Augmented reality},
doi={10.1109/ISMAR-Adjunct.2017.38},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088456,
author={I. {Makarov} and V. {Aliev} and O. {Gerasimova} and P. {Polyakov}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] Depth Map Interpolation Using Perceptual Loss},
year={2017},
volume={},
number={},
pages={93-94},
abstract={In this paper, we discuss a semi-dense depth map interpolation method based on convolutional neural network. We propose a compact neural network architecture with loss function defined as Euclidean distance in the feature space of VGG-16 neural network used for deep visual recognition. The suggested solution shows state-of-art performance on synthetic and real datasets. Together with LSD-SLAM, the method could be used to provide a dense depth map for interaction purposes, such as creating a first person game in AR/MR or perception module for autonomous vehicle.},
keywords={feature extraction;interpolation;neural nets;object tracking;SLAM (robots);stereo image processing;Euclidean distance;feature space;VGG-16 neural network;deep visual recognition;perceptual loss;semidense depth map interpolation method;convolutional neural network;compact neural network architecture;loss function;LSD-SLAM;first person game;AR/MR;autonomous vehicle perception module;Interpolation;Neural networks;Image reconstruction;Simultaneous localization and mapping;Integrated circuits;Visualization;Computer vision;Depth Map;Semi-Dense Depth Map Interpolation;Deep Convolutional Neural Networks;Mixed Reality;FPS},
doi={10.1109/ISMAR-Adjunct.2017.39},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088457,
author={J. {Kim} and J. {Ryu} and S. {Ryu} and K. {Lee} and J. {Kim}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] Optimizing Background Subtraction for OST-HMD},
year={2017},
volume={},
number={},
pages={95-96},
abstract={Transparency of optical see-through head-mounted displays (OST-HMDs) makes them suffer from background blending. Existing works have tackled this problem by color correction, but have not addressed how to estimate the background color accurately. In this paper, we apply colorimetric estimation to the subtraction compensation for background blending. Moreover, we propose an optimization framework that trades-off the overall color distortion and clipping to achieve the optimal color correction, constrained to keeping image structures and details.},
keywords={helmet mounted displays;image colour analysis;optimisation;background subtraction;optical see-through head-mounted displays;optimal color correction;color distortion;optimization framework;subtraction compensation;colorimetric estimation;background color;background blending;OST-HMD;Image color analysis;Estimation;Cameras;Adaptive optics;Optical imaging;Optimization;OST-HMD;background blending;colorimetric estimation;background subtraction;clipping;optimization},
doi={10.1109/ISMAR-Adjunct.2017.40},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088458,
author={J. {Thoma} and M. {Havlena} and S. {Stalder} and L. {Van Gool}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] Augmented Reality for User-Friendly Intra-Oral Scanning},
year={2017},
volume={},
number={},
pages={97-102},
abstract={Digital impressions of teeth, obtained through intra-oral scanning, allow for more efficient and cost effective treatments of many dental indications. Current state-of-the-art intra-oral impression acquisition systems make use of a separate monitor to show the scanning progress, forcing the dentist to divert attention away from the scanner and the patient. In this paper, we present an augmented reality based solution to this problem. During the scanning process, an optical see-through head-mounted display is used to show an online overlay of the dynamic dental model onto the patient's teeth. The dentist can then fully focus on the patient and the scanner, while still being able to keep track of the current state of the model. This type of novel application, which fundamentally changes the humancomputer interaction of intra-oral scanning systems, requires a fast and accurate registration of a dynamically growing model onto a glossy, partially occluded surface at a very small scale. To meet this demand, we propose application tailored algorithms for indirect high accuracy online 3D teeth tracking and optical see-through head-mounted display calibration. Experimental results indicate that our system does have a potential to noticeably facilitate intraoral scanning in the future.},
keywords={augmented reality;biomedical equipment;dentistry;helmet mounted displays;human computer interaction;image registration;medical image processing;augmented reality;dental treatments;head-mounted display;intra-oral impression acquisition systems;human computer interaction;growing model registration;online 3D teeth tracking;dynamic dental model;user-friendly intra-oral scanning;Calibration;Teeth;Dentistry;Three-dimensional displays;Augmented reality;Glass;Cameras;Intra-oral scanning;augmented reality;HMD calibration;implicit pose estimation},
doi={10.1109/ISMAR-Adjunct.2017.41},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088459,
author={J. {Rambach} and A. {Pagani} and D. {Stricker}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] Augmented Things: Enhancing AR Applications leveraging the Internet of Things and Universal 3D Object Tracking},
year={2017},
volume={},
number={},
pages={103-108},
abstract={With Augmented Reality (AR) reaching its technological maturity, there is a constantly increasing request for AR applications that address a broadened user base. Current AR applications still tend to be spatially and temporally confined either by applying to a single use case or by their tracking requirements. At the same time, with the rise of the Internet of Things (IoT), more and more everyday objects are fitted with wireless connection interfaces. In this work, we present the concept of Augmented Things, in which objects carry all the necessary tracking and augmentation information required for AR applications. This allows a user to connect to them, load this information on his personal device and have augmentations such as maintenance instructions or product origin and usage displayed. For this, we also present and evaluate a universally robust 3D object tracking framework based on high quality 3D scans of the objects.},
keywords={augmented reality;Internet of Things;mobile computing;object detection;object tracking;universal 3D object tracking;tracking requirements;wireless connection interfaces;augmentation information;universally robust 3D object;high quality 3D scans;augmented reality;augmented things;Three-dimensional displays;Solid modeling;Object tracking;Target tracking;Cameras;Feature extraction;Rendering (computer graphics)},
doi={10.1109/ISMAR-Adjunct.2017.42},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088460,
author={J. {Rambach} and A. {Pagani} and S. {Lampe} and R. {Reiser} and M. {Pancholi} and D. {Stricker}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] Fusion of Unsynchronized Optical Tracker and Inertial Sensor in EKF Framework for In-car Augmented Reality Delay Reduction},
year={2017},
volume={},
number={},
pages={109-114},
abstract={In this paper we present a novel sensor fusion framework between unsychronized optical tracking systems and inertial measurement units based on an Extended Kalman Filter (EKF). The main benefit from the fusion is taking advantage of the faster speed of measurement availability of the inertial sensor in order to decrease the delay of the system while maintaining the accuracy of the optical tracking system. The tracking framework is applicable in a car cockpit Augmented Reality system displaying augmentations using the tracked 6 Degree of Freedom pose of a head mounted display.},
keywords={augmented reality;helmet mounted displays;image filtering;image fusion;image sensors;Kalman filters;nonlinear filters;optical tracking;sensor fusion;unsychronized optical tracking systems;inertial measurement units;measurement availability;optical tracking system;tracking framework;car cockpit;EKF framework;extended Kalman filter;reality system displaying augmentations;unsynchronized optical tracker-inertial sensor fusion;in-car augmented reality delay reduction;Delays;Optical sensors;Optical variables measurement;Calibration;Adaptive optics;Optical filters;Robot sensing systems},
doi={10.1109/ISMAR-Adjunct.2017.43},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088461,
author={J. {Goderie} and R. {Alashrafov} and P. {Jockin} and L. {Liu} and X. {Liu} and M. A. {Cidota} and S. G. {Lukosch}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] ChiroChroma: An Augmented Reality Game for the Assessment of Hand Motor Functionality},
year={2017},
volume={},
number={},
pages={115-120},
abstract={For a better understanding of how different diseases (e.g. neurovascular diseases, neurodegenerative diseases, and musculoskeletal pain conditions) affect human motor function, a uniform, standardized and objective evaluation is a desirable goal expressed within the clinical community. We explore the capabilities of an augmented reality (AR) game that uses free hand interaction to facilitate an objective assessment of the upper extremity motor dysfunction. First, the design process of the game and the system architecture are described. Second, a study about usability of the AR framework and game engagement is presented based on an experiment we conducted with five patients and ten healthy people. Lastly, a short analysis of the accuracy of the hand data when participants performed “fingers tapping” gesture is done. The results of the study show that even though users experienced the system as physically and mentally demanding, it was engaging enough to make them complete the game. The study also shows that hand data captured is accurate enough to allow a high degree (95%) of pinching gesture recognition.},
keywords={augmented reality;biomechanics;computer games;diseases;gesture recognition;human computer interaction;medical computing;medical disorders;neurophysiology;patient rehabilitation;augmented reality game;hand motor functionality;neurovascular diseases;neurodegenerative diseases;musculoskeletal pain conditions;human motor function;clinical community;free hand interaction;objective assessment;upper extremity motor dysfunction;design process;system architecture;hand data;ChiroChroma;Games;Thumb;Three-dimensional displays;Diseases;Augmented reality;Resists;Augmented Reality Games;Usability;Engagement;Upper Extremity Motor Dysfunction;Assessment;Parkinson's Disease;Stroke patients},
doi={10.1109/ISMAR-Adjunct.2017.44},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088462,
author={J. {Arroyo-Palacios} and R. {Marks}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] Believable Virtual Characters for Mixed Reality},
year={2017},
volume={},
number={},
pages={121-123},
abstract={This poster discusses the implementation and technical choices of a proof of concept experience demonstrating interactive virtual characters for immersive mixed reality (MR) applications. Our interactive virtual characters are made believable through their visual appearance, physical interaction with the environment, dynamic behavior to the user and both real and virtual stimuli, and by being able to affect actual change to the real world (using Internet of Things devices). We also propose that the inclusion of believable virtual characters can reinforce the overall plausibility of a MR scenario.},
keywords={Internet of Things;virtual reality;interactive virtual characters;immersive mixed reality applications;physical interaction;virtual stimuli;believable virtual characters;visual appearance;dynamic behavior;Internet of Things devices;MR scenario;Virtual reality;Three-dimensional displays;Navigation;Solid modeling;Cameras;Robot kinematics;Mixed reality;virtual characters;video games;agents},
doi={10.1109/ISMAR-Adjunct.2017.45},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088463,
author={J. {Dai} and X. {Tang} and L. {Oppermann}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] ORBFusion: Real-Time and Accurate Dense SLAM at Large Scale},
year={2017},
volume={},
number={},
pages={124-129},
abstract={We present a new SLAM system capable of producing high quality globally consistent surface reconstructions with accurate real-time tracking and localization abilities. The system works on an off the shelf laptop with a typical GPU. This paper proposes an approach to unify feature-based keyframe techniques with fused volumetric surface reconstruction methods to overcome both of their limitations. On one hand, feature-based keyframe SLAM techniques have reached a level of maturity and can guarantee accurate and real-time tracking and localization ability, but their raw RGB-D point clouds are too noisy. On the other hand, volumetric surface reconstruction methods can produce a dense surface reconstruction of the environment, which will be helpful for Augmented Reality (AR) applications and scene understanding. However, current dense SLAM systems have limited tracking ability, which is vital for the quality of surface reconstruction. Moreover most of the current dense SLAM systems have to run on a powerful desktop PC to guarantee realtime performance. By unifying the feature-based keyframe tracking ability and adopting a multi-threaded design, our system improves both the tracking ability and the real-time performance. We present results of a wide variety of aspects of our system and evaluate it using the widely used TUM RGB-D and ICL-NUIM Datasets. Our system achieves unprecedented performance in terms of trajectory estimation, surface reconstruction, real-time and computational performance in comparison to other start-of-the-art dense RGB-D SLAM systems.},
keywords={augmented reality;feature selection;image colour analysis;image fusion;image reconstruction;multi-threading;pose estimation;real-time systems;robot vision;SLAM (robots);surface reconstruction;real-time tracking;localization ability;RGB-D images;ORBFusion;feature-based keyframe SLAM;fused volumetric surface reconstruction;GPU;RGB-D point clouds;augmented reality applications;multi-threaded design;trajectory estimation;RGB-D camera pose estimation;SLAM system;Augmented reality;Feature-based;surface reconstruction;RGB-D;SLAM},
doi={10.1109/ISMAR-Adjunct.2017.46},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088464,
author={K. {Lee} and J. {Kim} and J. {Ryu} and J. {Kim}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] Two-Step Gamut Mapping for Optical See-Through Displays},
year={2017},
volume={},
number={},
pages={130-131},
abstract={In Optical See-Through (OST) displays, the chroma component in color space is often distorted. OST displays appear to have a uniquely different gamut feature compared with conventional color devices. In this paper, we proposed a novel two-step gamut mapping method for OST displays. The conventional CARISMA (Color Appearance Research for Interactive System Management and Application) algorithm is extended straightforwardly, and its process is divided into two separate steps (lightness mapping and chroma compression) by reflecting the characteristics of the newtype OST. We confirmed experimentally that the proposed gamut mapping method can reduce color distortion better than the existing CARISMA.},
keywords={colorimetry;colour displays;image colour analysis;printers;printing;chroma component;color space;OST displays;two-step gamut mapping method;lightness mapping;chroma compression;color distortion;two-step gamut mapping;optical see-through displays;CARISMA;color appearance research for interactive system management and application;Augmented reality;Gamut mapping;OST display;color distortion;chroma reproduction;lightness mapping},
doi={10.1109/ISMAR-Adjunct.2017.47},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088465,
author={K. K. {Thiel} and E. {Jundt} and G. {Klinker}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] Automated Evaluation and Configuration of Object Tracking for Augmented Reality},
year={2017},
volume={},
number={},
pages={132-134},
abstract={Object tracking configuration is a tedious task for users due to an overwhelming amount of parameters while solving a multicriterial optimisation problem for a black box. With increasing complexity of the algorithms and fast changing situations due to the upcoming fourth industrial revolution, even experts will find themselves struggling with this topic. For this reason we present a concept for an automated evaluation and configuration of object tracking algorithms used in augmented reality applications. Given an arbitrary but specified use case the best suitable parameter set shall be found. Therefore the concept utilises statistical analysis of the vast parameter space performing multicriterial optimisation regarding a formalisation of the tracking quality within a virtual testbed.},
keywords={augmented reality;object tracking;optimisation;statistical analysis;statistical analysis;augmented reality applications;object tracking algorithms;black box;multicriterial optimisation problem;Optimization;Target tracking;Augmented reality;Object tracking;Complexity theory;Industries},
doi={10.1109/ISMAR-Adjunct.2017.48},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088466,
author={L. {Chen} and K. {Francis} and W. {Tang}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] Semantic Augmented Reality Environment with Material-Aware Physical Interactions},
year={2017},
volume={},
number={},
pages={135-136},
abstract={In Augmented Reality (AR) environment, realistic interactions between the virtual and real objects play a crucial role in user experience. Much of recent advances in AR has been largely focused on developing geometry-aware environment, but little has been done in dealing with interactions at the semantic level. High-level scene understanding and semantic descriptions in AR would allow effective design of complex applications and enhanced user experience. In this paper, we present a novel approach and a prototype system that enables the deeper understanding of semantic properties of the real world environment, so that realistic physical interactions between the real and the virtual objects can be generated. A material-aware AR environment has been created based on the deep material learning using a fully convolutional network (FCN). The state-of-the-art dense Simultaneous Localisation and Mapping (SLAM) has been used for the semantic mapping. Together with efficient accelerated 3D ray casting, natural and realistic physical interactions are generated for interactive AR games. Our approach has significant impact on the future development of advanced AR systems and applications.},
keywords={augmented reality;computer games;feedforward neural nets;human factors;learning (artificial intelligence);SLAM (robots);virtual reality;high-level scene understanding;semantic descriptions;enhanced user experience;semantic properties;realistic physical interactions;virtual objects;material-aware AR environment;deep material learning;semantic mapping;interactive AR games;material-aware physical interactions;geometry-aware environment;semantic level;semantic Augmented Reality environment;accelerated 3D ray casting;simultaneous localisation and mapping;Semantics;Three-dimensional displays;Solid modeling;Cameras;Games;Real-time systems;Simultaneous localization and mapping},
doi={10.1109/ISMAR-Adjunct.2017.49},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088467,
author={L. {Lucignano} and P. {Dillenbourg}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] Double Reality: Shifting the Gaze Between the Physical Object and Its Digital Representation},
year={2017},
volume={},
number={},
pages={137-142},
abstract={We present an eye-tracking study centered on the shift of focus from digital to physical layer in case of a handheld magic-lens system. In particular, we investigate which variables might affect the occurrence of such events. The experiment involved 35 participants who used a tablet AR application to explore structural behaviour through problem solving exercises. The tablet augmented the physical model of a structure with the information about the applied loads and the internal forces. Participants were asked to identify the nature of some forces by visual inspection. During the task they could freely navigate around the structure and touch it. Our results suggest that the amount of shifts to real-world objects is related to the user's spatial orientation rather than on the user's spatial skills or the digital augmentation flaws. Specifically, the probability of looking at the physical layer increases when rapid movements are performed. The task difficulty and the perceived controllability, although not influencing the number of shifts, affected the mental effort during such moments.},
keywords={augmented reality;user interfaces;digital augmentation flaws;digital representation;eye-tracking study;handheld magic-lens system;problem solving exercises;visual inspection;double reality;tablet AR application;user spatial orientation;controllability;Switches;Visualization;Augmented reality;Stress;Navigation;Solid modeling;Magic lens;Mobile device;Visual focus;Eye tracking;Physical correspondence;Learning structural analysis},
doi={10.1109/ISMAR-Adjunct.2017.50},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088468,
author={M. {Chessa} and F. {Solari}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] Walking in Augmented Reality: An Experimental Evaluation by Playing with a Virtual Hopscotch},
year={2017},
volume={},
number={},
pages={143-148},
abstract={A crucial aspect when interacting in augmented reality (AR) scenarios is having a stable and coherent perception of the virtual contents overlaid on the real world. This is particularly challenging when the task in the AR environments involves walking onto virtual paths on the floor. In this paper, we present two experiments, conducted in both real-world and cluttered conditions and in a laboratory setup, in which non-trained people were asked to play with a virtual hopscotch. We perform both a qualitative evaluation, by means of a self-assessment questionnaire, and quantitative measures, by capturing the 3D positions of the observers, which show errors in the perception of the hopscotchs layout. The obtained results suggest us that besides the known issues of pose estimation, also the mismatch between the intrinsic parameters of the mobile camera and the ones of the observer eyes could be investigated and taken into account to design effective AR systems.},
keywords={augmented reality;pose estimation;POSTER;experimental evaluation;virtual hopscotch;augmented reality scenarios;stable perception;coherent perception;virtual contents;virtual paths;laboratory setup;nontrained people;qualitative evaluation;hopscotchs layout;Legged locomotion;Three-dimensional displays;Augmented reality;Games;Human computer interaction;Smart phones;Mobile device AR;Layout perception;Uncalibrated view in AR;AR user evaluation;Interaction in AR},
doi={10.1109/ISMAR-Adjunct.2017.51},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088469,
author={M. {Randall} and I. {Williams} and C. {Athwal}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] A Predictive Approach to On-line Time Warping of Motion},
year={2017},
volume={},
number={},
pages={149-154},
abstract={The paper presents a novel approach to real-time temporal alignment of motion sequences, called On-line Predictive Warping (OPW) and considers potential uses in interactive applications. The approach develops on the methods of aligning motions based on least cost, used in dynamic time warping (DTW), with the short term predictions of smoothing algorithms, in an iterative step through approach. The approach allows a recorded motion sequence to be warped to align it with a users motion as it is being captured. The paper demonstrates the potential feasibility of the approach to support applications in MR and VR, allowing virtual characters to perform and interact with users and live actors in a variety of rehearsal, training, visualisation and performance scenarios.},
keywords={image motion analysis;image sequences;virtual reality;users motion;On-line Time Warping;real-time temporal alignment;motion sequences;OPW;interactive applications;dynamic time;short term predictions;smoothing algorithms;iterative step;motion sequence;On-line Predictive Warping;dynamic time warping;Real-time systems;Time series analysis;Production;Smoothing methods;Virtual reality;Prediction algorithms;Cameras;On-line time warping;character animation;virtual production},
doi={10.1109/ISMAR-Adjunct.2017.52},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088470,
author={M. {Fradet} and C. {Baillard} and A. {Laurent} and T. {Luo} and P. {Robert} and V. {Alleaume} and P. {Jouet} and F. {Servant}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] MR TV Mozaik: A New Mixed Reality Interactive TV Experience},
year={2017},
volume={},
number={},
pages={155-159},
abstract={Technicolor has been investigating how Mixed Reality technology could impact the future of home entertainment. We have designed and implemented a system to extend a standard TV experience with AR content, using a consumer tablet or a headset. A virtual TV mosaic is displayed around the TV screen and used as a GUI to control both TV and MR content. Using this interface, the user is able to switch TV content, display meta-data in AR (subtitles, text information or program guide), enhance TV content with interactive 3D objects blended in the environment, or play a game in interaction with the real world. The interactions between the real and the virtual worlds are handled thanks to a scene analysis pre-processing stage, which provides information about both the geometry and the lighting of the real environment. The real-virtual interactions strongly contribute to reinforcement of the immersion feeling. User feedback shows that the concept is very promising.},
keywords={entertainment;graphical user interfaces;home computing;interactive television;virtual reality;home entertainment;standard TV experience;AR content;virtual TV mosaic;TV screen;TV content;interactive 3D objects;virtual worlds;real-virtual interactions;MR TV mozaik;mixed reality interactive TV experience;technicolor;consumer tablet;headset;mixed reality technology;GUI;MR content;meta-data display;real environment geometry;real environment lighting;TV;Three-dimensional displays;Virtual reality;Standards;Servers;Headphones;Switches;Mixed Reality;extended TV;GUI;spatial interactions;lighting estimation;3D modeling},
doi={10.1109/ISMAR-Adjunct.2017.53},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088471,
author={M. {Tanaka} and A. {Misra} and K. {Oshima} and S. {Hashiguchi} and S. {Mori} and F. {Shibata} and A. {Kimura} and H. {Tamura}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] Further Experiments and Considerations on Weight Perception Caused by Visual Diminishing of Real Objects},
year={2017},
volume={},
number={},
pages={160-161},
abstract={Mixed reality (MR) technologies can virtually change the appearance of real objects in real time without changing the material attributes of the objects and the associated haptic stimuli. In this study, we use MR-based visuo-haptic to investigate the mechanisms by which vision and haptics interact. In contrast to MR, diminished reality (DR) can virtually erase a real object from our sight. Because DR visual and haptic experiences do not occur in daily life, the effects of DR on haptic sensations have not been previously investigated. Thus, in this paper, we also study the relationship between various ranges of DR-based visual effects and haptic sensations using stick-shaped real objects. The results indicate that, the sticks were perceived to be heavier than their actual weight when the visual length presented was made shorter by visually diminishing.},
keywords={augmented reality;haptic interfaces;virtual reality;haptic sensations;weight perception;mixed reality technologies;material attributes;vision;haptics interact;diminished reality;haptic experiences;visual length;haptic stimuli;DR-based visual effects;stick-shaped real objects;Visualization;Virtual reality;Haptic interfaces;Psychology;Shape;Solid modeling;Three-dimensional displays;Diminished Reality;Mixed Reality;Weight Perception},
doi={10.1109/ISMAR-Adjunct.2017.54},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088472,
author={N. {Shinozuka} and Y. {Manabe} and N. {Yata}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] Consistency between Reflection on the Glass and Virtual Object in Augmented Reality},
year={2017},
volume={},
number={},
pages={162-165},
abstract={Augmented Reality (AR) technology is often used for arrangement simulation such as furniture before purchasing. This study focuses on the simulation of product layout in stores and exhibits in museums with show-window. Displaying the virtual information with correct position against real objects is important for realistic experience in AR applications. In this study, we keep the consistency between reflection on the glass and virtual object. Consequently, we extract the reflections on the glass using stereo images according to disparity, and represent it on the superimposed CG object in real-time. Experimental result shows the appearance of the CG object compared with that of the real object in show-window.},
keywords={augmented reality;stereo image processing;virtual object;product layout;virtual information;augmented reality technology;Reflection;Cameras;Handheld computers;Augmented reality;Human computer interaction;Visualization;Augmented Reality;Stereo Camera;Arrangement Simulation;Show-window;Reflection Removing},
doi={10.1109/ISMAR-Adjunct.2017.55},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088473,
author={N. {Haouchine} and A. {Petit} and F. {Roy} and S. {Cotin}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] Deformed Reality: Proof of Concept and Preliminary Results},
year={2017},
volume={},
number={},
pages={166-167},
abstract={We introduce “Deformed Reality”, a new paradigm to interactively manipulate objects in a scene in a deformable manner. Using the core principle of augmented reality to estimate rigid pose over time, our method enables the user to deform the targeted object while it is being rendered with its natural texture, giving the sense of a real-time object editing in user environment. The presented results show that our method can open new ways of using augmented reality by not only augmenting the scene but also interacting with it in a non-rigid manner.},
keywords={augmented reality;image texture;augmented reality;natural texture;real-time object editing;user environment;deformed reality;proof of concept;interactive object manipulation;Solid modeling;Three-dimensional displays;Deformable models;Computational modeling;Image edge detection;Real-time systems;Augmented reality},
doi={10.1109/ISMAR-Adjunct.2017.56},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088474,
author={N. {Antigny} and M. {Servières} and V. {Renaudin}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] An Inertial, Magnetic and Vision Based Trusted Pose Estimation for AR and 3D Data Qualification on Long Urban Pedestrian Displacements},
year={2017},
volume={},
number={},
pages={168-169},
abstract={In the context of pedestrian navigation, urban environment constitutes a challenging area for both localization and Augmented Reality (AR). In order to display 3D Geographic Information System (GIS) content in AR and to qualify them, we propose to fuse the pose estimated using vision thanks to a precisely known 3D urban furniture model with rotation estimated from inertial and magnetic measurements. An acquisition conducted in urban environment on a long pedestrian path permits to validate the contribution of sensors fusion and allows to qualify the pose estimation needed for AR 3D GIS content characterization.},
keywords={augmented reality;geographic information systems;geomagnetic navigation;inertial navigation;pedestrians;pose estimation;satellite navigation;sensor fusion;stereo image processing;pedestrian navigation;urban environment;inertial measurements;magnetic measurements;AR 3D GIS content characterization;3D data qualification;3D geographic information system content;3D urban furniture model;urban pedestrian displacements;augmented reality;sensor fusion;vision based trusted pose estimation;magnetic based trusted pose estimation;inertial trusted pose estimation;Three-dimensional displays;Sensors;Cameras;Pose estimation;Feature extraction;Urban areas;Solid modeling;Outdoor Augmented Reality;mobile device;Geographic Information Systems;city visualization},
doi={10.1109/ISMAR-Adjunct.2017.57},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088475,
author={P. {Perea} and D. {Morand} and L. {Nigay}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] Halo3D: A Technique for Visualizing Off-Screen Points of Interest in Mobile Augmented Reality},
year={2017},
volume={},
number={},
pages={170-175},
abstract={When working with mobile Augmented Reality (AR) applications, users need to be aware of relevant points of interest (POIs) that are located off-screen. These POIs belong to the context since they are not observable in the 3D first-person AR view on screen. The context in mobile AR can include a large number of POIs including locally dense clusters as in mobile AR applications for production plant machine maintenance. Existing solutions display 3D arrows or an area on the edges of the screen to represent the POIs of the context. These techniques display the direction but not the distance of each POI. We present Halo3D, a visualization technique that conveys the 3D direction and distance of off-screen POIs while avoiding overlap and clutter in a high-POI-density AR environment.},
keywords={augmented reality;data visualisation;industrial plants;machinery;maintenance engineering;mobile computing;production engineering computing;Halo3D;3D direction;production plant machine maintenance;mobile AR applications;locally dense clusters;mobile Augmented Reality applications;off-screen points;high-POI-density AR environment;off-screen POIs;visualization technique;Augmented reality;Augmented reality;mobility;visualization;off-screen point of interest;Halo},
doi={10.1109/ISMAR-Adjunct.2017.58},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088476,
author={P. {Renner} and T. {Pfeiffer}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] Augmented Reality Assistance in the Central Field-of-View Outperforms Peripheral Displays for Order Picking: Results from a Virtual Reality Simulation Study},
year={2017},
volume={},
number={},
pages={176-181},
abstract={One area in which glasses-based augmented reality (AR) is successfully applied in industry is order picking in logistics (pick-byvision). Here, the almost hands-free operation and the direct integration into the digital workflow provided by augmented reality glasses are direct advantages. A common non-AR guidance technique for order picking is pick-by-light. This is an efficient approach for single users and low numbers of alternative targets. AR glasses have the potential to overcome these limitations. However, making a grounded decision on the specific AR device and the particular guidance techniques to choose for a specific scenario is difficult, given the diversity of device characteristics and the lack of experience with smart glasses in industry at larger scale. The contributions of the paper are twofold. First, we present a virtual reality (VR) simulation approach to ground design decisions for AR-based solutions and apply it to the scenario of order picking. Second, we present results from a simulator study with implemented simulations for monocular and binocular head-mounted displays and compared existing techniques for attention guiding with our own SWave approach and the integration of eye tracking. Our results show clear benefits for the use of pick-by-vision compared to pick-by-light. In addition to that, we can show that binocular AR solutions outperform monocular ones in the attention guiding task.},
keywords={augmented reality;helmet mounted displays;logistics;pick-by-vision;pick-by-light;order picking;virtual reality simulation study;augmented reality glasses;nonAR guidance technique;smart glasses;virtual reality simulation approach;augmented reality assistance;AR-based solutions;glasses-based augmented reality;Glass;Visualization;Three-dimensional displays;Gaze tracking;Two dimensional displays;Augmented reality},
doi={10.1109/ISMAR-Adjunct.2017.59},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088477,
author={P. {Rojtberg} and A. {Kuijper}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] Efficient Pose Selection for Interactive Camera Calibration},
year={2017},
volume={},
number={},
pages={182-183},
abstract={The choice of poses for camera calibration with planar patterns is only rarely considered - yet the calibration precision heavily depend on it. This work presents a pose selection method that explicitly avoids singular pose configurations which would lead to an unreliable solution. Consequently camera poses are favoured that reduce the uncertainty of the calibration parameters most. For this purpose the quality of the calibration parameters is continuously estimated using uncertainty propagation. Our approach performs better than comparable solutions while requiring 30% less calibration frames.},
keywords={calibration;cameras;pose estimation;calibration frames;calibration parameters;camera poses;selection method;calibration precision;planar patterns;interactive camera calibration;Calibration;Cameras;Distortion;Image analysis;Uncertainty;Uncertain systems;Robot vision systems},
doi={10.1109/ISMAR-Adjunct.2017.60},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088478,
author={P. {Kim} and J. {Orlosky} and K. {Kiyokawa} and P. {Ratsamee} and T. {Mashita}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] DotWarp: Dynamic Object Timewarping for Video See-Through Augmented Reality},
year={2017},
volume={},
number={},
pages={184-185},
abstract={A significant issue associated with the use of video see-through head-mounted displays (VST-HMD) for augmented reality is the presence of latency between real-world images and the images displayed to the HMD. For a static scene, this latency provides no real problem, however for dynamic scenes, which arise when the HMD user moves their head, when real-world objects move, or a combination of the two, the accompanying delay may result in significant registration error. To address this issue, we present DotWarp, a novel latency reduction technique for VST-HMDs that does not rely on head motion and compensates for the delay arising from real-world object motion. The algorithm requires a two-camera setup and matches dynamic objects in both images by tracking on the faster image and warping the pixels of the slower image, with the fast and slow components being RGB and IR components, respectively, for our system. First, moving objects are extracted from the faster camera scene using a motioncompensating background subtraction algorithm and tracked using a robust correlation tracker. Then, temporal correspondence between the two camera images is computed using sensor update information and the objects' positions in the slower image are shifted to match the corresponding positions in the faster image. Finally, the gaps in the slower image left behind by the shifted objects are filled in with background pixel data from previous frames using homography from the background subtraction model. In this manner, the augmented image is more closely matched with the real-world image and the perceived registration of the camera is significantly improved, with initial results of an 81.64% reduction in registration error.},
keywords={augmented reality;cameras;helmet mounted displays;image colour analysis;image motion analysis;image recognition;image registration;image representation;image segmentation;image sequences;object detection;object tracking;video signal processing;dynamic object timewarping;displays;VST-HMD;real-world image;static scene;dynamic scenes;HMD user;real-world objects;latency reduction technique;head motion;real-world object motion;two-camera setup;moving objects;faster camera scene;motioncompensating background subtraction algorithm;camera images;shifted objects;augmented image;registration error;DotWarp;video see-through augmented reality;dynamic objects matching;video see-through head-mounted displays;Streaming media;Cameras;Tracking;Head;Augmented reality;Delays;Heuristic algorithms;Delay compensation;augmented reality;timewarping},
doi={10.1109/ISMAR-Adjunct.2017.61},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088479,
author={P. {Wu} and Y. {Lee} and H. {Tseng} and H. {Ho} and M. {Yang} and S. {Chien}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] A Benchmark Dataset for 6DoF Object Pose Tracking},
year={2017},
volume={},
number={},
pages={186-191},
abstract={Accurately tracking the six degree-of-freedom pose of an object in real scenes is an important task in computer vision and augmented reality with numerous applications. Although a variety of algorithms for this task have been proposed, it remains difficult to evaluate existing methods in the literature as oftentimes different sequences are used and no large benchmark datasets close to realworld scenarios are available. In this paper, we present a large object pose tracking benchmark dataset consisting of RGB-D video sequences of 2D and 3D targets with ground-truth information. The videos are recorded under various lighting conditions, different motion patterns and speeds with the help of a programmable robotic arm. We present extensive quantitative evaluation results of the state-of-the-art methods on this benchmark dataset and discuss the potential research directions in this field. The proposed benchmark dataset is available online at media.ee.ntu.edu.tw/research/OPT.},
keywords={image sequences;mobile robots;object detection;object tracking;pose estimation;robot vision;target tracking;video signal processing;6DoF object;RGB-D video sequences;six degree-of-freedom pose tracking;programmable robotic arm;motion patterns;lighting conditions;ground-truth information;2D targets;3D targets;large object pose tracking benchmark dataset;augmented reality;computer vision;Cameras;Benchmark testing;Three-dimensional displays;Robot sensing systems;Two dimensional displays;Videos;Target tracking},
doi={10.1109/ISMAR-Adjunct.2017.62},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088480,
author={S. {Jiddi} and P. {Robert} and E. {Marchand}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] Illumination Estimation Using Cast Shadows for Realistic Augmented Reality Applications},
year={2017},
volume={},
number={},
pages={192-193},
abstract={Augmented Reality (AR) scenarios aim to provide realistic blending between real world and virtual objects. A key factor for realistic AR is thus a correct illumination simulation. This consists in estimating the characteristics of real light sources and use them to model virtual lighting. In this paper, we briefly introduce a novel method for recovering both 3D position and intensity of multiple light sources using detected cast shadows. Our algorithm has been successfully tested on a set of real scenes where virtual objects have visually coherent shadows.},
keywords={augmented reality;digital simulation;lighting;object detection;virtual lighting;virtual objects;visually coherent shadows;realistic blending;light source intensity;illumination estimation;augmented reality scenarios;illumination simulation;realistic augmented reality applications;realistic AR;real world;real light sources;3D position;cast shadow detection;Lighting;Three-dimensional displays;Light sources;Estimation;Augmented reality;Geometry;Image color analysis},
doi={10.1109/ISMAR-Adjunct.2017.63},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088481,
author={S. {Sumiyoshi}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] Position Estimation of a Strongly Occluded Object by Using an Auxiliary Point Cloud in Occluded Space},
year={2017},
volume={},
number={},
pages={194-199},
abstract={A method is proposed for estimation of occluded space and generation of auxiliary points for 3D position estimation of strongly occluded objects. First, occlusion space detection calculates 3D keypoints at the rear side of a target object, thus obtaining a silhouette around the object on the near side, as found from a camera image by an object detector. The method calculates the space containing the 3D keypoints and defines it as the occluded space. The method then generates an auxiliary point cloud for unobserved regions of this space. By matching the detected point cloud and the auxiliary point cloud, the method can estimate the position of an occluded object that has been difficult to localize with a conventional method, thus giving a general matching technique. The centroid position accuracy of the proposed method was experimentally evaluated to demonstrate its effectiveness and confirm its validity.},
keywords={cameras;object detection;stereo image processing;occluded space;occlusion space detection;target object;object detector;auxiliary point cloud;occluded object;centroid position accuracy;3D position estimation;Three-dimensional displays;Estimation;Cameras;Image edge detection;Portable computers;Robustness;Sensors},
doi={10.1109/ISMAR-Adjunct.2017.64},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088482,
author={S. {Mori} and J. {Qie} and S. {Ikeda} and F. {Shibata} and A. {Kimura} and H. {Tamura}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] Background Image Registration as a Post-Processing Technique in Diminished Reality Rendering Procedures},
year={2017},
volume={},
number={},
pages={200-201},
abstract={Diminished reality (DR) is a technique to remove undesirable objects from a video stream in real time. DR methods calculate a user's camera pose using vision- or sensor-based approaches to recover and overlay a background image to the camera view. Relying on 6DoF camera registration methods, DR results are often ruined due to misregistration. To solve this problem, we propose a registration framework as a post-processing technique in diminished reality rendering procedures to reduce the misalignment in an image space. We also reproduce motion blur to improve the image space alignment in the post-processing. The results showed remarkable improvement compared to sensor-based DR results. We argue that this postprocessing-based approach can be applied to almost all existing DR methods to improve their quality.},
keywords={cameras;image registration;pose estimation;rendering (computer graphics);video signal processing;DR methods;background image registration;user camera pose;vision-based approach;sensor-based approach;image quality improvement;postprocessing-based approach;image space alignment;6DoF camera registration methods;camera view;video stream;diminished reality rendering procedures;post-processing technique;Cameras;Rendering (computer graphics);Sensors;Tracking;Streaming media;Image sensors;Pose estimation;Diminished reality;framework;image alignment},
doi={10.1109/ISMAR-Adjunct.2017.65},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088483,
author={S. {Mori} and S. {Ikeda} and C. {Sandor} and A. {Plopski}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] BrightView: Increasing Perceived Brightness in Optical See-Through Head-Mounted Displays},
year={2017},
volume={},
number={},
pages={202-203},
abstract={Virtual content on optical see-through head-mounted displays (OST-HMDs) appears dim in bright environments. In this paper, we demonstrate how a liquid crystal (LC) filter can be used to dynamically increase the perceived brightness of the virtual content. Continuously adjusting the LC filter opacity attenuates the real scene and increases the perceived brightness without being noticed by the user. The results of our psychophysical experiment with 16 participants validate our prototype OST-HMD. Our design could be combined with existing and future OST-HMDs to improve the visibility of the virtual content in augmented reality.},
keywords={helmet mounted displays;liquid crystal displays;perceived brightness;virtual content;liquid crystal filter;LC filter opacity;prototype OST-HMD;optical see-through head-mounted displays;BrightView;augmented reality;Brightness;Lighting;Optical attenuators;Optical variables control;Prototypes;Augmented reality;Electronic mail;OST-HMD;optical see-through displays;illumination shedding;liquid crystal visor;brightness adaptation},
doi={10.1109/ISMAR-Adjunct.2017.66},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088484,
author={S. C. {Lee} and K. {Tateno} and B. {Fuerst} and F. {Tombari} and J. {Fotouhi} and G. {Osgood} and A. {Johnson} and N. {Navab}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] Mixed Reality Support for Orthopaedic Surgery},
year={2017},
volume={},
number={},
pages={204-205},
abstract={This work presents a mixed reality environment for orthopaedic interventions that provides a 3D overlay of Cone-beam CT images, surgical site, and real-time tool tracking. The system uses an RGBD camera attached to the detector plane of a mobile C-arm, which is a typical device to acquire X-Ray images during surgery. Calibration of the two devices is done by acquiring simultaneous CBCT and RGBD scans of a calibration phantom and computing the rigid transformation between them. The markerless tracking of the surgical tool is computed in the RGBD view using real-time segmentation and Simultaneous Localization And Mapping. The RGBD view is then overlaid to the CBCT data with real-time point clouds of the surgical site. This visualization provides multiple desired views of the medical data, surgical site, and the tracking of surgical tools, which could be used to provide intuitive visualization for orthopedic procedures to place instrumentation and to assist surgeons with their localization and coordination. Our proposed opto-X-ray system can lead to x-ray radiation dose reduction as well as improved safety in minimally invasive orthopaedic procedures.},
keywords={augmented reality;biomedical optical imaging;cameras;computerised tomography;data visualisation;image segmentation;medical image processing;orthopaedics;phantoms;surgery;cone-beam CT images;mixed reality support;simultaneous-localization-and-mapping;optoX-ray system;X-ray radiation dose reduction;minimally invasive orthopaedic procedures;real-time point clouds;CBCT data;real-time segmentation;RGBD view;surgical tool;rigid transformation;calibration phantom;X-Ray images;detector plane;RGBD camera;real-time tool tracking;orthopaedic interventions;mixed reality environment;orthopaedic surgery;Surgery;Cameras;Virtual reality;Tools;Three-dimensional displays;X-ray imaging;Trajectory;Mixed Reality;Multi-modal Visualization;Cone-Beam CT;Tracking;Range Imaging;Intra-operative},
doi={10.1109/ISMAR-Adjunct.2017.67},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088485,
author={S. {Toyohara} and S. {Miyafuji} and H. {Koike}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] ARial Texture: Dynamic Projection Mapping on Drone Propellers},
year={2017},
volume={},
number={},
pages={206-211},
abstract={This paper presents ARial Texture: which is a drone display with dynamic projection mapping on the drone's propellers. In our prototype, a motion capture system tracks the drone's position and orientation and a projector projects an image on the drone's four propellers. To evaluate the visibility of the display, we conducted quantitative and qualitative experiments in which the propellers were covered with different reflective materials.},
keywords={augmented reality;autonomous aerial vehicles;image texture;optical projectors;propellers;reflection;retroreflectors;ARial texture;drone position;reflective materials;motion capture system;drone display;drone propellers;dynamic projection mapping;Drones;Propellers;Tracking;Three-dimensional displays;Cameras;Dynamics;Electronic mail},
doi={10.1109/ISMAR-Adjunct.2017.68},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088486,
author={T. {Morozumi} and S. {Mori} and S. {Ikeda} and F. {Shibata} and A. {Kimura} and H. {Tamura}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] Design and Implementation of a Common Dataset for Comparison and Evaluation of Diminished Reality Methods},
year={2017},
volume={},
number={},
pages={212-213},
abstract={Diminished reality (DR) is a technique to remove or inpaint real objects in a display. While DR is one of the active topics in the ISMAR community, a fair comparison between existing or emerging DR methods is difficult, that is, many methods rely on subjective evaluation that uses their own results to demonstrate their advantages. We, therefore, present a common dataset as a basis of the research for fair evaluations. In this article, we describe the design and implementation directions of the dataset. Our dataset is open to the public on the Web† (as of Oct. 9, 2017).},
keywords={augmented reality;image reconstruction;ISMAR community;subjective evaluation;fair evaluations;DR methods;common dataset design;diminished reality method evaluation;real object inpainting;Cameras;Lighting;Image sequences;Automobiles;Photography;Manipulators;Diminished Reality;Dataset;Ground Truth;Quantitative Evaluation},
doi={10.1109/ISMAR-Adjunct.2017.69},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088487,
author={T. {Okamoto} and Y. {Uranishi} and T. {Mashita} and P. {Ratsamee} and K. {Kiyokawa} and H. {Takemura}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] Realtime Generation of Caustic Images Using a Deep Neural Network},
year={2017},
volume={},
number={},
pages={214-215},
abstract={We propose a method for generating caustic images in real time using a deep/convolutional neural network (CNN). To do so, training images are first rendered using photon mapping, and the CNN learns the correspondences between the depth images and caustic images. After learning, the CNN generates a caustic image from a depth image within 55 milliseconds. In addition, the similarity between the generated caustic images and the ground truth shows that our method is very promising for the generation of caustic images for a number of known objects, though the method does not handle objects in which ground truth is not already known. This method can play an important role in scenes used for stage video production and interactive art in the future.},
keywords={convolution;image colour analysis;learning (artificial intelligence);neural nets;rendering (computer graphics);deep neural network;CNN;training images;depth image;convolutional neural network;real-time caustic image generation;stage video production;interactive art;time 55.0 ms;Shape;Real-time systems;Standards;Training;Neural networks;Rendering (computer graphics);Photonics},
doi={10.1109/ISMAR-Adjunct.2017.70},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088488,
author={T. {Oskiper} and S. {Samarasekera} and R. {Kumar}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] CamSLAM: Vision Aided Inertial Tracking and Mapping Framework for Large Scale AR Applications},
year={2017},
volume={},
number={},
pages={216-217},
abstract={In this paper, we introduce CamSLAM, a simultaneous localization and mapping (SLAM) framework composed of a powerful visualinertial odometry backbone using an error-state Extended Kalman Filter (EKF) for sensor fusion, and a very efficient and lightweight parallel mapping engine utilizing keyframe based pose graph data structure and binary descriptors for feature matching and indexing. The framework is capable of generating and maintaining large scale maps of several kilometers in length that are typically required for AR applications in military training sites and industrial applications in environments such as factory floors, office buildings or warehouses. Both the online map creation and refinement, and navigation based on a pre-loaded map is capable of running in real time on a mobile processor such as Nvidia Tegra X1.},
keywords={graph theory;image fusion;inertial navigation;Kalman filters;mobile robots;nonlinear filters;pose estimation;SLAM (robots);mapping framework;SLAM;sensor fusion;lightweight parallel mapping engine;graph data structure;binary descriptors;feature matching;military training sites;online map creation;simultaneous localization and mapping;CamSLAM;vision aided inertial tracking framework;large scale AR applications;visual inertial odometry backbone;error-state extended Kalman filter;mobile processor;Nvidia Tegra X1;Three-dimensional displays;Cameras;Indexes;Pipelines;Navigation;Two dimensional displays;Sensor fusion;inertial navigation;visual mapping;sensor fusion;large scale AR},
doi={10.1109/ISMAR-Adjunct.2017.71},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088489,
author={T. {Piumsomboon} and A. {Dey} and B. {Ens} and G. {Lee} and M. {Billinghurst}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] CoVAR: Mixed-Platform Remote Collaborative Augmented and Virtual Realities System with Shared Collaboration Cues},
year={2017},
volume={},
number={},
pages={218-219},
abstract={We present CoVAR, a novel Virtual Reality (VR) and Augmented Reality (AR) system for remote collaboration. It supports collaboration between AR and VR users by sharing a 3D reconstruction of the AR user's environment. To enhance this mixed platform collaboration, it provides natural inputs such as eye-gaze and hand gestures, remote embodiment through avatar's head and hands, and awareness cues of field-of-view and gaze cue. In this paper, we describe the system architecture, setup and calibration procedures, input methods and interaction, and collaboration enhancement features.},
keywords={augmented reality;avatars;calibration;groupware;shared collaboration;VR;remote collaboration;mixed platform collaboration;remote embodiment;awareness cues;gaze cue;collaboration enhancement features;CoVAR;mixed-platform remote collaborative augmented reality system;mixed-platform remote collaborative virtual reality system;AR user environment;3D reconstruction;Collaboration;Augmented reality;Calibration;Image reconstruction;Three-dimensional displays;Tracking;mixed reality;remote collaboration;eye tracking},
doi={10.1109/ISMAR-Adjunct.2017.72},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088490,
author={T. {Xie} and M. M. {Islam} and A. B. {Lumsden} and I. A. {Kakadiaris}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] Holographic iRay: Exploring Augmentation for Medical Applications},
year={2017},
volume={},
number={},
pages={220-222},
abstract={A Holographic iRay prototype focusing on medical augmented reality is presented. The prototype is built using Microsoft HoloLens and Unity engine based on a previous iRay system built for iPad. A human subject is scanned using magnetic resonance imaging and the torso surface is pre-operatively segmented for 3D registration. The registration is performed using the iterative closest point algorithm between the pre-operative torso surface and the active torso surface mesh provided by the HoloLens spatial mapping and the gaze interaction. A scanning box is visualized at the gazing point to help the user select the targeting area. The pre-operative torso surface and the sampled active vertices within the scanning box are additionally overlaid in the box as the auxiliary information for guidance. Several simple interactions are designed to control the rendering of the inner organs after the registration. The experimental results demonstrate the potential of the Holographic iRay for medical applications.},
keywords={augmented reality;biomedical MRI;data visualisation;human computer interaction;image registration;image segmentation;iterative methods;medical image processing;augmentation;Holographic iRay prototype;medical augmented reality;magnetic resonance imaging;iterative closest point algorithm;gaze interaction;scanning box;3D registration;Microsoft HoloLens spatial mapping;Unity engine;iPad;torso surface segmentation;Torso;Prototypes;Three-dimensional displays;Cameras;Surface reconstruction;Medical services;Biomedical equipment;Medical AR;Markerless Registration;ICP;HMD;HoloLens;Spatial Mapping},
doi={10.1109/ISMAR-Adjunct.2017.73},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088491,
author={T. {Bakker} and J. {Verlinden} and D. {Abbink} and R. {van Deventer}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Development of a Haptic Device with Tactile and Proprioceptive Feedback for Spatial Design Tasks},
year={2017},
volume={},
number={},
pages={223-228},
abstract={We present a novel, low-cost haptic feedback device for spatial design tasks that provides proprioceptive and tactile feedback. It uses the Manus VR datagloves and a custom VR CAD environment. Here, tactile feedback is provided to the index finger through a vibrating motor, which helps users in identifying points on a grid. This grid allows for alignment during the creation and manipulation of geometric shapes. Models can be adjusted by pinching at a vertex of the shape with index finger and thumb, and moving this to a different point on the grid. Here, proprioceptive feedback is provided by a solenoid locking mechanism. The system was evaluated through preliminary user testing. Results indicate that the device leads to more natural and intuitive interactions for both the point grid and vertex adjustment, but that the ergonomics needs to be improved. Future challenges involve further integration of the physical device and datagloves and refined, multi-finger feedback.},
keywords={CAD;force feedback;haptic interfaces;human computer interaction;virtual reality;spatial design tasks;tactile feedback;Manus VR datagloves;proprioceptive feedback;multifinger feedback;haptic feedback device;VR CAD environment;geometric shape manipulation;solenoid locking mechanism;ergonomics;Thumb;Shape;Indexes;Haptic interfaces;Solenoids;Three-dimensional displays;haptic feedback;virtual reality;dataglove;computer aided design;immersive design},
doi={10.1109/ISMAR-Adjunct.2017.74},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088492,
author={T. {Kusanagi} and S. {Kagami} and K. {Hashimoto}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] Lightning Markers: Synchronization-free Single-shot Detection of Imperceptible AR Markers Embedded in a High-Speed Video Display},
year={2017},
volume={},
number={},
pages={229-234},
abstract={This paper proposes a method of embedding AR (augmented reality) markers in a high-speed video sequence so that they are imperceptible to human eyes. The embedded markers appear for very short periods and keep changing their positions at lightning speed. By carefully designing the timings of marker display, a camera with a sufficiently short exposure time running at any frame rate is able to detect at least one marker in the image even if there is no means of synchronization between the display and the camera. Experimental results show that different types of markers such as 2D barcode markers and spatially distributed markers are able to be used with the proposed method.},
keywords={augmented reality;bar codes;cameras;image sequences;object detection;video signal processing;high-speed video display;augmented reality;high-speed video sequence;human eyes;embedded markers;lightning speed;frame rate;2D barcode markers;spatially distributed markers;lightning markers;synchronization-free single-shot detection;imperceptible AR markers;embedding AR marker;camera;marker display timing;Cameras;Synchronization;Two dimensional displays;Human computer interaction;Image color analysis;Micromirrors;Augmented reality;High-speed displaying;Information hiding;Steganography;Augmented reality},
doi={10.1109/ISMAR-Adjunct.2017.75},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088493,
author={V. {Jain} and R. {Perla} and R. {Hebbalaguppe}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] AirGestAR: Leveraging Deep Learning for Complex Hand Gestural Interaction with Frugal AR Devices},
year={2017},
volume={},
number={},
pages={235-239},
abstract={Hand gestures provide a natural and an intuitive way of user interaction in AR/VR applications. However, the most popular and commercially available devices such as the Google Cardboard and Wearality1 still employ only primitive modes of interaction such as the magnetic trigger, conductive lever and have limited user-input capability. The truly instinctual gestures work only with inordinately priced devices such as the Hololens, Magic Leap, and Meta Glasses2 which use proprietary hardware and are still not commercially available. In this paper, we explore the possibility of leveraging deep learning for recognizing complex 3-dimensional marker-less gestures (Bloom, Click, Zoom-In, Zoom-Out) in real-time using monocular camera input from a single smartphone. This framework can be used with frugal smartphones to build powerful AR/VR systems for large scale deployments that work in real-time, eliminating the need for specialized hardware. We have created a hand gesture dataset to train LSTM networks for gesture classification and published the same online. We also demonstrate the performance of our proposed method in terms of classification accuracy and computational time.},
keywords={augmented reality;cameras;gesture recognition;human computer interaction;learning (artificial intelligence);recurrent neural nets;smart phones;complex hand gestural interaction;frugal AR devices;user interaction;AR/VR applications;magnetic trigger;conductive lever;user-input capability;3-dimensional marker-less gestures;frugal smartphones;powerful AR/VR systems;gesture classification;monocular camera;AirGestAR;Three-dimensional displays;Cameras;Videos;Google;Performance evaluation;Gesture recognition;Computer architecture;Complex gesture classification;deep learning;dynamic 3D gesture;monocular vision;smartphones},
doi={10.1109/ISMAR-Adjunct.2017.76},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088494,
author={Y. {You} and A. {Boyer} and T. {Jokela} and P. {Piippo}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] SelfieWall: A Mixed Reality Advertising Platform},
year={2017},
volume={},
number={},
pages={240-244},
abstract={SelfieWall is a mixed reality advertising platform that is intended to capture the attention of passersby in events and venues with large crowds of people, such as in trade shows, sports events, or shopping malls. It aims to offer users with unique and fun branded experiences and allow them to share their experiences online, indirectly promoting an advertising message. The platform leverages large displays, 3D cameras, computer vision algorithms and creative content to transport the user to a virtual space by segmenting the user standing in front of the display from the background and super-imposing him/her into a layered image. A selfie of this experience is captured and can be shared on social media, or streamed in real-time to other public displays. With a very low barrier to entry, the user is motivated to visit the selfie website, share the selfie on his/her social media network and interact with the brand and messaging in the process. This paper presents the technical aspects of the end-to-end SelfieWall platform, the real-time content creation and on-site mobile calibration interfaces, and some typical real-world deployment. A demo proposal is submitted to the conference as well.},
keywords={advertising data processing;computer vision;image segmentation;mobile computing;multimedia computing;social networking (online);virtual reality;computer vision algorithms;mixed reality advertising platform;SelfieWall platform;3D cameras;background segmentation;social media;real-time content creation;on-site mobile calibration interfaces;Cameras;Three-dimensional displays;Mirrors;Real-time systems;Calibration;Green products;Virtual reality},
doi={10.1109/ISMAR-Adjunct.2017.77},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088495,
author={Z. {Zhang} and D. {Weng} and J. {Guo} and Y. {Liu} and Y. {Wang}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[POSTER] An Accurate Calibration Method for Optical See-Through Head-Mounted Displays Based on Actual Eye-Observation Model},
year={2017},
volume={},
number={},
pages={245-250},
abstract={Single point active alignment method (SPAAM) has become the basic calibration method for optical-see-through head-mounted displays since its appearance. However, SPAAM is based on a simple static pinhole camera model that assumes a static relationship between the user's eye and the HMD. Such theoretic defects lead to a limitation in calibration accuracy. We model the eye as a dynamic pinhole camera to account for the displacement of the eye during the calibration process. We use region-induced data enhancement (RIDE) to reduce the system error in the acquisition process. The experimental results prove that the proposed dynamic model performs better than the traditional static model, and the RIDE method can help users obtain a more accurate calibration result based on the dynamic model, which improves the accuracy significantly compared to the standard SPAAM.},
keywords={augmented reality;calibration;cameras;helmet mounted displays;region-induced data enhancement;RIDE method;dynamic model;acquisition process;calibration process;dynamic pinhole camera;calibration accuracy;simple static pinhole camera model;basic calibration method;single point active alignment method;actual eye-observation model;head-mounted displays;Calibration;Cameras;Optical imaging;Adaptive optics;Solid modeling;Resists;Standards;OST-HMD;calibration;data enhancement},
doi={10.1109/ISMAR-Adjunct.2017.78},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088496,
author={C. {Perey}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Workshop on enterprise AR adoption obstacles},
year={2017},
volume={},
number={},
pages={251-251},
abstract={Summary form only given, as follows. A complete record of the workshop was not made available for publication as part of the conference proceedings. This workshop focuses on the adoption of AR (augmented reality) for improving the operational performance of enterprise and industrial workplaces. Enterprise AR has been in pilot and prototype phase, largely led by research teams in large organizations, for over a decade. Nevertheless, wide scale deploy continues to face many obstacles. The workshop will seek to identify obstacles and to provide practical, data-driven measures and solutions to addressing obstacles identified by participants and members of the AR for Enterprise Alliance (AREA), the world’s only membership-driven organization focusing on enterprise AR adoption. The topics and questions on which this workshop will focus include: Identification and classification of current and future enterprise AR adoption barriers; Appropriate measurement of enterprise AR adoption (penetration levels); How to balance research and production-grade systems in enterprise environments; Design for success in enterprise AR introduction projects; Organizational measures to reduce enterprise AR adoption obstacles; and Technology-supported measures to reduce enterprise AR adoption obstacles.},
keywords={Augmented reality},
doi={10.1109/ISMAR-Adjunct.2017.79},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088497,
author={G. {Welch} and J. {Royan} and M. {Preda}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Workshop on Standards for Mixed and Augmented Reality Summary},
year={2017},
volume={},
number={},
pages={252-252},
abstract={Summary form only given, as follows. A complete record of the workshop was not made available for publication as part of the conference proceedings. Mixed and augmented reality (MAR) is on the brink of large-scale consumer level commercialization. Standards will be required for MAR to succeed and proliferate as an information media and new contents platform. Standards will enable the development of MAR system components able to interoperate through defined interfaces and hence will enable the development of end-to-end solutions built on system components easily plugged in together to achieve contents sharing and interoperability. The workshop will be a place to present existing standards and demonstrate how they could ease the adoption of MAR in many domains, a place to present recent initiatives in order to coordinate efforts and share requirements coming from the industry. The discussion will lay a foundation to many issues of standardization for MAR: proper subareas for standards and abstract levels, physical and environment object representation, content file format, calibration process, tracking and recognition, augmentation and display style standards, sensors and processing units dedicated to MAR, standards for nonvisual and multimodal augmentation, object feature presentation, benchmarking, Industry requirements, etc.},
keywords={Standards;Augmented reality;Industries;Commercialization;Media;Interoperability},
doi={10.1109/ISMAR-Adjunct.2017.80},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088498,
author={T. {Vigier} and C. {Moujan} and J. {Gilbert}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Workshop on VR and AR meet creative industries},
year={2017},
volume={},
number={},
pages={253-253},
abstract={Summary form only given, as follows. A complete record of the workshop was not made available for publication as part of the conference proceedings. The production of new virtual reality (VR) and augmented reality (AR) experiences tackle both technical, human and creative aspects. In this workshop, we would like to invite contributions mixing creative and technological viewpoints in order to share common understandings and lessons to provide better experiences for the final users. In this context, the workshop aims to foster participation of artists and designers as humanities scientists (philosophy, literature, etc.) to meet up with usual ISMAR attendance. Mainly we are interested with (but not limited to) the following themes and topics of interest: Innovative interaction design with consumer grade multimedia VR/AR systems; User feedback and Quality of Experience assessment for VR/AR content creation; Quality of Experience as an artistic intention in VR/AR; Usage of VR/AR technologies in art performances and design; Narrative studies/Storytelling in VR/AR; and Create in/with VR/AR, VR/AR platforms/tools to support design and art creation.},
keywords={Augmented reality;Virtual reality;Art},
doi={10.1109/ISMAR-Adjunct.2017.81},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088499,
author={L. {Lescop}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Narrative Grammar in 360},
year={2017},
volume={},
number={},
pages={254-257},
abstract={VR has now come from industry to everyday application. Mainstream software and devices allow artists to create contents with a fast learning curve. Since 2014, with the launch of Google Cardboard and 360 cameras at a reasonable price, with the massive success of Unity 3D and Unreal UDK, real-time immersion no longer stands in the hands of experts but spreads to creative enthusiasts which result in a huge production of content. Like at the early age of photography and then cinema, slowly emerge questions about composition, narrative structure and visual grammar. This article is a raw presentation of issues of narrative grammar in 360.},
keywords={grammars;mobile computing;virtual reality;VR;fast learning curve;Google Cardboard;real-time immersion;narrative structure;visual grammar;narrative grammar;Unity 3D;Unreal UDK;cinema;photography;Grammar;Motion pictures;Games;Cameras;Films;Real-time systems;Three-dimensional displays;narrative;360 images;immersion;VR;scenology;ambiance},
doi={10.1109/ISMAR-Adjunct.2017.86},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088500,
author={L. {Díaz-Kommonen}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Interactive Diorama: A Virtual Reality (VR) Reconstruction of The Anatomy Lesson of Doctor Nicolaes Tulp by Rembrandt, 1632},
year={2017},
volume={},
number={},
pages={258-261},
abstract={This document describes some aspects of a design and research project undertaken during the years 2013-2017 by the Systems of Representation research group in the Department of Media at Aalto University in Finland. The objective of the work has been to create an interactive diorama based on the painting The Anatomy Lesson of Dr. Nicolaes Tulp. The diorama concept comprises a virtual reality simulation of the artwork in which several of the characters in the painting are re-created as 3D avatars and combined with other audiovisual media including sound and video. Using the HTC-Vive virtual reality system as interface, it is possible for a guest in an exhibition to extent enter the space of the painting itself and to interact with the characters. It is intended that the diorama will be displayed in diverse venues and to a large variety of audiences. This implies a challenge for which there is a need to develop new design knowledge. In this essay I argue that information architecture (IA) can be used in the structuring of the participant's experience as well as in the organizing and management of contents.},
keywords={art;avatars;data visualisation;interactive systems;virtual reality;VR reconstruction;artwork;3D avatars;audiovisual media;information architecture;doctor nicolaes tulp;virtual reality reconstruction;HTC-Vive virtual reality system;virtual reality simulation;diorama concept;Dr. Nicolaes Tulp;anatomy lesson;interactive diorama;Aalto University;Virtual reality;Painting;Art;Media;Solid modeling;Information architecture;Navigation},
doi={10.1109/ISMAR-Adjunct.2017.85},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088501,
author={N. {O’Dwyer} and N. {Johnson} and E. {Bates} and R. {Pagés} and J. {Ondřej} and K. {Amplianitis} and D. {Monaghan} and A. {Smolić}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Virtual Play in Free-Viewpoint Video: Reinterpreting Samuel Beckett for Virtual Reality},
year={2017},
volume={},
number={},
pages={262-267},
abstract={Since the early years of the twenty-first century, the performing arts have been party to an increasing number of digital media projects that bring renewed attention to questions about, on one hand, new working processes involving capture and distribution techniques, and on the other hand, how particular works-with bespoke hard and software-can exert an efficacy over how work is created by the artist/producer or received by the audience. The evolution of author/audience criteria demand that digital arts practice modify aesthetic and storytelling strategies, to types that are more appropriate to communicating ideas over interactive digital networks, wherein AR/VR technologies are rapidly becoming the dominant interface. This project explores these redefined criteria through a reimagining of Samuel Becketts Play (1963) for digital culture. This paper offers an account of the working processes, the aesthetic and technical considerations that guide artistic decisions and how we attempt to place the overall work in the state of the art.},
keywords={art;human computer interaction;multimedia systems;video signal processing;virtual reality;free-viewpoint video;virtual Play;guide artistic decisions;digital culture;Samuel Becketts Play;AR/VR technologies;interactive digital networks;communicating ideas;digital arts practice;digital media projects;virtual reality;Art;Cameras;Media;Games;Augmented reality},
doi={10.1109/ISMAR-Adjunct.2017.87},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088502,
author={A. {Dey} and M. {Billinghurst} and G. {Welch}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Workshop on augmented reality for good},
year={2017},
volume={},
number={},
pages={268-268},
abstract={Summary form only given, as follows. A complete record of the workshop was not made available for publication as part of the conference proceedings. Augmented Reality has started to become mainstream. With the AR research and technological advances, it is now possible to use AR in almost all domains and places. This provides a bigger opportunity to create applications that intend to impact society in greater ways than beyond just entertainment. Today the world is facing different challenges in health, the environment, and education among others. Now is the time to explore how AR could be used to solve widespread societal challenges. We invite application and position papers, addressing the way that AR can solve real-world problems in various application domains including, but not limited to, health, the environment, education, sports, and applications in support of special needs such as assistive, adaptive, and rehabilitative applications. Our focus and preference will be on applications that are beyond general uses of AR. The workshop will have oral presentations, group work, and panel to explore ideas.},
keywords={Augmented reality;Entertainment industry},
doi={10.1109/ISMAR-Adjunct.2017.82},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088503,
author={S. L. {Perkins} and M. A. {Lin} and S. {Srinivasan} and A. J. {Wheeler} and B. A. {Hargreaves} and B. L. {Daniel}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={A Mixed-Reality System for Breast Surgical Planning},
year={2017},
volume={},
number={},
pages={269-274},
abstract={One quarter of women who undergo breast lumpectomy to treat early-stage breast cancer in the United States undergo a repeat surgery due to concerns that residual tumor was left behind. This has led to a significant increase in women choosing mastectomy operations in the United States. We have developed a mixed-reality system that projects a 3D “hologram” of images from a breast MRI onto a patient using the Microsoft HoloLens. The goal of this system is to reduce the number of repeated surgeries by improving surgeons' ability to determine tumor extent. We are conducting a pilot study in patients with palpable tumors that tests a surgeon's ability to accurately identify the tumor location via mixed-reality visualization during surgical planning. Although early results are promising, it is critical but not straightforward to align holograms to the breast and to account for tissue deformations. More work is needed to improve the registration and holographic display at arm's-length working distance. Nonetheless, first results from breast cancer surgeries have shown that mixed-reality guidance can indeed provide information about tumor location, and that this exciting new use for AR has the potential to improve the lives of many patients.},
keywords={augmented reality;biological organs;biological tissues;biomedical MRI;cancer;data visualisation;medical image processing;surgery;tumours;microsoft hololens;tissue deformations;3D hologram;breast lumpectomy;breast surgical planning;breast cancer surgeries;mixed-reality visualization;tumor location;palpable tumors;breast MRI;mixed-reality system;mastectomy operations;Surgery;Tumors;Breast;Shape;Virtual reality;Planning;Augmented reality;breast cancer;breast-conserving surgery;magnetic resonance imaging;mixed reality;surgical planning},
doi={10.1109/ISMAR-Adjunct.2017.92},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088504,
author={M. A. {Cidota} and S. G. {Lukosch} and P. J. M. {Bank} and P. W. {Ouwehand}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Towards Engaging Upper Extremity Motor Dysfunction Assessment Using Augmented Reality Games},
year={2017},
volume={},
number={},
pages={275-278},
abstract={Advances in technology offer new opportunities for a better understanding of how different disorders affect motor function. Our aim is to explore the potential of augmented reality (AR) using free hand and body tracking to develop engaging games for a uniform, cost-effective and objective evaluation of upper extremity motor dysfunction in different patient groups. Based on the insights from a study with 20 patients (10 Parkinson's Disease patients and 10 stroke patients) who performed hand/arm movement tasks in AR, we created a set of different augmented reality games for upper extremity motor dysfunction assessment.},
keywords={augmented reality;biomechanics;diseases;medical computing;medical disorders;patient rehabilitation;upper extremity motor dysfunction assessment;motor function;free hand;body tracking;engaging games;medical disorders;Parkinson disease patients;stroke patients;augmented reality games;hand-arm movement tasks;Games;Augmented reality;Tracking;Visualization;Usability;Resists;Augmented Reality Games;Engagement;Upper Extremity Motor Dysfunction;Assessment;Parkinson’s Disease;Stroke patients},
doi={10.1109/ISMAR-Adjunct.2017.88},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088505,
author={B. {Kerous} and F. {Liarokapis}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={BrainChat - A Collaborative Augmented Reality Brain Interface for Message Communication},
year={2017},
volume={},
number={},
pages={279-283},
abstract={This paper presents BrainChat, an augmented reality based multiuser concept for brain-computer interfaces. The goal is to provide seamless communication based only on thoughts. A working prototype is presented, which demonstrates two-person textual communication based on non-invasive brain computer interfaces. Design choices are discussed and directions for future work are provided, considering the relevant research directions in Brain-Computer Interfaces based on Electroencephalography.},
keywords={augmented reality;brain-computer interfaces;electroencephalography;noninvasive brain computer interfaces;BrainChat;collaborative augmented reality Brain interface;message communication;multiuser concept;seamless communication;working prototype;two-person textual communication;design choices;electroencephalography;Electroencephalography;Augmented reality;Brain-computer interfaces;Brain;Electrodes;Training;Calibration},
doi={10.1109/ISMAR-Adjunct.2017.91},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088506,
author={R. {Dinic} and T. {Stütz}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={EatAR Tango: Results on the Accuracy of Portion Estimation},
year={2017},
volume={},
number={},
pages={284-287},
abstract={Nutrition is an important factor in the prevention and treatment of many diseases. Nutrition is a key factor for obesity, which is a risk factor for cardio-vascular diseases, type-2 diabetes and even cancer. Many non-communicable diseases require patients to keep track of nutrition accurately. Diabetes type-2 and type-1 require tracking carbohydrate intake accurately. However, keeping track and even determining nutrition information in everyday life is not an easy task and support for patients is needed. Recently, mobile devices with depth sensors have been available to the consumer market and apps using this new technology for nutrition assess-ment have been proposed (EatAR Tango). In this work, we pre-sent first results on the accuracy achieved by users with the EatAR Tango app. The user study had 16 participants which used the app to assess the nutrition information of three different por-tions of rice.},
keywords={biomedical measurement;cancer;cardiovascular system;medical computing;mobile computing;mobile radio;molecular biophysics;sensors;type-2 diabetes;noncommunicable diseases;nutrition information;EatAR Tango app;portion estimation;risk factor;carbohydrate intake;cardiovascular diseases;nutrition assessment;depth sensors;mobile devices;Estimation;Diabetes;Three-dimensional displays;Sensors;Mobile communication;Diseases;Image reconstruction;Augmented reality; mobile; portion estimation},
doi={10.1109/ISMAR-Adjunct.2017.90},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088507,
author={J. M. {Coughlan} and J. {Miele}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={AR4VI: AR as an Accessibility Tool for People with Visual Impairments},
year={2017},
volume={},
number={},
pages={288-292},
abstract={Although AR technology has been largely dominated by visual media, a number of AR tools using both visual and auditory feedback have been developed specifically to assist people with low vision or blindness - an application domain that we term Augmented Reality for Visual Impairment (AR4VI). We describe two AR4VI tools developed at Smith-Kettlewell, as well as a number of pre-existing examples. We emphasize that AR4VI is a powerful tool with the potential to remove or significantly reduce a range of accessibility barriers. Rather than being restricted to use by people with visual impairments, AR4VI is a compelling universal design approach offering benefits for mainstream applications as well.},
keywords={augmented reality;handicapped aids;visual perception;universal design approach;accessibility barriers;AR4VI;augmented reality for visual impairment;visually impaired people;auditory feedback;visual feedback;AR tools;visual media;accessibility tool;Visualization;Fingers;Tools;Image enhancement;Navigation;Three-dimensional displays;Haptics;tactile graphics;accessibility;visual impairment;low vision;blindness},
doi={10.1109/ISMAR-Adjunct.2017.89},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088508,
author={K. {Chang} and J. {Zhang} and T. {Liu}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Workshop on Virtual, Augmented and Mixed Reality in Education (VAMrE 2017) Summary},
year={2017},
volume={},
number={},
pages={293-293},
abstract={Summary form only given, as follows. A complete record of the workshop was not made available for publication as part of the conference proceedings. This Workshop on Virtual, Augmented and Mixed Reality in Education (VAMrE 2017) offers an opportunity to exchange, publish, and discuss ideas and thoughts regarding the scope of research related to the applications of VAMR in education. The theme is focused on, but not limited to, applications of VAMR in the development of teaching strategies, establishment of a teaching environment, improvement of learning effectiveness, application of psychological factors related to learning, and other related research.},
keywords={Augmented reality},
doi={10.1109/ISMAR-Adjunct.2017.83},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088509,
author={A. {Adjorlu} and E. R. {Høeg} and L. {Mangano} and S. {Serafin}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Daily Living Skills Training in Virtual Reality to Help Children with Autism Spectrum Disorder in a Real Shopping Scenario},
year={2017},
volume={},
number={},
pages={294-302},
abstract={In this paper, we present a study conducted to investigate the feasibility and effectiveness of Virtual Reality (VR) applied to daily living skills (DLS) training of individuals diagnosed with Autism Spectrum Disorder (ASD). In collaboration with a teacher at a school for children and adolescents with mental disorders, a head-mounted display based VR simulation of a supermarket was built and evaluated with the purpose of developing the shopping skills of students diagnosed with ASD. A comparative between-group experiment was conducted on 9 participants, with initiated VR training following a baseline assessment in a real supermarket. After running seven VR training sessions over 10 days for the treatment group, participants were assessed again in the real supermarket. Results show some benefit of training DLS using VR as discussed in the paper.},
keywords={computer based training;handicapped aids;helmet mounted displays;medical disorders;virtual reality;Virtual Reality;children;Autism Spectrum Disorder;shopping scenario;ASD;mental disorders;head-mounted display;VR simulation;supermarket;comparative between-group experiment;initiated VR training;training DLS;VR training sessions;daily living skills training;Training;Virtual environments;Solid modeling;Autism},
doi={10.1109/ISMAR-Adjunct.2017.93},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088510,
author={J. {Choi} and B. {Yoon} and C. {Jung} and W. {Woo}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={ARClassNote: Augmented Reality Based Remote Education Solution with Tag Recognition and Shared Hand-Written Note},
year={2017},
volume={},
number={},
pages={303-309},
abstract={We present `ARClassNote', an Augmented Reality application that enables users to save and share handwritten notes across multiple optical see-through Head Mounted Devices. In an augmented classroom environment, `ARClassNote' makes it easier to achieve bilateral communication between instructors and students, and share written class materials without occlusion. We showcase how each component of `ARClassNote' operates. We then propose suitable user interface for Augmented Reality head mounted devices. Evaluation of our applications' capabilities show that 'ARClassNote' is suitable for Augmented Reality Remote Education Solution.},
keywords={augmented reality;computer aided instruction;groupware;helmet mounted displays;user interfaces;tag recognition;shared hand-written note;augmented classroom environment;augmented reality application;user interface;ARClassNote;augmented reality based remote education solution;optical see-through head mounted devices;bilateral communication;written class materials;augmented reality head mounted devices;Resists;Servers;Optical character recognition software;Augmented reality;User interfaces;Cameras;Handwriting recognition},
doi={10.1109/ISMAR-Adjunct.2017.94},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088511,
author={L. {Jayaraj} and J. {Wood} and M. {Gibson}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Improving the Immersion in Virtual Reality with Real-Time Avatar and Haptic Feedback in a Cricket Simulation},
year={2017},
volume={},
number={},
pages={310-314},
abstract={The basis of this research is concerned with designing and implementing a system that allows a player to engage in a virtual reality (VR) game with better immersion. The research was based on the idea that an avatar generated using real-time motion capture would improve the player's immersion by increasing the perception of presence. When playing the VR games a common problem was observed. The user's avatar (Virtual agent) was not improved as most of the games were played using limited controllers. The inputs from these controllers were noted as insufficient to generate the entire body's animation. This research attempts to solve this problem by proposing/implementing full body motion capture and the establishment of the self-avatar in real time in a VR game. This involved designing a system that utilizes the effective technologies for 3D imaging, transmission and haptic feedback. The research attempts to measure the immersion by enhancing measuring instruments (Norman, 2010). It is complimented by a user-based study that involves/involved collecting both qualitative and quantitative data through questionnaire and observation.},
keywords={avatars;computer animation;computer games;haptic interfaces;human computer interaction;image motion analysis;sport;virtual reality;real-time motion capture;VR game;body motion capture;haptic feedback;real-time avatar;cricket simulation;3D imaging;Three-dimensional displays;Games;Cameras;Solid modeling;Virtual reality;Real-time systems;Haptic interfaces;Virtual reality;motion capture;games development;immersion;commercial systems},
doi={10.1109/ISMAR-Adjunct.2017.95},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088512,
author={V. T. {Nguyen} and T. {Dang}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Setting up Virtual Reality and Augmented Reality Learning Environment in Unity},
year={2017},
volume={},
number={},
pages={315-320},
abstract={We propose a framework and a setup for presenting complex models for curriculum contents in both augmented reality and virtual reality environment. After constructing some three-dimensional models representing real world objects such as trees, stones, rivers, dams, and buildings, our workflow uses the Unity engine in combination with Virtual Reality headset devices to create interactive applications for both Virtual Reality and Augmented Reality environments to support students understanding the curriculum contents through their surrounding. Typical challenges are addressed when creating 3D curriculum contents, integrating these models into Unity and solutions are proposed where possible. The overall structure of the project is described with some functionalities added to Unity for visualization and interaction with the models.},
keywords={augmented reality;computer aided instruction;data visualisation;solid modelling;Unity engine;Virtual Reality headset devices;Augmented Reality learning environment;visualization;3D curriculum contents;3D models;Solid modeling;Three-dimensional displays;Augmented reality;Games;Buildings;Google;Mixed reality;computational thinking;curriculum contents;Unity engine;watershed},
doi={10.1109/ISMAR-Adjunct.2017.97},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088513,
author={W. {Jung} and W. {Cho} and H. {Kim} and W. {Woo}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={BoostHand : Distance-free Object Manipulation System with Switchable Non-linear Mapping for Augmented Reality Classrooms},
year={2017},
volume={},
number={},
pages={321-325},
abstract={In this paper, we propose BoostHand, a freehand, distance-free object-manipulation system that supports simple trigger gestures using Leap Motion. In AR classrooms, it is necessary to allow both lecturers and students to utilize virtual teaching materials without any spatial restrictions, while handling virtual objects easily, regardless of distance. To provide efficient and accurate methods of handling AR classroom objects, our system requires only simple intuitive freehand gestures to control the users virtual hands in an enlarged, shared control space of users. We modified the GoGo interaction technique [5] by adding simple trigger gestures, and we evaluated performance against gaze-assisted selection (GaS) capabilities. Our proposed system enables both lecturers and students to utilize virtual teaching materials easily from their remote positions.},
keywords={augmented reality;computer aided instruction;gesture recognition;human computer interaction;image motion analysis;multimedia computing;teaching;BoostHand;distance-free object manipulation system;augmented reality classrooms;virtual teaching materials;AR classroom objects;virtual hands;switchable nonlinear mapping;trigger gestures;intuitive freehand gestures;Leap Motion;GoGo interaction technique;multimedia;Three-dimensional displays;Aerospace electronics;Avatars;Switches;Education;Electronic mail},
doi={10.1109/ISMAR-Adjunct.2017.96},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088514,
author={H. {Saito} and S. {Mori} and S. {Ikeda}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Workshop on highly diverse cameras and displays for mixed and augmented reality (HDCD4MAR)},
year={2017},
volume={},
number={},
pages={326-326},
abstract={Summary form only given, as follows. A complete record of the workshop was not made available for publication as part of the conference proceedings. Computer vision technology has evolved AR/MR to a point and given AR/MR (augmented reality/mixed reality) communities fruits to solve unique problems in AR/MR. To bring AR/MR to the next level, we need to investigate a way to fully utilize these fruits. From this point of view, in this workshop, HDCD4MAR, we focus on bringing opportunities to researchers to discuss experiences and findings on vision-based approaches especially regarding the diversity of input and output devices that would exist in the AR/MR environments or be brought there by the AR/MR users. We expect participants from various types of research fields such as multi-view stereo, computational photography, vSLAM, light field displays, etc. and workshop will give chances them for future collaborations.},
keywords={Augmented reality;Cameras;Computer vision;Photography;Collaboration},
doi={10.1109/ISMAR-Adjunct.2017.84},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088515,
author={H. {Shishido} and K. {Yamanaka} and Y. {Kameda} and I. {Kitahara}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Pseudo-Dolly-In Video Generation Combining 3D Modeling and Image Reconstruction},
year={2017},
volume={},
number={},
pages={327-333},
abstract={This paper proposes a pseudo-dolly-in video generation method that reproduces motion parallax by applying image reconstruction processing to multi-view videos. Since dolly-in video is taken by moving a camera forward to reproduce motion parallax, we can present a sense of immersion. However, at a sporting event in a large-scale space, moving a camera is difficult. Our research generates dolly-in video from multi-view images captured by fixed cameras. By applying the Image-Based Modeling technique, dolly-in video can be generated. Unfortunately, the video quality is often damaged by the 3D estimation error. On the other hand, Bullet-Time realizes high-quality video observation. However, moving the virtual-viewpoint from the capturing positions is difficult. To solve these problems, we propose a method to generate a pseudo-dolly-in image by installing 3D estimation and image reconstruction techniques into Bullet-Time and show its effectiveness by applying it to multi-view videos captured at an actual soccer stadium. In the experiment, we compared the proposed method with digital zoom images and with the dolly-in video generated from the Image-Based Modeling and Rendering method.},
keywords={cameras;image reconstruction;image resolution;rendering (computer graphics);sport;stereo image processing;video signal processing;image reconstruction;video generation method;motion parallax;multiview videos;multiview images;video quality;high-quality video observation;digital zoom images;3D modeling;psuedo-dolly-in video generation;virtual viewpoint;image-based modeling;rendering method;Cameras;Three-dimensional displays;Image reconstruction;Solid modeling;Estimation;Cloud computing;Image restoration},
doi={10.1109/ISMAR-Adjunct.2017.100},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088516,
author={K. {Yagi} and K. {Hasegawa} and H. {Saito}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Diminished Reality for Privacy Protection by Hiding Pedestrians in Motion Image Sequences Using Structure from Motion},
year={2017},
volume={},
number={},
pages={334-337},
abstract={We present a method for generating images in which people are hidden from image sequences taken with a hand-held camera. Our method is basically used for privacy protection of people whose images are unintentionally captured in image sequences. We hide people from images by reconstructing a 3D model of background and projecting it to 2D images. By detecting the area in which people are present beforehand, we can reconstruct a 3D model of the background without people. In the experiment, We compare our method with some conventional approaches for diminished reality.},
keywords={cameras;data privacy;image motion analysis;image reconstruction;image sequences;pedestrians;diminished reality;privacy protection;motion image sequences;hand-held camera;hiding pedestrians;structure from motion;image reconstruction;Three-dimensional displays;Solid modeling;Cameras;Image reconstruction;Image sequences;Privacy;Diminished reality;structure from motion;privacy protection;multi-view stereo;inpainting},
doi={10.1109/ISMAR-Adjunct.2017.101},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8088517,
author={Y. {Nakajima} and S. {Mori} and H. {Saito}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Semantic Object Selection and Detection for Diminished Reality Based on SLAM with Viewpoint Class},
year={2017},
volume={},
number={},
pages={338-343},
abstract={We propose a novel diminished reality method which is able to (i) automatically recognize the region to be diminished, (ii) work with a single RGB-D sensor, and (iii) work without pre-processing to generate a 3D model of the target scene by utilizing SLAM, segmentation, and recognition framework. Especially, regarding the recognition of the area to be diminished, our method is able to maintain high accuracy no matter how the camera moves by distributing the viewpoints for each object uniformly and aggregating recognition results from each distributed viewpoint as the same weight. These advantages are demonstrated on the UW RGB-D Dataset and Scenes.},
keywords={augmented reality;cameras;feature extraction;image colour analysis;image motion analysis;image reconstruction;image segmentation;image sensors;image texture;object detection;pose estimation;SLAM (robots);solid modelling;video signal processing;semantic object selection;SLAM;viewpoint class;single RGB-D sensor;target scene;segmentation;recognition framework;distributed viewpoint;semantic object detection;diminished reality method;region recognition;3D model;cameras;UW RGB-D dataset;Cameras;Simultaneous localization and mapping;Three-dimensional displays;GSM;Image segmentation;Solid modeling;Image recognition;Diminished Reality;Object Recognition;Convolutional Neural Network;SLAM;Segmentation},
doi={10.1109/ISMAR-Adjunct.2017.98},
ISSN={},
month={Oct},}