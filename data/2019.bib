%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for Rafael Radkowski at 2021-03-13 08:54:08 -0600 


%% Saved with string encoding Unicode (UTF-8) 



@inproceedings{8943728,
	Abstract = {In this paper, we address the topic of outdoor localization and tracking using monocular camera setups with poor GPS priors. We leverage 2.5D building maps, which are freely available from open-source databases such as OpenStreetMap. The main contributions of our work are a fast initialization method and a non-linear optimization scheme. The initialization upgrades a visual SLAM reconstruction with an absolute scale. The non-linear optimization uses the 2.5D building model footprint, which further improves the tracking accuracy and the scale estimation. A pose optimization step relates the vision-based camera pose estimation from SLAM to the position information received through GPS, in order to fix the common problem of drift. We evaluate our approach on a set of challenging scenarios. The experimental results show that our approach achieves improved accuracy and robustness with an advantage in run-time over previous setups.},
	Author = {R. {Liu} and J. {Zhang} and S. {Chen} and C. {Arth}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2019.00016},
	Issn = {1554-7868},
	Keywords = {cameras;computer vision;Global Positioning System;image reconstruction;mobile robots;optimisation;pose estimation;robot vision;SLAM (robots);initialization upgrades;nonlinear optimization scheme;fast initialization method;open-source databases;2.5D building maps;poor GPS priors;monocular camera setups;outdoor localization;2.5D building models;towards SLAM-based;previous setups;vision-based camera;optimization step;scale estimation;tracking accuracy;2.5D building model footprint;absolute scale;visual SLAM reconstruction;Simultaneous localization and mapping;Cameras;Optimization;Global Positioning System;Buildings;Image reconstruction;Solid modeling;Outdoor Localization and tracking;hybrid SLAM system;fast initialization;outdoor augmented reality},
	Month = {Oct},
	Pages = {1-7},
	Title = {Towards SLAM-Based Outdoor Localization using Poor GPS and 2.5D Building Models},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2019.00016}}

@inproceedings{8943719,
	Abstract = {We are interested in AR applications which take place in man-made GPS-denied environments, as industrial or indoor scenes. In such environments, relocalization may fail due to repeated patterns and large changes in appearance which occur even for small changes in viewpoint. We investigate in this paper a new method for relocalization which operates at the level of objects and takes advantage of the impressive progress realized in object detection. Recent works have opened the way towards object oriented reconstruction from elliptic approximation of objects detected in images. We go one step further and propose a new method for pose computation based on ellipse/ellipsoid correspondences. We consider in this paper the practical common case where an initial guess of the rotation matrix of the pose is known, for instance with an inertial sensor or from the estimation of orthogonal vanishing points. Our contributions are twofold: we prove that a closed form estimate of the translation can be computed from one ellipse-ellipsoid correspondence. The accuracy of the method is assessed on the LINEMOD database using only one correspondence. Second, we prove the effectiveness of the method on real scenes from a set of object detections generated by YOLO. A robust framework that is able to choose the best set of hypotheses is proposed and is based on an appropriate estimation of the reprojection error of ellipsoids. Globally, considering pose at the level of object allows us to avoid common failures due to repeated structures. In addition, due to the small combinatory induced by object correspondences, our method is well suited to fast rough localization even in large environments.},
	Author = {V. {Gaudilli{\`e}re} and G. {Simon} and M. {Berger}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2019.00017},
	Issn = {1554-7868},
	Keywords = {cameras;computer vision;Global Positioning System;image matching;image reconstruction;image sensors;matrix algebra;object detection;pose estimation;camera relocalization;object correspondences;ellipse-ellipsoid correspondence;closed form estimate;orthogonal vanishing points;practical common case;elliptic approximation;object oriented reconstruction;object detection;impressive progress;repeated patterns;indoor scenes;industrial scenes;man-made GPS-denied environments;ellipsoidal abstraction;Cameras;Ellipsoids;Three-dimensional displays;Object detection;Estimation;Computational modeling;Solid modeling;Camera relocalization;Ellipsoids;Ellipses;Augmented reality},
	Month = {Oct},
	Pages = {8-18},
	Title = {Camera Relocalization with Ellipsoidal Abstraction of Objects},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2019.00017}}

@inproceedings{8943776,
	Abstract = {Sharing live telepresence experiences for teleconferencing or remote collaboration receives increasing interest with the recent progress in capturing and AR/VR technology. Whereas impressive telepresence systems have been proposed on top of on-the-fly scene capture, data transmission and visualization, these systems are restricted to the immersion of single or up to a low number of users into the respective scenarios. In this paper, we direct our attention on immersing significantly larger groups of people into live-captured scenes as required in education, entertainment or collaboration scenarios. For this purpose, rather than abandoning previous approaches, we present a range of optimizations of the involved reconstruction and streaming components that allow the immersion of a group of more than 24 users within the same scene - which is about a factor of 6 higher than in previous work - without introducing further latency or changing the involved consumer hardware setup. We demonstrate that our optimized system is capable of generating high-quality scene reconstructions as well as providing an immersive viewing experience to a large group of people within these live-captured scenes.},
	Author = {P. {Stotko} and S. {Krumpen} and M. {Weinmann} and R. {Klein}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2019.00018},
	Issn = {1554-7868},
	Keywords = {groupware;image reconstruction;teleconferencing;virtual reality;AR/VR technology;high-quality scene reconstructions;optimized system;involved consumer hardware setup;streaming components;entertainment;live-captured scenes;respective scenarios;visualization;data transmission;on-the-fly scene capture;impressive telepresence systems;remote collaboration;teleconferencing;live telepresence experiences;group-scale multiclient live telepresence;efficient 3D reconstruction;immersive viewing experience;Three-dimensional displays;Telepresence;Image reconstruction;Data models;Servers;Scalability;Cameras;Human centered computing Human computer interaction (HCI) Interaction paradigms Virtual reality, Human centered computing Human computer interaction (HCI) Interaction paradigms Collaborative interaction, Computing methodologies Computer graphics Graphics systems and interfaces Virtual reality, Computing methodologies Computer vision Computer vision problems Reconstruction},
	Month = {Oct},
	Pages = {19-25},
	Title = {Efficient 3D Reconstruction and Streaming for Group-Scale Multi-Client Live Telepresence},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2019.00018}}

@inproceedings{8943645,
	Abstract = {Many crucial applications in the fields of filmmaking, game design, education, and cultural preservation - among others - involve the modeling, authoring, or editing of 3D objects and scenes. The two major methods of creating 3D models are 1) modeling, using computer software, and 2) reconstruction, generally using high-quality 3D scanners. Scanners of sufficient quality to support the latter method remain unaffordable to the general public. Since the emergence of consumer-grade RGBD cameras, there has been a growing interest in 3D reconstruction systems using depth cameras. However, most such systems are not user-friendly, and require intense efforts and practice if good reconstruction results are to be obtained. In this paper, we propose to increase the accessibility of depth-camera-based 3D reconstruction by assisting its users with augmented reality (AR) technology. Specifically, the proposed approach will allow users to rotate/move a target object freely with their hands and see the object being overlapped with its reconstructing model during the reconstruction process. As well as being more instinctual than conventional reconstruction systems, our proposed system will provide useful hints on complete 3D reconstruction of an object, including the best capturing range; reminder of moving and rotating the object at a steady speed; and which model regions are complex enough to require zooming-in. We evaluated our system via a user study that compared its performance against those of three other stateof- the-art approaches, and found our system outperforms the other approaches. Specifically, the participants rated it highest in usability, understandability, and model satisfaction.},
	Author = {Y. {Wu} and L. {Chan} and W. {Lin}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2019.00-30},
	Issn = {1554-7868},
	Keywords = {augmented reality;cameras;computer vision;image motion analysis;image reconstruction;image sensors;solid modelling;scanners;high-quality 3D;computer software;cultural preservation;game design;model satisfaction;model regions;complete 3D reconstruction;conventional reconstruction systems;reconstruction process;reconstructing model;target object;augmented reality;depth-camera-based 3D reconstruction;depth cameras;3D reconstruction systems;consumer-grade RGBD cameras;general public;Three-dimensional displays;Image reconstruction;Solid modeling;Cameras;Skin;Glass;Geometry},
	Month = {Oct},
	Pages = {26-36},
	Title = {Tangible and Visible 3D Object Reconstruction in Augmented Reality},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2019.00-30}}

@inproceedings{8943678,
	Abstract = {We present a novel and efficient approach that automatically constructs 3D models for garments using only RGB images. Most of the previous methods deal with input photos in which the garment is on a human body or mannequin. Our approach can work with various types of input garment photos: photos in which the garment is worn by a model, or photos in which the garment is laid on a flat surface. To construct a complete 3D model, our approach requires minimum two images as input: one front view and one rear view. We propose a multi-task learning network called JFNet that jointly identifies the landmarks of the garment as well as parses the garment into semantic part segments. The predicted landmarks are used to estimate the garment size thus a template mesh can be deformed accordingly to construct the 3D mesh model. Color and textures of the model are extracted by exploiting the parsed semantic parts from input images. Our approach can be applied in various Virtual Reality and Mixed Reality applications involving garment modeling.},
	Author = {Y. {Xu} and S. {Yang} and W. {Sun} and L. {Tan} and K. {Li} and H. {Zhou}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2019.00-28},
	Issn = {1554-7868},
	Keywords = {augmented reality;clothing;clothing industry;image colour analysis;image segmentation;learning (artificial intelligence);mesh generation;production engineering computing;solid modelling;RGB images;input photos;input garment photos;garment size;3D mesh model;input images;3D virtual garment modeling;multitask learning network;JFNet;parsed semantic parts;virtual reality applications;mixed reality applications;Clothing;Three-dimensional displays;Solid modeling;Semantics;Shape;Image segmentation;Computational modeling;landmark prediction, semantic part segmentation, 3D garment modeling},
	Month = {Oct},
	Pages = {37-45},
	Title = {3D Virtual Garment Modeling from RGB Images},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2019.00-28}}

@inproceedings{8943701,
	Abstract = {Image-based 3D modelling using Structure-form-Motion (SfM) has matured significantly over the last decade. Standard SfM methods create the object's texture from the appearance of the physical object at the time of acquisition. We propose a method for acquiring the diffuse per-point reflectance of the modelled object, as part of the image acquisition work flow, only adding one extra captured image and an irradiance rendering step, making it easy for anyone to digitize physical objects to create 3D content for AR/VR using only consumer grade hardware. Current state of the art of spatially varying reflectance capture requires either large, expensive, and purpose built setups or are optimization based approaches, whereas the proposed approach is model based. This paper proposes adding a render of irradiance with modelled camera and light source, using off the shelf hardware for image capture. The key element is taking two images at each imaging location: one with just the ambient illumination conditions, and one where the light from an on-camera flash is included. It is demonstrated how to get the ambient illumination to cancel out, and by assuming Lambertian materials, render the irradiance corresponding to the flash-only image, enabling computation of spatially varying diffuse reflectance rather than appearance. Qualitative results demonstrate the added realism of the modelled objects when used as assets in renders under varying illumination conditions, including limited outdoor scenarios. Quantitative tests demonstrate that the reflectance can be estimated correctly to within a few percent even in cases with severe un-even ambient illumination.},
	Author = {K. S. {Ladefoged} and C. B. {Madsen}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2019.00-27},
	Issn = {1554-7868},
	Keywords = {augmented reality;cameras;image capture;image motion analysis;image reconstruction;image texture;rendering (computer graphics);solid modelling;stereo image processing;image-based modeling applications;standard SfM methods;physical object;image acquisition work flow;consumer grade hardware;off the shelf hardware;image capture;imaging location;ambient illumination conditions;flash-only image;spatially-varying diffuse reflectance capture;irradiance map rendering;structure-form-motion;objects texture;AR-VR;optimization based approaches;on-camera flash;Lambertian materials;Lighting;Cameras;Three-dimensional displays;Light sources;Image reconstruction;Rendering (computer graphics);Solid modeling;Reflectance;Spatially Varying;Diffuse Reflectance;Image Based Modeling;Irradiance map Rendering;SfM;image based 3D modelling;albedo generation;diffuse albedo},
	Month = {Oct},
	Pages = {46-54},
	Title = {Spatially-Varying Diffuse Reflectance Capture Using Irradiance Map Rendering for Image-Based Modeling Applications},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2019.00-27}}

@inproceedings{8943682,
	Abstract = {We propose a novel spatial augmented reality (SAR) framework to edit the appearance of physical glossy surfaces. The key idea is utilizing the specular reflection, which was a major distractor in conventional SAR systems. Namely, we spatially manipulate the appearance of an environmental surface, which is observed through the specular reflection. We use a stereoscopic display to present two appearances with disparity on the environmental surface, by which the depth of the specularly reflected visual information corresponds to the glossy surface. We refer to this method as augmented environment mapping (AEM). The paper describes its principle, followed by three different implementation approaches inspired by typical virtual and augmented reality approaches. We confirmed the feasibility of AEM through both quantitative and qualitative experiments using prototype systems.},
	Author = {T. {Kaminokado} and D. {Iwai} and K. {Sato}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2019.00-26},
	Issn = {1554-7868},
	Keywords = {augmented reality;prototype systems;physical glossy surfaces;appearance editing;augmented reality approaches;virtual reality;augmented environment mapping;glossy surface;specularly reflected visual information corresponds;stereoscopic display;specular reflection;environmental surface;SAR systems;Observers;Surface treatment;Mirrors;Three-dimensional displays;Prototypes;Optical imaging;Cameras;Augmented reality;Projection mapping;Environment mapping},
	Month = {Oct},
	Pages = {55-65},
	Title = {Augmented Environment Mapping for Appearance Editing of Glossy Surfaces},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2019.00-26}}

@inproceedings{8943754,
	Abstract = {Coherent rendering in augmented reality deals with synthesizing virtual content that seamlessly blends in with the real content. Unfortunately, capturing or modeling every real aspect in the virtual rendering process is often unfeasible or too expensive. We present a post-processing method that improves the look of rendered overlays in a dental virtual try-on application. We combine the original frame and the default rendered frame in an autoencoder neural network in order to obtain a more natural output, inspired by artistic style transfer research. Specifically, we apply the original frame as style on the rendered frame as content, repeating the process with each new pair of frames. Our method requires only a single forward pass, our shallow architecture ensures fast execution, and our internal feedback loop inherently enforces temporal consistency.},
	Author = {V. {Vasiliu} and G. {S{\"o}r{\"o}s}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2019.00-25},
	Issn = {1554-7868},
	Keywords = {augmented reality;neural nets;rendering (computer graphics);rendered frame;artistic style transfer research;autoencoder neural network;original frame;dental virtual try-on application;rendered overlays;post-processing method;virtual rendering process;virtual content;augmented reality;fast neural style transfer;virtual smile previews;coherent rendering;Rendering (computer graphics);Image color analysis;Dentistry;Training;Augmented reality;Cameras;Task analysis;augmented reality;coherent rendering;style transfer;convolutional neural network;autoencoder},
	Month = {Oct},
	Pages = {66-73},
	Title = {Coherent Rendering of Virtual Smile Previews with Fast Neural Style Transfer},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2019.00-25}}

@inproceedings{8943651,
	Abstract = {We present a novel mixed reality (MR) rendering and composition solution that illuminates and blends virtual objects into underwater 360$\,^{\circ}$ videos (360-video) in real-time. Real-time underwater lighting (caustics, god rays, fog, and particulates) were developed to improve the overall lighting and blending quality. We also provide a MR toolkit, an interface to tune the parameters of the underwater lighting so the user can match the lighting observed in the 360-video. Our image based lighting provides automatic ambient and high frequency underwater lighting. This ensures that the virtual objects are lit and blend similarly to each frame of the video semi-automatically and in real-time. We conducted a user study by having participants rate our method based on the visual quality and presence using a five point Likert Scale. The results show that our underwater lighting is preferred over no underwater effects or using naive ambient lighting. We also have a few takeaways on what elements of our underwater lighting and interaction have a significant impact on visual quality and presence in underwater MR.},
	Author = {S. {Thompson} and A. {Chalmers} and T. {Rhee}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2019.00-24},
	Issn = {1554-7868},
	Keywords = {augmented reality;image sensors;rendering (computer graphics);user interfaces;virtual reality;naive ambient lighting;five point Likert Scale;MR toolkit;real time mixed reality rendering;underwater MR;using naive ambient lighting;underwater effects;visual quality;video semiautomatically;high frequency underwater lighting;image based lighting;blending quality;real-time underwater lighting;blends virtual objects;illuminates;composition solution;underwater 360$\,^{\circ}$ videos;Lighting;Videos;Real-time systems;Rendering (computer graphics);Sea surface;Virtual reality;Visualization;Mixed reality;Virtual reality;360$\,^{\circ}$ video;underwater lighting;caustics;fog;god rays;particles;composition;lighting;image-based lighting},
	Month = {Oct},
	Pages = {74-82},
	Title = {Real-Time Mixed Reality Rendering for Underwater 360$\,^{\circ}$ Videos},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2019.00-24}}

@inproceedings{8943647,
	Abstract = {Improvements in both software and hardware, as well as an increase in consumer suitable equipment, have resulted in great advances in the fields of virtual and augmented reality. Typically, systems use controllers or hand gestures to interact with virtual objects. However, these motions are often unnatural and diminish the immersion of the experience. Moreover, these approaches offer limited tactile feedback. There does not currently exist a platform to bring an arbitrary physical object into the virtual world without additional peripherals or the use of expensive motion capture systems. Such a system could be used for immersive experiences within the entertainment industry as well as being applied to VR or AR training experiences, in the fields of health and engineering. We propose an end-to-end pipeline for creating an interactive virtual prop from rigid and non-rigid physical objects. This includes a novel method for tracking the deformations of rigid and non-rigid objects at interactive rates using a single RGBD camera. We scan our physical object and process the point cloud to produce a triangular mesh. A range of possible deformations can be obtained by using a finite element method simulation and these are reduced to a low dimensional basis using principal component analysis. Machine learning approaches, in particular neural networks, have become key tools in computer vision and have been used on a range of tasks. Moreover, there has been an increased trend in training networks on synthetic data. To this end, we use a convolutional neural network, trained on synthetic data, to track the movement and potential deformations of an object in unlabelled RGB images from a single RGBD camera. We demonstrate our results for several objects with different sizes and appearances.},
	Author = {C. {Taylor} and C. {Mullany} and R. {McNicholas} and D. {Cosker}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2019.00-22},
	Issn = {1554-7868},
	Keywords = {augmented reality;cameras;computer animation;computer vision;convolutional neural nets;feature extraction;finite element analysis;gesture recognition;image colour analysis;image motion analysis;image sensors;interactive systems;learning (artificial intelligence);object recognition;principal component analysis;nonrigid physical objects;nonrigid objects;interactive rates;single RGBD camera;finite element method simulation;interactive virtual prop;AR training experiences;immersive experiences;expensive motion capture systems;virtual world;arbitrary physical object;virtual objects;hand gestures;augmented reality;virtual reality;consumer suitable equipment;augmented environments;virtual environments;end-to-end pipeline;VR props;Strain;Tracking;Pipelines;Training;Deformable models;Computational modeling;Solid modeling;Virtual Reality;Real Time Tracking;VR Props},
	Month = {Oct},
	Pages = {83-92},
	Title = {VR Props: An End-to-End Pipeline for Transporting Real Objects Into Virtual and Augmented Environments},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2019.00-22}}

@inproceedings{8943734,
	Abstract = {Many AR and VR task domains involve manipulating virtual objects; for example, to perform 3D geometric transformations. These operations are typically accomplished with tracked hands or hand-held controllers. However, there are some activities in which the user's hands are already busy with another task, requiring the user to temporarily stop what they are doing to perform the second task, while also taking time to disengage and reengage with the original task (e.g., putting down and picking up tools). To avoid the need to overload the user's hands this way in an AR system for guiding a physician performing a surgical procedure, we developed a hands-free approach to performing 3D transformations on patient-specific virtual organ models. Our approach uses small head motions to accomplish first-order and zero-order control, in conjunction with voice commands to establish the type of transformation. To show the effectiveness of this approach for translating, scaling, and rotating 3D virtual models, we conducted a within-subject study comparing the hands-free approach with one based on conventional manual techniques, both running on a Microsoft HoloLens and using the same voice commands to specify transformation type. Independent of any additional time to transition between tasks, users were significantly faster overall using the hands-free approach, significantly faster for hands-free translation and scaling, and faster (although not significantly) for hands-free rotation.},
	Author = {S. {Sadri} and S. A. {Kohen} and C. {Elvezio} and S. H. {Sun} and A. {Grinshpoon} and G. J. {Loeb} and N. {Basu} and S. K. {Feiner}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2019.00-21},
	Issn = {1554-7868},
	Keywords = {augmented reality;medical image processing;solid modelling;surgery;manipulating 3D anatomic models;manual approach;3D geometric transformations;hand-held controllers;patient-specific virtual organ models;3D virtual models;hands-free translation;hands-free rotation;AR;VR;surgical procedure;small head motions;voice commands;Microsoft HoloLens;Augmented Reality;Vascular Interventions;Hands Free Interaction;User Study;Multimodal Interaction;Head gaze Control},
	Month = {Oct},
	Pages = {93-102},
	Title = {Manipulating 3D Anatomic Models in Augmented Reality: Comparing a Hands-Free Approach and a Manual Approach},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2019.00-21}}

@inproceedings{8943683,
	Abstract = {Head-based interactions are very handy for virtual reality (VR) head-worn display (HWD) systems. A useful head-based interaction technique could help users to interact with VR environments in a hands-free manner (i.e., without the need of a hand-held de-vice). Moreover, it can sometimes be seamlessly integrated with other input modalities to provide richer interaction possibilities. This paper explores the potential of a new approach that we call DepthMove to allow interactions that are based on head motions along the depth dimension. With DepthMove, a user can interact with a VR system proactively by moving the head perpendicular to the VR HWD forward or backward. We use two user studies to investigate, model, and optimize DepthMove by taking into con-sideration user performance, subjective response, and social ac-ceptability. The results allow us to determine the optimal and comfortable DepthMove range. We also distill recommendations that can be used to guide the design interfaces that use DepthMove for efficient and accurate interaction in VR HWD systems. A third study is conducted to demonstrate the usefulness of DepthMove relative to other techniques in four application scenarios.},
	Author = {D. {Yu} and H. {Liang} and X. {Lu} and T. {Zhang} and W. {Xu}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2019.00-20},
	Issn = {1554-7868},
	Keywords = {augmented reality;helmet mounted displays;human computer interaction;head motions;depth dimension;virtual reality head-worn displays;virtual reality head-worn display systems;VR environments;hands-free manner;VR system;user performance;optimal DepthMove range;comfortable DepthMove range;VR HWD systems;DepthMove relative;head-based interaction technique;C++ languages;Augmented reality;Virtual reality;target selection;head-based interaction;hands-free interaction;head-worn displays;3D position tracking},
	Month = {Oct},
	Pages = {103-114},
	Title = {DepthMove: Leveraging Head Motions in the Depth Dimension to Interact with Virtual Reality Head-Worn Displays},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2019.00-20}}

@inproceedings{8943756,
	Abstract = {While perceptual biases have been widely investigated in Virtual Reality (VR), very few studies have considered the challenging environment of Optical See-through Augmented Reality (OST-AR). Moreover, regarding distance perception, existing works mainly focus on the assessment of egocentric distance perception, i.e. distance between the observer and a real or a virtual object. In this paper, we study exocentric distance perception in AR, hereby considered as the distance between two objects, none of them being directly linked to the user. We report a user study (n=29) aiming at estimating distances between two objects lying in a frontoparallel plane at 2.1m from the observer (i.e. in the medium-field perceptual space). Four conditions were tested in our study: real objects on the left and on the right of the participant (called real-real), virtual objects on both sides (virtual-virtual), a real object on the left and a virtual one on the right (real-virtual) and finally a virtual object on the left and a real object on the right (virtual-real). Participants had to reproduce the distance between the objects by spreading two real identical objects presented in front of them. The main findings of this study are the overestimation (20%) of exocentric distances for all tested conditions. Surprisingly, the real-real condition was significantly more overestimated (by about 4%, p=.0166) compared to the virtual-virtual condition, i.e. participants obtained better estimates of the exocentric distance for the virtual-virtual condition. Finally, for the virtual-real/real-virtual conditions, the analysis showed a non-symmetrical behavior, which suggests that the relationship between real and virtual objects with respect to the user might be affected by other external factors. Considered together, these unexpected results illustrate the need for additional experiments to better understand the perceptual phenomena involved in exocentric distance perception with real and virtual objects.},
	Author = {E. {Peillard} and F. {Argelaguet} and J. {Normand} and A. {L{\'e}cuyer} and G. {Moreau}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2019.00-13},
	Issn = {1554-7868},
	Keywords = {augmented reality;visual perception;augmented reality;Virtual Reality;egocentric distance perception;virtual object;exocentric distance perception;virtual-virtual condition;optical see-through augmented reality;exocentric distances;Augmented reality;Perception;Distance;Augmented Reality;User Experiment;Psychophysical Study},
	Month = {Oct},
	Pages = {115-122},
	Title = {Studying Exocentric Distance Perception in Optical See-Through Augmented Reality},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2019.00-13}}

@inproceedings{8943738,
	Abstract = {With the increasing use of avatars (i.e. the virtual representation of the user in a virtual environment) in virtual reality, it is important to identify the factors eliciting the sense of embodiment or the factors that can disrupt this feeling. This paper reports an exploratory study aiming at identifying internal factors (personality traits and body awareness) that might cause either a resistance or a predisposition to feel a sense of embodiment towards a virtual avatar. To this purpose, we conducted an experiment (n=123) in which participants were immersed in a virtual environment and embodied in a gender-matched generic virtual avatar through a head-mounted display. After an exposure phase in which they had to perform a number of visuomotor tasks (during 2 minutes) a virtual character entered the virtual scene and stabbed the participants' virtual hand with a knife. The participants' sense of embodiment was measured, as well as several personality traits (Big Five traits and locus of control) and body awareness, to evaluate the influence of participants' personality on the acceptance of the virtual body. The major finding of the experiment is that the locus of control is linked to several components of embodiment: the sense of agency is positively correlated with an internal locus of control and the sense of body ownership is positively correlated with an external locus of control. Interestingly, both components are not influenced by the same traits, which confirms that they can appear independently. Taken together our results suggest that the locus of control could be a good predictor of the sense of embodiment when the user embodies an avatar with a similar physical appearance. Yet, further studies are required to confirm these results.},
	Author = {D. {Dewez} and R. {Fribourg} and F. {Argelaguet} and L. {Hoyet} and D. {Mestre} and M. {Slater} and A. {L{\'e}cuyer}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2019.00-12},
	Issn = {1554-7868},
	Keywords = {avatars;helmet mounted displays;virtual reality;virtual representation;virtual environment;virtual reality;embodiment;internal factors;personality traits;body awareness;gender-matched generic virtual avatar;virtual character;virtual scene;virtual body;body ownership;Avatars;Virtual environments;Task analysis;Visualization;Atmospheric measurements;Particle measurements;Avatar;Virtual Reality;Embodiment;Personality;Body Awareness},
	Month = {Oct},
	Pages = {123-134},
	Title = {Influence of Personality Traits and Body Awareness on the Sense of Embodiment in Virtual Reality},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2019.00-12}}

@inproceedings{8943577,
	Abstract = {One of the main challenges in creating narrative-driven Augmented Reality (AR) content for Head Mounted Displays (HMDs) is to make them equally accessible and enjoyable in different types of indoor environments. However, little has been studied in regards to whether such content can indeed provide similar, if not the same, levels of experience across different spaces. To gain more understanding towards this issue, we examine the effect of room size and furniture on the player experience of Fragments, a space-adaptive, indoor AR crime-solving game created for the Microsoft HoloLens. The study compares factors of player experience in four types of spatial conditions: (1) Large Room - Fully Furnished; (2) Large Room - Scarcely Furnished; (3) Small Room - Fully Furnished; and (4) Small Room - Scarcely Furnished. Our results show that while large spaces facilitate a higher sense of presence and narrative engagement, fully-furnished rooms raise perceived workload. Based on our findings, we propose design suggestions that can support narrative-driven, space-adaptive indoor HMD-based AR content in delivering optimal experiences for various types of rooms.},
	Author = {J. {Shin} and H. {Kim} and C. {Parker} and H. {Kim} and S. {Oh} and W. {Woo}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2019.00-11},
	Issn = {1554-7868},
	Keywords = {augmented reality;computer games;helmet mounted displays;indoor environment;user experience;fully-furnished rooms;space-adaptive indoor HMD-based;room size;furniture;narrative engagement;Head Mounted Displays;indoor environments;player experience;narrative-driven augmented reality content;space-adaptive augmented reality game;Usability;Games;Augmented reality;Indoor environment;Media;Mobile handsets;Tracking;Augmented Reality;Augmented Reality Game;Augmented Narrative},
	Month = {Oct},
	Pages = {135-144},
	Title = {Is Any Room Really OK? The Effect of Room Size and Furniture on Presence, Narrative Engagement, and Usability During a Space-Adaptive Augmented Reality Game},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2019.00-11}}

@inproceedings{8943689,
	Abstract = {Recent AR research efforts have explored the use of virtual environments to test augmented reality (AR) user interfaces. However, it is yet to be seen what effects the visual fidelity of such virtual environments may have on AR interface assessment, and specifically to what degree assessment results observed in a virtual world would apply to the real world. Automotive AR head-up (HUD) interfaces provide a meaningful application area to examine this problem, especially given that immersive, 3D-graphics-based driving simulators are established tools to examine in-vehicle interfaces safely before testing in real vehicles. In this work, we present an argument that adequately assessing AR interfaces requires a suite of different measures, and that such measures should be considered when debating the appropriateness of virtual environments for AR interface assessment. We present a case study that examines how an AR interface presented via HUD effects driver performance and behavior in different virtual and real environments. Twelve participants completed the study measuring driver task performance, eye gaze behavior and situational awareness during AR guided navigation in low-and high-fidelity virtual simulation, and an on-road environment. Our results suggest that the visual fidelity of the environmental in which an AR interface is assessed, could impact some measures of effectiveness. Discussion is guided by a proposed initial assessment classification for AR user interfaces that may serve to guide future discussions on AR interface evaluation, as well as the suitability of virtual environments for AR assessment.},
	Author = {C. {Merenda} and C. {Suga} and J. L. {Gabbard} and T. {Misu}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2019.00-10},
	Issn = {1554-7868},
	Keywords = {augmented reality;data visualisation;head-up displays;user interfaces;virtual reality;augmented reality user interfaces;AR interface assessment;degree assessment results;virtual world;immersive D-graphics-based driving simulators;3D-graphics-based driving simulators;in-vehicle interfaces;virtual environments;high-fidelity virtual simulation;on-road environment;initial assessment classification;AR user interfaces;AR interface evaluation;AR assessment;real-world visual fidelity;AR head-up display graphics;Augmented reality;Augmented Reality;Virtual Reality;Head-up Displays;User Interface Assessment},
	Month = {Oct},
	Pages = {145-156},
	Title = {Effects of "Real-World" Visual Fidelity on AR Interface Assessment: A Case Study Using AR Head-up Display Graphics in Driving},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2019.00-10}}

@inproceedings{8943625,
	Abstract = {Domestic animals have a long history of enriching human lives physically and mentally by filling a variety of different roles, such as service animals, emotional support animals, companions, and pets. Despite this, technological realizations of such animals in augmented reality (AR) are largely underexplored in terms of their behavior and interactions as well as effects they might have on human users' perception or behavior. In this paper, we describe a simulated virtual companion animal, in the form of a dog, in a shared AR space. We investigated its effects on participants' perception and behavior, including locomotion related to proxemics, with respect to their AR dog and other real people in the environment. We conducted a 2 by 2 mixed factorial human-subject study, in which we varied (i) the AR dog's awareness and behavior with respect to other people in the physical environment and (ii) the awareness and behavior of those people with respect to the AR dog. Our results show that having an AR companion dog changes participants' locomotion behavior, proxemics, and social interaction with other people who can or can not see the AR dog. We also show that the AR dog's simulated awareness and behaviors have an impact on participants' perception, including co-presence, animalism, perceived physicality, and dog's perceived awareness of the participant and environment. We discuss our findings and present insights and implications for the realization of effective AR animal companions.},
	Author = {N. {Norouzi} and K. {Kim} and M. {Lee} and R. {Schubert} and A. {Erickson} and J. {Bailenson} and G. {Bruder} and G. {Welch}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2019.000-8},
	Issn = {1554-7868},
	Keywords = {augmented reality;behavioural sciences;computer animation;computer games;groupware;human factors;mixed factorial human-subject study;emotional support animals;service animals;human lives;domestic animals;simulated support animals;virtual dog;effective AR animal companions;perceived physicality;animalism;social interaction;AR companion dog changes participants;physical environment;AR dog;proxemics;shared AR space;simulated virtual companion animal;human users;augmented reality;technological realizations;Dogs;Games;Augmented reality;Legged locomotion;Visualization;Augmented Reality;Augmented Reality Animals;Virtual Animals;Companion Animals;Virtual Reality},
	Month = {Oct},
	Pages = {157-168},
	Title = {Walking Your Virtual Dog: Analysis of Awareness and Proxemics with Simulated Support Animals in Augmented Reality},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2019.000-8}}

@inproceedings{8943774,
	Abstract = {We consider the problem of automatic assessment of visually induced motion sickness in virtual reality applications. In particular, we study the impact on visual discomfort due to camera motion or egomotion present in the video displayed through a head mounted display. We develop a database of 100 short duration videos with different camera trajectories, speeds and shake levels and conduct a large scale subjective study by collecting more than 4000 human ratings of discomfort levels. The videos are generated synthetically by applying different camera trajectories. We then use the subjective study to learn to predict discomfort by designing features describing the camera motion. The features are based on the ground truth camera trajectory and estimate the camera velocity and shake and depth of the visual scene. We show that these features can be effectively used to predict discomfort by obtaining a high correlation with the subjective discomfort scores provided by humans.},
	Author = {S. {Balasubramanian} and R. {Soundararajan}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2019.000-7},
	Issn = {1554-7868},
	Keywords = {cameras;helmet mounted displays;image motion analysis;video signal processing;virtual reality;immersive videos;visually induced motion sickness;virtual reality applications;camera motion;egomotion;ground truth camera trajectory;camera velocity;visual scene;head mounted display;Videos;Cameras;Trajectory;Databases;Visualization;Virtual reality;Resists;Virtual reality, discomfort assessment, egomotion, monocular immersive videos},
	Month = {Oct},
	Pages = {169-177},
	Title = {Prediction of Discomfort due to Egomotion in Immersive Videos for Virtual Reality},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2019.000-7}}

@inproceedings{8943618,
	Abstract = {This work explores the use of foot gestures for locomotion in virtual environments. Foot gestures are represented as the distribution of plantar pressure and detected by three sparsely-located sensors on each insole. The Long Short-Term Memory model is chosen as the classifier to recognize the performer's foot gesture based on the captured signals of pressure information. The trained classifier directly takes the noisy and sparse input of sensor data, and handles seven categories of foot gestures (stand, walk forward/backward, run, jump, slide left and right) without manual definition of signal features for classifying these gestures. This classifier is capable of recognizing the foot gestures, even with the existence of large sensor-specific, inter-person and intra-person variations. Results show that an accuracy of ~80% can be achieved across different users with different shoe sizes and ~85% for users with the same shoe size. A novel method, Dual-Check Till Consensus, is proposed to reduce the latency of gesture recognition from 2 seconds to 0.5 seconds and increase the accuracy to over 97%. This method offers a promising solution to achieve lower latency and higher accuracy at a minor cost of computation workload. The characteristics of high accuracy and fast classification of our method could lead to wider applications of using foot patterns for human-computer interaction.},
	Author = {X. {Shi} and J. {Pan} and Z. {Hu} and J. {Lin} and S. {Guo} and M. {Liao} and Y. {Pan} and L. {Liu}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2019.000-6},
	Issn = {1554-7868},
	Keywords = {feature extraction;gesture recognition;learning (artificial intelligence);pattern classification;virtual reality;gesture recognition;foot gestures;virtual locomotion;plantar pressure;sparsely-located sensors;long short-term memory model;pressure information;human-computer interaction;Foot;Legged locomotion;Footwear;Virtual reality;Pressure sensors;Three-dimensional displays;Human centered computing;Human computer interaction (HCI);Interaction techniques;Gestural input;Interactive systems and tools;User interface programming},
	Month = {Oct},
	Pages = {178-189},
	Title = {Accurate and Fast Classification of Foot Gestures for Virtual Locomotion},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2019.000-6}}

@inproceedings{8943620,
	Abstract = {Head-worn devices with a narrow field of view are common commodity for Augmented Reality. However, their limited screen space makes view management difficult. Especially in dense information spaces this potentially leads to visual conflicts such as overlapping labels (occlusion) and visual clutter. In this paper, we look into the potential of using audio and vibrotactile feedback to guide search and information localization. Our results indicate users can be guided with high accuracy using audio-tactile feedback with maximum median deviations of only 2$\,^{\circ}$ on longitude, 3.6$\,^{\circ}$ on latitude and 0.07 meter in depth. Regarding the encoding of latitude we found a superior performance when using audio, resulting in an improvement of 61% and fastest search times. When interpreting localization cues the maximum median deviation was 9.9$\,^{\circ}$ on longitude and 18% of a selected distance to be encoded which could be reduced to 14% when using audio.},
	Author = {A. {Marquardt} and C. {Trepkowski} and T. D. {Eibich} and J. {Maiero} and E. {Kruijff}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2019.000-3},
	Issn = {1554-7868},
	Keywords = {augmented reality;data visualisation;haptic interfaces;helmet mounted displays;localization cues;fastest search times;maximum median deviation;audio-tactile feedback;information localization;vibrotactile feedback;visual clutter;visual conflicts;dense information spaces;view management difficult;screen space;common commodity;head-worn devices;nonvisual cues;size 0.07 inch;Visualization;Vibrations;Task analysis;Three-dimensional displays;Augmented reality;Navigation;Forehead;augmented reality, audio-tactile feedback, guidance, depth perception},
	Month = {Oct},
	Pages = {190-201},
	Title = {Non-Visual Cues for View Management in Narrow Field of View Augmented Reality Displays},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2019.000-3}}

@inproceedings{8943693,
	Abstract = {Modern augmented reality (AR) head-mounted displays comprise a multitude of sensors that allow them to sense the environment around them. We have extended these capabilities by mounting two heat-wavelength infrared cameras to a Microsoft HoloLens, facilitating the acquisition of thermal data and enabling stereoscopic thermal overlays in the user's augmented view. The ability to visualize live thermal information opens several avenues of investigation on how that thermal awareness may affect a user's thermoception. We present a human-subject study, in which we simulated different temperature shifts using either heat vision overlays or 3D AR virtual effects associated with thermal cause-effect relationships (e.g., flames burn and ice cools). We further investigated differences in estimated temperatures when the stimuli were applied to either the user's body or their environment. Our analysis showed significant effects and first trends for the AR virtual effects and heat vision, respectively, on participants' temperature estimates for their body and the environment though with different strengths and characteristics, which we discuss in this paper.},
	Author = {A. {Erickson} and K. {Kim} and R. {Schubert} and G. {Bruder} and G. {Welch}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2019.000-2},
	Issn = {1554-7868},
	Keywords = {augmented reality;data acquisition;data visualisation;flames;helmet mounted displays;infrared imaging;stereo image processing;thermal information;thermal awareness;temperature shifts;heat vision;3D AR virtual effects;thermal cause-effect relationships;estimated temperatures;augmented reality temperature visualization;computer-mediated;modern augmented reality head-mounted displays;heat-wavelength;Microsoft HoloLens;stereoscopic thermal overlays;user thermoception;Temperature sensors;Visualization;Cameras;Temperature measurement;Image color analysis;Heating systems;Mediation},
	Month = {Oct},
	Pages = {202-211},
	Title = {Is It Cold in Here or Is It Just Me? Analysis of Augmented Reality Temperature Visualization for Computer-Mediated Thermoception},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2019.000-2}}

@inproceedings{8943779,
	Abstract = {We have been studying augmented reality (AR)-based gustatory manipulation interfaces and previously proposed a gustatory manipulation interface using generative adversarial network (GAN)-based real time image-to-image translation. Unlike three-dimensional (3D) food model-based systems that only change the color or texture pattern of a particular type of food in an inflexible manner, our GAN-based system changes the appearance of food into multiple types of food in real time flexibly, dynamically, and interactively. In the present paper, we first describe in detail a user study on a vision-induced gustatory manipulation system using a 3D food model and report its successful experimental results. We then summarize identified problems of the 3D model-based system and describe implementation details of the GAN-based system. We finally report in detail the main user study in which we investigated the impact of the GAN-based system on gustatory sensations and food recognition when somen noodles were turned into ramen noodles or fried noodles, and steamed rice into curry and rice or fried rice. The experimental results revealed that our system successfully manipulates gustatory sensations to some extent and that the effectiveness seems to depend on the original and target types of food as well as the experience of each individual with the food.},
	Author = {K. {Nakano} and D. {Horita} and N. {Sakata} and K. {Kiyokawa} and K. {Yanai} and T. {Narumi}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2019.000-1},
	Issn = {1554-7868},
	Keywords = {augmented reality;chemioception;computer vision;food products;image texture;mobile computing;3D model-based system;vision-induced gustatory manipulation system;three-dimensional food model-based systems;image-to-image translation;generative adversarial network-based;gustatory manipulation interface;augmented reality-based gustatory manipulation interfaces;GAN-based real-time food-to-food;augmented reality gustatory manipulation;food recognition;gustatory sensations;GAN-based system;Three-dimensional displays;Solid modeling;Real-time systems;Visualization;Resists;Image color analysis;Modulation;Human centered computing Human computer interaction (HCI) Interaction paradigms Mixed / augmented reality Computing methodologies Computer graphics Graphics systems and interfaces Perception Computing methodologies Machine learning Machine learning approaches Neural networks},
	Month = {Oct},
	Pages = {212-223},
	Title = {DeepTaste: Augmented Reality Gustatory Manipulation with GAN-Based Real-Time Food-to-Food Translation},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2019.000-1}}

@inproceedings{8943608,
	Abstract = {In a mixed reality (MR) environment that combines the physical objects with the virtual environments, users' feelings are immersed in the virtual world, while their bodies remain in the physical world. Compared to the purely physical environments, such characteristic has led to some special needs for users' long-term immersion. However, the deficiency needs that we have to face for long-term immersion still need further research. In this paper, we apply the theory of Maslow's Hierarchy of Needs (MHN) to guide the design of MR systems for long-term immersion. Taking the normal biological rhythm of human beings as the basic unit (24 hours), we propose the fundamental needs for long-term immersion in VEs through combining the theory of MHN with the special needs of virtual reality (VR). In order to verify whether those needs can satisfy users' long-term immersion, we design an MR office system for basic operations based on the theory of MHN. A long-term exposure experiment (duration of 8 hours) is designed to evaluate those needs by comparing the results with a physical work environment after a short-term preliminary study. The physiological and psychological effects are tested in both two environments and the deficiency needs for short-term immersion and long-term immersion are also compared. The results showed that the design based on the theory of MHN can support users' long-term immersion, which means that it can be a guideline for long-term use of MR systems.},
	Author = {J. {Guo} and D. {Weng} and Z. {Zhang} and H. {Jiang} and Y. {Liu} and Y. {Wang} and H. B. {Duh}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2019.00019},
	Issn = {1554-7868},
	Keywords = {augmented reality;psychology;user experience;long-term immersion;virtual environments;mixed reality office system;Maslow's hierarchy of needs;user feelings;VE;MR office systems;long-term exposure experiment;physiological effects;psychological effects;Virtual reality;Psychology;Physiology;Health and safety;Visualization;Systematics;Human-centered-computing;Human-computer-interaction-(HCI);Interaction-paradigms;Mixed-/-augmented-reality;HCI-design-and-evaluation-methods;User-studies},
	Month = {Oct},
	Pages = {224-235},
	Title = {Mixed Reality Office System Based on Maslow's Hierarchy of Needs: Towards the Long-Term Immersion in Virtual Environments},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2019.00019}}

@inproceedings{8943760,
	Abstract = {Remote collaboration systems allow users at different sites to perform joint tasks, which are required by many real-life applications. For example, environmental pollution is a complex problem requiring many kinds of expertise to fully understand, as pollutants disperse not only locally but also regionally or even globally. This paper presents a remote collaborative visualization system through providing co-presence, information sharing, and collaborative analysis functions based on mixed reality techniques. We start with developing an immersive visualization approach for analyzing multi-attribute and geo-spatial data with intuitive multi-model interactions, simulating co-located collaboration effects. We then go beyond by designing a set of information sharing and collaborative analysis functions to support different users to share and analyze their sensemaking processes collaboratively. We provide example results and usage scenario to demonstrate that our system enables users to perform a variety of immersive and collaborative analytics tasks effectively. Through two small user studies focusing on evaluating our design of information sharing and system usability, the evaluation results confirm the effectiveness of comprehensive sharing among user, data, physical, and interaction spaces for improving remote collaborative analysis experience.},
	Author = {T. {Mahmood} and W. {Fulmer} and N. {Mungoli} and J. {Huang} and A. {Lu}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2019.00021},
	Issn = {1554-7868},
	Keywords = {data visualisation;geographic information systems;groupware;virtual reality;collaborative analysis functions;mixed reality techniques;immersive visualization approach;multiattribute;geo-spatial data;intuitive multimodel interactions;immersive analytics tasks;collaborative analytics tasks;system usability;comprehensive sharing;remote collaborative analysis experience;information sharing;remote GeoSpatial visualization;remote collaboration systems;joint tasks;real-life applications;environmental pollution;complex problem;pollutants;remote collaborative visualization system;colocated collaboration effects;Collaboration;Data visualization;Task analysis;Visualization;Information management;Three-dimensional displays;Avatars;Remote collaboration;immersive analytics;geospatial visualization;mixed reality},
	Month = {Oct},
	Pages = {236-247},
	Title = {Improving Information Sharing and Collaborative Analysis for Remote GeoSpatial Visualization Using Mixed Reality},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2019.00021}}

@inproceedings{8943737,
	Abstract = {We have explored the effects of sharing manipulated heart rate feedback in collaborative virtual environments. In our study, we created two types of different virtual environments (active and passive) with different levels of interactions and provided three levels of manipulated heart rate feedback (decreased, unchanged, and increased). We measured the effects of manipulated feedback on Social Presence, affect, physical heart rate, and overall experience. We noticed a significant effect of the manipulated heart rate feedback in affecting scariness and nervousness. The perception of the collaborator's valance and arousal was also affected where increased heart rate feedback perceived as a higher valance and lower arousal. Increased heart rate feedback decreased the real heart rate. The type of virtual environments had a significant effect on social presence, heart rate, and affect where the active environment had better performances across these measurements. We discuss the implications of this and directions for future research.},
	Author = {A. {Dey} and H. {Chen} and A. {Hayati} and M. {Billinghurst} and R. W. {Lindeman}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2019.00022},
	Issn = {1554-7868},
	Keywords = {cardiology;feedback;groupware;medical information systems;virtual reality;manipulated heart rate feedback sharing;manipulated feedback;different virtual environments;collaborative virtual environments;increased heart rate feedback;physical heart rate;Collaboration;Real-time systems;Physiology;Heart beat;Games;Skin;Virtual Reality;Physiological Feedback;Collaboration;Heartrate;Empathic Cues},
	Month = {Oct},
	Pages = {248-257},
	Title = {Sharing Manipulated Heart Rate Feedback in Collaborative Virtual Environments},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2019.00022}}

@inproceedings{8943686,
	Abstract = {While virtual reality (VR) tools provide an immersive learning experience for students, it is difficult for an instructor to observe the students' learning activities in a virtual environment (VE). Thus, it hinders interactions that could occur between the instructor and students, which are usually required in a classroom environment to understand how each student learns. Previous work has added virtual awareness cues that can help a small group of students to collaborate in a VE. However, when the number of students increases, such virtual awareness cues can cause visual clutter and confuse the instructor. We propose ObserVAR, a visualization system that allows the instructor to observe students in a VE at scale. ObserVAR uses augmented reality techniques to visualize each student's gaze in a VE and improves the instructor's awareness of the entire class. The visualizations are then optimized to reduce visual clutter in the scene using a force-directed graph drawing algorithm. In designing ObserVAR, we first investigated visualizations that can provide the instructor with an overall awareness of the VE that can be scaled up as the number of users increases. Second, we optimized the visualization of students by leveraging a graph drawing algorithm to reduce the visual clutter in the class scene. We compared the performance of our prototype with some commercially available user interfaces for VE classrooms. In our study, ObserVAR has demonstrated improvement and flexibility in several application scenarios.},
	Author = {S. {Thanyadit} and P. {Punpongsanon} and T. {Pong}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2019.00023},
	Issn = {1554-7868},
	Keywords = {augmented reality;computer aided instruction;data visualisation;directed graphs;user interfaces;ObserVAR;visualization system;virtual reality users;virtual reality tools;immersive learning experience;virtual environment;classroom environment;virtual awareness cues;visual clutter;augmented reality techniques;student;instructor;force-directed graph drawing algorithm;VE classrooms;Visualization;Avatars;Task analysis;Two dimensional displays;Collaboration;Augmented reality;Monitoring;Visualization;Remote Collaboration;VR for Education;Asymmetric Interaction},
	Month = {Oct},
	Pages = {258-268},
	Title = {ObserVAR: Visualization System for Observing Virtual Reality Users using Augmented Reality},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2019.00023}}

@inproceedings{8943602,
	Abstract = {We examine users' preferences for Augmented Reality for television and report findings from an exploratory study with 172 participants we conducted to understand the perceived value of twenty distinct AR-TV scenarios. We connect our findings to participants' overall perceptions of and experience with AR technology as well as to their self-reported television watching behavior. Our results reveal high perceived value for AR-TV scenarios involving interactive content, wall-sized and room-sized video projections, multiple perspectives of the same movie scene, and for virtual objects coming out of the TV screen into the room. Other scenarios, such as live video of remote friends watching the same broadcast or multiple virtual channels displayed around the TV screen were rated less valuable.},
	Author = {I. {Popovici} and R. {Vatavu}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2019.00024},
	Issn = {1554-7868},
	Keywords = {human computer interaction;interactive television;television broadcasting;user interfaces;video signal processing;virtual reality;users preferences;high perceived value;interactive content;room-sized video projections;virtual objects;multiple virtual channels;augmented reality television;AR-TV scenarios;AR technology;self-reported television watching behavior;Augmented reality;Smart TV;Real-time systems;Entertainment industry;Prototypes;Motion pictures;augmented reality;television;study;iTV;questionnaire},
	Month = {Oct},
	Pages = {269-278},
	Title = {Understanding Users' Preferences for Augmented Reality Television},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2019.00024}}

@inproceedings{8943748,
	Abstract = {Augmented reality (AR) is on the rise with consumer-level head-mounted displays (HMDs) becoming available in recent years. Text entry is an essential activity for AR systems, but it is still relatively underexplored. Although it is possible to use a physical keyboard to enter text in AR systems, it is not the most optimal and ideal way because it confines the uses to a stationary position and within indoor environments. Instead, a virtual keyboard seems more suitable. Text entry via virtual keyboards requires a pointing method and a selection mechanism. Although there exist various combinations of pointing+selection mechanisms, it is not well understood how well suited each combination is to support fast text entry speed with low error rates and positive usability (regarding workload, user experience, motion sickness, and immersion). In this research, we perform an empirical study to investigate user preference and text entry performance of four pointing methods (Controller, Head, Hand, and Hybrid) in combination with two input mechanisms (Swype and Tap). Our research represents a first systematic investigation of these eight possible combinations. Our results show that Controller outperforms all the other device-free methods in both text entry performance and user experience. However, device-free pointing methods can be usable depending on task requirements and users' preferences and physical condition.},
	Author = {W. {Xu} and H. {Liang} and A. {He} and Z. {Wang}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2019.00026},
	Issn = {1554-7868},
	Keywords = {augmented reality;helmet mounted displays;human computer interaction;keyboards;mobile computing;text analysis;user interfaces;augmented reality head mounted displays;consumer-level head-mounted displays;AR systems;virtual keyboard;pointing method;selection mechanism;pointing+selection mechanisms;fast text entry speed;text entry performance;device-free pointing methods;user preference;Keyboards;Resists;Performance evaluation;Augmented reality;User experience;Task analysis;Handheld computers;Augmented Reality;Text Entry;User Performance;User Preference;Pointing Methods;Selection Mechanisms},
	Month = {Oct},
	Pages = {279-288},
	Title = {Pointing and Selection Methods for Text Entry in Augmented Reality Head Mounted Displays},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2019.00026}}

@inproceedings{8943750,
	Abstract = {Virtual and Augmented Reality deliver engaging interaction experiences that can transport and extend the capabilities of the user. To ensure these paradigms are more broadly usable and effective, however, it is necessary to also deliver many of the conventional functions of a smartphone or personal computer. It remains unclear how conventional input tasks, such as text entry, can best be translated into virtual and augmented reality. In this paper we examine the performance potential of four alternative text entry strategies in virtual reality (VR). These four strategies are selected to provide full coverage of two fundamental design dimensions: i) physical surface association; and ii) number of engaged fingers. Specifically, we examine typing with index fingers on a surface and in mid-air and typing using all ten fingers on a surface and in mid-air. The central objective is to evaluate the human performance potential of these four typing strategies without being constrained by current tracking and statistical text decoding limitations. To this end we introduce an auto-correction simulator that uses knowledge of the stimulus to emulate statistical text decoding within constrained experimental parameters and use high-precision motion tracking hardware to visualise and detect fingertip interactions. We find that alignment of the virtual keyboard with a physical surface delivers significantly faster entry rates over a mid-air keyboard. Also, users overwhelmingly fail to effectively engage all ten fingers in mid-air typing, resulting in slower entry rates and higher error rates compared to just using two index fingers. In addition to identifying the envelopes of human performance for the four strategies investigated, we also provide a detailed analysis of the underlying features that distinguish each strategy in terms of its performance and behaviour.},
	Author = {J. {Dudley} and H. {Benko} and D. {Wigdor} and P. O. {Kristensson}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2019.00027},
	Issn = {1554-7868},
	Keywords = {augmented reality;human computer interaction;keyboards;statistical analysis;text analysis;user experience;user interfaces;performance envelopes;virtual keyboard text input strategies;virtual reality;interaction experiences;smartphone;augmented reality;engaged fingers;index fingers;human performance potential;statistical text decoding limitations;high-precision motion tracking hardware;physical surface;faster entry rates;mid-air keyboard;mid-air typing;entry rates;text entry strategies;Keyboards;Tracking;Layout;Decoding;Augmented reality;Performance evaluation;Indexes;virtual reality;text entry;head mounted display},
	Month = {Oct},
	Pages = {289-300},
	Title = {Performance Envelopes of Virtual Keyboard Text Input Strategies in Virtual Reality},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2019.00027}}

@inproceedings{8943627,
	Abstract = {Specifying points in three-dimensional (3D) space is an essential function in many augmented reality (AR) applications. When an environment model is not available, a straightforward solution is to perform geometric triangulation using two rays. However, na{\"\i}ve implementations suffer from low accuracy caused by technical limitations of AR devices and human motor constraints. To overcome these issues, we designed and evaluated two enhanced geometric techniques for 3D point marking. VectorCloud uses multiple rays to reduce the effects of pointing jitter, and ImageRefinement improves the accuracy by allowing users to refine the 3D direction of the two rays. We conducted studies to understand the characteristics of these techniques in both ecologically valid outdoor settings using a mobile AR display and in more controlled setting using virtual reality simulation. Our experiments demonstrate that both techniques improve the precision of 3D point marking, and that ImageRefinement is superior to VectorCloud overall. These results are particularly relevant in the design of mobile AR systems intended for use in large outdoor areas.},
	Author = {W. {Lages} and Y. {Li} and L. {Lisle} and T. {H{\"o}llerer} and D. {Bowman}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2019.00028},
	Issn = {1554-7868},
	Keywords = {augmented reality;computational geometry;image motion analysis;jitter;multiple rays;ImageRefinement;virtual reality simulation;3D point marking;VectorCloud;model-free augmented reality;three-dimensional space;environment model;geometric triangulation;na{\"\i}ve implementations;human motor constraints;AR devices;pointing jitter;mobile AR display;Three-dimensional displays;Solid modeling;Biological system modeling;Augmented reality;Head;Estimation;Computational modeling;Point marking;Augmented Reality;user interface design},
	Month = {Oct},
	Pages = {301-309},
	Title = {Enhanced Geometric Techniques for Point Marking in Model-Free Augmented Reality},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2019.00028}}

@inproceedings{8943611,
	Abstract = {Some of the most widely used selection techniques for extended reality (XR) are based on virtual hand interactions. Many existing XR frameworks provide this functionality by default; however, their implementation can differ in slight, but important ways. When preparing to make a selection with a virtual hand technique, a user's desired selection can potentially be ambiguous due to multiple intersections. Systems with varying underlying virtual hand implementations may yield contrasting selections due to resolving multiple intersections differently. This is particularly an issue when objects are smaller in size than the virtual hand representation and in dense environments. To demonstrate the importance of these differences, we present a virtual hand selection study comparing three methods that are currently used in popular XR frameworks for disambiguating selections: Closest Intersected, First Intersected, and Last Intersected. The results of our study show that the Closest Intersected method affords significantly faster selections, significantly fewer incorrect and missed selections, and yields significantly better effective throughput than the other two methods. These results show that using a framework's built-in selection technique can significantly affect an XR application's usability.},
	Author = {A. {Moore} and M. {Kodeih} and A. {Singhania} and A. {Wu} and T. {Bashir} and R. {McMahan}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2019.00029},
	Issn = {1554-7868},
	Keywords = {augmented reality;haptic interfaces;intersection disambiguation;virtual hand technique;virtual hand interactions;existing XR frameworks;multiple intersections;varying underlying virtual hand implementations;virtual hand representation;virtual hand selection study;popular XR frameworks;disambiguating selections;fewer incorrect missed selections;selection technique;closest intersected method affords;first intersected;last intersected;Task analysis;Three-dimensional displays;Grasping;X reality;Visualization;Taxonomy;Manuals;Intersection disambiguation;virtual hand;selection},
	Month = {Oct},
	Pages = {310-317},
	Title = {The Importance of Intersection Disambiguation for Virtual Hand Techniques},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2019.00029}}

@inproceedings{8943763,
	Abstract = {The following topics are dealt with: augmented reality; virtual reality; helmet mounted displays; cameras; user interfaces; human computer interaction; image reconstruction; image motion analysis; computer vision; data visualisation.},
	Author = {H. {Lee} and H. {Kim} and D. V. {Monteiro} and Y. {Goh} and D. {Han} and H. -N. {Liang} and H. S. {Yang} and J. {Jung}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2019.00030},
	Issn = {1554-7868},
	Keywords = {augmented reality;cameras;helmet mounted displays;user interfaces;virtual reality;augmented reality;virtual reality;helmet mounted displays;cameras;user interfaces;human computer interaction;image reconstruction;image motion analysis;computer vision;data visualisation;Task analysis;Visualization;Three-dimensional displays;Training;Annotations;Virtual reality;Cranes;Virtual reality;Evaluation;Visual guidance;Computer aided instruction;Human-computer interaction},
	Month = {Oct},
	Pages = {318-327},
	Title = {Annotation vs. Virtual Tutor: Comparative Analysis on the Effectiveness of Visual Instructions in Immersive Virtual Reality},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2019.00030}}

@inproceedings{8943767,
	Abstract = {Purpose: It is hypothesized that cyclical stereoscopy (displaying stereoscopy or 2D cyclically) has effect over visual fatigue, learning curves and quality of experience, and that those effects are different from regular stereoscopy. Materials and Methods: 59 participants played a serious game simulating a job interview with a Samsung Gear VR Head Mounted Display (HMD). Participants were randomly assigned to 3 groups: HMD with regular stereoscopy (S3D) and HMD with cyclical stereoscopy (cycles of 1 or 3 minutes). Participants played the game thrice (third try on a PC one month later). Visual discomfort, Flow, Presence, were measured with questionnaires. Visual Fatigue was assessed pre-and post-exposure with optometric measures. Learning traces were obtained in-game. Results: Visual discomfort and flow are lower with cyclical-S3D than S3D but not Presence. Cyclical stereoscopy every 1 minute is more tiring than stereoscopy. Cyclical stereoscopy every 3 minutes tends to be more tiring than stereoscopy. Cyclical stereoscopy groups improved during Short-Term Learning. None of the statistical tests showed a difference between groups in either Short-Term Learning or Long-Term Learning curves. Conclusion: cyclical stereoscopy displayed cyclically had a positive impact on Visual Comfort and Flow, but not Presence. It affects oculomotor functions in a HMD while learning with a serious game with low disparities and easy visual tasks. Other visual tasks should be tested, and eye-tracking should be considered to assess visual fatigue during exposure. Results in ecological conditions seem to support models suggesting that activating cyclically stereopsis in a HMD is more tiring than maintaining it.},
	Author = {A. {Souchet} and S. {Philippe} and F. {Ober} and A. {L{\'e}v{\^e}que} and L. {Leroy}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2019.00031},
	Issn = {1554-7868},
	Keywords = {eye;helmet mounted displays;human computer interaction;stereo image processing;three-dimensional displays;virtual reality;visual perception;serious game;virtual reality while learning;short-term learning;cyclical stereoscopy groups;S3D;regular stereoscopy;displaying stereoscopy;visual discomfort;cyclical stereoscopy effects;HMD;visual fatigue;time 3.0 min;time 1.0 min;Visualization;Fatigue;Task analysis;Resists;Games;Interviews;Imaging;virtual reality;serious game;visual fatigue;head mounted display;cyclical stereoscopy},
	Month = {Oct},
	Pages = {328-338},
	Title = {Investigating Cyclical Stereoscopy Effects Over Visual Discomfort and Fatigue in Virtual Reality While Learning},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2019.00031}}

@inproceedings{8943730,
	Abstract = {This work presents a study that explores the differences between authoring Scenario-Based Training (SBT) simulation content using Augmented Reality (AR) and a Desktop interface. Through an iterative design process two interface conditions were developed and then evaluated qualitatively and quantitatively. Our conceptual model is a graph based visualization that is presented to help designers understand the scenario learning artifacts and relationships. Our major contribution relies on the comparison made between the two authoring tools (AR, Desktop) with the same capabilities. Results show that no significant difference was found in time taken to complete tasks nor on the perceived usability of the systems. However, as expected the Desktop interface was perceived as more efficient. Based on these findings, insights on future directions for building AR immersive authoring tools are provided.},
	Author = {A. {Vargas Gonz{\'a}lez} and S. {Koh} and K. {Kapalo} and R. {Sottilare} and P. {Garrity} and M. {Billinghurst} and J. {LaViola}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2019.00032},
	Issn = {1554-7868},
	Keywords = {augmented reality;authoring systems;computer based training;augmented reality scenario;Desktop interface;iterative design process;conceptual model;scenario learning artifacts;AR immersive authoring tools;authoring scenario-based training simulation content;training authoring;interface conditions;Tools;Training;Visualization;Authoring systems;Augmented reality;Task analysis;Three-dimensional displays;Augmented Reality;Desktop;Scenario Based Training;Web;Usability Study;Programming/Development Support},
	Month = {Oct},
	Pages = {339-350},
	Title = {A Comparison of Desktop and Augmented Reality Scenario Based Training Authoring Tools},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2019.00032}}

@inproceedings{8943688,
	Abstract = {Recent improvements of Virtual Reality (VR) technology have enabled researchers to investigate the benefits VR may provide for various domains such as health, entertainment, training, and education. A significant proportion of VR system evaluations rely on perception-based measures such as user pre-and post-questionnaires and interviews. While these self-reports provide valuable insights into users' perceptions of VR environments, recent developments in digital sensors and data collection techniques afford researchers access to measures of physiological response. This work explores the merits of physiological measures in the evaluation of emotional responses in virtual environments (ERVE). We include and place at the center of our ERVE methodology emotional response data by way of electrodermal activity and heart-rate detection which are analyzed in conjunction with event-driven data to derive further measures. In this paper, we present our ERVE methodology together with a case study within the context of VR-based learning in which we derive measures of cognitive load and moments of insight. We discuss our methodology, and its potential for use in many other application and research domains to provide more in-depth and objective analyses of experiences within VR.},
	Author = {J. {Collins} and H. {Regenbrecht} and T. {Langlotz} and Y. {Said Can} and C. {Ersoy} and R. {Butson}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2019.00033},
	Issn = {1554-7868},
	Keywords = {cognition;computer aided instruction;virtual reality;VR-based;event-driven data;heart-rate detection;ERVE methodology emotional response data;virtual environments;emotional responses;physiological measures;physiological response;data collection techniques;digital sensors;VR environments;post-questionnaires;perception-based measures;VR system evaluations;virtual reality technology;cognitive load;Time measurement;Physiology;Emotional responses;Task analysis;Usability;Biomedical monitoring;Virtual environments;Virtual Reality;Interactive Learning Environments;Methodology},
	Month = {Oct},
	Pages = {351-362},
	Title = {Measuring Cognitive Load and Insight: A Methodology Exemplified in a Virtual Reality Learning Context},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2019.00033}}

@inproceedings{8943733,
	Abstract = {Virtual Reality (VR) has been gaining importance due to its numerous advantages as an immersive technology for learning applications. Next to being used in fields like manufacturing, transportation, communication, retail and real estate, it has also increased in significance for human resources development. Virtual human training programs offer exposure to difficult situations without supervision in a safe and controlled environment, as they simulate nuanced interpersonal situations and therefore allow users to gain new skills and apply them to real-life situations. Furthermore, VR training engages employees in a training session by being presented with a realistic situation designed to challenge and engage them. Practising in a controlled virtual environment also allows for easy and objective measurements of user development and the identification of strengths and weaknesses. In this paper, we evaluate if there is acceptance for practising a presentation in a VR-Speech Training (VR-ST) session using six Degrees of Freedom (6DoF) and if the 44 participants of this study improved, from a subjective point of view, valuable soft skills needed to give a convincing presentation in real life. We observed a positive tendency in the acceptance and effectiveness of the VR-ST.},
	Author = {F. {Palmas} and J. {Cichor} and D. A. {Plecher} and G. {Klinker}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2019.00034},
	Issn = {1554-7868},
	Keywords = {computer based training;virtual reality;immersive technology;learning applications;real estate;human resources development;virtual human training programs;safe environment;controlled environment;nuanced interpersonal situations;real-life situations;VR training engages employees;controlled virtual environment;user development;VR-Speech Training session;VR-ST;valuable soft skills;virtual reality public speaking training;Training;Public speaking;Games;Engines;Virtual environments;Atmospheric measurements;Virtual Reality;Virtual Training;Leadership;Social Presence;Uncanny Valley;Soft Skills;Experimental Study;Public Speaking},
	Month = {Oct},
	Pages = {363-371},
	Title = {Acceptance and Effectiveness of a Virtual Reality Public Speaking Training},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2019.00034}}
