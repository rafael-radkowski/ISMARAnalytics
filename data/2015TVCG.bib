@ARTICLE{7164337,  author={J. {Orlosky} and T. {Toyama} and K. {Kiyokawa} and D. {Sonntag}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={ModulAR: Eye-Controlled Vision Augmentations for Head Mounted Displays},   year={2015},  volume={21},  number={11},  pages={1259-1268},  abstract={In the last few years, the advancement of head mounted display technology and optics has opened up many new possibilities for the field of Augmented Reality. However, many commercial and prototype systems often have a single display modality, fixed field of view, or inflexible form factor. In this paper, we introduce Modular Augmented Reality (ModulAR), a hardware and software framework designed to improve flexibility and hands-free control of video see-through augmented reality displays and augmentative functionality. To accomplish this goal, we introduce the use of integrated eye tracking for on-demand control of vision augmentations such as optical zoom or field of view expansion. Physical modification of the device's configuration can be accomplished on the fly using interchangeable camera-lens modules that provide different types of vision enhancements. We implement and test functionality for several primary configurations using telescopic and fisheye camera-lens systems, though many other customizations are possible. We also implement a number of eye-based interactions in order to engage and control the vision augmentations in real time, and explore different methods for merging streams of augmented vision into the user's normal field of view. In a series of experiments, we conduct an in depth analysis of visual acuity and head and eye movement during search and recognition tasks. Results show that methods with larger field of view that utilize binary on/off and gradual zoom mechanisms outperform snapshot and sub-windowed methods and that type of eye engagement has little effect on performance.},  keywords={augmented reality;computer vision;helmet mounted displays;ModulAR framework;eye-controlled vision augmentation;head mounted display technology;modular augmented reality;video see-through augmented reality displays;augmentative functionality;camera-lens module;telescopic system;fisheye camera-lens system;eye-based interaction;gradual zoom mechanism;binary on-off mechanism;snapshot method;sub-windowed method;Cameras;Lenses;Standards;Software;Visualization;Hardware;Augmented reality;modular display;eye tracking;Augmented reality;modular display;eye tracking;augmented vision;visualization;hands-free interaction;Adolescent;Adult;Computer Graphics;Equipment Design;Eye Movements;Female;Head;Humans;Image Processing, Computer-Assisted;Male;User-Computer Interface;Young Adult},  doi={10.1109/TVCG.2015.2459852},  ISSN={1941-0506},  month={Nov},}



@ARTICLE{7165643,  author={Y. {Itoh} and M. {Dzitsiuk} and T. {Amano} and G. {Klinker}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Semi-Parametric Color Reproduction Method for Optical See-Through Head-Mounted Displays},   year={2015},  volume={21},  number={11},  pages={1269-1278},  abstract={The fundamental issues in Augmented Reality (AR) are on how to naturally mediate the reality with virtual content as seen by users. In AR applications with Optical See-Through Head-Mounted Displays (OST-HMD), the issues often raise the problem of rendering color on the OST-HMD consistently to input colors. However, due to various display constraints and eye properties, it is still a challenging task to indistinguishably reproduce the colors on OST-HMDs. An approach to solve this problem is to pre-process the input color so that a user perceives the output color on the display to be the same as the input. We propose a color calibration method for OST-HMDs. We start from modeling the physical optics in the rendering and perception process between the HMD and the eye. We treat the color distortion as a semi-parametric model which separates the non-linear color distortion and the linear color shift. We demonstrate that calibrated images regain their original appearance on two OST-HMD setups with both synthetic and real datasets. Furthermore, we analyze the limitations of the proposed method and remaining problems of the color reproduction in OST-HMDs. We then discuss how to realize more practical color reproduction methods for future HMD-eye system.},  keywords={augmented reality;calibration;colour displays;helmet mounted displays;optical distortion;physical optics;semiparametric color reproduction method;optical see-through head-mounted display;augmented reality;AR;OST-HMD-eye system;color rendering;color calibration method;physical optics;nonlinear color distortion;Image color analysis;Cameras;Calibration;Rendering (computer graphics);Optical distortion;Nonlinear distortion;OST-HMD;color replication;color calibration;optical see-through display;OST-HMD;color replication;color calibration;optical see-through display;Algorithms;Color;Computer Graphics;Equipment Design;Head;Humans;Image Processing, Computer-Assisted;User-Computer Interface;Video Recording},  doi={10.1109/TVCG.2015.2459892},  ISSN={1941-0506},  month={Nov},}


@ARTICLE{7165661,  author={T. {Nguyen} and G. {Reitmayr} and D. {Schmalstieg}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Structural Modeling from Depth Images},   year={2015},  volume={21},  number={11},  pages={1230-1240},  abstract={In this work, we present a new automatic system for scene reconstruction of high-level structural models. We start with identifying planar regions in depth images obtained with a SLAM system. Our main contribution is an approach which identifies constraints such as incidence and orthogonality of planar surfaces and uses them in an incremental optimization framework to extract high-level structural models. The result is a manifold mesh with a low number of polygons, immediately useful in many Augmented Reality applications such as inspection, interior design or spatial interaction.},  keywords={augmented reality;computational geometry;image reconstruction;mesh generation;SLAM (robots);solid modelling;structural modeling;depth image;automatic system;scene reconstruction;high-level structural model;planar region;SLAM system;planar surface;incremental optimization framework;manifold mesh;polygon;augmented reality application;Three-dimensional displays;Image reconstruction;Computational modeling;Simultaneous localization and mapping;Feature extraction;Solid modeling;Cameras;Structural modeling;geometric scene understanding;topology construction;Structural modeling;geometric scene understanding;topology construction},  doi={10.1109/TVCG.2015.2459831},  ISSN={1941-0506},  month={Nov},}


@ARTICLE{7165673,  author={O. {Kähler} and V. {Adrian Prisacariu} and C. {Yuheng Ren} and X. {Sun} and P. {Torr} and D. {Murray}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Very High Frame Rate Volumetric Integration of Depth Images on Mobile Devices},   year={2015},  volume={21},  number={11},  pages={1241-1250},  abstract={Volumetric methods provide efficient, flexible and simple ways of integrating multiple depth images into a full 3D model. They provide dense and photorealistic 3D reconstructions, and parallelised implementations on GPUs achieve real-time performance on modern graphics hardware. To run such methods on mobile devices, providing users with freedom of movement and instantaneous reconstruction feedback, remains challenging however. In this paper we present a range of modifications to existing volumetric integration methods based on voxel block hashing, considerably improving their performance and making them applicable to tablet computer applications. We present (i) optimisations for the basic data structure, and its allocation and integration; (ii) a highly optimised raycasting pipeline; and (iii) extensions to the camera tracker to incorporate IMU data. In total, our system thus achieves frame rates up 47 Hz on a Nvidia Shield Tablet and 910 Hz on a Nvidia GTX Titan XGPU, or even beyond 1.1 kHz without visualisation.},  keywords={image reconstruction;very high frame rate volumetric integration;depth images;mobile devices;image reconstruction;volumetric integration methods;voxel block hashing;Cameras;Three-dimensional displays;Arrays;Resource management;Rendering (computer graphics);Solid modeling;Interpolation;3D modelling;volumetric;real-time;mobile devices;camera tracking;Kinect;3D modelling;volumetric;real-time;mobile devices;camera tracking;Kinect},  doi={10.1109/TVCG.2015.2459891},  ISSN={1941-0506},  month={Nov},}


@ARTICLE{7165662,  author={P. {Ondrúška} and P. {Kohli} and S. {Izadi}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={MobileFusion: Real-Time Volumetric Surface Reconstruction and Dense Tracking on Mobile Phones},   year={2015},  volume={21},  number={11},  pages={1251-1258},  abstract={We present the first pipeline for real-time volumetric surface reconstruction and dense 6DoF camera tracking running purely on standard, off-the-shelf mobile phones. Using only the embedded RGB camera, our system allows users to scan objects of varying shape, size, and appearance in seconds, with real-time feedback during the capture process. Unlike existing state of the art methods, which produce only point-based 3D models on the phone, or require cloud-based processing, our hybrid GPU/CPU pipeline is unique in that it creates a connected 3D surface model directly on the device at 25Hz. In each frame, we perform dense 6DoF tracking, which continuously registers the RGB input to the incrementally built 3D model, minimizing a noise aware photoconsistency error metric. This is followed by efficient key-frame selection, and dense per-frame stereo matching. These depth maps are fused volumetrically using a method akin to KinectFusion, producing compelling surface models. For each frame, the implicit surface is extracted for live user feedback and pose estimation. We demonstrate scans of a variety of objects, and compare to a Kinect-based baseline, showing on average ~ 1.5cm error. We qualitatively compare to a state of the art point-based mobile phone method, demonstrating an order of magnitude faster scanning times, and fully connected surface models.},  keywords={graphics processing units;image capture;image colour analysis;image fusion;image reconstruction;image registration;mobile computing;object tracking;pose estimation;solid modelling;stereo image processing;MobileFusion;real-time volumetric surface reconstruction;dense tracking;mobile phones;dense 6DoF camera tracking;embedded RGB camera;object scanning;object shape;object size;object appearance;real-time feedback;image capture;point-based 3D model;cloud-based processing;hybrid GPU/CPU pipeline;connected 3D surface model;RGB input registration;noise aware photoconsistency error metric;key-frame selection;dense per-frame stereo matching;volumetric depth map fusion;KinectFusion;implicit surface extraction;live user feedback;pose estimation;point-based mobile phone method;scanning time;Cameras;Three-dimensional displays;Computational modeling;Mobile handsets;Surface reconstruction;Real-time systems;Pipelines;3D object scanning;surface reconstruction;mobile computing;3D object scanning;surface reconstruction;mobile computing;Algorithms;Cell Phones;Humans;Imaging, Three-Dimensional},  doi={10.1109/TVCG.2015.2459902},  ISSN={1941-0506},  month={Nov},}


@ARTICLE{7165658,  author={S. {Magnenat} and D. T. {Ngo} and F. {Zünd} and M. {Ryffel} and G. {Noris} and G. {Rothlin} and A. {Marra} and M. {Nitti} and P. {Fua} and M. {Gross} and R. W. {Sumner}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Live Texturing of Augmented Reality Characters from Colored Drawings},   year={2015},  volume={21},  number={11},  pages={1201-1210},  abstract={Coloring books capture the imagination of children and provide them with one of their earliest opportunities for creative expression. However, given the proliferation and popularity of digital devices, real-world activities like coloring can seem unexciting, and children become less engaged in them. Augmented reality holds unique potential to impact this situation by providing a bridge between real-world activities and digital enhancements. In this paper, we present an augmented reality coloring book App in which children color characters in a printed coloring book and inspect their work using a mobile device. The drawing is detected and tracked, and the video stream is augmented with an animated 3-D version of the character that is textured according to the child's coloring. This is possible thanks to several novel technical contributions. We present a texturing process that applies the captured texture from a 2-D colored drawing to both the visible and occluded regions of a 3-D character in real time. We develop a deformable surface tracking method designed for colored drawings that uses a new outlier rejection algorithm for real-time tracking and surface deformation recovery. We present a content creation pipeline to efficiently create the 2-D and 3-D content. And, finally, we validate our work with two user studies that examine the quality of our texturing algorithm and the overall App experience.},  keywords={augmented reality;computer animation;image texture;interactive systems;video streaming;live texturing;augmented reality characters;children imagination;creative expression;digital devices;real-world activities;digital enhancements;augmented reality coloring book app;printed coloring book;mobile device;video stream;animated 3D version;texturing process;2D colored drawing;visible regions;occluded regions;3D character;deformable surface tracking method;outlier rejection algorithm;real-time tracking;surface deformation recovery;content creation pipeline;interactive books;Shape;Image color analysis;Real-time systems;Pipelines;Feature extraction;Books;Cameras;Augmented reality;deformable surface tracking;inpainting;interactive books;drawing coloring;Augmented reality;deformable surface tracking;inpainting;interactive books;drawing coloring},  doi={10.1109/TVCG.2015.2459871},  ISSN={1941-0506},  month={Nov},}


@ARTICLE{7164353,  author={C. {Resch} and H. {Naik} and P. {Keitler} and S. {Benkhardt} and G. {Klinker}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={On-Site Semi-Automatic Calibration and Registration of a Projector-Camera System Using Arbitrary Objects with Known Geometry},   year={2015},  volume={21},  number={11},  pages={1211-1220},  abstract={In the Shader Lamps concept, a projector-camera system augments physical objects with projected virtual textures, provided that a precise intrinsic and extrinsic calibration of the system is available. Calibrating such systems has been an elaborate and lengthy task in the past and required a special calibration apparatus. Self-calibration methods in turn are able to estimate calibration parameters automatically with no effort. However they inherently lack global scale and are fairly sensitive to input data. We propose a new semi-automatic calibration approach for projector-camera systems that - unlike existing auto-calibration approaches - additionally recovers the necessary global scale by projecting on an arbitrary object of known geometry. To this end our method combines surface registration with bundle adjustment optimization on points reconstructed from structured light projections to refine a solution that is computed from the decomposition of the fundamental matrix. In simulations on virtual data and experiments with real data we demonstrate that our approach estimates the global scale robustly and is furthermore able to improve incorrectly guessed intrinsic and extrinsic calibration parameters thus outperforming comparable metric rectification algorithms.},  keywords={calibration;cameras;image registration;matrix decomposition;optical projectors;semiautomatic calibration approach;projector-camera system registration;Shader Lamps concept;intrinsic calibration;extrinsic calibration;structured light projection;surface registration;bundle adjustment optimization;fundamental matrix decomposition;metric rectification algorithm;intrinsic calibration parameter;extrinsic calibration parameter;Calibration;Cameras;Image reconstruction;Iterative closest point algorithm;Three-dimensional displays;Distortion;Mathematical model;Projector-Camera Systems;Calibration;Registration;Shader Lamps;SAR;Projector-Camera Systems;Calibration;Registration;Shader Lamps;SAR},  doi={10.1109/TVCG.2015.2459898},  ISSN={1941-0506},  month={Nov},}


@ARTICLE{7165660,  author={P. {Punpongsanon} and D. {Iwai} and K. {Sato}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={SoftAR: Visually Manipulating Haptic Softness Perception in Spatial Augmented Reality},   year={2015},  volume={21},  number={11},  pages={1279-1288},  abstract={We present SoftAR, a novel spatial augmented reality (AR) technique based on a pseudo-haptics mechanism that visually manipulates the sense of softness perceived by a user pushing a soft physical object. Considering the limitations of projection-based approaches that change only the surface appearance of a physical object, we propose two projection visual effects, i.e., surface deformation effect (SDE) and body appearance effect (BAE), on the basis of the observations of humans pushing physical objects. The SDE visualizes a two-dimensional deformation of the object surface with a controlled softness parameter, and BAE changes the color of the pushing hand. Through psychophysical experiments, we confirm that the SDE can manipulate softness perception such that the participant perceives significantly greater softness than the actual softness. Furthermore, fBAE, in which BAE is applied only for the finger area, significantly enhances manipulation of the perception of softness. We create a computational model that estimates perceived softness when SDE+fBAE is applied. We construct a prototype SoftAR system in which two application frameworks are implemented. The softness adjustment allows a user to adjust the softness parameter of a physical object, and the softness transfer allows the user to replace the softness with that of another object.},  keywords={augmented reality;deformation;haptic interfaces;SoftAR;haptic softness perception visual manipulation;spatial augmented reality technique;pseudo-haptics mechanism;soft physical object;projection-based approaches;surface deformation effect;body appearance effect;humans pushing physical objects;2D deformation;object surface;controlled softness parameter;SDE+fBAE;Force;Visualization;Visual effects;Image color analysis;Haptic interfaces;Barium;Computational modeling;Spatial augmented reality;pseudo-haptic feedback;softness perception;product visualization;Spatial augmented reality;pseudo-haptic feedback;softness perception;product visualization},  doi={10.1109/TVCG.2015.2459792},  ISSN={1941-0506},  month={Nov},}


@ARTICLE{7164348,  author={J. E. {Swan} and G. {Singh} and S. R. {Ellis}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Matching and Reaching Depth Judgments with Real and Augmented Reality Targets},   year={2015},  volume={21},  number={11},  pages={1289-1298},  abstract={Many compelling augmented reality (AR) applications require users to correctly perceive the location of virtual objects, some with accuracies as tight as 1 mm. However, measuring the perceived depth of AR objects at these accuracies has not yet been demonstrated. In this paper, we address this challenge by employing two different depth judgment methods, perceptual matching and blind reaching, in a series of three experiments, where observers judged the depth of real and AR target objects presented at reaching distances. Our experiments found that observers can accurately match the distance of a real target, but when viewing an AR target through collimating optics, their matches systematically overestimate the distance by 0.5 to 4.0 cm. However, these results can be explained by a model where the collimation causes the eyes' vergence angle to rotate outward by a constant angular amount. These findings give error bounds for using collimating AR displays at reaching distances, and suggest that for these applications, AR displays need to provide an adjustable focus. Our experiments further found that observers initially reach ~4 cm too short, but reaching accuracy improves with both consistent proprioception and corrective visual feedback, and eventually becomes nearly as accurate as matching.},  keywords={augmented reality;display devices;real target;augmented reality target;virtual object;depth judgment method;perceptual matching;blind reaching;observer;collimating optics;constant angular amount;collimating AR display;visual feedback;Observers;Accuracy;Visualization;Surgery;Target tracking;Augmented reality;Servers;Depth judgment;perceptual matching;blind reaching;accommodation;vergence;augmented reality;Depth judgment;perceptual matching;blind reaching;accommodation;vergence;augmented reality},  doi={10.1109/TVCG.2015.2459895},  ISSN={1941-0506},  month={Nov},}

