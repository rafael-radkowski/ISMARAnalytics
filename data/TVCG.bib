@ARTICLE{9199574,  author={L. R. {Luidolt} and M. {Wimmer} and K. {Kr√∂sl}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Gaze-Dependent Simulation of Light Perception in Virtual Reality},   year={2020},  volume={26},  number={12},  pages={3557-3567},  abstract={The perception of light is inherently different inside a virtual reality (VR) or augmented reality (AR) simulation when compared to the real world. Conventional head-worn displays (HWDs) are not able to display the same high dynamic range of brightness and color as the human eye can perceive in the real world. To mimic the perception of real-world scenes in virtual scenes, it is crucial to reproduce the effects of incident light on the human visual system. In order to advance virtual simulations towards perceptual realism, we present an eye-tracked VR/AR simulation comprising effects for gaze-dependent temporal eye adaption, perceptual glare, visual acuity reduction, and scotopic color vision. Our simulation is based on medical expert knowledge and medical studies of the healthy human eye. We conducted the first user study comparing the perception of light in a real-world low-light scene to a VR simulation. Our results show that the proposed combination of simulated visual effects is well received by users and also indicate that an individual adaptation is necessary, because perception of light is highly subjective.},  keywords={augmented reality;colour vision;eye;helmet mounted displays;human factors;vision defects;visual perception;light perception;virtual reality;conventional head-worn displays;high dynamic range;brightness;virtual scenes;human visual system;virtual simulations;perceptual realism;gaze-dependent temporal eye adaption;visual acuity reduction;scotopic color vision;real-world low-light scene;VR simulation;simulated visual effects;gaze-dependent simulation;Adaptation models;Solid modeling;Lighting;Virtual reality;Retina;Augmented reality;Computational modeling;Head-mounted displays;Perception;virtual reality;user studies},  doi={10.1109/TVCG.2020.3023604},  ISSN={1941-0506},  month={Dec},}


@ARTICLE{9199573,  author={J. {Luo} and Z. {Huang} and Y. {Li} and X. {Zhou} and G. {Zhang} and H. {Bao}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={NIID-Net: Adapting Surface Normal Knowledge for Intrinsic Image Decomposition in Indoor Scenes},   year={2020},  volume={26},  number={12},  pages={3434-3445},  abstract={Intrinsic image decomposition, i.e., decomposing a natural image into a reflectance image and a shading image, is used in many augmented reality applications for achieving better visual coherence between virtual contents and real scenes. The main challenge is that the decomposition is ill-posed, especially in indoor scenes where lighting conditions are complicated, while real training data is inadequate. To solve this challenge, we propose NIID-Net, a novel learning-based framework that adapts surface normal knowledge for improving the decomposition. The knowledge learned from relatively more abundant data for surface normal estimation is integrated into intrinsic image decomposition in two novel ways. First, normal feature adapters are proposed to incorporate scene geometry features when decomposing the image. Secondly, a map of integrated lighting is proposed for propagating object contour and planarity information during shading rendering. Furthermore, this map is capable of representing spatially-varying lighting conditions indoors. Experiments show that NIID-Net achieves competitive performance in reflectance estimation and outperforms all previous methods in shading estimation quantitatively and qualitatively. The source code of our implementation is released at https://github.com/zju3dv/NIID-Net.},  keywords={augmented reality;feature extraction;image representation;image resolution;image segmentation;learning (artificial intelligence);lighting;object recognition;rendering (computer graphics);video signal processing;natural image;reflectance image;shading image;indoor scenes;surface normal knowledge;surface normal estimation;intrinsic image decomposition;lighting conditions;NIID-Net;scene geometry features;feature adapters;Image decomposition;Estimation;Image reconstruction;Augmented reality;Training data;Intrinsic image decomposition;image processing;augmented reality},  doi={10.1109/TVCG.2020.3023565},  ISSN={1941-0506},  month={Dec},}


@ARTICLE{9201064,  author={X. {Yang} and L. {Zhou} and H. {Jiang} and Z. {Tang} and Y. {Wang} and H. {Bao} and G. {Zhang}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Mobile3DRecon: Real-time Monocular 3D Reconstruction on a Mobile Phone},   year={2020},  volume={26},  number={12},  pages={3446-3456},  abstract={We present a real-time monocular 3D reconstruction system on a mobile phone, called Mobile3DRecon. Using an embedded monocular camera, our system provides an online mesh generation capability on back end together with real-time 6DoF pose tracking on front end for users to achieve realistic AR effects and interactions on mobile phones. Unlike most existing state-of-the-art systems which produce only point cloud based 3D models online or surface mesh offline, we propose a novel online incremental mesh generation approach to achieve fast online dense surface mesh reconstruction to satisfy the demand of real-time AR applications. For each keyframe of 6DoF tracking, we perform a robust monocular depth estimation, with a multi-view semi-global matching method followed by a depth refinement post-processing. The proposed mesh generation module incrementally fuses each estimated keyframe depth map to an online dense surface mesh, which is useful for achieving realistic AR effects such as occlusions and collisions. We verify our real-time reconstruction results on two mid-range mobile platforms. The experiments with quantitative and qualitative evaluation demonstrate the effectiveness of the proposed monocular 3D reconstruction system, which can handle the occlusions and collisions between virtual objects and real scenes to achieve realistic AR effects.},  keywords={cameras;image matching;image reconstruction;mesh generation;pose estimation;robot vision;stereo image processing;mid-range mobile platforms;real-time reconstruction results;online dense surface mesh;estimated keyframe depth map;mesh generation module;depth refinement post-processing;multiview semiglobal matching method;robust monocular depth estimation;6DoF tracking;real-time AR applications;dense surface mesh reconstruction;mesh generation approach;3D models online;existing state-of-the-art systems;realistic AR effects;real-time 6DoF;online mesh generation capability;embedded monocular camera;called Mobile3DRecon;real-time monocular 3D reconstruction system;mobile phone;Real-time systems;Three-dimensional displays;Mobile handsets;Cameras;Surface reconstruction;Estimation;Mesh generation;real-time reconstruction;monocular depth estimation;incremental mesh generation},  doi={10.1109/TVCG.2020.3023634},  ISSN={1941-0506},  month={Dec},}


@ARTICLE{9199560,  author={X. {Wen} and M. {Wang} and C. {Richardt} and Z. -Y. {Chen} and S. -M. {Hu}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Photorealistic Audio-driven Video Portraits},   year={2020},  volume={26},  number={12},  pages={3457-3466},  abstract={Video portraits are common in a variety of applications, such as videoconferencing, news broadcasting, and virtual education and training. We present a novel method to synthesize photorealistic video portraits for an input portrait video, automatically driven by a person's voice. The main challenge in this task is the hallucination of plausible, photorealistic facial expressions from input speech audio. To address this challenge, we employ a parametric 3D face model represented by geometry, facial expression, illumination, etc., and learn a mapping from audio features to model parameters. The input source audio is first represented as a high-dimensional feature, which is used to predict facial expression parameters of the 3D face model. We then replace the expression parameters computed from the original target video with the predicted one, and rerender the reenacted face. Finally, we generate a photorealistic video portrait from the reenacted synthetic face sequence via a neural face renderer. One appealing feature of our approach is the generalization capability for various input speech audio, including synthetic speech audio from text-to-speech software. Extensive experimental results show that our approach outperforms previous general-purpose audio-driven video portrait methods. This includes a user study demonstrating that our results are rated as more realistic than previous methods.},  keywords={image representation;image sequences;text-to-speech software;input speech audio synthesis;synthetic face sequence;general-purpose audio-driven video portrait methods;neural face renderer;facial expression parameters;parametric 3D face model;photorealistic facial expressions;virtual education;photorealistic audio-driven video portraits;Face recognition;Three-dimensional displays;Visualization;Streaming media;Rendering (computer graphics);Image reconstruction;Training data;Audio-driven animation;facial reenactment;generative models;talking-head video generation},  doi={10.1109/TVCG.2020.3023573},  ISSN={1941-0506},  month={Dec},}

@ARTICLE{9199563,  author={X. {Sun} and Y. {Zhang} and P. -C. {Huang} and N. {Acharjee} and M. {Dagenais} and M. {Peckerar} and A. {Varshney}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Correcting the Proximity Effect in Nanophotonic Phased Arrays},   year={2020},  volume={26},  number={12},  pages={3503-3513},  abstract={Thermally modulated Nanophotonic Phased Arrays (NPAs) can be used as phase-only holographic displays. Compared to the holographic displays based on Liquid Crystal on Silicon Spatial Light Modulators (LCoS SLMs), NPAs have the advantage of integrated light source and high refresh rate. However, the formation of the desired wavefront requires accurate modulation of the phase which is distorted by the thermal proximity effect. This problem has been largely overlooked and existing approaches to similar problems are either slow or do not provide a good result in the setting of NPAs. We propose two new algorithms based on the iterative phase retrieval algorithm and the proximal algorithm to address this challenge. We have carried out computational simulations to compare and contrast various algorithms in terms of image quality and computational efficiency. This work is going to benefit the research on NPAs and enable the use of large-scale NPAs as holographic displays.},  keywords={holographic displays;iterative methods;nanophotonics;optical arrays;integrated light source;high refresh rate;wavefront;thermal proximity effect;iterative phase retrieval algorithm;proximal algorithm;thermally modulated nanophotonic phased arrays;phase-only holographic displays;liquid crystal on silicon spatial light modulators;computational simulations;image quality;Proximity effects;Holography;Phased arrays;Holographic optical components;Optical imaging;Adaptive optics;Phase modulation;Nanophotonics;Nanophotonic phased array;proximity effect correction;proximal algorithms;phase-only hologram},  doi={10.1109/TVCG.2020.3023601},  ISSN={1941-0506},  month={Dec},}

@ARTICLE{9199564,  author={G. {Ma} and S. {Li} and C. {Chen} and A. {Hao} and H. {Qin}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Stage-wise Salient Object Detection in 360¬∞ Omnidirectional Image via Object-level Semantical Saliency Ranking},   year={2020},  volume={26},  number={12},  pages={3535-3545},  abstract={The 2D image based salient object detection (SOD) has been extensively explored, while the 360¬∞ omnidirectional image based SOD has received less research attention and there exist three major bottlenecks that are limiting its performance. Firstly, the currently available training data is insufficient for the training of 360¬∞ SOD deep model. Secondly, the visual distortions in 360¬∞ omnidirectional images usually result in large feature gap between 360¬∞ images and 2D images; consequently, the widely used stage-wise training-a widely-used solution to alleviate the training data shortage problem, becomes infeasible when conducing SOD in 360¬∞ omnidirectional images. Thirdly, the existing 360¬∞ SOD approach has followed a multi-task methodology that performs salient object localization and segmentation-like saliency refinement at the same time, being faced with extremely large problem domain, making the training data shortage dilemma even worse. To tackle all these issues, this paper divides the 360¬∞ SOD into a multi-staqe task, the key rationale of which is to decompose the original complex problem domain into sequential easy sub problems that only demand for small-scale training data. Meanwhile, we learn how to rank the ‚Äúobject-level semantical saliency‚Äù, aiming to locate salient viewpoints and objects accurately. Specifically, to alleviate the training data shortage problem, we have released a novel dataset named 360-SSOD, containing 1,105 360¬∞ omnidirectional images with manually annotated object-level saliency ground truth, whose semantical distribution is more balanced than that of the existing dataset. Also, we have compared the proposed method with 13 SOTA methods, and all quantitative results have demonstrated the performance superiority.},  keywords={feature extraction;image segmentation;learning (artificial intelligence);object detection;360¬∞ omnidirectional image;training data shortage problem;salient object localization;segmentation-like saliency refinement;omnidirectional images;object-level saliency ground truth;stage-wise salient object detection;object-level semantical saliency ranking;SOD deep model;SOD approach;Two dimensional displays;Visualization;Distortion;Training data;Object detection;Task analysis;Virtual Reality;360¬∞ Omnidirectional Image;Multi-stage Salient Object Detection;Semantical Saliency},  doi={10.1109/TVCG.2020.3023636},  ISSN={1941-0506},  month={Dec},}

@ARTICLE{9207831,  author={D. {Yu} and Q. {Zhou} and J. {Newn} and T. {Dingler} and E. {Velloso} and J. {Goncalves}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Fully-Occluded Target Selection in Virtual Reality},   year={2020},  volume={26},  number={12},  pages={3402-3413},  abstract={The presence of fully-occluded targets is common within virtual environments, ranging from a virtual object located behind a wall to a datapoint of interest hidden in a complex visualization. However, efficient input techniques for locating and selecting these targets are mostly underexplored in virtual reality (VR) systems. In this paper, we developed an initial set of seven techniques techniques for fully-occluded target selection in VR. We then evaluated their performance in a user study and derived a set of design implications for simple and more complex tasks from our results. Based on these insights, we refined the most promising techniques and conducted a second, more comprehensive user study. Our results show how factors, such as occlusion layers, target depths, object densities, and the estimation of target locations, can affect technique performance. Our findings from both studies and distilled recommendations can inform the design of future VR systems that offer selections for fully-occluded targets.},  keywords={augmented reality;human computer interaction;virtual environments;virtual object;complex visualization;virtual reality systems;fully-occluded target selection;user study;VR systems;Data visualization;Virtual environments;Three-dimensional displays;Task analysis;Distance measurement;Pointing selection;object selection;visualization;occlusion;virtual reality;hidden target;head-mounted displays},  doi={10.1109/TVCG.2020.3023606},  ISSN={1941-0506},  month={Dec},}

@ARTICLE{9199566,  author={T. {Luo} and M. {Zhang} and Z. {Pan} and Z. {Li} and N. {Cai} and J. {Miao} and Y. {Chen} and M. {Xu}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Dream-Experiment: A MR User Interface with Natural Multi-channel Interaction for Virtual Experiments},   year={2020},  volume={26},  number={12},  pages={3524-3534},  abstract={This paper studies a set of MR technologies for middle school experimental teaching environments and develops a multi-channel MR user interface called Dream-Experiment. The goal of Dream-Experiment is to improve the traditional MR user interface, so that users can get a real, natural 3D interactive experience like real experiments, but without danger and pollution. In terms of visual presentation, we design multi-camera collaborative registration to realize robust 6-DoF MR interactive space, and also define a complete rendering pipeline to provide improved processing of virtual-real objects' occlusion including translucent devices. In the virtual-real interaction, we provide six interaction modes that support visual interaction, tangible interaction, virtual-real gestures with touching, voice, thermal feeling, and olfactory feeling. After users' testing, we find that Dream-Experiment has better interactive efficiency and user experience than traditional MR environments.},  keywords={cameras;computer aided instruction;data visualisation;human computer interaction;interactive systems;rendering (computer graphics);teaching;virtual reality;multicamera collaborative registration;MR interactive space;user experience;tangible interaction;visual interaction;interaction modes;virtual-real interaction;natural 3D interactive experience;multichannel MR user interface;middle school experimental teaching environments;virtual experiments;natural multichannel interaction;Dream-Experiment;Cameras;Three-dimensional displays;Image edge detection;Electron tubes;Temperature sensors;Visualization;User interfaces;Multi-channel interaction;virtual-real occlusion;multi-camera collaboration;MR experiments},  doi={10.1109/TVCG.2020.3023602},  ISSN={1941-0506},  month={Dec},}

@ARTICLE{9199555,  author={A. S. {Williams} and J. {Garcia} and F. {Ortega}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Understanding Multimodal User Gesture and Speech Behavior for Object Manipulation in Augmented Reality Using Elicitation},   year={2020},  volume={26},  number={12},  pages={3479-3489},  abstract={The primary objective of this research is to understand how users manipulate virtual objects in augmented reality using multimodal interaction (gesture and speech) and unimodal interaction (gesture). Through this understanding, natural-feeling interactions can be designed for this technology. These findings are derived from an elicitation study employing Wizard of Oz design aimed at developing user-defined multimodal interaction sets for building tasks in 3D environments using optical see-through augmented reality headsets. The modalities tested are gesture and speech combined, gesture only, and speech only. The study was conducted with 24 participants. The canonical referents for translation, rotation, and scale were used along with some abstract referents (create, destroy, and select). A consensus set of gestures for interactions is provided. Findings include the types of gestures performed, the timing between co-occurring gestures and speech (130 milliseconds), perceived workload by modality (using NASA TLX), and design guidelines arising from this study. Multimodal interaction, in particular gesture and speech interactions for augmented reality headsets, are essential as this technology becomes the future of interactive computing. It is possible that in the near future, augmented reality glasses will become pervasive.},  keywords={augmented reality;gesture recognition;user interfaces;speech behavior;multimodal user gesture;augmented reality glasses;interactive computing;speech interactions;augmented reality headsets;user-defined multimodal interaction sets;natural-feeling interactions;virtual objects;object manipulation;time 130.0 ms;Two dimensional displays;Augmented reality;Optical design;Gesture recognition;Speech recognition;Headphones},  doi={10.1109/TVCG.2020.3023566},  ISSN={1941-0506},  month={Dec},}

@ARTICLE{9199565,  author={C. {George} and P. {Tamunjoh} and H. {Hussmann}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Invisible Boundaries for VR: Auditory and Haptic Signals as Indicators for Real World Boundaries},   year={2020},  volume={26},  number={12},  pages={3414-3422},  abstract={Maintaining awareness of real world boundaries whilst being immersed in virtual reality (VR) with head mounted displays (HMDs), is a necessity for the physical integrity of the user. This paper explores whether individual human senses can be allocated to the real and the virtual world and what effect this has on workload, presence, performance and perceived safety. We present the results of a lab study (N=33) where the auditory and haptic sense of participants was trained to be an indicator for real world boundaries, while their visual sense was bound to a VR experience with an HMD. Our results suggests that allocating senses increases workload. However, while performance is comparable to purely visual indications of boundaries, sense allocation seems to improve presence. Participants prefer the signals to be separate or combined subsequently, depending on the priority and proximity to the boundary. This exploratory study is valuable for developers and researchers who want to start including audio and haptic signals as indicators for real world boundaries.},  keywords={haptic interfaces;helmet mounted displays;virtual reality;HMD;invisible boundaries;visual sense;haptic sense;virtual world;haptic signals;auditory signals;VR;Virtual reality;Haptic interfaces;Safety;Games;Resource management;Virtual Reality;Augmented Reality;Audio and Haptic Modality;Chaperone System},  doi={10.1109/TVCG.2020.3023607},  ISSN={1941-0506},  month={Dec},}

@ARTICLE{9199567,  author={T. {Kaminokado} and Y. {Hiroi} and Y. {Itoh}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={StainedView: Variable-Intensity Light-Attenuation Display with Cascaded Spatial Color Filtering for Improved Color Fidelity},   year={2020},  volume={26},  number={12},  pages={3576-3586},  abstract={We present StainedView, an optical see-through display that spatially filters the spectral distribution of light to form an image with improved color fidelity. Existing light-attenuation displays have limited color fidelity and contrast, resulting in a degraded appearance of virtual images. To use these displays to present virtual images that are more consistent with the real world, we require three things: intensity modulation of incoming light, spatial color filtering with narrower bandwidth, and appropriate light modulation for incoming light with an arbitrary spectral distribution. In StainedView, we address the three requirements by cascading two phase-only spatial light modulators (PSLMs), a digital micromirror device, and polarization optics to control both light intensity and spectrum distribution. We show that our design has a 1.8 times wider color gamut fidelity (75.8% fulfillment of sRGB color space) compared to the existing single-PSLM approach (41.4%) under a reference white light. We demonstrated the design with a proof-of-concept display system. We further introduce our optics design and pixel-selection algorithm for the given light input, evaluate the spatial color filter, and discuss the limitation of the current prototype.},  keywords={colour displays;image colour analysis;light attenuation;light polarisation;micromirrors;spatial filters;spatial light modulators;virtual images;intensity modulation;incoming light;light modulation;arbitrary spectral distribution;StainedView;phase-only spatial light modulators;light intensity;spectrum distribution;sRGB color space;reference white light;proof-of-concept display system;optics design;light input;variable-intensity light-attenuation display;cascaded spatial color filtering;color gamut fidelity;optical see-through display;digital micromirror device;polarization optics;pixel-selection algorithm;Image color analysis;Optical imaging;Optical polarization;Adaptive optics;Optical distortion;Optical modulation;Nonlinear optics;Light attenuation display;phase modulation;see-through display;vision augmentation;augmented reality},  doi={10.1109/TVCG.2020.3023569},  ISSN={1941-0506},  month={Dec},}

@ARTICLE{9206143,  author={N. {Khenak} and J. {V√©zien} and P. {Bourdot}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Spatial Presence, Performance, and Behavior between Real, Remote, and Virtual Immersive Environments},   year={2020},  volume={26},  number={12},  pages={3467-3478},  abstract={Spatial presence encompasses the user's ability to experience a sense of ‚Äúbeing there‚Äù. While particular attention was given to assess spatial presence in real and virtual environments, few have been interested in measuring it in telepresence situations. To bridge this gap, the present work introduces a study that compares the execution of a task in three conditions: a real physical environment, a remote environment via a telepresence system, and a virtual simulation of the real environment. Following a within-subject design, 27 participants performed a navigation task consisting in following a route while avoiding obstacles. Spatial presence and five related factors (affordance, enjoyment, attention allocation, reality, and cybersickness) were evaluated using a presence questionnaire. In addition, performance measures were gathered regarding environment recollection and task execution. The evaluation also included a behavioral metric measured by obstacle avoidance distance extracted from participants' trajectories. Results indicated a higher presence in the real environment, along with the best performance measures. No difference was found in spatial presence between the remote and the virtual conditions, although a higher degree of affordance and enjoyment was attributed to the virtual environment, and a higher degree of reality was attributed to the remote environment. The number of collisions was found to be lower in the remote condition compared to the virtual condition. Similarly, the avoidance distance was also bigger (and almost similar) in the real and the remote environments compared to the virtual environment indicating a greater caution of participants. These cues highlight that the behavior of participants in the remote condition was closer to their behavior in the real situation than it was in the virtual condition. Furthermore, positive correlations were found between the reality factor and two of the three performance measures, as well as with the behavioral metric. This suggests that the degree of physical existence of the space in which participants operate can influence their performance and behavior.},  keywords={user experience;virtual reality;virtual condition;virtual environment;remote environment;remote condition;spatial presence;virtual immersive environments;physical environment;presence questionnaire;environment recollection;Virtual environments;Atmospheric measurements;Particle measurements;Behavioral sciences;Spatial augmented reality;Spatial presence;remote and virtual environments;user evaluation;performance and behavioral mesures},  doi={10.1109/TVCG.2020.3023574},  ISSN={1941-0506},  month={Dec},}

@ARTICLE{9199571,  author={D. {Roth} and M. E. {Latoschik}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Construction of the Virtual Embodiment Questionnaire (VEQ)},   year={2020},  volume={26},  number={12},  pages={3546-3556},  abstract={User embodiment is important for many virtual reality (VR) applications, for example, in the context of social interaction, therapy, training, or entertainment. However, there is no data-driven and validated instrument to empirically measure the perceptual aspects of embodiment, necessary to reliably evaluate this important phenomenon. To provide a method to assess components of virtual embodiment in a reliable and consistent fashion, we constructed a Virtual Embodiment Questionnaire (VEQ). We reviewed previous literature to identify applicable constructs and questionnaire items, and performed a confirmatory factor analysis (CFA) on the data from three experiments (N = 196). The analysis confirmed three factors: (1) ownership of a virtual body, (2) agency over a virtual body, and (3) the perceived change in the body schema. A fourth study (N = 22) was conducted to confirm the reliability and validity of the scale, by investigating the impacts of latency and latency jitter present in the simulation. We present the proposed scale and study results and discuss resulting implications.},  keywords={statistical analysis;virtual reality;virtual embodiment questionnaire;VEQ;user embodiment;virtual reality applications;confirmatory factor analysis;virtual body;Avatars;User centered design;User interfaces;Social factors;Virtualization;Virtual Embodiment;Body Ownership;Agency;Avatars;Virtual Reality},  doi={10.1109/TVCG.2020.3023603},  ISSN={1941-0506},  month={Dec},}

@ARTICLE{9199570,  author={A. {Marquardt} and C. {Trepkowski} and T. D. {Eibich} and J. {Maiero} and E. {Kruijff} and J. {Sch√∂ning}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Comparing Non-Visual and Visual Guidance Methods for Narrow Field of View Augmented Reality Displays},   year={2020},  volume={26},  number={12},  pages={3389-3401},  abstract={Current augmented reality displays still have a very limited field of view compared to the human vision. In order to localize out-of-view objects, researchers have predominantly explored visual guidance approaches to visualize information in the limited (in-view) screen space. Unfortunately, visual conflicts like cluttering or occlusion of information often arise, which can lead to search performance issues and a decreased awareness about the physical environment. In this paper, we compare an innovative non-visual guidance approach based on audio-tactile cues with the state-of-the-art visual guidance technique EyeSee360 for localizing out-of-view objects in augmented reality displays with limited field of view. In our user study, we evaluate both guidance methods in terms of search performance and situation awareness. We show that although audio-tactile guidance is generally slower than the well-performing EyeSee360 in terms of search times, it is on a par regarding the hit rate. Even more so, the audio-tactile method provides a significant improvement in situation awareness compared to the visual approach.},  keywords={augmented reality;data visualisation;augmented reality displays;human vision;out-of-view objects;visual guidance approaches;search performance issues;nonvisual guidance approach;audio-tactile cues;guidance methods;situation awareness;audio-tactile guidance;audio-tactile method;visual approach;visual guidance technique;EyeSee360 technique;Visualization;Task analysis;Augmented reality;Navigation;Three-dimensional displays;Search problems;Augmented Reality;view-management;guidance;audio-tactile cues;performance;situation awareness},  doi={10.1109/TVCG.2020.3023605},  ISSN={1941-0506},  month={Dec},}


@ARTICLE{9199568,  author={B. {Zhou} and S. {G√ºven}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Fine-Grained Visual Recognition in Mobile Augmented Reality for Technical Support},   year={2020},  volume={26},  number={12},  pages={3514-3523},  abstract={Augmented Reality is increasingly explored as the new medium for two-way remote collaboration applications to guide the participants more effectively and efficiently via visual instructions. As users strive for more natural interaction and automation in augmented reality applications, new visual recognition techniques are needed to enhance the user experience. Although simple object recognition is often used in augmented reality towards this goal, most collaboration tasks are too complex for such recognition algorithms to suffice. In this paper, we propose a fine-grained visual recognition approach for mobile augmented reality, which leverages RGB video frames and sparse depth feature points identified in real-time, as well as camera pose data to detect various visual states of an object. We demonstrate the value of our approach through a mobile application designed for hardware support, which automatically detects the state of an object to present the right set of information in the right context.},  keywords={augmented reality;cameras;feature extraction;image colour analysis;mobile computing;object recognition;mobile augmented reality;two-way remote collaboration applications;augmented reality applications;visual recognition techniques;user experience;object recognition;collaboration tasks;recognition algorithms;fine-grained visual recognition approach;mobile application;RGB video frames;sparse depth feature points;Visualization;Cameras;Three-dimensional displays;Image recognition;Augmented reality;Maintenance engineering;Visual recognition;augmented reality;mobile},  doi={10.1109/TVCG.2020.3023635},  ISSN={1941-0506},  month={Dec},}


@ARTICLE{9199575,  author={Q. {Zhou} and D. {Yu} and M. N. {Reinoso} and J. {Newn} and J. {Goncalves} and E. {Velloso}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Eyes-free Target Acquisition During Walking in Immersive Mixed Reality},   year={2020},  volume={26},  number={12},  pages={3423-3433},  abstract={Reaching towards out-of-sight objects during walking is a common task in daily life, however the same task can be challenging when wearing immersive Head-Mounted Displays (HMD). In this paper, we investigate the effects of spatial reference frame, walking path curvature, and target placement relative to the body on user performance of manually acquiring out-of-sight targets located around their bodies, as they walk in a spatial-mapping Mixed Reality (MR) environment wearing an immersive HMD. We found that walking and increased path curvature negatively affected the overall spatial accuracy of the performance, and that the performance benefited more from using the torso as the reference frame than the head. We also found that targets placed at maximum reaching distance yielded less error in angular rotation and depth of the reaching arm. We discuss our findings with regard to human walking kinesthetics and the sensory integration in the peripersonal space during locomotion in immersive MR. We provide design guidelines for future immersive MR experience featuring spatial mapping and full-body motion tracking to provide better embodied experience.},  keywords={gait analysis;helmet mounted displays;user interfaces;virtual reality;immersive MR experience;spatial-mapping mixed reality environment;angular rotation;reaching distance;immersive HMD;user performance;walking path curvature;spatial reference frame;immersive head-mounted displays;out-of-sight objects;immersive mixed reality;eyes-free target acquisition;full-body motion tracking;spatial mapping;human walking kinesthetics;reaching arm;Legged locomotion;Virtual reality;Immersive experience;Augmented reality;Target tracking;Mixed Reality;Virtual Reality;Target Acquisition;Motion Tracking;Proprioception;Locomotion;Sensory Integration},  doi={10.1109/TVCG.2020.3023570},  ISSN={1941-0506},  month={Dec},}


@ARTICLE{9211732,  author={F. {Heinrich} and L. {Schwenderling} and F. {Joeres} and K. {Lawonn} and C. {Hansen}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Comparison of Augmented Reality Display Techniques to Support Medical Needle Insertion},   year={2020},  volume={26},  number={12},  pages={3568-3575},  abstract={Augmented reality (AR) may be a useful technique to overcome issues of conventionally used navigation systems supporting medical needle insertions, like increased mental workload and complicated hand-eye coordination. Previous research primarily focused on the development of AR navigation systems designed for specific displaying devices, but differences between employed methods have not been investigated before. To this end, a user study involving a needle insertion task was conducted comparing different AR display techniques with a monitor-based approach as baseline condition for the visualization of navigation information. A video see-through stationary display, an optical see-through head-mounted display and a spatial AR projector-camera-system were investigated in this comparison. Results suggest advantages of using projected navigation information in terms of lower task completion time, lower angular deviation and affirmative subjective participant feedback. Techniques requiring the intermediate view on screens, i.e. the stationary display and the baseline condition, showed less favorable results. Thus, benefits of providing AR navigation information compared to a conventionally used method could be identified. Significant objective measures results, as well as an identification of advantages and disadvantages of individual display techniques contribute to the development and design of improved needle navigation systems.},  keywords={augmented reality;data visualisation;helmet mounted displays;human factors;medical computing;needles;specific displaying devices;needle insertion task;stationary display;head-mounted display;projector-camera-system;navigation information;task completion time;angular deviation;AR navigation information;conventionally used method;needle navigation systems;AR display techniques;AR navigation systems;hand-eye coordination;mental workload;medical needle insertion;augmented reality display techniques;Navigation;Augmented reality;Biomedical monitoring;Navigation;Data visualization;Phantoms;Optical imaging;Medical augmented reality;display techniques;surgical navigation systems;needle guidance;visuospatial task},  doi={10.1109/TVCG.2020.3023637},  ISSN={1941-0506},  month={Dec},}


@ARTICLE{9212653,  author={V. {Biener} and D. {Schneider} and T. {Gesslein} and A. {Otte} and B. {Kuth} and P. O. {Kristensson} and E. {Ofek} and M. {Pahud} and J. {Grubert}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Breaking the Screen: Interaction Across Touchscreen Boundaries in Virtual Reality for Mobile Knowledge Workers},   year={2020},  volume={26},  number={12},  pages={3490-3502},  abstract={Virtual Reality (VR) has the potential to transform knowledge work. One advantage of VR knowledge work is that it allows extending 2D displays into the third dimension, enabling new operations, such as selecting overlapping objects or displaying additional layers of information. On the other hand, mobile knowledge workers often work on established mobile devices, such as tablets, limiting interaction with those devices to a small input space. This challenge of a constrained input space is intensified in situations when VR knowledge work is situated in cramped environments, such as airplanes and touchdown spaces. In this paper, we investigate the feasibility of interacting jointly between an immersive VR head-mounted display and a tablet within the context of knowledge work. Specifically, we 1) design, implement and study how to interact with information that reaches beyond a single physical touchscreen in VR; 2) design and evaluate a set of interaction concepts; and 3) build example applications and gather user feedback on those applications.},  keywords={helmet mounted displays;mobile computing;virtual reality;touchscreen boundaries;mobile knowledge workers;virtual reality;VR knowledge work;mobile devices;immersive VR head-mounted display;interaction concepts;Three-dimensional displays;Two dimensional displays;Virtual reality;Microsoft Windows;User interfaces;Touch sensitive screens;Sensors;virtual reality;knowledge work;mobile office;window management;eye tracking;multimodal interaction},  doi={10.1109/TVCG.2020.3023567},  ISSN={1941-0506},  month={Dec},}


@ARTICLE{8794595,  author={J. {Zhang} and M. {Gui} and Q. {Wang} and R. {Liu} and J. {Xu} and S. {Chen}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Hierarchical Topic Model Based Object Association for Semantic SLAM},   year={2019},  volume={25},  number={11},  pages={3052-3062},  abstract={Object-based simultaneous localization and mapping (SLAM) is a more natural and robust way for agents to interact with their surrounding environment. However, it introduces a problem of semantic objects association. Correct object association is the key factor to achieve a successful object SLAM system because object association and SLAM are inherently coupled and have not been well tackled yet. A novel formulation of the object association problem based on a hierarchical Dirichlet process (HDP) is proposed. Through the HDP, we can hierarchically associate the grouped object measurements. This can improve the object association accuracy and computation efficiency. Thanks to the novel formulation, the proposed method is also able to correct failure object associations according to its sampling inference algorithm. Furthermore, we introduce object poses to the processing of pose optimization. The object association and pose optimization are then solved in a tightly coupled way, by which both aspects can promote each other. The proposed method is evaluated on indoor and outdoor datasets and the experimental results show a very impressive improvement with respect to the traditional SLAM.},  keywords={inference mechanisms;SLAM (robots);semantic SLAM;semantic objects association;grouped object measurements;computation efficiency;failure object associations;hierarchical topic model based object association;object-based simultaneous localization and mapping;object SLAM system;sampling inference algorithm;Simultaneous localization and mapping;Semantics;Optimization;Cameras;Image reconstruction;Atmospheric modeling;Computer science;Visual Semantic SLAM;Object Association;Hierarchical Dirichlet Process},  doi={10.1109/TVCG.2019.2932216},  ISSN={1941-0506},  month={Nov},}


@ARTICLE{8794622,  author={O. {Erat} and M. {Hoell} and K. {Haubenwallner} and C. {Pirchheim} and D. {Schmalstieg}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Real-Time View Planning for Unstructured Lumigraph Modeling},   year={2019},  volume={25},  number={11},  pages={3063-3072},  abstract={We propose an algorithm for generating an unstructured lumigraph in real-time from an image stream. This problem has important applications in mixed reality, such as telepresence, interior design or as-built documentation. Unlike conventional texture optimization in structure from motion, our method must choose views from the input stream in a strictly incremental manner, since only a small number of views can be stored or transmitted. This requires formulating an online variant of the well-known view-planning problem, which must take into account what parts of the scene have already been seen and how the lumigraph sample distribution could improve in the future. We address this highly unconstrained problem by regularizing the scene structure using a regular grid structure. Upon the grid structure, we define a coverage metric describing how well the lumigraph samples cover the grid in terms of spatial and angular resolution, and we greedily keep incoming views if they improve the coverage. We evaluate the performance of our algorithm quantitatively and qualitatively on a variety of synthetic and real scenes, and demonstrate visually appealing results obtained at real-time frame rates (in the range of 3Hz-100Hz per incoming image, depending on configuration).},  keywords={graph theory;image resolution;image texture;interactive systems;rendering (computer graphics);virtual reality;time view planning;unstructured lumigraph modeling;image stream;mixed reality;telepresence;interior design;conventional texture optimization;strictly incremental manner;online variant;well-known view-planning problem;lumigraph sample distribution;highly unconstrained problem;scene structure;regular grid structure;lumigraph samples;spatial resolution;angular resolution;incoming views;synthetic scenes;real scenes;real-time frame rates;Planning;Image reconstruction;Real-time systems;Geometry;Rendering (computer graphics);Computational modeling;Image color analysis;Lumigraph;virtual reality;rendering;real-time;view planning;keyframe selection;multi-view},  doi={10.1109/TVCG.2019.2932237},  ISSN={1941-0506},  month={Nov},}


@ARTICLE{8794561,  author={D. {Andersen} and P. {Villano} and V. {Popescu}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={AR HMD Guidance for Controlled Hand-Held 3D Acquisition},   year={2019},  volume={25},  number={11},  pages={3073-3082},  abstract={Photogrammetry is a popular method of 3D reconstruction that uses conventional photos as input. This method can achieve high quality reconstructions so long as the scene is densely acquired from multiple views with sufficient overlap between nearby images. However, it is challenging for a human operator to know during acquisition if sufficient coverage has been achieved. Insufficient coverage of the scene can result in holes, missing regions, or even a complete failure of reconstruction. These errors require manually repairing the model or returning to the scene to acquire additional views, which is time-consuming and often infeasible. We present a novel approach to photogrammetric acquisition that uses an AR HMD to predict a set of covering views and to interactively guide an operator to capture imagery from each view. The operator wears an AR HMD and uses a handheld camera rig that is tracked relative to the AR HMD with a fiducial marker. The AR HMD tracks its pose relative to the environment and automatically generates a coarse geometric model of the scene, which our approach analyzes at runtime to generate a set of human-reachable acquisition views covering the scene with consistent camera-to-scene distance and image overlap. The generated view locations are rendered to the operator on the AR HMD. Interactive visual feedback informs the operator how to align the camera to assume each suggested pose. When the camera is in range, an image is automatically captured. In this way, a set of images suitable for 3D reconstruction can be captured in a matter of minutes. In a user study, participants who were novices at photogrammetry were tasked with acquiring a challenging and complex scene either without guidance or with our AR HMD based guidance. Participants using our guidance achieved improved reconstructions without cases of reconstruction failure as in the control condition. Our AR HMD based approach is self-contained, portable, and provides specific acquisition guidance tailored to the geometry of the scene being captured.},  keywords={cameras;computer vision;data visualisation;helmet mounted displays;image capture;image reconstruction;image sensors;photogrammetry;rendering (computer graphics);stereo image processing;high quality reconstructions;nearby images;human operator;photogrammetric acquisition;handheld camera rig;human-reachable acquisition views;consistent camera-to-scene distance;AR HMD based guidance;hand-held 3D acquisition control;3D reconstruction;coarse geometric model;image overlap;interactive visual feedback;Image reconstruction;Cameras;Resists;Three-dimensional displays;Visualization;Smart phones;Geometry;Augmented reality;head-mounted display;photogrammetry;3D reconstruction},  doi={10.1109/TVCG.2019.2932172},  ISSN={1941-0506},  month={Nov},}

@ARTICLE{8794580,  author={X. {Min} and W. {Zhang} and S. {Sun} and N. {Zhao} and S. {Tang} and Y. {Zhuang}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={VPModel: High-Fidelity Product Simulation in a Virtual-Physical Environment},   year={2019},  volume={25},  number={11},  pages={3083-3093},  abstract={In the development of a new product, the design team must describe the expected effects of the final products to potential users and stakeholders. However, existing prototyping tools can only present a product imperfectly, due to limitations at different levels. Specifically, the physical product model, which may be the product of 3D printing, could lack a visual interface; the presentation of the product through modeling software such as Rhinoceros 3D does not provide good realistic tactile perception; or the interface platforms, such as Axure RP, used to display the interactive effects differ from those to be used in the actual operation. Thus, we present the VPModel, a high-fidelity prototyping tool, able to integrate multiple prototyping methods simultaneously. It combines a touchable 3D-printed product model (3DPM) and a corresponding visualized virtual model, and the interactive interfaces are rendered synchronously in a mixed-reality device. Through the tangible, visual, and interactive demonstration, designers and normal users can each obtain a similar experience to the experience of the finished product. Furthermore, the VPModel also enhances design practices by enabling comparisons between modular models. However, the implementation of this system is a challenging task, which subsumes several fundamental problems as sub-tasks: object detection, real-time matching, hand-gesture detection and action recognition. To achieve the expected goals of the VPModel, this system uses physical hardware (a Microsoft MR HoloLens headset, a Leap Motion Controller, and a 3D printer) and existing machine learning algorithms. To evaluate our VPModel, we report the user experience of 16 participants, evaluated using a closed-ended questionnaire survey, a quantitative analysis of task performance, and a qualitative analysis of open-ended interviews. The results show a significant improvement in realism and enjoyment using the VPModel over the two traditional camera prototype approaches. In summary, the VPModel can be used to support design strategy and to convey design concepts fully and efficiently, which indicates a potential use for the VPModel in shortening product development cycles and reducing communication costs.},  keywords={data visualisation;gesture recognition;haptic interfaces;image matching;object detection;product development;production engineering computing;stereo image processing;three-dimensional printing;user experience;virtual reality;VPModel;high-fidelity product simulation;virtual-physical environment;physical product model;visual interface;Rhinoceros 3D;high-fidelity prototyping tool;3DPM;modular models;physical hardware;user experience;product development cycles;tactile perception;communication costs;3D-printed product model;object detection;real-time matching;hand-gesture detection;action recognition;machine learning algorithms;visualized virtual model;camera prototype approaches;Solid modeling;Three-dimensional displays;Tools;Prototypes;Visualization;Real-time systems;Software;Designers;high-fidelity prototyping tool;3D printed model;mixed reality;Adult;Augmented Reality;Computer Graphics;Consumer Behavior;Female;Humans;Male;Printing, Three-Dimensional;Surveys and Questionnaires;User-Computer Interface;Virtual Reality;Young Adult},  doi={10.1109/TVCG.2019.2932276},  ISSN={1941-0506},  month={Nov},}


@ARTICLE{8821571,  author={S. {Kagami} and K. {Hashimoto}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Animated Stickies: Fast Video Projection Mapping onto a Markerless Plane through a Direct Closed-Loop Alignment},   year={2019},  volume={25},  number={11},  pages={3094-3104},  abstract={This paper presents a fast projection mapping method for moving image content projected onto a markerless planar surface using a low-latency Digital Micromirror Device (DMD) projector. By adopting a closed-loop alignment approach, in which not only the surface texture but also the projected image is tracked by a camera, the proposed method is free from a calibration or position adjustment between the camera and projector. We designed fiducial patterns to be inserted into a fast flapping sequence of binary frames of the DMD projector, which allows the simultaneous tracking of the surface texture and a fiducial geometry separate from a single image captured by the camera. The proposed method implemented on a CPU runs at 400 fps and enables arbitrary video contents to be ‚Äústuck‚Äù onto a variety of textured surfaces.},  keywords={calibration;cameras;closed loop systems;image capture;image sensors;micromirrors;optical design techniques;optical images;optical projectors;photodetectors;surface texture;video signal processing;single image capture;fiducial geometry;surface texture tracking;direct closed-loop alignment approach;low-latency digital micromirror device projector;arbitrary video contents;DMD projector;fast flapping sequence;fiducial patterns;calibration;markerless planar surface;fast video projection mapping;animated stickies;Cameras;Target tracking;Surface texture;Calibration;Sensors;Optimization;Visualization;Spatial augmented reality;high-speed vision;projector-camera system;visual tracking},  doi={10.1109/TVCG.2019.2932248},  ISSN={1941-0506},  month={Nov},}


@ARTICLE{8794641,  author={N. {Gard} and A. {Hilsmann} and P. {Eisert}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Projection Distortion-based Object Tracking in Shader Lamp Scenarios},   year={2019},  volume={25},  number={11},  pages={3105-3113},  abstract={Shader lamp systems augment the real environment by projecting new textures on known target geometries. In dynamic scenes, object tracking maintains the illusion if the physical and virtual objects are well aligned. However, traditional trackers based on texture or contour information are often distracted by the projected content and tend to fail. In this paper, we present a model-based tracking strategy, which directly takes advantage from the projected content for pose estimation in a projector-camera system. An iterative pose estimation algorithm captures and exploits visible distortions caused by object movements. In a closed-loop, the corrected pose allows the update of the projection for the subsequent frame. Synthetic frames simulating the projection on the model are rendered and an optical flow-based method minimizes the difference between edges of the rendered and the camera image. Since the thresholds automatically adapt to the synthetic image, a complicated radiometric calibration can be avoided. The pixel-wise linear optimization is designed to be easily implemented on the GPU. Our approach can be combined with a regular contour-based tracker and is transferable to other problems, like the estimation of the extrinsic pose between projector and camera. We evaluate our procedure with real and synthetic images and obtain very precise registration results.},  keywords={calibration;cameras;image registration;image segmentation;image sequences;image texture;iterative methods;linear programming;object tracking;pose estimation;rendering (computer graphics);object movements;synthetic frames;optical flow-based method;camera image;synthetic image;regular contour-based tracker;projection distortion-based object tracking;shader lamp scenarios;shader lamp systems;dynamic scenes;physical objects;virtual objects;texture;model-based tracking strategy;projected content;projector-camera system;iterative pose estimation;Cameras;Three-dimensional displays;Pose estimation;Mathematical model;Calibration;Object tracking;Projector-camera systems;projector-camera calibration;shader lamp systems;object tracking;object registration;spatial augmented reality;projection mapping},  doi={10.1109/TVCG.2019.2932223},  ISSN={1941-0506},  month={Nov},}

@ARTICLE{8794584,  author={X. {Xia} and Y. {Guan} and A. {State} and P. {Chakravarthula} and K. {Rathinavel} and T. -J. {Cham} and H. {Fuchs}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Towards a Switchable AR/VR Near-eye Display with Accommodation-Vergence and Eyeglass Prescription Support},   year={2019},  volume={25},  number={11},  pages={3114-3124},  abstract={In this paper, we present our novel design for switchable AR/VR near-eye displays which can help solve the vergence-accommodation-conflict issue. The principal idea is to time-multiplex virtual imagery and real-world imagery and use a tunable lens to adjust focus for the virtual display and the see-through scene separately. With this novel design, prescription eyeglasses for near- and far-sighted users become unnecessary. This is achieved by integrating the wearer's corrective optical prescription into the tunable lens for both virtual display and see-through environment. We built a prototype based on the design, comprised of micro-display, optical systems, a tunable lens, and active shutters. The experimental results confirm that the proposed near-eye display design can switch between AR and VR and can provide correct accommodation for both.},  keywords={eye;eye protection;handicapped aids;lenses;patient care;virtual reality;AR-VR near-eye displays;optical systems;microdisplay;prescription eyeglasses;virtual display;real-world imagery;time-multiplex virtual imagery;eyeglass prescription support;near-eye display design;Three-dimensional displays;Lenses;Optical switches;Optical imaging;Holography;Holographic optical components;Optical distortion;Near-eye displays;Augmented reality;Virtual reality;Focus accommodation;Prescription correction;Augmented Reality;Computer Graphics;Equipment Design;Eyeglasses;Holography;Humans;Image Processing, Computer-Assisted;Virtual Reality},  doi={10.1109/TVCG.2019.2932238},  ISSN={1941-0506},  month={Nov},}

@ARTICLE{8827571,  author={K. {Rathinavel} and G. {Wetzstein} and H. {Fuchs}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Varifocal Occlusion-Capable Optical See-through Augmented Reality Display based on Focus-tunable Optics},   year={2019},  volume={25},  number={11},  pages={3125-3134},  abstract={Optical see-through augmented reality (AR) systems are a next-generation computing platform that offer unprecedented user experiences by seamlessly combining physical and digital content. Many of the traditional challenges of these displays have been significantly improved over the last few years, but AR experiences offered by today's systems are far from seamless and perceptually realistic. Mutually consistent occlusions between physical and digital objects are typically not supported. When mutual occlusion is supported, it is only supported for a fixed depth. We propose a new optical see-through AR display system that renders mutual occlusion in a depth-dependent, perceptually realistic manner. To this end, we introduce varifocal occlusion displays based on focus-tunable optics, which comprise a varifocal lens system and spatial light modulators that enable depth-corrected hard-edge occlusions for AR experiences. We derive formal optimization methods and closed-form solutions for driving this tunable lens system and demonstrate a monocular varifocal occlusion-capable optical see-through AR display capable of perceptually realistic occlusion across a large depth range.},  keywords={augmented reality;computer displays;lenses;optimisation;spatial light modulators;perceptually realistic occlusion;monocular varifocal occlusion-capable;tunable lens system;depth-corrected hard-edge occlusions;varifocal lens system;varifocal occlusion displays;perceptually realistic manner;AR display system;fixed depth;mutual occlusion;digital objects;physical objects;mutually consistent occlusions;AR experiences;digital content;physical content;focus-tunable optics;augmented reality display;Optical imaging;Adaptive optics;Optical design;Optical distortion;Lenses;Optical diffraction;Augmented Reality;Computational Displays;Varifocal Display;Occlusion},  doi={10.1109/TVCG.2019.2933120},  ISSN={1941-0506},  month={Nov},}


@ARTICLE{8794519,  author={T. {Randhavane} and A. {Bera} and K. {Kapsaskis} and K. {Gray} and D. {Manocha}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={FVA: Modeling Perceived Friendliness of Virtual Agents Using Movement Characteristics},   year={2019},  volume={25},  number={11},  pages={3135-3145},  abstract={We present a new approach for improving the friendliness and warmth of a virtual agent in an AR environment by generating appropriate movement characteristics. Our algorithm is based on a novel data-driven friendliness model that is computed using a user-study and psychological characteristics. We use our model to control the movements corresponding to the gaits, gestures, and gazing of friendly virtual agents (FVAs) as they interact with the user's avatar and other agents in the environment. We have integrated FVA agents with an AR environment using with a Microsoft HoloLens. Our algorithm can generate plausible movements at interactive rates to increase the social presence. We also investigate the perception of a user in an AR setting and observe that an FVA has a statistically significant improvement in terms of the perceived friendliness and social presence of a user compared to an agent without the friendliness modeling. We observe an increment of 5.71% in the mean responses to a friendliness measure and an improvement of 4.03% in the mean responses to a social presence measure.},  keywords={augmented reality;avatars;human computer interaction;multi-agent systems;psychology;social aspects of automation;AR environment;appropriate movement characteristics;novel data-driven friendliness model;user-study;psychological characteristics;friendly virtual agents;FVA agents;plausible movements;interactive rates;statistically significant improvement;friendliness modeling;friendliness measure;social presence measure;modeling perceived friendliness;virtual agent;Microsoft HoloLens;AR setting;Task analysis;Psychology;Computational modeling;Three-dimensional displays;Skeleton;Computer science;Avatars;Social perception;intelligent virtual agents;friendliness;gaits;gestures;gazing;Adult;Algorithms;Augmented Reality;Computer Graphics;Female;Fixation, Ocular;Friends;Gait;Gestures;Humans;Male;Models, Psychological;Movement;Social Perception;Virtual Reality},  doi={10.1109/TVCG.2019.2932235},  ISSN={1941-0506},  month={Nov},}


@ARTICLE{8798880,  author={T. {Feigl} and D. {Roth} and S. {Gradl} and M. {Wirth} and M. E. {Latoschik} and B. M. {Eskofier} and M. {Philippsen} and C. {Mutschler}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Sick Moves! Motion Parameters as Indicators of Simulator Sickness},   year={2019},  volume={25},  number={11},  pages={3146-3157},  abstract={We explore motion parameters, more specifically gait parameters, as an objective indicator to assess simulator sickness in Virtual Reality (VR). We discuss the potential relationships between simulator sickness, immersion, and presence. We used two different camera pose (position and orientation) estimation methods for the evaluation of motion tasks in a large-scale VR environment: a simple model and an optimized model that allows for a more accurate and natural mapping of human senses. Participants performed multiple motion tasks (walking, balancing, running) in three conditions: a physical reality baseline condition, a VR condition with the simple model, and a VR condition with the optimized model. We compared these conditions with regard to the resulting sickness and gait, as well as the perceived presence in the VR conditions. The subjective measures confirmed that the optimized pose estimation model reduces simulator sickness and increases the perceived presence. The results further show that both models affect the gait parameters and simulator sickness, which is why we further investigated a classification approach that deals with non-linear correlation dependencies between gait parameters and simulator sickness. We argue that our approach could be used to assess and predict simulator sickness based on human gait parameters and we provide implications for future research.},  keywords={gait analysis;pose estimation;virtual reality;sick moves;motion parameters;simulator sickness;large-scale VR environment;optimized model;multiple motion tasks;physical reality baseline condition;VR condition;perceived presence;optimized pose estimation model;human gait parameters;Legged locomotion;Solid modeling;Atmospheric measurements;Particle measurements;Computational modeling;Reliability;Pose estimation;Human-centered computing;virtual reality;user studies;computing methodologies;perception;machine learning;Adult;Algorithms;Computer Graphics;Female;Gait;Humans;Image Processing, Computer-Assisted;Machine Learning;Male;Models, Statistical;Motion Sickness;Movement;Psychomotor Performance;Virtual Reality;Young Adult},  doi={10.1109/TVCG.2019.2932224},  ISSN={1941-0506},  month={Nov},}

@ARTICLE{8794563,  author={N. L. {Williams} and T. C. {Peck}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Estimation of Rotation Gain Thresholds Considering FOV, Gender, and Distractors},   year={2019},  volume={25},  number={11},  pages={3158-3168},  abstract={Redirected walking techniques enable users to naturally locomote in virtual environments (VEs) that are larger than the tracked space. Redirected walking imperceptibly transforms the VE around the user with predefined estimated threshold gains. Previously estimated gains were evaluated with a 40¬∞ field of view (FOV), and have not been evaluated in the presence of a distractor-a moving object in the VE that may capture the user's attention. We conducted a 2 (FOV: 40¬∞, 110¬∞) √ó 2 (Gender: female, male) √ó 2 (Distractor: without, with) user study to estimate and compare thresholds for rotation gains. Significant differences in detection thresholds were found between FOVs, and significant differences were found between female and male gains with a 110¬∞ FOV. Males had significantly wider gains using a 110¬∞ FOV compared to a 40¬∞ FOV, and distractors affected females differently than males. Finally, strong correlations were found between simulator sickness scores and threshold gains.},  keywords={gait analysis;virtual reality;predefined estimated threshold gains;rotation gains;detection thresholds;male gains;distractors;redirected walking techniques;rotation gain thresholds;moving object;Visualization;Legged locomotion;Observers;Usability;Virtual environments;Stimulated emission;Virtual reality;Locomotion;Perception;Detection thresholds;Distractors;Gender differences;Simulator sickness;Adult;Computer Graphics;Female;Humans;Male;Middle Aged;Rotation;Task Performance and Analysis;Video Games;Virtual Reality;Visual Fields;Walking;Young Adult},  doi={10.1109/TVCG.2019.2932213},  ISSN={1941-0506},  month={Nov},}


@ARTICLE{8794644,  author={D. {Wolf} and M. {Rietzler} and L. {Hnatek} and E. {Rukzio}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Face/On: Multi-Modal Haptic Feedback for Head-Mounted Displays in Virtual Reality},   year={2019},  volume={25},  number={11},  pages={3169-3177},  abstract={While the real world provides humans with a huge variety of sensory stimuli, virtual worlds most of all communicate their properties by visual and auditory feedback due to the design of current head mounted displays (HMDs). Since HMDs offer sufficient contact area to integrate additional actuators, prior works utilised a limited amount of haptic actuators to integrate respective information about the virtual world. With the Face/On prototype complex feedback patterns are introduced that combine a high number of vibration motors with additional thermal sources to transport multi-modal and spatial information. A pre-study determining the boundaries of the feedbacks' intensities as well as a user study showing a significant increase of presence and enjoyment validate Face/On's approach.},  keywords={haptic interfaces;helmet mounted displays;virtual reality;head-mounted displays;virtual reality;sensory stimuli;visual feedback;auditory feedback;HMDs;haptic actuators;spatial information;multimodal haptic feedback;thermal sources;vibration motors;Face-On prototype complex feedback patterns;Haptic interfaces;Actuators;Resists;Face;Visualization;Skin;Virtual environments;VR;haptic feedback;multi-modal;thermal feedback;Adult;Computer Graphics;Equipment Design;Face;Feedback, Sensory;Female;Head;Humans;Male;Vibration;Virtual Reality;Young Adult},  doi={10.1109/TVCG.2019.2932215},  ISSN={1941-0506},  month={Nov},}


@ARTICLE{8799015,  author={A. {Irlitti} and T. {Piumsomboon} and D. {Jackson} and B. H. {Thomas}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Conveying spatial awareness cues in xR collaborations},   year={2019},  volume={25},  number={11},  pages={3178-3189},  abstract={Spatial Augmented Reality (SAR) systems can be suitably combined with other existing Extended Reality (xR) technologies to support collaboration. In existing strategies, users unencumbered by a viewing technology, such as a tablet interface or a head-mounted display, must rely on the transmission of their collaborators' positioning through interpreting a first-person camera view. This design creates a seam between a user's experience of the augmented physical environment in SAR, and their collaborators' experience inside the virtual environment. To assist in development and evaluation of spatial cues to support spatial awareness in SAR environments, an egocentric spatial-communication taxonomy is presented given two determining dimensions, a cue's attachment (physical/virtual) and animation (local/world). We developed four egocentric cues which characterize the four independent dimensions of the matrix: arrow, path, glow, and radial, and a single exocentric world in miniature visualization. Our study shows that virtual attachment cues are preferred, providing the highest accuracy, highest performance when collaborators are occluded, and produce the least mental effort when used with a single virtual collaborator. For multiple collaborators however, the virtual attached, world animated radial cue produces significant increases in mental load and reductions in preference, demonstrating the impact of visual augmentation clutter. The single exocentric visualization produced higher levels of head movement, and poorer accuracy, however the novelty of the visualization produced positive qualitative results.},  keywords={augmented reality;data visualisation;helmet mounted displays;spatial augmented reality systems;extended reality technologies;single exocentric visualization;visual augmentation clutter;world animated radial cue;multiple collaborators;single virtual collaborator;virtual attachment cues;single exocentric world;independent dimensions;egocentric cues;determining dimensions;spatial-communication taxonomy;SAR environments;spatial cues;virtual environment;augmented physical environment;first-person camera view;head-mounted display;tablet interface;viewing technology;xR collaborations;spatial awareness cues;Collaboration;Visualization;Taxonomy;Stakeholders;Spatial augmented reality;Virtual environments;Animation;Extended reality;spatial augmented reality;awareness;augmented reality;collaboration;virtual reality;Adult;Computer Graphics;Cues;Female;Head Movements;Humans;Male;Middle Aged;Video Games;Virtual Reality;Young Adult},  doi={10.1109/TVCG.2019.2932173},  ISSN={1941-0506},  month={Nov},}



@ARTICLE{8794572,  author={D. {Schneider} and A. {Otte} and T. {Gesslein} and P. {Gagel} and B. {Kuth} and M. S. {Damlakhi} and O. {Dietz} and E. {Ofek} and M. {Pahud} and P. O. {Kristensson} and J. {M√ºller} and J. {Grubert}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={ReconViguRation: Reconfiguring Physical Keyboards in Virtual Reality},   year={2019},  volume={25},  number={11},  pages={3190-3201},  abstract={Physical keyboards are common peripherals for personal computers and are efficient standard text entry devices. Recent research has investigated how physical keyboards can be used in immersive head-mounted display-based Virtual Reality (VR). So far, the physical layout of keyboards has typically been transplanted into VR for replicating typing experiences in a standard desktop environment. In this paper, we explore how to fully leverage the immersiveness of VR to change the input and output characteristics of physical keyboard interaction within a VR environment. This allows individual physical keys to be reconfigured to the same or different actions and visual output to be distributed in various ways across the VR representation of the keyboard. We explore a set of input and output mappings for reconfiguring the virtual presentation of physical keyboards and probe the resulting design space by specifically designing, implementing and evaluating nine VR-relevant applications: emojis, languages and special characters, application shortcuts, virtual text processing macros, a window manager, a photo browser, a whack-a-mole game, secure password entry and a virtual touch bar. We investigate the feasibility of the applications in a user study with 20 participants and find that, among other things, they are usable in VR. We discuss the limitations and possibilities of remapping the input and output characteristics of physical keyboards in VR based on empirical findings and analysis and suggest future research directions in this area.},  keywords={helmet mounted displays;keyboards;virtual reality;efficient standard text entry devices;physical keyboard interaction;VR environment;standard text entry devices;immersive head-mounted display-based virtual reality;standard desktop environment;VR representation;virtual text processing macros;window manager;photo browser;whack-a-mole game;secure password entry;virtual touch bar;emojis;languages;ReconViguRation;input-output mappings;Keyboards;Visualization;Virtual reality;Sensors;Tracking;Password;Task analysis;Virtual Reality;Text Entry;Physical Keyboards},  doi={10.1109/TVCG.2019.2932239},  ISSN={1941-0506},  month={Nov},}


@ARTICLE{8497080,  author={P. {Pjanic} and S. {Willi} and D. {Iwai} and A. {Grundh√∂fer}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Seamless Multi-Projection Revisited},   year={2018},  volume={24},  number={11},  pages={2963-2973},  abstract={This paper introduces a novel photometric compensation technique for inter-projector luminance and chrominance variations. Although it sounds as a classical technical issue, to the best of our knowledge there is no existing solution to alleviate the spatial non-uniformity among strongly heterogeneous projectors at perceptually acceptable quality. Primary goal of our method is increasing the perceived seamlessness of the projection system by automatically generating an improved and consistent visual quality. It builds upon the existing research of multi-projection systems, but instead of working with perceptually non-uniform color spaces such as CIEXYZ, the overall computation is carried out using the RLab [10, pp. 243-254] color appearance model which models the color processing in an adaptive, perceptual manner. Besides, we propose an adaptive color gamut acquisition, spatially varying gamut mapping, and optimization framework for edge blending. The paper describes the overall workflow and detailed algorithm of each component, followed by an evaluation validating the proposed method. The experimental results both qualitatively and quantitatively show the proposed method significant improved the visual quality of projected results of a multi-projection display with projectors with severely heterogeneous color processing.},  keywords={colour displays;image colour analysis;optical projectors;photometry;color appearance model;RLab color appearance model;heterogeneous color processing;heterogeneous projectors;multiprojection display;edge blending;optimization framework;gamut mapping;adaptive color gamut acquisition;nonuniform color spaces;consistent visual quality;chrominance variations;inter-projector luminance;photometric compensation technique;seamless multiprojection;Image color analysis;Color;Adaptation models;Computational modeling;Visualization;Optimization;Three-dimensional displays;Projector-camera systems;colorimetric calibration;3D stereoscopic and multi-user entertainment},  doi={10.1109/TVCG.2018.2868597},  ISSN={1941-0506},  month={Nov},}


@ARTICLE{8466021,  author={P. {Kurth} and V. {Lange} and C. {Siegl} and M. {Stamminger} and F. {Bauer}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Auto-Calibration for Dynamic Multi-Projection Mapping on Arbitrary Surfaces},   year={2018},  volume={24},  number={11},  pages={2886-2894},  abstract={The quality of every dynamic multi-projection mapping system is limited by the quality of the projector to tracking device calibration. Common problems with poor calibration result in noticeable artifacts for the user, such as ghosting and seams. In this work we introduce a new, fully automated calibration algorithm that is tailored to reduce these artifacts, based on consumer-grade hardware. We achieve this goal by repurposing a structured-light scanning setup. A structured-light scanner can generate 3D geometry based on a known intrinsic and extrinsic calibration of its components (projector and RGB camera). We revert this process by providing the resulting 3D model to determine the intrinsic and extrinsic parameters of our setup (including those of a variety of tracking systems). Our system matches features and solves for all parameters in a single pass while respecting the lower quality of our sensory input.},  keywords={calibration;optical projectors;optical scanners;tracking systems;dynamic multiprojection mapping system;consumer-grade hardware;structured-light scanning setup;automated calibration algorithm;3D geometry generation model;RGB camera;Cameras;Calibration;Target tracking;Three-dimensional displays;Distortion;Heuristic algorithms;Geometry;Calibration;SAR;multi-projection mapping;mixed reality},  doi={10.1109/TVCG.2018.2868530},  ISSN={1941-0506},  month={Nov},}



@ARTICLE{8493594,  author={B. {Volmer} and J. {Baumeister} and S. {Von Itzstein} and I. {Bornkessel-Schlesewsky} and M. {Schlesewsky} and M. {Billinghurst} and B. H. {Thomas}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={A Comparison of Predictive Spatial Augmented Reality Cues for Procedural Tasks},   year={2018},  volume={24},  number={11},  pages={2846-2856},  abstract={Previous research has demonstrated that Augmented Reality can reduce a user's task response time and mental effort when completing a procedural task. This paper investigates techniques to improve user performance and reduce mental effort by providing projector-based Spatial Augmented Reality predictive cues for future responses. The objective of the two experiments conducted in this study was to isolate the performance and mental effort differences from several different annotation cueing techniques for simple (Experiment 1) and complex (Experiment 2) button-pressing tasks. Comporting with existing cognitive neuroscience literature on prediction, attentional orienting, and interference, we hypothesized that for both simple procedural tasks and complex search-based tasks, having a visual cue guiding to the next task's location would positively impact performance relative to a baseline, no-cue condition. Additionally, we predicted that direction-based cues would provide a more significant positive impact than target-based cues. The results indicated that providing a line to the next task was the most effective technique for improving the users' task time and mental effort in both the simple and complex tasks.},  keywords={augmented reality;cognition;human computer interaction;complex tasks;predictive spatial augmented reality cues;procedural task;user performance;future responses;mental effort differences;complex button-pressing tasks;simple procedural tasks;complex search-based tasks;visual cue;no-cue condition;direction-based cues;target-based cues;projector-based spatial augmented reality predictive cues;Task analysis;Visualization;Resists;Monitoring;Augmented reality;Complexity theory;Maintenance engineering;Spatial augmented reality;predictive cue;mental effort;procedural task},  doi={10.1109/TVCG.2018.2868587},  ISSN={1941-0506},  month={Nov},}


@ARTICLE{8458443,  author={Y. -W. {Cha} and T. {Price} and Z. {Wei} and X. {Lu} and N. {Rewkowski} and R. {Chabra} and Z. {Qin} and H. {Kim} and Z. {Su} and Y. {Liu} and A. {Ilie} and A. {State} and Z. {Xu} and J. -M. {Frahm} and H. {Fuchs}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Towards Fully Mobile 3D Face, Body, and Environment Capture Using Only Head-worn Cameras},   year={2018},  volume={24},  number={11},  pages={2993-3004},  abstract={We propose a new approach for 3D reconstruction of dynamic indoor and outdoor scenes in everyday environments, leveraging only cameras worn by a user. This approach allows 3D reconstruction of experiences at any location and virtual tours from anywhere. The key innovation of the proposed ego-centric reconstruction system is to capture the wearer's body pose and facial expression from near-body views, e.g. cameras on the user's glasses, and to capture the surrounding environment using outward-facing views. The main challenge of the ego-centric reconstruction, however, is the poor coverage of the near-body views - that is, the user's body and face are observed from vantage points that are convenient for wear but inconvenient for capture. To overcome these challenges, we propose a parametric-model-based approach to user motion estimation. This approach utilizes convolutional neural networks (CNNs) for near-view body pose estimation, and we introduce a CNN-based approach for facial expression estimation that combines audio and video. For each time-point during capture, the intermediate model-based reconstructions from these systems are used to re-target a high-fidelity pre-scanned model of the user. We demonstrate that the proposed self-sufficient, head-worn capture system is capable of reconstructing the wearer's movements and their surrounding environment in both indoor and outdoor situations without any additional views. As a proof of concept, we show how the resulting 3D-plus-time reconstruction can be immersively experienced within a virtual reality system (e.g., the HTC Vive). We expect that the size of the proposed egocentric capture-and-reconstruction system will eventually be reduced to fit within future AR glasses, and will be widely useful for immersive 3D telepresence, virtual tours, and general use-anywhere 3D content creation.},  keywords={cameras;computer vision;convolution;face recognition;feedforward neural nets;image reconstruction;mobile computing;motion estimation;pose estimation;solid modelling;virtual reality;environment capture;head-worn cameras;dynamic indoor scenes;virtual tours;ego-centric reconstruction system;near-body views;parametric-model-based approach;user motion estimation;convolutional neural networks;CNN-based approach;facial expression estimation;intermediate model-based reconstructions;high-fidelity pre-scanned model;head-worn capture system;3D-plus-time reconstruction;virtual reality system;immersive 3D telepresence;use-anywhere 3D content creation;capture-and-reconstruction system;fully mobile 3D face capture;AR glasses;HTC Vive;near-view body pose estimation;Cameras;Three-dimensional displays;Image reconstruction;Face;Solid modeling;Pose estimation;Deformable models;Telepresence;Ego-centric Vision;Motion Capture;Convolutional Neural Networks;Facial Expression;Humans;Imaging, Three-Dimensional;Internet;Neural Networks, Computer;Posture;User-Computer Interface;Video Recording},  doi={10.1109/TVCG.2018.2868527},  ISSN={1941-0506},  month={Nov},}


@ARTICLE{8492363,  author={S. {Golodetz} and T. {Cavallari} and N. A. {Lord} and V. A. {Prisacariu} and D. W. {Murray} and P. H. S. {Torr}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Collaborative Large-Scale Dense 3D Reconstruction with Online Inter-Agent Pose Optimisation},   year={2018},  volume={24},  number={11},  pages={2895-2905},  abstract={Reconstructing dense, volumetric models of real-world 3D scenes is important for many tasks, but capturing large scenes can take significant time, and the risk of transient changes to the scene goes up as the capture time increases. These are good reasons to want instead to capture several smaller sub-scenes that can be joined to make the whole scene. Achieving this has traditionally been difficult: joining sub-scenes that may never have been viewed from the same angle requires a high-quality camera relocaliser that can cope with novel poses, and tracking drift in each sub-scene can prevent them from being joined to make a consistent overall scene. Recent advances, however, have significantly improved our ability to capture medium-sized sub-scenes with little to no tracking drift: real-time globally consistent reconstruction systems can close loops and re-integrate the scene surface on the fly, whilst new visual-inertial odometry approaches can significantly reduce tracking drift during live reconstruction. Moreover, high-quality regression forest-based relocalisers have recently been made more practical by the introduction of a method to allow them to be trained and used online. In this paper, we leverage these advances to present what to our knowledge is the first system to allow multiple users to collaborate interactively to reconstruct dense, voxel-based models of whole buildings using only consumer-grade hardware, a task that has traditionally been both time-consuming and dependent on the availability of specialised hardware. Using our system, an entire house or lab can be reconstructed in under half an hour and at a far lower cost than was previously possible.},  keywords={cameras;distance measurement;image reconstruction;optimisation;pose estimation;regression analysis;large-scale dense 3D reconstruction;online inter-agent pose optimisation;volumetric models;high-quality camera relocaliser;tracking drift;medium-sized sub-scenes;high-quality regression forest-based relocalisers;visual-inertial odometry approach;voxel-based model;consumer-grade hardware;Robots;Collaboration;Three-dimensional displays;Cameras;Buildings;Visualization;Hardware;Collaborative;large-scale;dense 3D reconstruction;inter-agent relocalisation;pose graph optimisation},  doi={10.1109/TVCG.2018.2868533},  ISSN={1941-0506},  month={Nov},}



@ARTICLE{8456525,  author={F. {Bork} and C. {Schnelzer} and U. {Eck} and N. {Navab}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Towards Efficient Visual Guidance in Limited Field-of-View Head-Mounted Displays},   year={2018},  volume={24},  number={11},  pages={2983-2992},  abstract={Understanding, navigating, and performing goal-oriented actions in Mixed Reality (MR) environments is a challenging task and requires adequate information conveyance about the location of all virtual objects in a scene. Current Head-Mounted Displays (HMDs) have a limited field-of-view where augmented objects may be displayed. Furthermore, complex MR environments may be comprised of a large number of objects which can be distributed in the extended surrounding space of the user. This paper presents two novel techniques for visually guiding the attention of users towards out-of-view objects in HMD-based MR: the 3D Radar and the Mirror Ball. We evaluate our approaches against existing techniques during three different object collection scenarios, which simulate real-world exploratory and goal-oriented visual search tasks. To better understand how the different visualizations guide the attention of users, we analyzed the head rotation data for all techniques and introduce a novel method to evaluate and classify head rotation trajectories. Our findings provide supporting evidence that the type of visual guidance technique impacts the way users search for virtual objects in MR.},  keywords={augmented reality;helmet mounted displays;field-of-view head-mounted displays;Mixed Reality environments;virtual objects;augmented objects;complex MR environments;out-of-view objects;real-world exploratory;goal-oriented visual search tasks;head rotation data;head rotation trajectories;visual guidance technique;visual guidance;information conveyance;Head-Mounted Displays;visualizations;object collection scenarios;goal-oriented actions;Three-dimensional displays;Trajectory;Virtual reality;Two dimensional displays;Radar;Visualization;Task analysis;Mixed / Augmented reality;Visualization design and evaluation methods;Adult;Algorithms;Attention;Computer Graphics;Female;Head;Humans;Imaging, Three-Dimensional;Male;Middle Aged;Virtual Reality;Young Adult},  doi={10.1109/TVCG.2018.2868584},  ISSN={1941-0506},  month={Nov},}


@ARTICLE{8456571,  author={L. {Qian} and A. {Plopski} and N. {Navab} and P. {Kazanzides}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Restoring the Awareness in the Occluded Visual Field for Optical See-Through Head-Mounted Displays},   year={2018},  volume={24},  number={11},  pages={2936-2946},  abstract={Recent technical advancements support the application of Optical See-Through Head-Mounted Displays (OST-HMDs) in critical situations like navigation and manufacturing. However, while the form-factor of an OST-HMD occupies less of the user's visual field than in the past, it can still result in critical oversights, e.g., missing a pedestrian while driving a car. In this paper, we design and compare two methods to compensate for the loss of awareness due to the occlusion caused by OST-HMDs. Instead of presenting the occluded content to the user, we detect motion that is not visible to the user and highlight its direction either on the edge of the HMD screen, or by activating LEDs placed in the user's peripheral vision. The methods involve an offline stage, where the occluded visual field and location of each indicator and its associated occluded region of interest (OROI) are determined, and an online stage, where an enhanced optical flow algorithm tracks the motion in the occluded visual field. We have implemented both methods on a Microsoft HoloLens and an ODG R-9. Our prototype systems achieved success rates of 100% in an objective evaluation, and 98.90% in a pilot user study. Our methods are able to compensate for the loss of safety-critical information in the occluded visual field for state-of-the-art OST-HMDs and can be extended for their future generations.},  keywords={helmet mounted displays;image sequences;LED displays;optical see-through head-mounted displays;state-of-the-art OST-HMD;HMD screen;user peripheral vision;occluded region of interest;optical flow algorithm;Microsoft HoloLens;ODG R-9;safety-critical information;enhanced optical flow algorithm;associated occluded region;OST-HMD;occluded visual field;Head-mounted displays;Optical distortion;Light emitting diodes;Context awareness;Image edge detection;Location awareness;View Expansion;Prototype;Optical See-Through Head-Mounted Display;Algorithms;Computer Graphics;Equipment Design;Head;Humans;Image Processing, Computer-Assisted;User-Computer Interface;Virtual Reality;Visual Fields},  doi={10.1109/TVCG.2018.2868559},  ISSN={1941-0506},  month={Nov},}



@ARTICLE{8456852,  author={K. {Rathinavel} and H. {Wang} and A. {Blate} and H. {Fuchs}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={An Extended Depth-at-Field Volumetric Near-Eye Augmented Reality Display},   year={2018},  volume={24},  number={11},  pages={2857-2866},  abstract={We introduce an optical design and a rendering pipeline for a full-color volumetric near-eye display which simultaneously presents imagery with near-accurate per-pixel focus across an extended volume ranging from 15cm (6.7 diopters) to 4M (0.25 diopters), allowing the viewer to accommodate freely across this entire depth range. This is achieved using a focus-tunable lens that continuously sweeps a sequence of 280 synchronized binary images from a high-speed, Digital Micromirror Device (DMD) projector and a high-speed, high dynamic range (HDR) light source that illuminates the DMD images with a distinct color and brightness at each binary frame. Our rendering pipeline converts 3-D scene information into a 2-D surface of color voxels, which are decomposed into 280 binary images in a voxel-oriented manner, such that 280 distinct depth positions for full-color voxels can be displayed.},  keywords={augmented reality;display instrumentation;image colour analysis;image sequences;lenses;micromirrors;optical focusing;optical projectors;rendering (computer graphics);depth-at-field volumetric near-eye augmented reality display;optical design;focus-tunable lens;high dynamic range light source;DMD images;binary frame;full-color voxels;voxel-oriented manner;3-D scene information;HDR;synchronized binary images sequence;full-color volumetric near-eye display;distinct depth positions;rendering pipeline;digital micromirror device projector;Three-dimensional displays;Lenses;Rendering (computer graphics);Pipelines;Light sources;Image color analysis;Hardware;Near-Eye Displays;Augmented Reality;Rendering Pipeline;Algorithms;Computer Graphics;Equipment Design;Imaging, Three-Dimensional;Photography;Video Recording;Virtual Reality},  doi={10.1109/TVCG.2018.2868570},  ISSN={1941-0506},  month={Nov},}


@ARTICLE{8458263,  author={P. {Chakravarthula} and D. {Dunn} and K. {Ak≈üit} and H. {Fuchs}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={FocusAR: Auto-focus Augmented Reality Eyeglasses for both Real World and Virtual Imagery},   year={2018},  volume={24},  number={11},  pages={2906-2916},  abstract={We describe a system which corrects dynamically for the focus of the real world surrounding the near-eye display of the user and simultaneously the internal display for augmented synthetic imagery, with an aim of completely replacing the user prescription eyeglasses. The ability to adjust focus for both real and virtual stimuli will be useful for a wide variety of users, but especially for users over 40 years of age who have limited accommodation range. Our proposed solution employs a tunable-focus lens for dynamic prescription vision correction, and a varifocal internal display for setting the virtual imagery at appropriate spatially registered depths. We also demonstrate a proof of concept prototype to verify our design and discuss the challenges to building an auto-focus augmented reality eyeglasses for both real and virtual.},  keywords={augmented reality;eye;helmet mounted displays;lenses;medical computing;ophthalmic lenses;optical focusing;optical tuning;autofocus augmented reality eyeglasses;real world imagery;varifocal internal display;dynamic prescription vision correction;tunable-focus lens;accommodation range;virtual stimuli;user prescription eyeglasses;augmented synthetic imagery;near-eye display;virtual imagery;FocusAR;Lenses;Meters;Liquids;Prototypes;Augmented reality;Glass;Apertures;Augmented Reality;Displays;Auto-focus;Focus accommodation;Prescription correction;Adult;Computer Graphics;Eyeglasses;Humans;Image Processing, Computer-Assisted;User-Computer Interface;Virtual Reality},  doi={10.1109/TVCG.2018.2868532},  ISSN={1941-0506},  month={Nov},}


@ARTICLE{8466636,  author={T. {Piumsomboon} and G. A. {Lee} and B. {Ens} and B. H. {Thomas} and M. {Billinghurst}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Superman vs Giant: A Study on Spatial Perception for a Multi-Scale Mixed Reality Flying Telepresence Interface},   year={2018},  volume={24},  number={11},  pages={2974-2982},  abstract={The advancements in Mixed Reality (MR), Unmanned Aerial Vehicle, and multi-scale collaborative virtual environments have led to new interface opportunities for remote collaboration. This paper explores a novel concept of flying telepresence for multi-scale mixed reality remote collaboration. This work could enable remote collaboration at a larger scale such as building construction. We conducted a user study with three experiments. The first experiment compared two interfaces, static and dynamic IPD, on simulator sickness and body size perception. The second experiment tested the user perception of a virtual object size under three levels of IPD and movement gain manipulation with a fixed eye height in a virtual environment having reduced or rich visual cues. Our last experiment investigated the participant's body size perception for two levels of manipulation of the IPDs and heights using stereo video footage to simulate a flying telepresence experience. The studies found that manipulating IPDs and eye height influenced the user's size perception. We present our findings and share the recommendations for designing a multi-scale MR flying telepresence interface.},  keywords={autonomous aerial vehicles;groupware;human computer interaction;mobile robots;telerobotics;virtual reality;spatial perception;multiscale collaborative virtual environments;unmanned aerial vehicle;stereo video footage;interpupillary distance;multiscale MR flying telepresence interface;movement gain manipulation;virtual object size;user perception;body size perception;simulator sickness;dynamic IPD;static IPD;multiscale mixed reality remote collaboration;Collaboration;Telepresence;Avatars;Navigation;Magnetic heads;Virtual environments;Mixed reality;remote collaboration;flying telepresence;multi-scale collaborative environment;Adult;Algorithms;Computer Graphics;Female;Humans;Imaging, Three-Dimensional;Male;Space Perception;Spatial Navigation;User-Computer Interface;Virtual Reality;Young Adult},  doi={10.1109/TVCG.2018.2868594},  ISSN={1941-0506},  month={Nov},}


@ARTICLE{8456570,  author={D. {Yu} and K. {Fan} and H. {Zhang} and D. {Monteiro} and W. {Xu} and H. {Liang}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={PizzaText: Text Entry for Virtual Reality Systems Using Dual Thumbsticks},   year={2018},  volume={24},  number={11},  pages={2927-2935},  abstract={We present PizzaText, a circular keyboard layout technique for text entry in virtual reality (VR) environments that uses the dual thumbsticks of a hand-held game controller. Text entry is a common activity in VR environments but remains challenging with existing techniques and keyboard layouts that is largely based on QWERTY. Our technique makes text entry simple, easy, and efficient, even for novice users. The technique uses a hand-held controller because it is still an important input device for users to interact with VR environments. To allow rapid search of characters, PizzaText divides a circle into slices and each slice contains 4 characters. To enable fast selection, the user uses the right thumbstick for traversing the slices, and the left thumbstick for choosing the letters. The design of PizzaText is based on three criteria: efficiency, learnability, and ease-of-use. In our first study, six potential layouts are considered and evaluated. The results lead to a design with 7 slices and 4 letters per slice. The final design is evaluated in a five-day study with 10 participants. The results show that novice users can achieve an average of 8.59 Words per Minute (WPM), while expert users are able to reach 15.85 WPM, with just two hours of training.},  keywords={computer games;keyboards;virtual reality;virtual reality systems;dual thumbsticks;circular keyboard layout technique;virtual reality environments;hand-held game controller;VR environments;keyboard layouts;hand-held controller;text entry;PizzaText;QWERTY;Layout;Keyboards;Games;Training;Virtual reality;Google;Fans;Virtual reality;text entry;game controller;dual-joystick input;selection keyboard;circular keyboard layout},  doi={10.1109/TVCG.2018.2868581},  ISSN={1941-0506},  month={Nov},}




@ARTICLE{8456568,  author={K. {Kim} and M. {Billinghurst} and G. {Bruder} and H. B. {Duh} and G. F. {Welch}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Revisiting Trends in Augmented Reality Research: A Review of the 2nd Decade of ISMAR (2008‚Äì2017)},   year={2018},  volume={24},  number={11},  pages={2947-2962},  abstract={In 2008, Zhou et al. presented a survey paper summarizing the previous ten years of ISMAR publications, which provided invaluable insights into the research challenges and trends associated with that time period. Ten years later, we review the research that has been presented at ISMAR conferences since the survey of Zhou et al., at a time when both academia and the AR industry are enjoying dramatic technological changes. Here we consider the research results and trends of the last decade of ISMAR by carefully reviewing the ISMAR publications from the period of 2008-2017, in the context of the first ten years. The numbers of papers for different research topics and their impacts by citations were analyzed while reviewing them-which reveals that there is a sharp increase in AR evaluation and rendering research. Based on this review we offer some observations related to potential future research areas or trends, which could be helpful to AR researchers and industry members looking ahead.},  keywords={augmented reality;rendering (computer graphics);ISMAR publications;ISMAR conferences;augmented reality;rendering;International Symposium on Mixed and Augmented Reality;Market research;Rendering (computer graphics);Calibration;Augmented reality;Industries;Sensors;Indexes;Augmented reality;mixed reality;survey;trends},  doi={10.1109/TVCG.2018.2868591},  ISSN={1941-0506},  month={Nov},}


@ARTICLE{8466859,  author={C. {Merenda} and H. {Kim} and K. {Tanous} and J. L. {Gabbard} and B. {Feichtl} and T. {Misu} and C. {Suga}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Augmented Reality Interface Design Approaches for Goal-directed and Stimulus-driven Driving Tasks},   year={2018},  volume={24},  number={11},  pages={2875-2885},  abstract={The automotive industry is rapidly developing new in-vehicle technologies that can provide drivers with information to aid awareness and promote quicker response times. Particularly, vehicles with augmented reality (AR) graphics delivered via head-up displays (HUDs) are nearing mainstream commercial feasibility and will be widely implemented over the next decade. Though AR graphics have been shown to provide tangible benefits to drivers in scenarios like forward collision warnings and navigation, they also create many new perceptual and sensory issues for drivers. For some time now, designers have focused on increasing the realism and quality of virtual graphics delivered via HUDs, and recently have begun testing more advanced 3D HUD systems that deliver volumetric spatial information to drivers. However, the realization of volumetric graphics adds further complexity to the design and delivery of AR cues, and moreover, parameters in this new design space must be clearly and operationally defined and explored. In this work, we present two user studies that examine how driver performance and visual attention are affected when using fixed and animated AR HUD interface design approaches in driving scenarios that require top-down and bottom-up cognitive processing. Results demonstrate that animated design approaches can produce some driving gains (e.g., in goal-directed navigation tasks) but often come at the cost of response time and distance. Our discussion yields AR HUD design recommendations and challenges some of the existing assumptions of world-fixed conformal graphic approaches to design.},  keywords={augmented reality;cognition;computer animation;driver information systems;head-up displays;user interfaces;augmented reality interface design approaches;driving tasks;automotive industry;head-up displays;HUDs;AR graphics;sensory issues;virtual graphics;volumetric spatial information;volumetric graphics;driver performance;AR HUD interface design approaches;animated design approaches;vehicle technologies;graphic approaches;3D HUD systems;bottom-up cognitive processing;top-down cognitive processing;Vehicles;Task analysis;Visualization;Navigation;Roads;Augmented reality;Mixed-reality;augmented reality;driving;head-up displays},  doi={10.1109/TVCG.2018.2868531},  ISSN={1941-0506},  month={Nov},}


@ARTICLE{8462799,  author={C. {Reichherzer} and A. {Cunningham} and J. {Walsh} and M. {Kohler} and M. {Billinghurst} and B. H. {Thomas}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Narrative and Spatial Memory for Jury Viewings in a Reconstructed Virtual Environment},   year={2018},  volume={24},  number={11},  pages={2917-2926},  abstract={This paper showcases one way of how virtual reconstruction can be used in a courtroom. The results of a pilot study on narrative and spatial memory are presented in the context of viewing real and virtual copies of a simulated crime scene. Based on current court procedures, three different viewing options were compared: photographs, a real life visit, and a 3D virtual reconstruction of the scene viewed in a Virtual Reality headset. Participants were also given a written narrative that included the spatial locations of stolen goods and were measured on their ability to recall and understand these spatial relationships of those stolen items. The results suggest that Virtual Reality is more reliable for spatial memory compared to photographs and that Virtual Reality provides a compromise for when physical viewing of crime scenes are not possible. We conclude that Virtual Reality is a promising medium for the court.},  keywords={criminal law;forensic science;virtual reality;virtual reconstruction;written narrative;spatial locations;spatial relationships;spatial memory;photographs;physical viewing;jury viewings;virtual copies;current court procedures;virtual reality headset;crime scene simulation;narrative memory;virtual environment reconstruction;viewing options;courtroom;Visualization;Three-dimensional displays;Law enforcement;Virtual environments;Image reconstruction;Videos;Virtual Reality;virtual environments;narrative memory;spatial memory;crime scene viewing;Adult;Crime;Female;Humans;Imaging, Three-Dimensional;Jurisprudence;Male;Mental Recall;Middle Aged;Narration;Spatial Memory;User-Computer Interface;Virtual Reality;Young Adult},  doi={10.1109/TVCG.2018.2868569},  ISSN={1941-0506},  month={Nov},}


@ARTICLE{8457524,  author={A. {Ibrahim} and B. {Huynh} and J. {Downey} and T. {H√∂llerer} and D. {Chun} and J. {O'donovan}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={ARbis Pictus: A Study of Vocabulary Learning with Augmented Reality},   year={2018},  volume={24},  number={11},  pages={2867-2874},  abstract={We conducted a fundamental user study to assess potential benefits of AR technology for immersive vocabulary learning. With the idea that AR systems will soon be able to label real-world objects in any language in real time, our within-subjects (N=52) lab-based study explores the effect of such an AR vocabulary prompter on participants learning nouns in an unfamiliar foreign language, compared to a traditional flashcard-based learning approach. Our results show that the immersive AR experience of learning with virtual labels on real-world objects is both more effective and more enjoyable for the majority of participants, compared to flashcards. Specifically, when participants learned through augmented reality, they scored significantly better on both same-day and 4-day delayed productive recall tests than when they learned using the flashcard method. We believe this result is an indication of the strong potential for language learning in augmented reality, particularly because of the improvement shown in sustained recall compared to the traditional approach.},  keywords={augmented reality;computer aided instruction;natural language processing;vocabulary;augmented reality;flashcard method;language learning;ARbis pictus;AR technology;immersive vocabulary learning;AR vocabulary prompter;virtual labels;foreign language;flashcard-based learning;Augmented reality;Vocabulary;Education;Task analysis;Games;Labeling;Language learning;education;augmented reality;HCl;experimentation},  doi={10.1109/TVCG.2018.2868568},  ISSN={1941-0506},  month={Nov},}