@ARTICLE{7523389,  author={S. {Willi} and A. {Grundh√∂fer}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Spatio-Temporal Point Path Analysis and Optimization of a Galvanoscopic Scanning Laser Projector},   year={2016},  volume={22},  number={11},  pages={2377-2384},  abstract={Galvanoscopic scanning laser projectors are powerful vector graphic devices offering a tremendous local brightness advantage compared to standard video projection systems. However, such devices have inherent problems, such as temporal flicker and spatially inaccurate rendering. We propose a method to generate an accurate point-based projection with such devices. To overcome the mentioned problems, we present a camera-based method to automatically analyze the laser projector's motion behavior. With this information, a model database is generated that is used to optimize the scanning path of projected point sequences. The optimization considers the overall path length, its angular shape, acceleration behavior, and the spatio-temporal point neighborhood. The method minimizes perceived visual flickering while guaranteeing an accurate spatial point projection at the same time. Comparisons and timing measurements prove the effectiveness of our method. An informal user evaluation shows substantial visual quality improvement as well.},  keywords={brightness;computer graphic equipment;image motion analysis;laser beam applications;minimisation;optical projectors;optical scanners;spatiotemporal phenomena;spatiotemporal point path analysis;spatiotemporal point path optimization;galvanoscopic scanning laser projector;vector graphic devices;local brightness;point-based projection;camera-based method;automatic motion behavior analysis;model database;scanning path optimization;point sequences;overall path length;angular shape;acceleration behavior;spatiotemporal point neighborhood;perceived visual flickering minimization;timing measurements;informal user evaluation;visual quality improvement;Laser modes;Calibration;Cameras;Mirrors;Optimization;Analytical models;Projector-camera systems;Calibration and registration of sensing systems;Display hardware;including 3D;stereoscopic and multi-user Entertainment;broadcast},  doi={10.1109/TVCG.2016.2593766},  ISSN={1941-0506},  month={Nov},}


@ARTICLE{7523411,  author={A. {Bapat} and E. {Dunn} and J. {Frahm}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Towards Kilo-Hertz 6-DoF Visual Tracking Using an Egocentric Cluster of Rolling Shutter Cameras},   year={2016},  volume={22},  number={11},  pages={2358-2367},  abstract={To maintain a reliable registration of the virtual world with the real world, augmented reality (AR) applications require highly accurate, low-latency tracking of the device. In this paper, we propose a novel method for performing this fast 6-DOF head pose tracking using a cluster of rolling shutter cameras. The key idea is that a rolling shutter camera works by capturing the rows of an image in rapid succession, essentially acting as a high-frequency 1D image sensor. By integrating multiple rolling shutter cameras on the AR device, our tracker is able to perform 6-DOF markerless tracking in a static indoor environment with minimal latency. Compared to state-of-the-art tracking systems, this tracking approach performs at significantly higher frequency, and it works in generalized environments. To demonstrate the feasibility of our system, we present thorough evaluations on synthetically generated data with tracking frequencies reaching 56.7 kHz. We further validate the method's accuracy on real-world images collected from a prototype of our tracking system against ground truth data using standard commodity GoPro cameras capturing at 120 Hz frame rate.},  keywords={augmented reality;image registration;image sensors;object tracking;pose estimation;kilo-hertz 6-DoF visual tracking;egocentric cluster;multiple rolling shutter camera integration;virtual world;augmented reality applications;low-latency device tracking;6-DOF head pose tracking;high-frequency 1D image sensor;6-DOF markerless tracking;standard commodity GoPro cameras;frame rate;frequency 56.7 kHz;frequency 120 Hz;Tracking;Cameras;Visualization;Performance evaluation;Simultaneous localization and mapping;Light emitting diodes;High frequency;Visual inside-out tracking;Rolling shutter},  doi={10.1109/TVCG.2016.2593757},  ISSN={1941-0506},  month={Nov},}


@ARTICLE{7523400,  author={K. {Gupta} and G. A. {Lee} and M. {Billinghurst}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Do You See What I See? The Effect of Gaze Tracking on Task Space Remote Collaboration},   year={2016},  volume={22},  number={11},  pages={2413-2422},  abstract={We present results from research exploring the effect of sharing virtual gaze and pointing cues in a wearable interface for remote collaboration. A local worker wears a Head-mounted Camera, Eye-tracking camera and a Head-Mounted Display and shares video and virtual gaze information with a remote helper. The remote helper can provide feedback using a virtual pointer on the live video view. The prototype system was evaluated with a formal user study. Comparing four conditions, (1) NONE (no cue), (2) POINTER, (3) EYE-TRACKER and (4) BOTH (both pointer and eye-tracker cues), we observed that the task completion performance was best in the BOTH condition with a significant difference of POINTER and EYETRACKER individually. The use of eye-tracking and a pointer also significantly improved the co-presence felt between the users. We discuss the implications of this research and the limitations of the developed system that could be improved in further work.},  keywords={cameras;gaze tracking;helmet mounted displays;gaze tracking;task space remote collaboration;wearable interface;remote collaboration;head-mounted camera;eye-tracking camera;head-mounted display;virtual gaze information;virtual pointer;Collaboration;Cameras;Teleconferencing;Prototypes;Head;Gaze tracking;Computers;Computer-supported collaborative work;Computer conferencing;teleconferencing;videoconferencing},  doi={10.1109/TVCG.2016.2593778},  ISSN={1941-0506},  month={Nov},}



@ARTICLE{7523376,  author={T. {Langlotz} and M. {Cook} and H. {Regenbrecht}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Real-Time Radiometric Compensation for Optical See-Through Head-Mounted Displays},   year={2016},  volume={22},  number={11},  pages={2385-2394},  abstract={Optical see-through head-mounted displays are currently seeing a transition out of research labs towards the consumer-oriented market. However, whilst availability has improved and prices have decreased, the technology has not matured much. Most commercially available optical see-through head mounted displays follow a similar principle and use an optical combiner blending the physical environment with digital information. This approach yields problems as the colors for the overlaid digital information can not be correctly reproduced. The perceived pixel colors are always a result of the displayed pixel color and the color of the current physical environment seen through the head-mounted display. In this paper we present an initial approach for mitigating the effect of color-blending in optical see-through head-mounted displays by introducing a real-time radiometric compensation. Our approach is based on a novel prototype for an optical see-through head-mounted display that allows the capture of the current environment as seen by the user's eye. We present three different algorithms using this prototype to compensate color blending in real-time and with pixel-accuracy. We demonstrate the benefits and performance as well as the results of a user study. We see application for all common Augmented Reality scenarios but also for other areas such as Diminished Reality or supporting color-blind people.},  keywords={augmented reality;helmet mounted displays;image colour analysis;radiometry;optical see-through head-mounted displays;consumer-oriented market;optical combiner blending;physical environment;digital information;color problem;overlaid digital information;perceived pixel colors;color-blending;real-time radiometric compensation;pixel-accuracy;augmented reality scenario;diminished reality;color-blind people;Image color analysis;Adaptive optics;Optical imaging;Prototypes;Radiometry;Cameras;Real-time systems;Radiosity;global illumination;constant time},  doi={10.1109/TVCG.2016.2593781},  ISSN={1941-0506},  month={Nov},}


@ARTICLE{7523375,
  author={Y. {Itoh} and T. {Amano} and D. {Iwai} and G. {Klinker}},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={Gaussian Light Field: Estimation of Viewpoint-Dependent Blur for Optical See-Through Head-Mounted Displays}, 
  year={2016},
  volume={22},
  number={11},
  pages={2368-2376},
  abstract={We propose a method to calibrate viewpoint-dependent, channel-wise image blur of near-eye displays, especially of Optical See-Through Head-Mounted Displays (OST-HMDs). Imperfections in HMD optics cause channel-wise image shift and blur that degrade the image quality of the display at a user's viewpoint. If we can estimate such characteristics perfectly, we could mitigate the effect by applying correction techniques from the computational photography in computer vision as analogous to cameras. Unfortunately, directly applying existing calibration techniques of cameras to OST-HMDs is not a straightforward task. Unlike ordinary imaging systems, image blur in OST-HMDs is viewpoint-dependent, i.e., the optical characteristic of a display dynamically changes depending on the current viewpoint of the user. This constraint makes the problem challenging since we must measure image blur of an HMD, ideally, over the entire 3D eyebox in which a user can see an image. To overcome this problem, we model the viewpoint-dependent blur as a Gaussian Light Field (GLF) that stores spatial information of the display screen as a (4D) light field with depth information and the blur as point-spread functions in the form of Gaussian kernels, respectively. We first describe both our GLF model and a calibration procedure to learn a GLF for a given OST-HMD. We then apply our calibration method to two HMDs that use different optics: a cubic prism or holographic gratings. The results show that our method achieves significantly better accuracy in Point-Spread Function (PSF) estimations with an accuracy about 2 to 7 dB in Peak SNR.},
  keywords={calibration;computer vision;Gaussian processes;helmet mounted displays;holographic displays;holographic gratings;image restoration;optical prisms;optical transfer function;screens (display);Gaussian light field;viewpoint-dependent blur estimation;optical see-through head-mounted displays;viewpoint dependent channel-wise image blur calibration;near-eye displays;OST-HMD;HMD optics;channel-wise image shift;image quality degradation;correction techniques;computational photography;computer vision;optical characteristics;3D eyebox;spatial information;display screen;4D light field;depth information;point-spread functions;GLF model;cubic prism;holographic gratings;PSF estimations;Optical imaging;Adaptive optics;Optical distortion;Calibration;Cameras;Holography;OST-HMD;calibration;optical see-through;chromatic aberration;point-spread functions;light field},
  doi={10.1109/TVCG.2016.2593779},
  ISSN={1941-0506},
  month={Nov},}
  

  
  
  @ARTICLE{7523447,  author={Y. {Kim} and H. {Park} and S. {Bang} and S. {Lee}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Retargeting Human-Object Interaction to Virtual Avatars},   year={2016},  volume={22},  number={11},  pages={2405-2412},  abstract={In augmented reality (AR) applications, a virtual avatar serves as a useful medium to represent a human in a different place. This paper deals with the problem of retargeting a human motion to an avatar. In particular, we present a novel method that retargets a human motion with respect to an object to that of an avatar with respect to a different object of a similar shape. To achieve this, we developed a spatial map that defines the correspondences between any points in the 3D spaces around the respective objects. The key advantage of the spatial map is that it identifies the desired locations of the avatar's body parts for any input motion of a human. Once the spatial map is created offline, the motion retargeting can be performed in real-time. The retargeted motion preserves important features of the original motion such as the human pose and the spatial relation with the object. We report the results of a number of experiments that demonstrate the effectiveness of the proposed method.},  keywords={augmented reality;avatars;human-object interaction retargeting;virtual avatars;augmented reality;AR applications;human motion retargeting;Avatars;Shape;Three-dimensional displays;Trajectory;Mesh generation;Real-time systems;Geometry;Motion retargeting;human-object interaction;augmented reality;telepresence;avatar animation},  doi={10.1109/TVCG.2016.2593780},  ISSN={1941-0506},  month={Nov},}
  
  
  
  @ARTICLE{7523388,  author={F. {Rameau} and H. {Ha} and K. {Joo} and J. {Choi} and K. {Park} and I. S. {Kweon}},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={A Real-Time Augmented Reality System to See-Through Cars},   year={2016},  volume={22},  number={11},  pages={2395-2404},  abstract={One of the most hazardous driving scenario is the overtaking of a slower vehicle, indeed, in this case the front vehicle (being overtaken) can occlude an important part of the field of view of the rear vehicle's driver. This lack of visibility is the most probable cause of accidents in this context. Recent research works tend to prove that augmented reality applied to assisted driving can significantly reduce the risk of accidents. In this paper, we present a real-time marker-less system to see through cars. For this purpose, two cars are equipped with cameras and an appropriate wireless communication system. The stereo vision system mounted on the front car allows to create a sparse 3D map of the environment where the rear car can be localized. Using this inter-car pose estimation, a synthetic image is generated to overcome the occlusion and to create a seamless see-through effect which preserves the structure of the scene.},  keywords={augmented reality;driver information systems;pose estimation;stereo image processing;real-time augmented reality system;see-through cars;assisted driving;real-time marker-less system;wireless communication system;sparse 3D map;inter-car pose estimation;synthetic image;stereo vision system;Automobiles;Three-dimensional displays;Cameras;Accidents;Delays;Real-time systems;Augmented reality;collaborative vehicle;see-through},  doi={10.1109/TVCG.2016.2593768},  ISSN={1941-0506},  month={Nov},}