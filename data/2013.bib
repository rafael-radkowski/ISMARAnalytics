@INPROCEEDINGS{6671746,
author={},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) [Copyright notice]},
year={2013},
volume={},
number={},
pages={1-1},
abstract={Copyright and Reprint Permission: Abstracting is permitted with credit to the source. Libraries are permitted to photocopy beyond the limit of U.S. copyright law for private use of patrons those articles in this volume that carry a code at the bottom of the first page, provided the per-copy fee indicated in the code is paid through Copyright Clearance Center, 222 Rosewood Drive, Danvers, MA 01923. For reprint or republication permission, email to IEEE Copyrights Manager at pubs-permissions@ieee.org. All rights reserved. Copyright (c)2013 by IEEE. 978-1-4799-2869-9 / CFP13MAR-ART.},
keywords={},
doi={10.1109/ISMAR.2013.6671746},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671747,
author={M. {Billinghurst} and B. H. {Thomas}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={General chair},
year={2013},
volume={},
number={},
pages={1-2},
abstract={Welcome to the Twelfth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR 2013).},
keywords={Educational institutions;Augmented reality;Australia;Media;Europe;Cities and towns;Laboratories},
doi={10.1109/ISMAR.2013.6671747},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671748,
author={M. {Gandy} and S. {Julier} and K. {Kiyokawa}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Program chairs},
year={2013},
volume={},
number={},
pages={1-4},
abstract={We are delighted to welcome you to ISMAR 2013, the 12th symposium on Mixed and Augmented Reality! This year's symposium continues a long tradition of ISMAR meetings, a series that itself followed a related series of IWAR, ISMR, and ISAR meetings.},
keywords={},
doi={10.1109/ISMAR.2013.6671748},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671749,
author={T. {Drummond} and M. {Adcock}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Tutorial chairs},
year={2013},
volume={},
number={},
pages={1-1},
abstract={It is our great pleasure to present the ISMAR Tutorials. We proudly host two tutorials that provide sharing of knowledge from seasoned researchers. Our tutorials cover open standards and development using HTML and instantAR. Through these exciting tutorials we hope to expand the minds of ISMAR 2013 attendees and help to foster the next generation of Mixed and Augmented Reality researchers, practitioners, and artists.},
keywords={},
doi={10.1109/ISMAR.2013.6671749},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671750,
author={H. {Seichter} and D. {Kalkofen}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Workshop chairs},
year={2013},
volume={},
number={},
pages={1-1},
abstract={It is our pleasure to present the workshops associated with ISMAR 2013. These events provide a chance to thoroughly examine specific research areas in the exciting field of Mixed and Augmented Reality.},
keywords={},
doi={10.1109/ISMAR.2013.6671750},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671751,
author={R. T. {Smith} and M. {Sugimoto} and R. {Grasset}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Demo chairs},
year={2013},
volume={},
number={},
pages={1-1},
abstract={We are delighted to present the 12th edition of the demonstration program of the IEEE International Symposium on Mixed and Augmented Reality. The ISMAR demonstration program provide hands-on experience to the community of the most recent technical and applied advancement in Augmented Reality.},
keywords={},
doi={10.1109/ISMAR.2013.6671751},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671752,
author={W. {Chinthammit} and S. J. {Kim}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Doctoral chairs},
year={2013},
volume={},
number={},
pages={1-1},
abstract={We are proud to present the 2nd ISMAR Doctoral Consortium (DC). We are continuing to build on the success of the 1st DC at ISMAR 2012, where DC students received invaluable inputs from DC mentors to help improve their research. The goal of the DC is to create an opportunity for PhD students to present their research, discuss their current progresses and future plans, and receive constructive criticism and guidances regarding their current work, future work, and career perspectives.},
keywords={},
doi={10.1109/ISMAR.2013.6671752},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671753,
author={S. {Diverdi} and J. {Park}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Wip chairs},
year={2013},
volume={},
number={},
pages={1-1},
abstract={New this year to ISMAR 2013, we are proud to present the Works In Progress (WIP) Program. Augmented Reality is rapidly growing into many new areas, so the WIP is a platform to present the field's latest, emerging results to the larger community before the work has reached its final form. This year, the program includes bread and butter AR technologies such as remote collaboration interfaces, fiducial marker design, and perceptual studies, to even loftier applications like AR interactions aboard the International Space Station. Passive haptics, bare-handed gesture interfaces, and realistic rendering round out the offerings. So come to the WIP sessions to hear about active AR research and find the spark of inspiration!},
keywords={},
doi={10.1109/ISMAR.2013.6671753},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671754,
author={I. {Poupyrev}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Not just augmentation: How to re-make the world?},
year={2013},
volume={},
number={},
pages={1-1},
abstract={From the dawn of the industrial revolution and establishment of mass production the world has been fixed and immutable. The objects and devices around us were designed by engineers, manufactured in factories, sold in stores and brought to our homes and offices to serve the purpose that they had been made for. Our roles were that of passive consumers. Recently, however, we have come to expect that our objects and environments are interactive, engaging and hackable anytime and anywhere. We are no longer passive consumers. With new rapid prototyping tools, open source software, readily available sensors and microprocessors, novel materials, 3D printers and printed electronics our world can be hacked, twisted, connected, re-connected and extended with functionality that it is not supposed to have. We can make living plants play digital music and human bodies transmit sound, we can build touch screens on water, extend our world by 3D printing whatever we need, create virtual objects that we can feel in free air with bare hands and produce electrical energy from ordinary paper. Everyday physical objects, both living and artificial, and entire environments can be made interactive, responsive and digital. Never before in our collective history have we as individuals had so much power to re-make the world around us, enhance it with new experiences and functionalities that educate, delight, entertain and make our lives better in countless ways. And most of them are yet to be invented. In this talk I will look back at the relation of digital technology and physical world, speculate about it's future and discuss some of the recent explorations that myself and my group have been conducting in merging digital computing and physical environments. Some of the topics include new technologies for tactile augmentation, free-air haptics, deformable and compliant computers, ad-hoc sensor augmentation, biologically inspired interfaces, energy harvesting and others. The talk will cover both early projects that I conducted at Sony Corporation and current research efforts by the Interaction Group at Disney Research, Pittsburgh.},
keywords={},
doi={10.1109/ISMAR.2013.6671754},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671755,
author={J. {Rekimoto}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={From augmented reality to augmented human},
year={2013},
volume={},
number={},
pages={1-1},
abstract={Traditionally, the field of Human Computer Interaction (HCI) was primarily concerned with designing and investigating interfaces between humans and machines. However, with recent technological advances the concept of “enhancing”, “augmenting” or even “redesigning” humans themselves is becoming not only interesting and intriguing but also very feasible and serious topic of scientific research and development. “Augmented Human” is term that I use today to refer to this overall research direction. Although the term “augmentation” has long been used in HCI and AR communities since Douglas Engelbert's landmark research on augmenting intelligence, I think the possibility of human augmentation is not limited to intellectual abilities and can be expound to physical abilities. I believe Augmented Human introduces a fundamental paradigm shift in HCI: from human-computer-interaction to human-computer-integration. In this talk, I will discuss rich possibilities and distinct challenges in enhancing human abilities with technology. I will introduce recent projects conducted by our research group including design and applications if wearable eye sensing for augmenting our perception and memory abilities, design of flying cameras as our external eyes, a home appliance that can increase your happiness, an organic physical wall/window that dynamically mediates the environment, and a human hand control system based on functional electrical stimulation.},
keywords={},
doi={10.1109/ISMAR.2013.6671755},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671756,
author={},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={IEEE Visualization Graphics Technical Committee (VGTC)},
year={2013},
volume={},
number={},
pages={1-4},
abstract={The IEEE Visualization and Graphics Technical Committee (VGTC) is a formal subcommittee of the Technical Activities Board (TAB) of the IEEE Computer Society. The VGTC provides technical leadership and organizes technical activities in the areas of visualization, computer graphics, virtual and augmented reality, and interaction.},
keywords={},
doi={10.1109/ISMAR.2013.6671756},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671757,
author={},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Conference officers]},
year={2013},
volume={},
number={},
pages={1-2},
abstract={Provides a listing of current committee members and society officers.},
keywords={},
doi={10.1109/ISMAR.2013.6671757},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671758,
author={D. {Kalkofen} and E. {Veas} and S. {Zollmann} and M. {Steinberger} and D. {Schmalstieg}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Adaptive ghosted views for Augmented Reality},
year={2013},
volume={},
number={},
pages={1-9},
abstract={In Augmented Reality (AR), ghosted views allow a viewer to explore hidden structure within the real-world environment. A body of previous work has explored which features are suitable to support the structural interplay between occluding and occluded elements. However, the dynamics of AR environments pose serious challenges to the presentation of ghosted views. While a model of the real world may help determine distinctive structural features, changes in appearance or illumination detriment the composition of occluding and occluded structure. In this paper, we present an approach that considers the information value of the scene before and after generating the ghosted view. Hereby, a contrast adjustment of preserved occluding features is calculated, which adaptively varies their visual saliency within the ghosted view visualization. This allows us to not only preserve important features, but to also support their prominence after revealing occluded structure, thus achieving a positive effect on the perception of ghosted views.},
keywords={augmented reality;data visualisation;adaptive ghosted view visualization;augmented reality;occluding-occluded elements structural interplay;AR environment dynamics;structural features;occluding features contrast adjustment;visual saliency;Visualization;Color;Image color analysis;Solid modeling;Three-dimensional displays;Rendering (computer graphics);Augmented reality;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, Augmented, and Virtual Realities},
doi={10.1109/ISMAR.2013.6671758},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671759,
author={ {Zhuwen Li} and {Yuxi Wang} and {Jiaming Guo} and {Loong-Fah Cheong} and S. Z. {Zhou}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Diminished reality using appearance and 3D geometry of internet photo collections},
year={2013},
volume={},
number={},
pages={11-19},
abstract={This paper presents a new system level framework for Diminished Reality, leveraging for the first time both the appearance and 3D information provided by large photo collections on the Internet. Recent computer vision techniques have made it possible to automatically reconstruct 3-D structure-from-motion points from large and unordered photo collections. Using these point clouds and a prior provided by GPS, reasonably accurate 6 degree of freedom camera poses can be obtained, thus allowing localization. Once the camera (and hence the user) is correctly localized, photos depicting scenes visible from the user's viewpoint can be used to remove unwanted objects indicated by the user in the video sequences. Existing methods based on texture synthesis bring undesirable artifacts and video inconsistency when the background is heterogeneous; the task is rendered even harder for these methods when the background contains complex structures. On the other hand, methods based on plane warping fail when the background has arbitrary shape. Unlike these methods, our algorithm copes with these problems by making use of internet photos, registering them in 3D space and obtaining the 3D scene structure in an offline process. We carefully design the various components during the online phase so as to meet both speed and quality requirements of the task. Experiments on real data collected demonstrate the superiority of our system.},
keywords={cameras;computer vision;digital photography;image reconstruction;image sequences;image texture;Internet;video signal processing;virtual reality;diminished reality;3D geometry;Internet photo collections;system level framework;3D information;computer vision techniques;3D structure-from-motion points reconstruction;point clouds;GPS;6 degree of freedom camera poses;video sequences;texture synthesis;video inconsistency;complex structures;plane warping;arbitrary shape;Internet photos;3D space;3D scene structure;Cameras;Three-dimensional displays;Internet;Image reconstruction;Correlation;Feature extraction;Global Positioning System;H.5.1 [Information Systems]: Multimedia Information Systems—Augmented Reality;I.4.8 [Image Processing and Computer Vision]: Scene Analysis—Sensor Fusion, Tracking;I.4.9 [Computing Methodologies]: Image Processing and Computer Vision—Application},
doi={10.1109/ISMAR.2013.6671759},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671760,
author={M. {Tomioka} and S. {Ikeda} and K. {Sato}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Approximated user-perspective rendering in tablet-based augmented reality},
year={2013},
volume={},
number={},
pages={21-28},
abstract={This study addresses the problem of geometric consistency between displayed images and real scenes in augmented reality using a video see-through hand-held display or tablet. To solve this problem, we present approximated user-perspective images rendered by homography transformation of camera images. Homography approximation has major advantages not only in terms of computational costs, but also in the quality of image rendering. However, it can lead to an inconsistency between the real image and virtual objects. This study also introduces a variety of rendering methods for virtual objects and discusses the differences between them. We implemented two prototypes and designed three types of user studies on matching tasks between real scenes and displayed images. We have confirmed that the proposed method works in real time on an off-the-shelf tablet. Our pilot tests show the potential to improve users' visibility, even in real environments, by using our method.},
keywords={augmented reality;image processing;notebook computers;rendering (computer graphics);user interfaces;approximated user perspective rendering;tablet based augmented reality;geometric consistency;real scenes;video see-through handheld display;user perspective images;homography transformation;camera images;homography approximation;computational costs;image rendering;virtual objects;Cameras;Prototypes;Rendering (computer graphics);Three-dimensional displays;Approximation methods;Calibration;Augmented reality;User-perspective rendering;augmented reality;geometric consistency;video see-through;tablet-based AR},
doi={10.1109/ISMAR.2013.6671760},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671761,
author={A. {Maimone} and H. {Fuchs}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Computational augmented reality eyeglasses},
year={2013},
volume={},
number={},
pages={29-38},
abstract={In this paper we discuss the design of an optical see-through head-worn display supporting a wide field of view, selective occlusion, and multiple simultaneous focal depths that can be constructed in a compact eyeglasses-like form factor. Building on recent developments in multilayer desktop 3D displays, our approach requires no reflective, refractive, or diffractive components, but instead relies on a set of optimized patterns to produce a focused image when displayed on a stack of spatial light modulators positioned closer than the eye accommodation distance. We extend existing multilayer display ray constraint and optimization formulations while also purposing the spatial light modulators both as a display and as a selective occlusion mask. We verify the design on an experimental prototype and discuss challenges to building a practical display.},
keywords={augmented reality;eye;helmet mounted displays;optimisation;three-dimensional displays;computational augmented reality eyeglasses;optical see-through head-worn display;focal depths;compact eyeglasses-like form factor;multilayer desktop 3D displays;focused image;spatial light modulators;eye accommodation distance;multilayer display ray constraint;optimization formulations;selective occlusion mask;Optimization;Nonhomogeneous media;Optical imaging;Modulation;Optical refraction;Optical diffraction;Lenses;augmented reality;three-dimensional displays},
doi={10.1109/ISMAR.2013.6671761},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671762,
author={M. R. {Marner} and A. {Irlitti} and B. H. {Thomas}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Improving procedural task performance with Augmented Reality annotations},
year={2013},
volume={},
number={},
pages={39-48},
abstract={This paper presents results of a study measuring user performance in a procedural task using Spatial Augmented Reality (SAR). The task required participants to press sequences of buttons on two control panel designs in the correct order. Instructions for the task were shown either on a computer monitor, or projected directly onto the control panels. This work was motivated by discrepancies between the expectations from AR proponents and experimental findings. AR is often promoted as a way of improving user performance and understanding. With notable exceptions however, experimental results do not confirm these expectations. Reasons cited for results include limitations of current display technologies and misregistration caused by tracking and calibration errors. Our experiment utilizes SAR to remove these effects. Our results show that augmented annotations lead to significantly faster task completion speed, fewer errors, and reduced head movement, when compared to monitor based instructions. Subjectively, our results show augmented annotations are preferred by users.},
keywords={augmented reality;procedural task performance improvement;augmented reality annotations;user performance measurement;spatial augmented reality;computer monitor;augmented annotations;Monitoring;Presses;Augmented reality;Assembly;Pressing;Atmospheric measurements;Particle measurements;Spatial Augmented Reality;User Interfaces;User Study},
doi={10.1109/ISMAR.2013.6671762},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671763,
author={Z. {Bai} and A. F. {Blackwell} and G. {Coulouris}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Through the looking glass: Pretend play for children with autism},
year={2013},
volume={},
number={},
pages={49-58},
abstract={Lack of spontaneous pretend play is an early diagnostic indicator of autism spectrum conditions (ASC) along with impaired communication and social interaction. In a previous ISMAR poster [2] we proposed an Augmented Reality (AR) system to encourage pretend play, based on an analogy between imaginative interpretation of physical objects (pretense) and the superimposition of virtual content on the physical world in AR. This paper reports an empirical experiment evaluating that proposal, involving children between the ages of 4 and 7 who have been diagnosed with ASC. Results find significantly more pretend play, and higher engagement, using the AR system by comparison to a non-augmented condition. We also discuss usability issues and design implications for AR systems that aim to support children with ASC and other pervasive developmental disorders.},
keywords={augmented reality;handicapped aids;medical disorders;spontaneous pretend play;diagnostic indicator;autism spectrum condition;ASC;impaired communication;social interaction;augmented reality;pervasive developmental disorder;Autism;Vehicles;Materials;Airplanes;Educational institutions;Visualization;Fires;Augmented Reality;pretend play;autism;children},
doi={10.1109/ISMAR.2013.6671763},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671764,
author={ {Wai-Tat Fu} and J. {Gasper} and S. {Kim}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Effects of an in-car augmented reality system on improving safety of younger and older drivers},
year={2013},
volume={},
number={},
pages={59-66},
abstract={We designed and tested the effects of an in-car augmented reality system (ARS) on younger and older drivers, with and without a secondary distraction task. When potential danger is detected, the ARS alerts the driver by progressively indicating the time to collision to the lead vehicle as well as merging vehicles from side lanes by an AR display that overlaps with the lead or merging vehicles. We tested the ARS with younger (18-30) and older (65-75) drivers in a high-fidelity driving simulator. Results showed that the ARS could significantly reduce collisions caused by hazard events such as sudden slowing of the lead vehicle or merging of vehicles from sides lanes. Consistent with previous results, older drivers, despite age-related decline in cognitive and motor abilities, could leverage their driving experience to avoid forward collisions with the lead vehicle as much as younger drivers. However, older drivers were poorer in avoiding collisions caused by sudden merging events than younger drivers. The ARS was found to be most useful in helping older adults to avoid collision caused by sudden hazard events, especially with the presence of a distraction task. The ARS was also more effective for older than younger drivers to encourage a safe driving distance with the lead vehicle. Interestingly, there seemed to be differential effects of the ARS on the general driving behavior of younger and older drivers. While older drivers in general became more careful and safer in how they drive with the ARS, younger drivers seemed to rely on the ARS to alert them to potential hazard events without adopting safer driving behavior.},
keywords={accident prevention;augmented reality;human computer interaction;traffic engineering computing;in-car augmented reality system;younger driver;older driver;distraction task;high-fidelity driving simulator;hazard event;cognitive ability;motor ability;forward collision;driving behavior;Vehicles;Augmented reality;Merging;Hazards;Visualization;Accidents;Analysis of variance;Human-Centered Computing [Human-Computer Interaction]: HCI design and evaluation methods—},
doi={10.1109/ISMAR.2013.6671764},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671765,
author={G. {Bruder} and P. {Wieland} and B. {Bolte} and M. {Lappe} and F. {Steinicke}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Going with the flow: Modifying self-motion perception with computer-mediated optic flow},
year={2013},
volume={},
number={},
pages={67-74},
abstract={One major benefit of wearable computers is that users can naturally move and explore computer-mediated realities. However, researchers often observe that users' space and motion perception severely differ in such environments compared to the real world, an effect that is often attributed to slight discrepancies in sensory cues, for instance, caused by tracking inaccuracy or system latency. This is particularly true for virtual reality (VR), but such conflicts are also inherent to augmented reality (AR) technologies. Although, head-worn displays will become more and more available soon, the effects on motion perception have rarely been studied, and techniques to modify self-motion in AR environments have not been leveraged so far. In this paper we introduce the concept of computer-mediated optic flow, and analyze its effects on self-motion perception in AR environments. First, we introduce different techniques to modify optic flow patterns and velocity. We present a psychophysical experiment which reveals differences in self-motion perception with a video see-through head-worn display compared to the real-world viewing condition. We show that computer-mediated optic flow has the potential to make a user perceive self-motion as faster or slower than it actually is, and we discuss its potential for future AR setups.},
keywords={augmented reality;helmet mounted displays;image motion analysis;image sequences;video signal processing;wearable computers;computer-mediated realities;users space;sensory cues;virtual reality;VR;augmented reality;AR environments;computer-mediated optic flow;self-motion perception;optic flow patterns;psychophysical experiment;video see-through head-worn display;real-world viewing condition;Visualization;Optical sensors;Cameras;Optical imaging;Legged locomotion;Observers;Optical feedback;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism—Virtual reality},
doi={10.1109/ISMAR.2013.6671765},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671766,
author={A. {Hartl} and J. {Grubert} and D. {Schmalstieg} and G. {Reitmayr}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Mobile interactive hologram verification},
year={2013},
volume={},
number={},
pages={75-82},
abstract={Verification of paper documents is an important part of checking a person's identity, authorization for access or simply establishing a trusted currency. Many documents such as passports or paper bills include holograms or other view-dependent elements that are difficult to forge and therefore are used to verify the genuineness of that document. View-dependent elements change their appearance based both on viewing direction and dominant light sources, thus it requires special knowledge and training to accurately distinguish original elements from forgeries. We present an interactive application for mobile devices that integrates the recognition of the documents with the interactive verification of view-dependent elements. The system recognizes and tracks the paper document, provides user guidance for view alignment and presents a stored image of the element's appearance depending on the current view of the document also recording user decisions. We describe how to model and capture the underlying spatially varying BRDF representation of view-dependent elements. Furthermore, we evaluate this approach within a user study and demonstrate that such a setup captures images that are recognizable and that can be correctly verified.},
keywords={document image processing;holography;interactive systems;mobile computing;mobile radio;view-dependent elements;document genuineness;viewing direction;light sources;forgeries;interactive application;mobile devices;documents recognition;user guidance;view alignment;BRDF representation;paper bills;passports;trusted currency;authorization;person identity checking;paper documents verification;mobile interactive hologram verification;Cameras;Mobile handsets;Light sources;Mobile communication;Security;Inspection;Light emitting diodes},
doi={10.1109/ISMAR.2013.6671766},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671767,
author={V. {Pradeep} and C. {Rhemann} and S. {Izadi} and C. {Zach} and M. {Bleyer} and S. {Bathiche}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={MonoFusion: Real-time 3D reconstruction of small scenes with a single web camera},
year={2013},
volume={},
number={},
pages={83-88},
abstract={MonoFusion allows a user to build dense 3D reconstructions of their environment in real-time, utilizing only a single, off-the-shelf web camera as the input sensor. The camera could be one already available in a tablet, phone, or a standalone device. No additional input hardware is required. This removes the need for power intensive active sensors that do not work robustly in natural outdoor lighting. Using the input stream of the camera we first estimate the 6DoF camera pose using a sparse tracking method. These poses are then used for efficient dense stereo matching between the input frame and a key frame (extracted previously). The resulting dense depth maps are directly fused into a voxel-based implicit model (using a computationally inexpensive method) and surfaces are extracted per frame. The system is able to recover from tracking failures as well as filter out geometrically inconsistent noise from the 3D reconstruction. Our method is both simple to implement and efficient, making such systems even more accessible. This paper details the algorithmic components that make up our system and a GPU implementation of our approach. Qualitative results demonstrate high quality reconstructions even visually comparable to active depth sensor-based systems such as KinectFusion.},
keywords={cameras;graphics processing units;image reconstruction;image sensors;pose estimation;real-time systems;target tracking;GPU implementation;voxel-based implicit model;sparse tracking method;6DoF camera pose;natural outdoor lighting;power intensive active sensors;off-the-shelf Web camera;single Web camera;small scenes;real-time 3D reconstruction;MonoFusion;Cameras;Three-dimensional displays;Real-time systems;Image reconstruction;Robustness;Optimization;Streaming media},
doi={10.1109/ISMAR.2013.6671767},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671768,
author={V. A. {Prisacariu} and O. {Kähler} and D. W. {Murray} and I. D. {Reid}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Simultaneous 3D tracking and reconstruction on a mobile phone},
year={2013},
volume={},
number={},
pages={89-98},
abstract={A novel framework for joint monocular 3D tracking and reconstruction is described that can handle untextured objects, occlusions, motion blur, changing background and imperfect lighting, and that can run at frame rate on a mobile phone. The method runs in parallel (i) level set based pose estimation and (ii) continuous max flow based shape optimisation. By avoiding a global computation of distance transforms typically used in level set methods, tracking rates here exceed 100Hz and 20Hz on a desktop and mobile phone, respectively, without needing a GPU. Tracking ambiguities are reduced by augmenting orientation information from the phone's inertial sensor. Reconstruction involves probabilistic integration of the 2D image statistics from keyframes into a 3D volume. Per-voxel posteriors are used instead of the standard likelihoods, giving increased accuracy and robustness. Shape coherency and compactness is then imposed using a total variational approach solved using globally optimal continuous max flow.},
keywords={image motion analysis;image reconstruction;mobile computing;pose estimation;probability;tracking;variational techniques;3D reconstruction;mobile phone;monocular 3D tracking;motion blur;level set based pose estimation;shape optimisation;tracking ambiguities;inertial sensor;probabilistic integration;2D image statistics;3D volume;per-voxel posteriors;shape coherency;total variational approach;optimal continuous max flow;Three-dimensional displays;Shape;Image reconstruction;Mobile handsets;Optimization;Cameras;Rendering (computer graphics)},
doi={10.1109/ISMAR.2013.6671768},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671769,
author={ {Mingsong Dou} and H. {Fuchs} and J. {Frahm}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Scanning and tracking dynamic objects with commodity depth cameras},
year={2013},
volume={},
number={},
pages={99-106},
abstract={The 3D data collected using state-of-the-art algorithms often suffers from various problems, such as incompletion and inaccuracy. Using temporal information has been proven effective for improving the reconstruction quality; for example, KinectFusion [21] shows significant improvements for static scenes. In this work, we present a system that uses commodity depth and color cameras, such as Microsoft Kinects, to fuse the 3D data captured over time for dynamic objects to build a complete and accurate model, and then tracks the model to match later observations. The key ingredients of our system include a nonrigid matching algorithm that aligns 3D observations of dynamic objects by using both geometry and texture measurements, and a volumetric fusion algorithm that fuses noisy 3D data. We demonstrate that the quality of the model improves dramatically by fusing a sequence of noisy and incomplete depth data of human and that by deforming this fused model to later observations, noise-and-hole-free 3D models are generated for the human moving freely.},
keywords={data visualisation;image colour analysis;image reconstruction;image texture;object tracking;solid modelling;dynamic object scanning;dynamic object tracking;commodity depth camera;temporal information;reconstruction quality;KinectFusion;static scene;color camera;Microsoft Kinects;nonrigid matching algorithm;geometry;texture measurement;volumetric fusion algorithm;noisy 3D data;noise-and-hole-free 3D model;Three-dimensional displays;Image color analysis;Deformable models;Cameras;Solid modeling;Heuristic algorithms;Data models},
doi={10.1109/ISMAR.2013.6671769},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671770,
author={T. {Nguyen} and R. {Grasset} and D. {Schmalstieg} and G. {Reitmayr}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Interactive syntactic modeling with a single-point laser range finder and camera},
year={2013},
volume={},
number={},
pages={107-116},
abstract={In-situ 3D Modeling becomes increasingly prominent in current Augmented Reality research, particularly for mobile scenarios. However, real-time performance and qualitative modeling remain highly challenging. In this work, we propose a new interactive 3D modeling approach for indoor environments, combining an assistive user interface and constrained reconstruction with a device consisting of a single-point laser range finder and a camera. Using our system, a user pans around capturing a panorama of the environment, while simultaneously measuring the distance to a single point per frame. An automatic detection process estimates planes from these sparse 3D measurements. The user can highlight specific geometric features in the environment, such as 2- or 3-way corners, with simple gestures, adding more 3D points to the estimation. The segmented planes are refined using a constrained optimization, enforcing orthogonality and parallel constraints as well as minimizing the number of planes used in the reconstruction. Finally a volumetric space-carving approach determines the geometry of the environment. Our reconstruction approach can output highly accurate models built only from simple, clean geometry. To examine the quantitative performance of our approach, we run evaluations on both synthetic and real data.},
keywords={augmented reality;geometry;image reconstruction;image segmentation;interactive systems;laser ranging;solid modelling;interactive syntactic modeling;augmented reality;mobile scenarios;real-time performance;interactive 3D modeling;indoor environments;assistive user interface;single-point laser range finder;camera;panorama;automatic detection process;sparse 3D measurements;geometric features;2-way corners;3-way corners;gestures;3D points;segmented planes;constrained optimization;orthogonality constraints;parallel constraints;volumetric space-carving approach;reconstruction approach;Three-dimensional displays;Cameras;Solid modeling;Geometry;Computational modeling;Measurement by laser beam;Laser modes;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial augmented, and virtual realities;I.2.10 [Artificial Intelligence]: Vision and Scene Understanding—Modeling and recovery of physical attributes},
doi={10.1109/ISMAR.2013.6671770},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671771,
author={N. {Petersen} and A. {Pagani} and D. {Stricker}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Real-time modeling and tracking manual workflows from first-person vision},
year={2013},
volume={},
number={},
pages={117-124},
abstract={Recognizing previously observed actions in video sequences can lead to Augmented Reality manuals that (1) automatically follow the progress of the user and (2) can be created from video examples of the workflow. Modeling is challenging, as the environment is susceptible to change drastically due to user interaction and camera motion may not provide sufficient translation to robustly estimate geometry. We propose a piecewise homographic transform that projects the given video material onto a series of distinct planar subsets of the scene. These subsets are selected by segmenting the largest image region that is consistent with a homographic model and contains a given region of interest. We are then able to model the state of the environment and user actions using simple 2D region descriptors. The model elegantly handles estimation errors due to incomplete observation and is robust towards occlusions, e.g., due to the user's hands. We demonstrate the effectiveness of our approach quantitatively and compare it to the current state of the art. Further, we show how we apply the approach to visualize automatically assessed correctness criteria during run-time.},
keywords={augmented reality;image motion analysis;image sequences;tracking;transforms;video signal processing;real-time modeling;manual workflow tracking;first-person vision;video sequences;augmented reality manuals;user interaction;camera motion;piecewise homographic transform;homographic model;2D region descriptors;Cameras;Image segmentation;Visualization;Tracking;Manuals;Optical distortion;Optical imaging},
doi={10.1109/ISMAR.2013.6671771},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671772,
author={T. A. {Franke}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Delta Light Propagation Volumes for mixed reality},
year={2013},
volume={},
number={},
pages={125-132},
abstract={Indirect illumination is an important visual cue which has traditionally been neglected in mixed reality applications. We present Delta Light Propagation Volumes, a novel volumetric relighting method for real-time mixed reality applications which allows to simulate the effect of first bounce indirect illumination of synthetic objects onto a real geometry and vice versa. Inspired by Radiance Transfer Fields, we modify Light Propagation Volumes in such a way as to propagate the change in illumination caused by the introduction of a synthetic object into a real scene. This method combines real and virtual light in one representation, provides improved temporal coherence for indirect light compared to previous solutions and implicitly includes smooth shadows.},
keywords={augmented reality;rendering (computer graphics);delta light propagation volume;real-time mixed reality;first bounce indirect illumination;visual cue;volumetric relighting method;radiance transfer field;indirect light;Lighting;Light sources;Image reconstruction;Geometry;Surface reconstruction;Virtual reality;Cameras;Mixed Reality;Real-time Global Illumination},
doi={10.1109/ISMAR.2013.6671772},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671773,
author={P. {Kán} and H. {Kaufmann}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Differential Irradiance Caching for fast high-quality light transport between virtual and real worlds},
year={2013},
volume={},
number={},
pages={133-141},
abstract={Fast and realistic synthesis of real videos with computer generated content has been a challenging problem in computer graphics. It involves computationally expensive light transport calculations. We present a novel and efficient algorithm for diffuse light transport calculation between virtual and real worlds called Differential Irradiance Caching. Our algorithm produces a high-quality result while preserving interactivity and allowing dynamic geometry, materials, lighting, and camera movement. The problem of expensive differential irradiance evaluation is solved by exploiting the spatial coherence in indirect illumination using irradiance caching. We enable multiple bounces of global illumination by using Monte Carlo integration in GPU ray-tracing to evaluate differential irradiance at irradiance cache records in one pass. The combination of ray-tracing and rasterization is used in an extended irradiance cache splatting algorithm to provide a fast GPU-based solution of indirect illumination. Limited information stored in the irradiance splat buffer causes errors for pixels on edges in case of depth of field rendering. We propose a solution to this problem using a reprojection technique to access the irradiance splat buffer. A novel cache miss detection technique is introduced which allows for a linear irradiance cache data structure. We demonstrate the integration of differential irradiance caching into a rendering framework for Mixed Reality applications capable of simulating complex global illumination effects.},
keywords={augmented reality;cache storage;data structures;Monte Carlo methods;ray tracing;rendering (computer graphics);differential irradiance caching;high-quality light transport;virtual world;real world;computer generated content;computer graphics;diffuse light transport calculation;differential irradiance evaluation;spatial coherence;indirect illumination;Monte Carlo integration;GPU ray-tracing;irradiance cache records;rasterization;irradiance cache splatting algorithm;GPU-based solution;irradiance splat buffer;field rendering;reprojection technique;cache miss detection technique;linear irradiance cache data structure;rendering framework;mixed reality applications;complex global illumination effects;Lighting;Ray tracing;Rendering (computer graphics);Virtual reality;Graphics processing units;Mathematical model;Real-time systems;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism —Raytracing;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems —Artificial, augmented, and virtual realities},
doi={10.1109/ISMAR.2013.6671773},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671774,
author={M. {Meilland} and C. {Barat} and A. {Comport}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={3D High Dynamic Range dense visual SLAM and its application to real-time object re-lighting},
year={2013},
volume={},
number={},
pages={143-152},
abstract={Acquiring High Dynamic Range (HDR) light-fields from several images with different exposures (sensor integration periods) has been widely considered for static camera positions. In this paper a new approach is proposed that enables 3D HDR environment maps to be acquired directly from a dynamic set of images in real-time. In particular a method will be proposed to use an RGB-D camera as a dynamic light-field sensor, based on a dense real-time 3D tracking and mapping approach, that avoids the need for a light-probe or the observation of reflective surfaces. The 6dof pose and dense scene structure will be estimated simultaneously with the observed dynamic range so as to compute the radiance map of the scene and fuse a stream of low dynamic range images (LDR) into an HDR image. This will then be used to create an arbitrary number of virtual omni-directional light-probes that will be placed at the positions where virtual augmented objects will be rendered. In addition, a solution is provided for the problem of automatic shutter variations in visual SLAM. Augmented reality results are provided which demonstrate real-time 3D HDR mapping, virtual light-probe synthesis and light source detection for rendering reflective objects with shadows seamlessly with the real video stream in real-time.},
keywords={augmented reality;object tracking;rendering (computer graphics);SLAM (robots);3D high dynamic range dense visual SLAM;real-time object relighting;HDR light-field;sensor integration period;static camera position;3D HDR environment maps;RGB-D camera;dynamic light-field sensor;dense real-time 3D tracking;reflective surface;pose structure;dense scene structure;radiance map;low dynamic range image;virtual omni-directional light-probe;virtual augmented object;automatic shutter variation;augmented reality;3D HDR mapping;virtual light-probe synthesis;light source detection;rendering;real video stream;Three-dimensional displays;Real-time systems;Rendering (computer graphics);Cameras;Lighting;Light sources;Solid modeling;Real-time rendering;photo-realistic rendering;vision-based registration and tracking;MR/AR for entertainment},
doi={10.1109/ISMAR.2013.6671774},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671775,
author={A. {Mulloni} and M. {Ramachandran} and G. {Reitmayr} and D. {Wagner} and R. {Grasset} and S. {Diaz}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={User friendly SLAM initialization},
year={2013},
volume={},
number={},
pages={153-162},
abstract={The development of new Simultaneous Localization and Mapping (SLAM) techniques is quickly advancing in research communities and rapidly transitioning into commercial products. Creating accurate and high-quality SLAM maps relies on a robust initialization process. However, the robustness and usability of SLAM initialization for end-users has often been disregarded. This paper presents and evaluates a novel tracking system for 6DOF pose tracking between a single keyframe and the current camera frame, without any prior scene knowledge. Our system is particularly suitable for SLAM initialization, since it allows 6DOF pose tracking in the intermediate frames before a wide-enough baseline between two keyframes has formed. We investigate how our tracking system can be used to interactively guide users in performing an optimal motion for SLAM initialization. However, our findings from a pilot study indicate that the need for such motion can be completely hidden from the user and outsourced to our tracking system. Results from a second user study show that letting our tracking system create a SLAM map as soon as possible is a viable and usable solution. Our work provides important insight for SLAM systems, showing how our novel tracking system can be integrated with a user interface to support fast, robust and user-friendly SLAM initialization.},
keywords={augmented reality;SLAM (robots);tracking;SLAM initialization;simultaneous localization and mapping techniques;robust initialization process;tracking system;6DOF pose tracking;single keyframe;camera frame;intermediate frames;Cameras;Simultaneous localization and mapping;Tracking;Three-dimensional displays;Robustness;Accuracy;Radar tracking;Augmented Reality;SLAM},
doi={10.1109/ISMAR.2013.6671775},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671776,
author={A. {Steed} and S. {Julier}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Behaviour-aware sensor fusion: Continuously inferring the alignment of coordinate systems from user behaviour},
year={2013},
volume={},
number={},
pages={163-172},
abstract={Within mobile mixed reality experiences, we would like to engage the user's head and hands for interaction. However, this requires the use of multiple tracking systems. These must be aligned, both as part of initial system setup and to counteract inter-tracking system drift that can accumulate over time. Traditional approaches to alignment use obtrusive procedures that introduce explicit constraints between the different tracking systems. These can be highly disruptive for the user's experience. In this paper, we propose another type of information which can be exploited to effect alignment: the behaviour of the user. The crucial insight is that user behaviours - such as selection through pointing - introduce implicit constraints between tracking systems. These constraints can be used as the user continually interacts with the system to infer alignment without the need for disruptive procedures. We call this concept behaviour-aware sensor fusion. We introduce two different interaction techniques-the redirected pointing technique and the yaw fix technique - to illustrate this concept. Pilot experiments show that behaviour-aware sensor fusion can increase ease of use and speed of interaction in exemplar mixed-reality interaction tasks.},
keywords={mobile computing;sensor fusion;tracking;user interfaces;virtual reality;behaviour-aware sensor fusion;coordinate systems;user behaviour;mobile mixed reality experiences;inter-tracking system drift;obtrusive procedures;selection through pointing;interaction techniques;redirected pointing technique;yaw fix technique;exemplar mixed-reality interaction tasks;Heating;Abstracts;Indexes;Mobile communication;Tracking;Calibration;Animation;Mobile virtual reality;head-mounted display;3D user interaction;selection tasks;augmented reality},
doi={10.1109/ISMAR.2013.6671776},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671777,
author={B. {Glocker} and S. {Izadi} and J. {Shotton} and A. {Criminisi}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Real-time RGB-D camera relocalization},
year={2013},
volume={},
number={},
pages={173-179},
abstract={We introduce an efficient camera relocalization approach which can be easily integrated into real-time 3D reconstruction methods, such as KinectFusion. Our approach makes use of compact encoding of whole image frames which enables both online harvesting of keyframes in tracking mode, and fast retrieval of pose proposals when tracking is lost. The encoding scheme is based on randomized ferns and simple binary feature tests. Each fern generates a small block code, and the concatenation of codes yields a compact representation of each camera frame. Based on those representations we introduce an efficient frame dissimilarity measure which is defined via the block-wise hamming distance (BlockHD). We illustrate how BlockHDs between a query frame and a large set of keyframes can be simultaneously evaluated by traversing the nodes of the ferns and counting image co-occurrences in corresponding code tables. In tracking mode, this mechanism allows us to consider every frame/pose pair as a potential keyframe. A new keyframe is added only if it is sufficiently dissimilar from all previously stored keyframes. For tracking recovery, camera poses are retrieved that correspond to the keyframes with smallest BlockHDs. The pose proposals are then used to reinitialize the tracking algorithm. Harvesting of keyframes and pose retrieval are computationally efficient with only small impact on the run-time performance of the 3D reconstruction. Integrating our relocalization method into KinectFusion allows seamless continuation of mapping even when tracking is frequently lost. Additionally, we demonstrate how marker-free augmented reality, in particular, can benefit from this integration by enabling a smoother and continuous AR experience.},
keywords={augmented reality;cameras;concatenated codes;image reconstruction;image retrieval;object tracking;pose estimation;real-time systems;real-time RGB-D camera relocalization;real-time 3D reconstruction method;KinectFusion;compact encoding;image frames;online harvesting;tracking mode;pose proposal retrieval;encoding scheme;randomized ferns;binary feature tests;block code;code concatenation;compact representation;frame dissimilarity measure;block-wise hamming distance;BlockHD;query frame;image cooccurrence;code tables;stored keyframes;tracking recovery;pose retrieval;marker-free augmented reality;Cameras;Proposals;Pipelines;Three-dimensional displays;Encoding;Iterative closest point algorithm;Real-time systems},
doi={10.1109/ISMAR.2013.6671777},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671778,
author={ {Jiaolong Yang} and Y. {Dai} and H. {Li} and H. {Gardner} and {Yunde Jia}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Single-shot extrinsic calibration of a generically configured RGB-D camera rig from scene constraints},
year={2013},
volume={},
number={},
pages={181-188},
abstract={With the increasing use of commodity RGB-D cameras for computer vision, robotics, mixed and augmented reality and other areas, it is of significant practical interest to calibrate the relative pose between a depth (D) camera and an RGB camera in these types of setups. In this paper, we propose a new single-shot, correspondence-free method to extrinsically calibrate a generically configured RGB-D camera rig. We formulate the extrinsic calibration problem as one of geometric 2D-3D registration which exploits scene constraints to achieve single-shot extrinsic calibration. Our method first reconstructs sparse point clouds from a single-view 2D image. These sparse point clouds are then registered with dense point clouds from the depth camera. Finally, we directly optimize the warping quality by evaluating scene constraints in 3D point clouds. Our single-shot extrinsic calibration method does not require correspondences across multiple color images or across different modalities and it is more flexible than existing methods. The scene constraints can be very simple and we demonstrate that a scene containing three sheets of paper is sufficient to obtain reliable calibration and with a lower geometric error than existing methods.},
keywords={calibration;cameras;computer graphics;image colour analysis;image registration;RGB-D camera rig;scene constraints;commodity RGB-D cameras;computer vision;robotics;mixed reality;augmented reality;depth camera;RGB camera;extrinsic calibration problem;geometric 2D-3D registration;sparse point clouds;single-view 2D image;dense point clouds;warping quality;3D point cloud;single-shot extrinsic calibration method;color images;geometric error;Cameras;Three-dimensional displays;Calibration;Color;Image color analysis;Robot vision systems;Educational institutions;I.4.1 [IMAGE PROCESSING AND COMPUTER VISION ]: Digitization and Image Capture—Camera calibration;H.5.1 [INFORMATION INTERFACES AND PRESENTATION]: Multimedia Information Systems—Artificial, augmented, and virtual realities;I.4.8 [IMAGE PROCESSING AND COMPUTER VISION]: Scene Analysis—Range data;I.4.3 [IMAGE PROCESSING AND COMPUTER VISION ]: Enhancement—Registration},
doi={10.1109/ISMAR.2013.6671778},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671779,
author={F. {Wientapper} and H. {Wuest} and P. {Rojtberg} and D. {Fellner}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={A camera-based calibration for automotive augmented reality Head-Up-Displays},
year={2013},
volume={},
number={},
pages={189-197},
abstract={Using Head-up-Displays (HUD) for Augmented Reality requires to have an accurate internal model of the image generation process, so that 3D content can be visualized perspectively correct from the viewpoint of the user. We present a generic and cost-effective camera-based calibration for an automotive HUD which uses the windshield as a combiner. Our proposed calibration model encompasses the view-independent spatial geometry, i.e. the exact location, orientation and scaling of the virtual plane, and a view-dependent image warping transformation for correcting the distortions caused by the optics and the irregularly curved windshield. View-dependency is achieved by extending the classical polynomial distortion model for cameras and projectors to a generic five-variate mapping with the head position of the viewer as additional input. The calibration involves the capturing of an image sequence from varying viewpoints, while displaying a known target pattern on the HUD. The accurate registration of the camera path is retrieved with state-of-the-art vision-based tracking. As all necessary data is acquired directly from the images, no external tracking equipment needs to be installed. After calibration, the HUD can be used together with a head-tracker to form a head-coupled display which ensures a perspectively correct rendering of any 3D object in vehicle coordinates from a large range of possible viewpoints. We evaluate the accuracy of our model quantitatively and qualitatively.},
keywords={augmented reality;automotive engineering;calibration;cameras;driver information systems;geometry;head-up displays;image sequences;polynomials;rendering (computer graphics);camera-based calibration;automotive augmented reality;head-up-displays;image generation process;3D content;automotive HUD;view-independent spatial geometry;image warping transformation;polynomial distortion;image sequence;rendering;3D object;Cameras;Calibration;Vehicles;Image reconstruction;Three-dimensional displays;Automotive components;Solid modeling;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities;I.4.1 [Image Processing and Computer Vision]: Digitization and Image Capture—Camera calibration;I.4.8 [Image Processing and Computer Vision]: Scene Analysis—Tracking;G.1.6 [Numerical Analysis]: Optimization—Least squares methods},
doi={10.1109/ISMAR.2013.6671779},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671780,
author={N. {Haouchine} and J. {Dequidt} and I. {Peterlik} and E. {Kerrien} and M. {Berger} and S. {Cotin}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Image-guided simulation of heterogeneous tissue deformation for augmented reality during hepatic surgery},
year={2013},
volume={},
number={},
pages={199-208},
abstract={This paper presents a method for real-time augmentation of vascular network and tumors during minimally invasive liver surgery. Internal structures computed from pre-operative CT scans can be overlaid onto the laparoscopic view for surgery guidance. Compared to state-of-the-art methods, our method uses a real-time biomechanical model to compute a volumetric displacement field from partial three-dimensional liver surface motion. This permits to properly handle the motion of internal structures even in the case of anisotropic or heterogeneous tissues, as it is the case for the liver and many anatomical structures. Real-time augmentation results are presented on in vivo and phantom data and illustrate the benefits of such an approach for minimally invasive surgery.},
keywords={augmented reality;computerised tomography;liver;medical image processing;phantoms;real-time systems;solid modelling;surgery;tumours;image-guided simulation;heterogeneous tissue deformation;augmented reality;hepatic surgery;real-time augmentation;vascular network;tumors;minimally invasive liver surgery;internal structures;preoperative CT scans;laparoscopic view;surgery guidance;state-of-the-art method;real-time biomechanical model;volumetric displacement field;partial three-dimensional liver surface motion;anisotropic tissue;anatomical structures;phantom data;minimally invasive surgery;Liver;Biological system modeling;Computational modeling;Three-dimensional displays;Surgery;Deformable models;Biomechanics;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities;I.3.5 [Computer Graphics]: Computational Geometry and Object Modeling—Physically based modeling},
doi={10.1109/ISMAR.2013.6671780},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671781,
author={ {Wei Tan} and {Haomin Liu} and Z. {Dong} and G. {Zhang} and H. {Bao}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Robust monocular SLAM in dynamic environments},
year={2013},
volume={},
number={},
pages={209-218},
abstract={We present a novel real-time monocular SLAM system which can robustly work in dynamic environments. Different to the traditional methods, our system allows parts of the scene to be dynamic or the whole scene to gradually change. The key contribution is that we propose a novel online keyframe representation and updating method to adaptively model the dynamic environments, where the appearance or structure changes can be effectively detected and handled. We reliably detect the changed features by projecting them from the keyframes to current frame for appearance and structure comparison. The appearance change due to occlusions also can be reliably detected and handled. The keyframes with large changed areas will be replaced by newly selected frames. In addition, we propose a novel prior-based adaptive RANSAC algorithm (PARSAC) to efficiently remove outliers even when the inlier ratio is rather low, so that the camera pose can be reliably estimated even in very challenging situations. Experimental results demonstrate that the proposed system can robustly work in dynamic environments and outperforms the state-of-the-art SLAM systems (e.g. PTAM).},
keywords={iterative methods;robot vision;SLAM (robots);robust monocular SLAM;dynamic environments;online keyframe representation method;prior-based adaptive RANSAC algorithm;PARSAC;Three-dimensional displays;Cameras;Simultaneous localization and mapping;Feature extraction;Solid modeling;Robustness;Real-time systems;I.4.8 [Image Processing and Computer Vision]: Scene Analysis—Tracking;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities},
doi={10.1109/ISMAR.2013.6671781},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671782,
author={T. {Oskiper} and M. {Sizintsev} and V. {Branzoi} and S. {Samarasekera} and R. {Kumar}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Augmented Reality binoculars},
year={2013},
volume={},
number={},
pages={219-228},
abstract={In this paper we present an augmented reality binocular system to allow long range high precision augmentation of live telescopic imagery with aerial and terrain based synthetic objects, vehicles, people and effects. The inserted objects must appear stable in the display and must not jitter and drift as the user pans around and examines the scene with the binoculars. The design of the system is based on using two different cameras with wide field of view, and narrow field of view lenses enclosed in a binocular shaped shell. Using the wide field of view gives us context and enables us to recover the 3D location and orientation of the binoculars much more robustly, whereas the narrow field of view is used for the actual augmentation as well as to increase precision in tracking. We present our navigation algorithm that uses the two cameras in combination with an IMU and GPS in an Extended Kalman Filter (EKF) and provides jitter free, robust and real-time pose estimation for precise augmentation. We have demonstrated successful use of our system as part of a live simulated training system for observer training, in which fixed and rotary wing aircrafts, ground vehicles, and weapon effects are combined with real world scenes.},
keywords={augmented reality;Global Positioning System;image fusion;Kalman filters;pose estimation;augmented reality binocular system;high precision augmentation;live telescopic imagery;binocular shaped shell;3D location;3D orientation;navigation algorithm;IMU;GPS;extended Kalman filter;EKF;pose estimation;Cameras;Augmented reality;Global Positioning System;Visualization;Calibration;Tracking;Training;IMU;monocular wide and narrow field of view camera;GPS;inertial navigation;sensor fusion;EKF},
doi={10.1109/ISMAR.2013.6671782},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671783,
author={C. {Pirchheim} and D. {Schmalstieg} and G. {Reitmayr}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Handling pure camera rotation in keyframe-based SLAM},
year={2013},
volume={},
number={},
pages={229-238},
abstract={Handling degenerate rotation-only camera motion is a challenge for keyframe-based simultaneous localization and mapping with six degrees of freedom. Existing systems usually filter corresponding keyframe candidates, resulting in mapping starvation and tracking failure. We propose to employ these otherwise discarded keyframes to build up local panorama maps registered in the 3D map. Thus, the system is able to maintain tracking during rotational camera motions. Additionally, we seek to actively associate panoramic and 3D map data for improved 3D mapping through the triangulation of more new 3D map features. We demonstrate the efficacy of our approach in several evaluations that show how the combined system handles rotation only camera motion while creating larger and denser maps compared to a standard SLAM system.},
keywords={image motion analysis;SLAM (robots);rotation only camera motion;keyframe-based SLAM;keyframe-based simultaneous localization;keyframe-based simultaneous mapping;panorama maps;3D mapping;Cameras;Three-dimensional displays;Tracking;Simultaneous localization and mapping;Robustness;Feature extraction;History;monocular visual SLAM;hybrid 6DOF and panoramic mapping and tracking;general and rotation-only camera motion;mobile phone},
doi={10.1109/ISMAR.2013.6671783},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671784,
author={ {Zhen Bai} and A. F. {Blackwell}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={See-through window vs. magic mirror: A comparison in supporting visual-motor tasks},
year={2013},
volume={},
number={},
pages={239-240},
abstract={There are two alternative display metaphors for Augmented Reality (AR) screens: a see-through window or a magic mirror. Commonly used by task-support AR applications, the see-through display has not been compared with the mirror display in terms of user's task performance, even though the “mirror” hardware is more accessible to general users. We conducted a novel experiment to compare participants' performance when following object rotation cues with the two display metaphors. Results show that participants' overall performance under the mirror view was comparable to the see-through view, which indicates that the augmented mirror display may be a promising alternative to the window display for AR applications which guide moderately complex three-dimensional manipulations with physical objects.},
keywords={augmented reality;computer displays;mirrors;see-through window;magic mirror;visual-motor tasks;display metaphors;augmented reality screens;AR screens;task-support AR applications;see-through display;augmented mirror display;three-dimensional manipulations;physical objects;Mirrors;Cameras;Monitoring;Face;Augmented reality;Educational institutions;Three-dimensional displays;Augmented Reality;display metaphor;visual-motor},
doi={10.1109/ISMAR.2013.6671784},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671785,
author={B. {Brinkman} and S. {Brinkman}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={AR in the library: A pilot study of multi-target acquisition usability},
year={2013},
volume={},
number={},
pages={241-242},
abstract={Libraries use call numbers to organize their books and enable patrons to locate them. To keep the books in order, library workers conduct a time-consuming and tedious task called “shelf-reading.” Workers look at the call numbers on the spines of each book in the library, one at a time, to make sure they are in the correct places. ShelvAR is an augmented reality shelf-reading system for smart phones that reduces time spent, increases accuracy, and produces an inventory of the books on their shelves as a byproduct. Shelf-reading requires rapid acquisition of many targets (books). Unlike many target acquisition tasks considered in the AR literature, the user is not trying to select a single target from among many. Instead, the user is trying to scan all of the targets, and must be able to easily double-check that none were missed. Our goal is to explore the usability of augmented reality applications for this type of “multiple target acquisition” task. We present the results of a pilot study on the effectiveness of ShelvAR. We demonstrate that individuals with no library experience are just as fast and accurate, when using ShelvAR, as experienced library workers at the shelf-reading task.},
keywords={augmented reality;library automation;smart phones;multitarget acquisition usability;libraries;call numbers;books organization;library workers;ShelvAR;augmented reality shelf-reading system;smart phones;books inventory;target acquisition tasks;augmented reality applications;multiple target acquisition;Libraries;Augmented reality;Accuracy;Lenses;Usability;Cameras;Manuals},
doi={10.1109/ISMAR.2013.6671785},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671786,
author={R. {Budhiraja} and G. A. {Lee} and M. {Billinghurst}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Interaction techniques for HMD-HHD hybrid AR systems},
year={2013},
volume={},
number={},
pages={243-244},
abstract={Most mobile Augmented Reality (AR) systems use either a head mounted display (HMD) or a handheld display (HHD) as a hardware platform. As mobile devices become more affordable, it becomes more common that users own more than one mobile device and use them together. In this research, we investigate Hybrid AR systems that use both HMD and HHD for AR visualization and interaction. In addition to a simple approach of using HMD as a display and HHD as an input device (e.g. a touch pad or a pointer), we further explore novel interaction techniques that can take advantage of having both HMD and HHD closely integrated into one AR system, such as cross-device information sharing, situation adaptive visualization management, and multi-layered visualization.},
keywords={augmented reality;data visualisation;helmet mounted displays;human computer interaction;mobile computing;interaction techniques;HMD-HHD hybrid AR system;mobile augmented reality systems;head mounted display;handheld display;mobile devices;AR visualization;AR interaction;cross-device information sharing;situation adaptive visualization management;multilayered visualization;Visualization;Augmented reality;Mobile communication;Three-dimensional displays;Mobile handsets;Wearable computers;Hardware;Mobile augmented reality;wearable computer;headmounted display;handheld display},
doi={10.1109/ISMAR.2013.6671786},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671787,
author={A. G. {Campbell} and L. {Görgü} and B. {Kroon} and D. {Lillis} and D. {Carr} and G. M. P. {O'Hare}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Giving mobile devices a SIXTH sense: Introducing the SIXTH middleware for Augmented Reality applications},
year={2013},
volume={},
number={},
pages={245-246},
abstract={With the increasing availability of sensors within smartphones and within the world at large, a question arises about how this sensor data can be leveraged by Augmented Reality (AR) devices. AR devices have traditionally been limited by the capability of a given device's unique set of sensors. Connecting sensors from multiple devices using a Sensor Web could address this problem. Through leveraging this SensorWeb existing AR environments could be improved and new scenarios made possible, with devices that previously could not have being used as part of an AR environment. This paper proposes the use of SIXTH: a middleware designed to generate a Sensor Web, which allows a device to leverage heterogeneous external sensors within its environment to help facilitate the creation of richer AR experiences. This paper will present a worst case scenario, in which the device chosen will be a see-through, Android-based Head Mounted Display that has no access to sensors. This device is transformed into an AR device through the creation of a Sensor Web allowing it to sense its environment facilitated through the use of SIXTH.},
keywords={Android (operating system);augmented reality;helmet mounted displays;middleware;mobile computing;smart phones;mobile devices;SIXTH middleware;augmented reality applications;smartphones;sensor data;augmented reality devices;AR devices;connecting sensors;SensorWeb;AR environments;heterogeneous external sensors;AR experiences;Android-based head mounted display;Smart phones;Middleware;Augmented reality;Libraries;Internet;Educational institutions;Android Augmented Reality;Sensor Web;OSGi},
doi={10.1109/ISMAR.2013.6671787},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671788,
author={S. {Côté} and P. {Trudel}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Third person perspective augmented reality for high accuracy applications},
year={2013},
volume={},
number={},
pages={247-248},
abstract={We proposed a 6 degrees of freedom augmentation system aimed at meeting the high accuracy requirements of engineering tasks. A stationary panoramic video camera captures a stream that is augmented by a portable computer. A handheld tablet device located in the same area broadcasts its instantaneous orientation, and receives the augmented view in the corresponding orientation, in real time. The panoramic camera can also be moved to other locations and simultaneously tracked by the system, providing 6 degrees of freedom augmentation. This gives the user a third person perspective augmentation, which is very precise and potentially more accurate than handheld augmentation.},
keywords={augmented reality;tracking;third person perspective augmented reality;6 degrees of freedom augmentation system;engineering tasks;stationary panoramic video camera;portable computer;handheld tablet device;panoramic camera;Cameras;Solid modeling;Augmented reality;Accuracy;Portable computers;Tracking;Design automation;Augmented reality;third person perspective;accuracy;3D model;engineering},
doi={10.1109/ISMAR.2013.6671788},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671789,
author={M. {Dubská} and I. {Szentandrási} and M. {Zachariáš} and A. {Herout}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Poor man's SimulCam: Real-time and effortless matchmoving},
year={2013},
volume={},
number={},
pages={249-250},
abstract={In this article, we propose an instant matchmoving solution for green screen. It uses a recent technique of planar uniform marker fields. Marker fields are an extension of planar markers used in augmented reality, offering better reliability and performance suitable for our task: tolerance to occlusion, speed of detection, and use of arbitrary low-contrast colors. We show that marker fields of shades of green (or blue or other color) can be used to obtain an instant and effortless camera pose estimation. We provide exemplar applications of the presented technique: virtual camera/simulcam and live storyboarding or shot prototyping. The matchmoving technique based on marker fields of shades of green is very computationally efficient - our measurements show that the matchmoving preview and living storyboard editing and recording can be easily done on today's ultramobile devices. Our technique is thus available to anyone at low cost and with easy setup, opening space for new levels of filmmakers' creative expression.},
keywords={augmented reality;cameras;pose estimation;real-time systems;reliability;SimulCam;real-time matchmoving;effortless matchmoving;planar uniform marker fields;augmented reality;reliability;occlusion;speed of detection;camera pose estimation;virtual camera;storyboarding;shot prototyping;Cameras;Image color analysis;Green products;Reliability;Estimation;Color;Real-time systems;Chroma Key;Green Screen;Camera Pose Estimation;MatchMoving;Film Tricks;Movie Production;Storyboard},
doi={10.1109/ISMAR.2013.6671789},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671790,
author={T. {Engelke} and J. {Keil} and P. {Rojtberg} and F. {Wientapper} and S. {Webel} and U. {Bockholt}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Content first - A concept for industrial augmented reality maintenance applications using mobile devices},
year={2013},
volume={},
number={},
pages={251-252},
abstract={Although AR has a long history in the area of maintenance and service-support in industry, there still is a lack of lightweight, yet practical solutions for handheld AR systems in everyday workflows. Attempts to support complex maintenance tasks with AR still miss reliable tracking techniques, simple ways to be integrated into existing maintenance environments, and practical authoring solutions, which minimize costs for specialized content generation. We present a general, customisable application framework, allowing to employ AR and VR techniques in order to support technicians in their daily tasks. In contrast to other systems, we do not aim to replace existing support systems such as traditional manuals. Instead we integrate well-known AR- and novel presentation techniques with existing instruction media. To this end practical authoring solutions are crucial and hence we present an application development system based on web-standards such as HTML,CSS and X3D.},
keywords={augmented reality;maintenance engineering;mobile handsets;X3D;CSS;HTML;Web standards;application development system;VR techniques;customisable application framework;specialized content generation;maintenance environments;tracking techniques;complex maintenance tasks;handheld AR systems;service support;mobile devices;industrial augmented reality maintenance applications;Three-dimensional displays;Maintenance engineering;Solid modeling;Augmented reality;Context;Media;Documentation;H.5.1 [Multimedia Information Systems]: Artificial, augmented, and virtual realities—Life Cycle;H.5.2 [User Interfaces]: Training, help, and documentation—Generation and Evaluation},
doi={10.1109/ISMAR.2013.6671790},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671791,
author={M. {Ferreira} and P. {Gomes} and M. K. {Silvéria} and F. {Vieira}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Augmented Reality driving supported by Vehicular Ad Hoc Networking},
year={2013},
volume={},
number={},
pages={253-254},
abstract={The confined space of a car and the configuration of its controls and displays towards the driver, offer significant advantages for Augmented Reality (AR) systems in terms of the immersion level provided to the user. In addition, the inherent mobility and virtually unlimited power autonomy transform cars into perfect mobile computing platforms. However, the limited network connectivity that is currently available in automobiles leads to the design of Advanced Driver Assistance Systems (ADAS) that create AR objects based only on the information generated by on-board sensors, stored maps and databases, and eventually high-latency online content for Internet-enabled vehicles. By combining the new paradigm of Vehicular Ad Hoc Networking (VANET) with AR human machine interfaces, we show that it is possible to design novel cooperative ADAS, that base the creation of AR content on the information collected from neighbouring vehicles or roadside infrastructures. We provide a prototype implementation of a visual AR system that can significantly improve the driving experience.},
keywords={augmented reality;driver information systems;Internet;mobile computing;user interfaces;vehicular ad hoc networks;augmented reality driving;vehicular ad hoc networking;immersion level;mobile computing platforms;virtually unlimited power autonomy;advanced driver assistance systems;ADAS;AR objects;Internet-enabled vehicles;VANET;AR human machine interfaces;visual AR system;Vehicles;Glass;Augmented reality;Visualization;Streaming media;Acoustics;Vehicular ad hoc networks;C.6.1 [Network Architecture and Design]: [—Wireless communication;H.5.1 [Multimedia Information Systems]: Artificial, augmented, and virtual realities—Video;I.4.9 [Applications]: Miscellaneous—},
doi={10.1109/ISMAR.2013.6671791},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671792,
author={L. {Gruber} and P. {Sen} and T. {Höllerer} and D. {Schmalstieg}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Acceleration methods for radiance transfer in photorealistic augmented reality},
year={2013},
volume={},
number={},
pages={255-256},
abstract={Radiance transfer computation from unknown real-world environments is an intrinsic task in probe-less photometric registration for photorealistic augmented reality, which affects both the accuracy of the real-world light estimation and the quality of the rendering. We discuss acceleration methods that can reduce the overall ray-tracing costs for computing the radiance transfer for photometric registration in order to free up resources for more advanced augmented reality lighting. We also present evaluation metrics for a systematic evaluation.},
keywords={augmented reality;brightness;ray tracing;rendering (computer graphics);acceleration methods;radiance transfer;photorealistic augmented reality;ray-tracing costs reduction;photometric registration;augmented reality lighting;Lighting;Geometry;Acceleration;Cameras;Rendering (computer graphics);Vectors;Augmented reality;H.5.1 [Information Interfaces and Presentation]: Artificial, Augmented,Virtual Realities—;I.4.8 [Image Processing and Computer Vision]: Photometric registration—3D Reconstruction;I.3.3 [Computer Graphics]: Image Generation—Ray Tracing-Spherical Harmonics;},
doi={10.1109/ISMAR.2013.6671792},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671793,
author={T. N. {Hoang} and R. T. {Smith} and B. H. {Thomas}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Passive Deformable Haptic glove to support 3D interactions in mobile augmented reality environments},
year={2013},
volume={},
number={},
pages={257-258},
abstract={We present a passive deformable haptic (PDH) glove to enhance mobile immersive augmented reality manipulation with a sense of computer-captured touch, responding to objects in the physical environment. We extend our existing pinch glove design with a Digital Foam sensor, placed under the palm of the hand. The novel glove input device supports a range of touch-activated, precise, direct manipulation modeling techniques with tactile feedback including hole-punching, trench cutting, and chamfer creation. The PDH glove helps improve a user's task performance time, decrease error rate and erroneous hand movements, and reduce fatigue.},
keywords={augmented reality;data gloves;mobile computing;tactile sensors;passive deformable haptic glove;3D interactions;mobile augmented reality environments;PDH glove;mobile immersive augmented reality manipulation;computer-captured touch;pinch glove design;Digital Foam sensor;glove input device;direct manipulation modeling techniques;tactile feedback;hole-punching;trench cutting;chamfer creation;Haptic interfaces;Materials;Mobile communication;Augmented reality;Thumb;Nails;Passive Haptics;Augmented Reality;Pinch Gloves;Input Device;Interaction Technique},
doi={10.1109/ISMAR.2013.6671793},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671794,
author={N. {Kawai} and T. {Sato} and N. {Yokoya}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Diminished reality considering background structures},
year={2013},
volume={},
number={},
pages={259-260},
abstract={This paper proposes a new diminished reality method for 3D scenes considering background structures. Most conventional methods using image inpainting assumes that the background around a target object is almost planar. In this study, approximating the background structure by the combination of local planes, perspective distortion of texture is corrected and searching area is limited for improving the quality of image inpainting. The temporal coherence of texture is preserved using the estimated structures and camera pose estimated by Visual-SLAM.},
keywords={augmented reality;cameras;image reconstruction;image texture;pose estimation;diminished reality method;background structures;3D scenes;image inpainting;local planes;texture perspective distortion;texture temporal coherence;structure estimation;camera pose estimation;visual-SLAM;Cameras;Image segmentation;Vectors;Real-time systems;Coherence;Search problems;Three-dimensional displays;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities;I.4.9 [Computing Methodologies]: Image Processing and Computer Vision—Applications},
doi={10.1109/ISMAR.2013.6671794},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671795,
author={S. {Kim} and G. A. {Lee} and N. {Sakata} and A. {Dünser} and E. {Vartiainen} and M. {Billinghurst}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Study of augmented gesture communication cues and view sharing in remote collaboration},
year={2013},
volume={},
number={},
pages={261-262},
abstract={In this research, we explore how different types of augmented gesture communication cues can be used under different view sharing techniques in a remote collaboration system. In a pilot study, we compared four conditions: (1) Pointers on Still Image, (2) Pointers on Live Video, (3) Annotation on Still Image, and (4) Annotation on Live Video. Through this study, we found three results. First, users collaborate more efficiently using annotation cues than pointer cues for communicating object position and orientation information. Second, live video becomes more important when quick feedback is needed. Third, the type of gesture cue has more influence on performance and user preference than the type of view sharing method.},
keywords={augmented reality;gesture recognition;video communication;video signal processing;augmented gesture communication cues;view sharing techniques;remote collaboration system;still image annotation;live video annotation;still image pointers;live video pointers;annotation cues;pointer cues;object position;orientation information;quick feedback;user preference;video conferencing;Collaboration;Portable computers;Streaming media;Visualization;Augmented reality;Speech;Video Conferencing;Augmented Reality},
doi={10.1109/ISMAR.2013.6671795},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671796,
author={D. {Kurz} and P. G. {Meier} and A. {Plopski} and G. {Klinker}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={An outdoor ground truth evaluation dataset for sensor-aided visual handheld camera localization},
year={2013},
volume={},
number={},
pages={263-264},
abstract={We introduce the first publicly available test dataset for outdoor handheld camera localization comprising over 45,000 real camera images of an urban environment captured under natural camera motions and different illumination settings. For all these images the dataset not only contains readings of the sensors attached to the camera, but also ground truth information on the geometry and texture of the environment and the full 6DoF ground truth camera pose. This poster describes the extensive process of creating this comprehensive dataset that we have made available to the public. We hope this not only enables researchers to objectively evaluate their camera localization and tracking algorithms and frameworks on realistic data but also stimulates further research.},
keywords={cameras;geometry;image texture;pose estimation;6DoF ground truth camera pose;image texture;geometry;urban environment;camera images;sensor-aided visual handheld camera localization;outdoor ground truth evaluation dataset;Cameras;Three-dimensional displays;Mobile handsets;Solid modeling;Robot sensing systems;Electronic mail;Visualization},
doi={10.1109/ISMAR.2013.6671796},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671797,
author={D. {Larnaout} and V. {Gay-Bellile} and S. {Bourgeois} and B. {Labbé} and M. {Dhome}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Fast and automatic city-scale environment modeling for an accurate 6DOF vehicle localization},
year={2013},
volume={},
number={},
pages={265-266},
abstract={To provide high quality augmented reality service in a car navigation system, accurate 6DoF localization is required. To ensure such accuracy, most of current vision-based solutions rely on an off-line large scale modeling of the environment. Nevertheless, while existing solutions require expensive equipments and/or a prohibitive computation time, we propose in this paper a complete framework that automatically builds an accurate city scale database using only a standard camera, a GPS and Geographic Information System (GIS). As illustrated in the experiments, only few minutes are required to model large scale environments. The resulting databases can then be used during a localization algorithm for high quality Augmented Reality experiences.},
keywords={augmented reality;automobiles;computer vision;geographic information systems;traffic engineering computing;camera;GIS;geographic information system;GPS;city scale database;off-line large scale modeling;vision-based solutions;car navigation system;augmented reality service;6DOF vehicle localization;automatic city-scale environment modeling;Buildings;Three-dimensional displays;Databases;Global Positioning System;Solid modeling;Cameras;Accuracy;I.4.8 [Image Processing and Computer Vision]: Scene Analysis—Tracking},
doi={10.1109/ISMAR.2013.6671797},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671798,
author={F. {Lauber} and A. {Butz}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Are HMDs the better HUDs?},
year={2013},
volume={},
number={},
pages={267-268},
abstract={Head-mounted displays (HMDs) have the potential to overcome some of the technological limitations of currently existing automotive head-up displays (HUDs), such as the limited field of view and the restrictive boundaries of the windshield. In an initial study we evaluated the use of HMDs in cars by means of a typical HUD visualization, using a HUD as baseline output technology. We found no significant differences in terms of driving performance, physical uneasiness or visual distraction. User statements revealed several advantages and drawbacks of the different output technologies apart from technological maturity and ergonomics. These results will hopefully inspire researchers as well as application developers and even might lead us to novel HMD visualization approaches.},
keywords={automobiles;automotive components;data visualisation;ergonomics;head-up displays;helmet mounted displays;traffic engineering computing;head-mounted displays;technological limitations;automotive head-up displays;field of view;windshield;HUD visualization;baseline output technology;driving performance;physical uneasiness;visual distraction;technological maturity;ergonomics;HMD visualization;Visualization;Vehicles;Mirrors;Augmented reality;Data visualization;Educational institutions;Automotive engineering;head-mounted display;head-up display;mixed reality},
doi={10.1109/ISMAR.2013.6671798},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671799,
author={C. {Léonet} and G. {Simon} and M. {Berger}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={In-situ interactive modeling using a single-point laser rangefinder coupled with a new hybrid orientation tracker},
year={2013},
volume={},
number={},
pages={269-270},
abstract={We present a method for in situ modeling of polygonal scenes, using a laser rangefinder, an IMU and a camera. The main contributions of this work are a well-founded calibration procedure, a new hybrid, driftless orientation tracking method and an easy-to-use interface based on natural interactions.},
keywords={cameras;interactive systems;laser ranging;solid modelling;target tracking;in-situ interactive modeling;single-point laser rangefinder;orientation tracker;polygonal scenes;IMU;camera;Cameras;Laser beams;Three-dimensional displays;Measurement by laser beam;Calibration;Laser modes;Solid modeling;I.2.10 [Vision and Scene Understanding]: Modeling and recovery of physical attributes—Motion},
doi={10.1109/ISMAR.2013.6671799},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671800,
author={W. {Lu} and D. {Feng} and S. {Feiner} and Q. {Zhao} and H. B. {Duh}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Subtle cueing for visual search in head-tracked head worn displays},
year={2013},
volume={},
number={},
pages={271-272},
abstract={Goal-oriented visual search in augmented reality can be facilitated by using visual cues to call attention to a target. However, traditional use of explicit cues can degrade visual search performance due to scene distortion, occlusion and addition of visual clutter. In contrast, Subtle Cueing has been previously proposed as an alter-native to explicit cueing, but little is known about how well it works for head-tracked head worn displays (HWDs). We investigated the effect of Subtle Cueing for head-tracked head worn displays, using visual search research methods in simulated augmented reality environments. Our user study found that Subtle Cueing improves visual search performance, and serves as a feasible cueing mechanism for AR environments using HWDs.},
keywords={augmented reality;clutter;helmet mounted displays;query formulation;subtle cueing;head-tracked head worn displays;goal-oriented visual search;augmented reality;scene distortion;occlusion;visual clutter;Visualization;Augmented reality;Head;Magnetic heads;Clutter;Erbium;Mobile communication;Attention;Subtle visual cueing;Visual search},
doi={10.1109/ISMAR.2013.6671800},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671801,
author={K. {Makita} and J. {Nishida} and T. {Ishikawa} and T. {Okuma} and M. {Kourogi} and T. {Vincent} and L. {Nigay} and J. {Yamashita} and H. {Kuzuoka} and T. {Kurata}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Photo-shoot localization of a mobile camera based on registered frame data of virtualized reality models},
year={2013},
volume={},
number={},
pages={273-274},
abstract={This paper presents a study of a method for estimating the position and orientation of a photo-shoot in indoor environments for augmented reality applications. Our proposed localization method is based on registered frame data of virtualized reality models, which are photos with known photo-shoot positions and orientations, and depth data. Because registered frame data are secondary product of modeling process, additional works are not necessary to create registered frame data especially for the localization. In the method, a photo taken by a mobile camera is compared to registered frame data for the localization. Since registered frame data are linked with photo-shoot position, orientation, and depth data, 3D coordinates of each pixel on the photo of registered frame data is available. We conducted experiments with employing five techniques of the estimation for comparative evaluations.},
keywords={augmented reality;cameras;pose estimation;photo-shoot localization;mobile camera;registered frame data;virtualized reality models;position estimation;augmented reality;3D coordinates;Cameras;Image edge detection;Data models;Feature extraction;Estimation;Mobile communication;Solid modeling;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, Augmented, and Virtual Realities;1.4.8 [Image Processing and Computer Vision]: Scene Analysis—Sensor Fusion Tracking},
doi={10.1109/ISMAR.2013.6671801},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671802,
author={T. {Mashita} and H. {Yasuhara} and A. {Plopski} and K. {Kiyokawa} and H. {Takemura}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={In-situ lighting and reflectance estimations for indoor AR systems},
year={2013},
volume={},
number={},
pages={275-276},
abstract={We introduce an in-situ lighting and reflectance estimation method that does not require specific light probes and/or preliminary scanning. Our method uses images taken from multiple viewpoints while data accumulation and lighting and reflectance estimations run in the background of the primary AR system. As a result, our method requires little in the way of manipulations for image collection because it consists primarily of image processing and optimization. When used, lighting directions and initial optimization values are estimated via image processing. Eventually, the full parameters are obtained by optimization of the differences between real images. This system uses current best parameters because the parameter estimation and input image updates are run independently.},
keywords={augmented reality;image processing;optimisation;parameter estimation;indoor AR systems;in-situ lighting;reflectance estimation method;light probes;viewpoints;data accumulation;lighting estimations;image collection;image processing;lighting directions;optimization values;real images;parameter estimation;image updates;Lighting;Estimation;Optimization;Light sources;Image color analysis;Educational institutions;Electronic mail;H.5.1 [Information interfaces and presentation]: Artificial, Augmented, Virtual Realities;I.4.8 [Image Processing and Computer Vision]: Photometric registration},
doi={10.1109/ISMAR.2013.6671802},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671803,
author={ {Ma Meng} and P. {Fallavollita} and T. {Blum} and U. {Eck} and C. {Sandor} and S. {Weidert} and J. {Waschke} and N. {Navab}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Kinect for interactive AR anatomy learning},
year={2013},
volume={},
number={},
pages={277-278},
abstract={Education of anatomy is a challenging but crucial element in educating medical professionals, but also for general education of pupils. Our research group has previously developed a prototype of an Augmented Reality (AR) magic mirror which allows intuitive visualization of realistic anatomical information on the user. However, the current overlay is imprecise as the magic mirror depends on the skeleton output from Kinect. These imprecisions affect the quality of education and learning. Hence, together with clinicians we have defined bone landmarks which users can touch easily on their body while standing in front of the sensor. We demonstrate that these landmarks allow the proper deformation of medical data within the magic mirror and onto the human body, resulting in a more precise augmentation.},
keywords={augmented reality;biomedical education;bone;computer aided instruction;data visualisation;medical computing;interactive AR anatomy learning;anatomy education;augmented reality;AR magic mirror;intuitive visualization;realistic anatomical information;skeleton output;Kinect;education quality;learning quality;bone landmarks;sensor;medical data deformation;human body;Mirrors;Biomedical imaging;Education;Augmented reality;Bones;Computed tomography;Augmented Reality;Kinect;Anatomy Learning},
doi={10.1109/ISMAR.2013.6671803},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671804,
author={F. {Okura} and M. {Kanbara} and N. {Yokoya}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Interactive exploration of augmented aerial scenes with free-viewpoint image generation from pre-rendered images},
year={2013},
volume={},
number={},
pages={279-280},
abstract={This study proposes a framework to photorealistically synthesize virtual objects and virtualized real-world. We combine the offline rendering of virtual objects and the free-viewpoint image generation to take advantage of the higher quality of offline rendering without the computational cost of online computer graphics (CG) rendering; i.e., it incurs only the cost of the online computation for the free-viewpoint image generation. In addition, the generation of structured viewpoints (e.g., at every grid point) reduces the computational costs required to online process.},
keywords={geophysical image processing;rendering (computer graphics);virtual reality;interactive exploration;augmented aerial scenes;prerendered images;photorealistic synthesis;virtual objects;virtualized real-world;offline rendering;free-viewpoint image generation;online computer graphics rendering;CG rendering;structured viewpoints generation;grid point;Three-dimensional displays;Solid modeling;Rendering (computer graphics);Image generation;Cameras;Lighting;Computational efficiency},
doi={10.1109/ISMAR.2013.6671804},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671805,
author={J. {Orlosky} and K. {Kiyokawa} and H. {Takemura}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Towards intelligent view management: A study of manual text placement tendencies in mobile environments using video see-through displays},
year={2013},
volume={},
number={},
pages={281-282},
abstract={When viewing content in a see-through head mounted display (HMD), displaying readable information is still difficult when text is overlayed onto a changing background or lighted surface. Moving text or content to a more appropriate place on the screen through automation or intelligent algorithms is one viable solution to this kind of issue. However, many of these algorithms fail to act as a human would when placing text in a more appropriate location in real time. In order to improve these text and view management algorithms, we report the results and analysis of an experiment designed to evaluate user tendencies when placing virtual text in the real world through an HMD. In the conducted experiment, 20 users manually overlayed text in real time onto 4 different videos taken from the first-person perspective of a pedestrian. We find that users have a tendency to place overlayed text in locations near the center of the viewing field, gravitating towards a point just below the horizon. Common locations for text overlay such as walls, shaded areas, and pavement are classified and discussed.},
keywords={helmet mounted displays;mobile computing;intelligent view management;manual text placement;mobile environments;video see-through displays;see-through head mounted display;HMD;virtual text;pedestrian;Real-time systems;Algorithm design and analysis;Augmented reality;Streaming media;Head;Visualization;Roads;Text Overlay;View Management;Automation;Head Mounted Display;Heads Up Display},
doi={10.1109/ISMAR.2013.6671805},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671806,
author={M. {Otsuki} and P. {Milgram}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Psychophysical exploration of stereoscopic pseudo-transparency},
year={2013},
volume={},
number={},
pages={283-284},
abstract={We report an experiment related to perceiving (virtual) objects in the vicinity of (real) surfaces when using stereoscopic augmented reality displays. In particular, our goal was to explore the effect of various visual surface features on both perception of object location and perception of surface transparency. Surface features were manipulated using random dot patterns on a simulated real object surface, by manipulating dot size, dot density, and whether or not objects placed behind the surface were partially occluded by the surface.},
keywords={augmented reality;stereo image processing;three-dimensional displays;psychophysical exploration;stereoscopic pseudo-transparency;virtual objects;real object surface;stereoscopic augmented reality display;visual surface features;object location perception;surface transparency perception;surface features;random dot patterns;dot size manipulation;dot density manipulation;Augmented reality;Surface texture;Stereo image processing;Surface treatment;Visualization;Educational institutions;Three-dimensional displays;Human factors;stereoscopic augmented reality;pseudo-transparency;transparency perception},
doi={10.1109/ISMAR.2013.6671806},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671807,
author={F. {Pankratz} and A. {Dippon} and T. {Coskun} and G. {Klinker}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={User awareness of tracking uncertainties in AR navigation scenarios},
year={2013},
volume={},
number={},
pages={285-286},
abstract={Current Augmented Reality navigation applications for pedestrians usually do not visualize tracking errors. However, tracking uncertainties can accumulate so that the user is presented with a distorted impression of navigation accuracy. To increase the awareness of users about potential imperfections of the tracking at a given time, we alter the visualization of the navigation system. We developed four visualization and error visualization concepts and used a controlled Mixed Reality environment to conduct a pilot study. We found that, while error visualization has the potential to improve AR navigation systems, it is difficult to find suitable visualizations, which are correctly understood among the users.},
keywords={augmented reality;navigation;pedestrians;user awareness;tracking uncertainties;AR navigation scenarios;augmented reality navigation applications;pedestrians;tracking errors;navigation accuracy;error visualization;mixed reality environment;AR navigation systems;Visualization;Navigation;Target tracking;Augmented reality;Accuracy;Color;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities},
doi={10.1109/ISMAR.2013.6671807},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671808,
author={A. {Petit} and E. {Marchand} and K. {Kanani}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Augmenting markerless complex 3D objects by combining geometrical and color edge information},
year={2013},
volume={},
number={},
pages={287-288},
abstract={This paper presents a method to address the issue of augmenting a markerless 3D object with a complex shape. It relies on a model-based tracker which takes advantage of GPU acceleration and 3D rendering in order to handle the complete 3D model, whose sharp edges are efficiently extracted. In the pose estimation step, we propose to robustly combine geometrical and color edge-based features in the nonlinear minimization process, and to integrate multiple-hypotheses in the geometrical edge-based registration phase. Our tracking method shows promising results for augmented reality applications, with a Kinect-based reconstructed 3D model.},
keywords={augmented reality;edge detection;feature extraction;graphics processing units;image colour analysis;image reconstruction;image registration;object tracking;pose estimation;rendering (computer graphics);solid modelling;markerless complex 3D object augmentation;geometrical information;color edge information;model-based tracker;GPU acceleration;3D rendering;3D model;sharp edge extraction;pose estimation step;geometrical feature;color edge-based feature;nonlinear minimization process;geometrical edge-based registration phase;tracking method;augmented reality applications;Kinect-based reconstructed 3D model;Three-dimensional displays;Solid modeling;Image edge detection;Image color analysis;Robustness;Augmented reality;Computational modeling;3D visual tracking;model-based tracking},
doi={10.1109/ISMAR.2013.6671808},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671809,
author={T. {Piumsomboon} and A. {Clark} and M. {Billinghurst}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={KITE: Platform for mobile Augmented Reality gaming and interaction using magnetic tracking and depth sensing},
year={2013},
volume={},
number={},
pages={289-290},
abstract={In this paper, we describe the KITE, a mobile Augmented Reality (AR) platform that uses a magnetic tracker and a depth sensor for games and interaction development that is typically only available on a desktop system. We have achieved this using off-the-shelf hardware and efficient software that can be easily assembled and executed. We demonstrate four possible modalities based on hand input to provide a platform that game and interaction designers can use to explore new possibilities for gaming in AR.},
keywords={augmented reality;computer games;mobile computing;KITE;mobile augmented reality gaming;interaction development;magnetic tracking;depth sensing;AR platform;magnetic tracker;depth sensor;desktop system;Hardware;Games;Cameras;Mobile communication;Augmented reality;Software;Transmitters;Augmented reality;gaming platform;magnetic tracking;depth sensing},
doi={10.1109/ISMAR.2013.6671809},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671810,
author={G. {Reinisch} and C. {Arth} and D. {Schmalstieg}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Panoramic mapping on a mobile phone GPU},
year={2013},
volume={},
number={},
pages={291-292},
abstract={Creating panoramic images in real-time is an expensive operation for mobile devices. Mapping of individual pixels into the panoramic image is the main focus of this paper, since it is one of the most time consuming parts. The pixel-mapping process is transferred from the Central Processing Unit (CPU) to the Graphics Processing Unit (GPU). The independence of pixels being projected allows OpenGL shaders to perform this operation very efficiently. We propose a shader-based mapping approach and confront it with an existing solution. The application is implemented for Android phones and works fluently on current generation devices.},
keywords={graphics processing units;image processing;mobile computing;smart phones;panoramic mapping;mobile phone GPU;panoramic image creation;mobile device;pixel-mapping process;central processing unit;CPU;graphics processing unit;OpenGL shaders;shader-based mapping approach;Android phones;Brightness;Graphics processing units;Cameras;Mobile handsets;Real-time systems;Augmented reality;Central Processing Unit;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities; Evaluation/methodology},
doi={10.1109/ISMAR.2013.6671810},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671811,
author={A. {Suzuki} and Y. {Manabe} and N. {Yata}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Design of an AR marker for cylindrical surface},
year={2013},
volume={},
number={},
pages={293-294},
abstract={This paper proposes an augmented reality marker that can be robustly detected even on a cylindrical surface. The marker enables the surface normal estimation of a cylindrical object to realize the presentation of appropriate virtual information on the object. Conventional markers have difficulty detecting and obtaining accurate surface normal in the presence of occlusion or distortion of the marker in the image. Furthermore, it is difficult to identify a feature on a cylindrical object on which to position a marker. These problems are resolved by relying on the characteristic that a line parallel to the central axis of the cylinder maintains linearity. In addition, surface normal is calculated by estimating the object's shape by using transformation matrices.},
keywords={augmented reality;matrix algebra;AR marker;cylindrical surface;augmented reality marker;surface normal estimation;virtual information;transformation matrices;Estimation;Augmented reality;Vectors;Multimedia communication;Cameras;Silicon;Augmented reality;Marker;Cylindrical surface},
doi={10.1109/ISMAR.2013.6671811},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671812,
author={T. {Taketomi} and K. {Okada} and G. {Yamamoto} and J. {Miyazaki} and H. {Kato}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Geometric registration for zoomable camera using epipolar constraint and pre-calibrated intrinsic camera parameter change},
year={2013},
volume={},
number={},
pages={295-296},
abstract={In general, video see-through based augmented reality (AR) cannot change the magnification of camera zooming parameter due to the difficulty of dealing with changes in intrinsic camera parameters. To realize the usage of camera zooming in AR, we propose a novel simultaneous intrinsic and extrinsic camera parameter estimation method based on an energy minimization framework. Our method is composed of the online and offline stages. An intrinsic camera parameter change depending on the zoom values is calibrated in the offline stage. Intrinsic and extrinsic camera parameters are then estimated based on the energy minimization framework in the online stage. In our method, two energy terms are added to the conventional marker-based camera parameter estimation method. One is reprojection errors based on the epipolar constraint. The other is the constraint of continuity of zoom values. By using a novel energy function, our method can estimate accurate intrinsic and extrinsic camera parameters. In an experiment, we confirmed that the proposed method can achieve accurate camera parameter estimation during camera zooming.},
keywords={augmented reality;calibration;cameras;parameter estimation;geometric registration;zoomable camera;epipolar constraint;precalibrated intrinsic camera parameter change;AR;intrinsic camera parameter estimation method;extrinsic camera parameter estimation method;energy minimization framework;energy terms;marker-based camera parameter estimation method;reprojection errors;zoom values continuity constraint;energy function;Cameras;Parameter estimation;Estimation;Augmented reality;Electronic mail;Computer vision;Calibration;H.5.1 [Multimedia Information Systems]: Artificial, augmented, and virtual realities;I.4.8 [Image Processing and Computer Vision]: Scene Analysis—Tracking;I.5.4 [Pattern Recognition]: Applications—Computer Vision},
doi={10.1109/ISMAR.2013.6671812},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671813,
author={H. {Tanaka} and Y. {Sumi} and Y. {Matsumoto}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Further stabilization of a microlens-array-based fiducial marker},
year={2013},
volume={},
number={},
pages={297-298},
abstract={Fiducial markers (AR/visual markers) are still useful tools in many AR/MR applications. But conventional markers have two fundamental problems in orientation estimation. One is degradation of orientation accuracy in frontal observation. The other is “pose ambiguity” where the estimated orientation repeats switching between two values. We previously developed a novel marker “ArrayMark” which uses a microlens array and solved the former problem. This time we propose a practical solution to the latter problem by improving the ArrayMark. We attach an additional reference point to detect the occurrence of invalid estimation, and modify the orientation of the marker by inverting the zenith-angle of the visual-line. This marker enables stable pose estimation from a one-shot image without using any filtering techniques. The method is applicable to conventional markers, too. We demonstrated the availability of this improvement to the pose-ambiguity problem.},
keywords={augmented reality;filtering theory;microlenses;pose estimation;stabilization;microlens-array-based fiducial marker;AR marker;visual marker;orientation estimation;orientation accuracy;pose ambiguity;ArrayMark;reference point;zenith-angle;visual-line;pose estimation;filtering techniques;pose-ambiguity problem;Estimation;Cameras;Visualization;Augmented reality;Lenses;Microoptics;Accuracy;position and orientation tracking technology;vision-based registration and tracking;performance issues},
doi={10.1109/ISMAR.2013.6671813},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671814,
author={T. {Toyama} and W. {Suzuki} and A. {Dengel} and K. {Kise}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={User attention oriented augmented reality on documents with document dependent dynamic overlay},
year={2013},
volume={},
number={},
pages={299-300},
abstract={When we read a document (any kind of, scientific papers, novels, etc.), we often encounter a situation that the information from the reading document is too less to comprehend what the author(s) would like to convey. In this paper, we demonstrate how the combination of a wearable eye tracker, a see-through head-mounted display (HMD) and an image based document retrieval engine enhances people's reading experiences. By using our proposed system, the reader can get supportive information in the see-through HMD when he wants. A wearable eye tracker and a document retrieval engine are used to detect which line in the document the reader is reading. We propose a method to detect the reader's attention on a word in a reading document, in order to present information at a preferable moment. Furthermore, we also propose a method to project a point of the document to a point of the HMD screen, by calculating the pose of the reading document in the camera image. This projection enables the system to overlay the information dynamically in an augmented view on the reading line. The results from the user study and the experiments show the potential of the proposed system in a practical use case.},
keywords={augmented reality;helmet mounted displays;information retrieval;user attention oriented augmented reality;document dependent dynamic overlay;wearable eye tracker;see-through head-mounted display;HMD;image based document retrieval engine;camera image;Augmented reality;Calibration;Visualization;Feature extraction;Engines;Dictionaries;Tracking;H.5.2 [INFORMATION INTERFACES AND PRESENTATION (e.g., HCI)]: User Interfaces—Input devices and strategies},
doi={10.1109/ISMAR.2013.6671814},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671815,
author={R. {Berry} and E. {Edmonds} and A. {Johnston}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Representational systems with tangible and graphical elements},
year={2013},
volume={},
number={},
pages={1-4},
abstract={This research centres on the development of a number of prototype interactive systems, each of which uses a tangible means of representation and manipulation of musical elements in musical composition. Data gathered through collaborative prototyping and user studies is analysed using grounded theory methods. The resultant contribution to knowledge includes theory, design criteria and guidelines specific to tangible representations of music. This knowledge will be useful for future design of systems that use tangible representations, particularly for making music. The prototypes themselves also serve as a form of knowledge and as creative works.},
keywords={Augmented Reality;Tangible Interface;Music Composition;Creative Support Tools;Grounded Theory;Qualitative Research},
doi={10.1109/ISMAR.2013.6671815},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671816,
author={U. {Eck} and C. {Sandor} and H. {Laga}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Visuo-haptic augmented reality runtime environment for medical training},
year={2013},
volume={},
number={},
pages={1-4},
abstract={During the last decade, Visuo-Haptic Augmented Reality (VHAR) systems have emerged that enable users to see and touch digital information that is embedded in the real world. They pose unique problems to developers, including the need for precise augmentations, accurate colocation of haptic devices, and efficient concurrent processing of multiple, realtime sensor inputs to achieve low latency. We think that this complexity is one of the main reasons, why VHAR technology has only been used in few user interface research projects. The proposed project's main objective is to pioneer the development of a widely applicable VHAR runtime environment, which meets the requirements of realtime, low latency operation with precise co-location, haptic interaction with deformable bodies, and realistic rendering, while reducing the overall cost and complexity for developers. A further objective is to evaluate the benefits of VHAR user interfaces with a focus on medical training applications, so that creators of future medical simulators or other haptic applications recognize the potential of VHAR.},
keywords={augmented reality;mixed reality;haptic interaction;physically-based simulation;dataflow architectures;medical training},
doi={10.1109/ISMAR.2013.6671816},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671817,
author={N. A. M. {ElSayed} and C. {Sandor} and H. {Laga}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Visual analytics in Augmented Reality},
year={2013},
volume={},
number={},
pages={1-4},
abstract={In the last decade, Augmented Reality has become more mature and is widely adopted on mobile devices. Exploring the available information of a user's environment is one of the key applications. However, current mobile Augmented Reality interfaces are very limited compared to the recently emerging big data exploration tools for desktop computers. Our vision is to bring powerful Visual Analytic tools to mobile Augmented Reality.},
keywords={mixed reality;context-based;visualisation},
doi={10.1109/ISMAR.2013.6671817},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671818,
author={ {Youngkyoon Jang} and W. {Woo}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Unified Visual Perception Model for context-aware wearable AR},
year={2013},
volume={},
number={},
pages={1-4},
abstract={We propose Unified Visual Perception Model (UVPM), which imitates the human visual perception process, for the stable object recognition necessarily required for augmented reality (AR) in the field. The proposed model is designed based on the theoretical bases in the field of cognitive informatics, brain research and psychological science. The proposed model consists of Working Memory (WM) in charge of low-level processing (in a bottomup manner), Long-Term Memory (LTM) and Short-Term Memory (STM), which are in charge of high-level processing (in a top-down manner). WM and LTM/STM are mutually complementary to increase recognition accuracies. By implementing the initial prototype of each boxes of the model, we could know that the proposed model works for stable object recognition. The proposed model is available to support context-aware AR with the optical see-through HMD.},
keywords={H.5.1 [Information Interfaces and Representation (HCI)]: Multimedia Information Systems—Artificial, augmented, and virtual realities;I.4.8 [Image Processing and Computer Vision]: Scene Analysis—Object recognition},
doi={10.1109/ISMAR.2013.6671818},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671819,
author={J. {Orlosky} and K. {Kiyokawa} and H. {Takemura}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Management and manipulation of text in dynamic mixed reality workspaces},
year={2013},
volume={},
number={},
pages={1-4},
abstract={Viewing and interacting with text based content safely and easily while mobile has been an issue with see-through displays for many years. For example, in order to effectively use optical see through Head Mounted Displays (HMDs) in constantly changing dynamic environments, variables like lighting conditions, human or vehicular obstructions in a user's path, and scene variation must be dealt with effectively. My PhD research focuses on answering the following questions: 1) What are appropriate methods to intelligently move digital content such as e-mail, SMS messeges, and news articles, throughout the real world? 2) Once a user stops moving, in what way should dynamics of the current workspace change when migrated to a new static environment? 3) Lastly, how can users manipulate mobile content using the fewest number of interactions possible? My strategy for developing solutions to these problems primarily involves automatic or semi-automatic movement of digital content throughout the real world using camera tracking. I have already developed an intelligent text management system that actively manages movement of text in a user's field of view while mobile [11]. I am optimizing and expanding on this type of management system, developing appropriate interaction methodology, and conducting experiments to verify effectiveness, usability, and safety when used with an HMD in various environments.},
keywords={Scene Analysis;Wearable Display;Heads Up Display;Content Stabilization;Text Placement;View Management},
doi={10.1109/ISMAR.2013.6671819},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671820,
author={C. {Sweeney} and T. {Höllerer} and M. {Turk}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Improved outdoor augmented reality through “Globalization”},
year={2013},
volume={},
number={},
pages={1-4},
abstract={Despite the major interest in live tracking and mapping (e.g., SLAM), the field of augmented reality has yet to truly make use of the rich data provided from large-scale reconstructions generated by structure from motion. This dissertation focuses on extensible tracking and mapping for large-scale reconstructions that enables SfM and SLAM to operate cooperatively to mutually enhance the performance. We describe a multi-user, collaborative augmented reality system that will collectively extend and enhance reconstructions of urban environments at city-scales. Contrary to current outdoor augmented reality systems, this system is capable of continuous tracking through areas previously modeled as well as new, undiscovered areas. Further, we describe a new process called globalization that propagates new visual information back to the global model. Globalization allows for continuous updating of the 3D models with visual data from live users, providing data to fill coverage gaps that are common in 3D reconstructions and to provide the most current view of an environment as it changes over time. The proposed research is a crucial step toward enabling users to augment urban environments with location-specific information at any location in the world for a truly global augmented reality.},
keywords={},
doi={10.1109/ISMAR.2013.6671820},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671821,
author={K. {Waegel} and F. P. {Brooks}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Filling the gaps: Hybrid vision and inertial tracking},
year={2013},
volume={},
number={},
pages={1-4},
abstract={Existing head-tracking systems all suffer from various limitations, such as latency, cost, accuracy, or drift. I propose to address these limitations by using depth cameras and existing 3D reconstruction algorithms to simultaneously localize the camera position and build a map of the environment, providing stable and drift-free tracking. This method is enabled by the recent proliferation of light-weight, inexpensive depth cameras. Because these cameras have a relatively slow frame rate, I combine this technique with a low-latency inertial measurement unit to estimate movement between frames. Using the generated environment model, I further propose a collision avoidance system for use with real walking.},
keywords={I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism—Virtual Reality;I.4.8 [Image Processing and Computer Vision]: Scene Analysis—Tracking},
doi={10.1109/ISMAR.2013.6671821},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671822,
author={T. N. {Hoang} and R. T. {Smith} and B. H. {Thomas}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={3D interactions with a passive deformable haptic glove},
year={2013},
volume={},
number={},
pages={1-6},
abstract={This paper explores enhancing mobile immersive augmented reality manipulations by providing a sense of computer-captured touch through the use of a passive deformable haptic glove that responds to objects in the physical environment. The glove extends our existing pinch glove design with a Digital Foam sensor that is placed under the palm of the hand. The novel glove input device supports a range of touch-activated, precise, direct manipulation modeling techniques with tactile feedback including hole cutting, trench cutting, and chamfer creation. A user evaluation study comparing an image plane approach to our passive deformable haptic glove showed that the glove improves a user's task performance time, decreases error rate and erroneous hand movements, and reduces fatigue.},
keywords={Passive Haptics;Augmented Reality;Pinch Gloves;Input Device;Interaction Technique},
doi={10.1109/ISMAR.2013.6671822},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671823,
author={T. {Bleeker} and {Gun Lee} and M. {Billinghurst}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Ego- and Exocentric interaction for mobile AR conferencing},
year={2013},
volume={},
number={},
pages={1-6},
abstract={In this research we explore how a handheld display (HHD) can be used to provide input into an Augmented Reality (AR) conferencing application shown on a head mounted display (HMD). Although AR has successfully been used for many collaborative applications, there has been little research on using HHD and HMD together to enhance remote conferencing. This research investigates two different HHD interfaces and methods for supporting file sharing in an AR conferencing application. A formal evaluation compared four different conditions and found that an Exocentric view and using Visual cues for requesting content produced the best performance. The results were used to create a set of basic design guidelines for future research and application development.},
keywords={Augmented Reality;mobile;conferencing;hand held display;head mounted display},
doi={10.1109/ISMAR.2013.6671823},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671824,
author={M. {Hossny} and M. {Hossny} and S. {Nahavandi}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={CARMa: Content augmented reality marker},
year={2013},
volume={},
number={},
pages={1-5},
abstract={The current marker-based augmented reality (AR) rendering has demonstrated good results for online and special purpose applications such as computer-assisted tasks and virtual training. However, it fails to deliver a solution for off-line and generic applications such as augmented books, newspapers, and scientific articles. These applications feature too many markers that imposes a serious challenge on the recognition module. This paper proposes a novel design for augmented reality markers. The proposed marker design employs multi-view orthographic projection to derive dense depth maps and relies on splats rendering for visualisation. The main objective is to interpret the marker rather than recognising it. The proposed marker design stores six depth map projections of the 3D model along with their coloured textures in the marker.},
keywords={Marker-based Augmented Reality;Content-based Marker},
doi={10.1109/ISMAR.2013.6671824},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671825,
author={M. {Otsuki} and P. {Milgram}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Psychophysical exploration of stereoscopic pseudo-transparency},
year={2013},
volume={},
number={},
pages={1-6},
abstract={We report a two part experiment related to perceiving (virtual) objects in the vicinity of (real) surfaces when using stereoscopic augmented reality displays. In particular, our goal was to explore the effect of various visual surface features on both perception of object location and perception of surface transparency. Surface features were manipulated using random dot patterns on a simulated real object surface, by manipulating dot size, dot density, and whether or not objects placed behind the surface were partially occluded by the surface.},
keywords={Human factors;stereoscopic augmented reality pseudo-transparency;transparency perception},
doi={10.1109/ISMAR.2013.6671825},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671826,
author={M. {Broecker} and B. H. {Thomas} and R. T. {Smith}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Adapting ray tracing to Spatial Augmented Reality},
year={2013},
volume={},
number={},
pages={1-6},
abstract={Ray tracing is an elegant and intuitive image generation method. The introduction of GPU-accelerated ray tracing and corresponding software frameworks makes this rendering technique a viable option for Augmented Reality applications. Spatial Augmented Reality employs projectors to illuminate physical models and is used in fields that require photorealism, such as design and prototyping. Ray tracing can be used to great effect in this Augmented Reality environment to create scenes of high visual fidelity. However, the peculiarities of SAR systems require that core ray tracing algorithms be adapted to this new rendering environment. This paper highlights the problems involved in using ray tracing in a SAR environment and provides solutions to overcome them. In particular, the following issues are addressed: ray generation, hybrid rendering and view-dependent rendering.},
keywords={Ray tracing;Augmented Reality;Spatial AR},
doi={10.1109/ISMAR.2013.6671826},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671827,
author={Y. {Nakashima} and T. {Sato} and Y. {Uno} and N. {Yokoya} and N. {Kawai}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Augmented reality image generation with virtualized real objects using view-dependent texture and geometry},
year={2013},
volume={},
number={},
pages={1-6},
abstract={Augmented reality (AR) images with virtualized real objects can be used for various applications. However, such AR image generation requires hand-crafted 3D models of that objects, which are usually not available. This paper proposes a view-dependent texture (VDT)- and view-dependent geometry (VDG)-based method for generating high quality AR images, which uses 3D models automatically reconstructed from multiple images. Since the quality of reconstructed 3D models is usually insufficient, the proposed method inflates the objects in the depth map as VDG to repair chipped object boundaries and assigns a color to each pixel based on VDT to reproduce the detail of the objects. Background pixel exposure due to inflation is suppressed by the use of the foreground region extracted from the input images. Our experimental results have demonstrated that the proposed method can successfully reduce above visual artifacts.},
keywords={AR with virtualized real objects;view-dependent texture;view-dependent geometry},
doi={10.1109/ISMAR.2013.6671827},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671828,
author={F. {Lauber} and A. {Butz}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={View management for driver assistance in an HMD},
year={2013},
volume={},
number={},
pages={1-6},
abstract={Head-mounted displays (HMDs) have the potential to overcome some of the technological limitations of currently existing automotive head-up displays (HUDs), such as the limited field of view and the restrictive boundaries of the windshield. However, in a formative study, we identified other, partially known problems with HMDs regarding content stability and occlusion. As a counter-measure we propose a novel layout mechanism for HMD visualization, which, on the one hand, benefits from the unique characteristics of HMDs and, on the other, combines the advantages of head-stabilized and cockpit-stabilized content. By subdividing the HMD's field of view into different slots to which the content is dynamically assigned depending on the user's head rotation, we ensure that the driver's vision is effectively augmented in every possible direction.},
keywords={head-mounted display;view management;mixed reality;head-up display;driver assistance},
doi={10.1109/ISMAR.2013.6671828},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671829,
author={A. G. {Campbell} and D. {Carr} and L. {Görgü} and D. {Lillis} and B. {Kroon} and G. M. P. {O'Hare}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={SIXTH middleware for sensor web enabled AR applications},
year={2013},
volume={},
number={},
pages={1-6},
abstract={We increasingly live in a world where sensors have become truly ubiquitous in nature. Many of these sensors are an integral part of devices such as smartphones, which contain sufficient sensors to allow for their use as Augmented Reality (AR) devices. This AR experience is limited by the precision and functionality of an individual device's sensors and the its capacity to process the sensor data into a useable form. This paper discuss the current work on a mobile version of the SIXTH middleware which allows for creation of Sensor Web enabled AR applications. This paper discusses current work on mobile SIXTH, which involves the creation of a sensor web between different Android and non-Android devices. This has led to several small demonstrators which are discussed in this work in progress paper. Future work on the project will be outline the aims of the project to allow for the integration of additional devices so as to explore new abilities such as leveraging additional proprieties of those devices.},
keywords={Android Augmented Reality;Sensor Web;OSGi Android Augmented Reality;Sensor Web;OSGi},
doi={10.1109/ISMAR.2013.6671829},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671830,
author={I. {Damian} and R. {Bühling} and M. {Obaid} and R. {Buhling} and M. {Billinghurst} and E. {André}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Motion capturing empowered interaction with a virtual agent in an Augmented Reality environment},
year={2013},
volume={},
number={},
pages={1-6},
abstract={We present an Augmented Reality (AR) system where we immerse the user's whole body in the virtual scene using a motion capturing (MoCap) suit. The goal is to allow for seamless interaction with the virtual content within the AR environment. We describe an evaluation study of a prototype application featuring an interactive scenario with a virtual agent. The scenario contains two conditions: in one, the agent has access to the full tracking data of the MoCap suit and therefore is aware of the exact actions of the user, while in the second condition, the agent does not get this information. We then report and discuss the differences we were able to detect regarding the users' perception of the interaction with the agent and give future research directions.},
keywords={Augmented Reality;Motion Capturing;Virtual Agent;Full Body Interaction;Natural Interaction},
doi={10.1109/ISMAR.2013.6671830},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671831,
author={ {Lin Xueting} and T. {Ogawa}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Blur with depth: A depth cue method based on blur effect in augmented reality},
year={2013},
volume={},
number={},
pages={1-6},
abstract={In this paper, a depth cue method based on blur effect in augmented reality is proposed. Different from previous approaches, the proposed method offers an algorithm which estimates the blur effect in the whole scene based on the spatial information in the real world and the intrinsic parameters of the camera. We implemented a prototype using the proposed method and conducted two user tests on how the users might perceive the blur effect rendered by different blurring methods. The test settings are introduced and the results are discussed. The test results show that our blur estimation method is acceptable for moving virtual objects. We also find that the users might prefer a stronger contrast of blur than the blur consistent to the background.},
keywords={Augmented reality;depth cue;blur;thin lens model;user perception},
doi={10.1109/ISMAR.2013.6671831},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671832,
author={D. {Markov-Vetter} and O. {Staadt}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={A pilot study for Augmented Reality supported procedure guidance to operate payload racks on-board the International Space Station},
year={2013},
volume={},
number={},
pages={1-6},
abstract={We present our current state in developing and testing of Augmented Reality supported spaceflight procedures for intra-vehicular payload activities. Our vision is to support the ground team and the flight crew to author and operate easily AR guidelines without programming and AR knowledge. For visualization of the procedural instructions using an HMD, 3D registered visual aids are overlaid onto the payload model operated by additional voice control. Embedded informational resources (e.g., images and videos) are provided through a mobile tangible user interface. In a pilot study that was performed at the ESA European Astronaut Centre by application domain experts, we evaluated the performance, workload and acceptance by comparing our AR system with the conventional method of displaying PDF documents of the procedure.},
keywords={Augmented reality;aerospace;HCI;usability},
doi={10.1109/ISMAR.2013.6671832},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671833,
author={ {Seungwon Kim} and G. A. {Lee} and N. {Sakata}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Comparing pointing and drawing for remote collaboration},
year={2013},
volume={},
number={},
pages={1-6},
abstract={In this research, we explore using pointing and drawing in a remote collaboration system. Our application allows a local user with a tablet to communicate with a remote expert on a desktop computer. We compared performance in four conditions: (1) Pointers on Still Image, (2) Pointers on Live Video, (3) Annotation on Still Image, and (4) Annotation on Live Video. We found that using drawing annotations would require fewer inputs on an expert side, and would require less cognitive load on the local worker side. In a follow-on study we compared the conditions (2) and (4) using a more complicated task. We found that pointing input requires good verbal communication to be effective and that drawing annotations need to be erased after completing each step of a task.},
keywords={Video conferencing;Augmented Reality},
doi={10.1109/ISMAR.2013.6671833},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671834,
author={S. {Martedi} and B. {Thomas} and H. {Saito}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Region-based tracking using sequences of relevance measures},
year={2013},
volume={},
number={},
pages={1-6},
abstract={We present the preliminary results of our proposal: a region-based detection and tracking method of arbitrary shapes. The method is designed to be robust against orientation and scale changes and also occlusions. In this work, we study the effectiveness of sequence of shape descriptors for matching purpose. We detect and track surfaces by matching the sequences of descriptor so called relevance measures with their correspondences in the database. First, we extract stable shapes as the detection target using Maximally Stable Extreme Region (MSER) method. The keypoints on the stable shapes are then extracted by simplifying the outline of the stable regions. The relevance measures that are composed by three keypoints are then computed and the sequences of them are composed as descriptors. During runtime, the sequences of relevance measures are extracted from the captured image and are matched with those in the database. When a particular region is matched with one in the database, the orientation of the region is then estimated and virtual annotations can be superimposed. We apply this approach in an interactive task support system that helps users for creating paper craft objects.},
keywords={Artificial;augmented;virtual realities},
doi={10.1109/ISMAR.2013.6671834},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671835,
author={V. {Ferrer} and {Yifan Yang} and A. {Perdomo} and J. {Quarles}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Consider your clutter: Perception of virtual object motion in AR},
year={2013},
volume={},
number={},
pages={1-6},
abstract={Background motion and visual clutter are present in almost all augmented reality applications. However, there is minimal prior work that has investigated the effects that background motion and clutter (e.g., a busy city street) can have on the perception of virtual object motion in augmented reality. To investigate these issues, we conducted an experiment in which participants' perceptions of changes in overlaid virtual object velocity were evaluated on a black background and a high clutter/motion background. Results offer insights into the impact that background clutter and motion has on perception in augmented reality.},
keywords={Augmented Reality;perception;user study},
doi={10.1109/ISMAR.2013.6671835},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671836,
author={L. {Figueiredo} and R. {dos Anjos} and J. {Lindoso} and E. {Neto} and R. {Roberto} and M. {Silva} and V. {Teichrieb}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Bare hand natural interaction with augmented objects},
year={2013},
volume={},
number={},
pages={1-6},
abstract={In this work in progress we address the problem of interacting with augmented objects. A bare hand tracking technique is developed, which allied to gesture recognition heuristics, enables interaction with augmented objects in an intuitive way. The tracking algorithm uses a flock of features approach that tracks both hands in real time. The interaction occurs by the execution of grasp and release gestures. Physics simulation and photorealistic rendering are added to the pipeline. This way, the tool provides more coherent feedback in order to make the virtual objects look and respond more likely real ones. The pipeline was tested through specific tasks, designed to analyze its performance regarding the easiness of use, precision and response time.},
keywords={bare hand tracking;virtual grasp;augmented objects},
doi={10.1109/ISMAR.2013.6671836},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671837,
author={R. {Budhiraja} and G. A. {Lee} and M. {Billinghurst}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Using a HHD with a HMD for mobile AR interaction},
year={2013},
volume={},
number={},
pages={1-6},
abstract={Mobile Augmented Reality (AR) applications are typically deployed either on head mounted displays (HMD) or handheld displays (HHD). This paper explores novel interaction techniques for a combined HHD-HMD hybrid system that builds on the strengths of each type of device. We use the HMD for viewing AR content and a touch screen HHD for interacting with the content. A prototype system was developed and a user study was conducted comparing four interaction techniques for selection tasks.},
keywords={Augmented reality;wearable computer;head-mounted display;handheld display;gesture interaction},
doi={10.1109/ISMAR.2013.6671837},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671838,
author={M. {Tait} and T. {Tsai} and N. {Sakata} and M. {Billinghurst} and E. {Vartiainen}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={A projected augmented reality system for remote collaboration},
year={2013},
volume={},
number={},
pages={1-6},
abstract={This paper describes an AR system for remote collaboration using a captured 3D model of the local user's scene. In the system a remote user can manipulate the scene independently of the view of the local user and add AR annotations that appear projected into the real world. Results from a pilot study and the design of a further full study are presented.},
keywords={Remote collaboration;Kinect Fusion;Augmented Reality;Projection},
doi={10.1109/ISMAR.2013.6671838},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671839,
author={D. {Ranatunga} and D. {Feng} and M. {Adcock} and B. {Thomas}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Towards object based manipulation in remote guidance},
year={2013},
volume={},
number={},
pages={1-6},
abstract={This paper presents a method for using object based manipulation and spatial augmented reality for the purpose of remote guidance. Previous remote guidance methods have typically not made use of any semantic information about the physical properties of the environment and require the helper and worker to provide context. Our new prototype system introduces a level of abstraction to the remote expert, allowing them to directly specify the object movements required of a local worker. We use 3D tracking to create a hidden virtual reality scene, mirroring the real world, with which the remote expert interacts while viewing a camera feed of the physical workspace. The intended manipulations are then rendered to the local worker using Spatial Augmented Reality (SAR). We report on the implementation of a functional prototype that demonstrates an instance of this approach. We anticipate that techniques such as the one we present will allow more efficient collaborative remote guidance in a range of physical tasks.},
keywords={augmented reality;hidden feature removal;human computer interaction;object based manipulation;spatial augmented reality;semantic information;physical property;prototype system;remote expert;3D tracking;hidden virtual reality scene;camera feed;physical workspace;SAR;collaborative remote guidance;Cameras;Feeds;Three-dimensional displays;Visualization;Australia;Augmented reality;Prototypes;Spatially Augmented Reality;Remote Guidance;Object Manipulation;Multi touch interaction;3D CHI},
doi={10.1109/ISMAR.2013.6671839},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671840,
author={A. {Irlitti} and S. {Von Itzstein} and L. {Alem} and B. {Thomas}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Tangible interaction techniques to support asynchronous collaboration},
year={2013},
volume={},
number={},
pages={1-6},
abstract={Industrial uses of Augmented Reality (AR) are growing, however their uses are consistently fashioned with an emphasis on consumption, delivering additional information to the worker to assist them in the completion of their job. A promising alternative is to allow user data creation during the actual process by the worker performing their duties. This not only allows spatially located annotations to be produced, it also allows an AR scene to be developed in-situ and in real-time. Tangible markers offer a physical interface while also creating physical containers to allow for fluent interactions. This form factor allows both attached and detached annotations, whilst allowing the creation of an AR scene during the process. This annotated scene will allow asynchronous collaboration to be conducted between multiple stakeholders, both locally and remotely. In this paper we discuss our reasoning behind such an approach, and present the current work on our prototype created to test and validate our proposition.},
keywords={augmented reality;haptic interfaces;human computer interaction;tangible interaction techniques;asynchronous collaboration;augmented reality;job completion;user data creation;AR scene;tangible markers;physical interface;physical containers;fluent interactions;attached annotations;detached annotations;Collaboration;Prototypes;Mobile communication;Context;Australia;Augmented reality;Wearable computers;Augmented Reality;Asynchronous Collaboration;Spatial Annotations;Tangible Interaction},
doi={10.1109/ISMAR.2013.6671840},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6671841,
author={ {Huidong Bai} and {Lei Gao} and J. {El-Sana} and M. {Billinghurst}},
booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Markerless 3D gesture-based interaction for handheld Augmented Reality interfaces},
year={2013},
volume={},
number={},
pages={1-6},
abstract={Conventional 2D touch-based interaction methods for handheld Augmented Reality (AR) cannot provide intuitive 3D interaction due to a lack of natural gesture input with real-time depth information. The goal of this research is to develop a natural interaction technique for manipulating virtual objects in 3D space on handheld AR devices. We present a novel method that is based on identifying the positions and movements of the user's fingertips, and mapping these gestures onto corresponding manipulations of the virtual objects in the AR scene. We conducted a user study to evaluate this method by comparing it with a common touch-based interface under different AR scenarios. The results indicate that although our method takes longer time, it is more natural and enjoyable to use.},
keywords={augmented reality;gesture recognition;markerless 3D gesture-based interaction;handheld augmented reality interfaces;2D touch-based interaction methods;real-time depth information;natural interaction technique;virtual object manipulation;3D;handheld AR devices;user fingertip position identification;user fingertip movement identification;gesture mapping;touch-based interface;Three-dimensional displays;Thumb;Augmented reality;Cameras;Educational institutions;Mobile handsets;3D interaction technique;natural gesture interaction;fingertip detection;handheld augmented reality},
doi={10.1109/ISMAR.2013.6671841},
ISSN={},
month={Oct},}