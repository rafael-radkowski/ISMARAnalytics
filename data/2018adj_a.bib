%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for Rafael Radkowski at 2021-03-13 09:00:12 -0600 


%% Saved with string encoding Unicode (UTF-8) 



@inproceedings{8699176,
	Abstract = {In this paper, we propose a system that can reproduce the material appearance of real objects using mobile augmented reality (AR). Our proposed system allows a user to manipulate a virtual object, whose model is generated from the shape and reflectance of a real object, using the user's own hand. The shape of the real object is reconstructed by integrating depth images of the object, which are captured using an RGB-D camera from different directions. The reflectance of the object is obtained by estimating the parameters of a reflectance model from the reconstructed shape and color images, assuming that a single light source is attached to the camera. We measured the shape and reflectance of some real objects and presented the material appearance of the objects using mobile AR. It was confirmed that users were able to obtain the perception of materials from changes in gloss and burnish of the objects by rotating the objects using their own hand.},
	Author = {S. {Tsunezaki} and R. {Nomura} and T. {Komuro} and S. {Yamamoto} and N. {Tsumura}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2018.00065},
	Keywords = {augmented reality;image colour analysis;image reconstruction;mobile computing;object reflectance;RGB-D camera;shape reconstruction;depth images;color images;reflectance model;virtual object;mobile augmented reality;reproducing material appearance;Shape;Cameras;Shape measurement;Augmented reality;Image reconstruction;Light sources;Color;Reflectance property;reflectance measurement;object manipulation;mobile display;RGB-D camera;Human-centered computing;Human conputer interaction (HCI);Interaction paradigms;Mixed / augmented reality},
	Month = {Oct},
	Pages = {196-197},
	Title = {Reproducing Material Appearance of Real Objects Using Mobile Augmented Reality},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2018.00065}}

@inproceedings{8699329,
	Abstract = {Unrealistic Mixed Reality (MR) experiences can be caused by unprocessed occlusions between real and augmented objects during interactions. Depth knowledge is indeed key to achieve a seamless visualisation when a bare hand interacts with an augmented object. This can be addressed by blending real-time 3D finger tracking information with the visualisation of the hand in MR. We propose an approach that automatically localises the hand in RGB images and associates the respective depth, estimated with an auxiliary infrared stereo camera used for hand tracking, to each RGB hand pixel. Because misalignments between the outline of the hand and its depth may occur due to tracking errors, we use the distance transform algorithm to densely associate depth values to hand pixels. In this way hand and augmented object depths can be compared, and the object can be rendered accordingly. We evaluate our approach by using an MR setup composed of a smartphone and a Leap Motion mounted on it. We analyse several hand configurations and measure the erroneously classified pixels when occlusions occur.},
	Author = {C. {Battisti} and S. {Messelodi} and F. {Poiesi}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2018.00066},
	Keywords = {augmented reality;cameras;computer vision;gesture recognition;human computer interaction;image colour analysis;rendering (computer graphics);stereo image processing;hand tracking;RGB hand pixel;hand pixels;object depths;realtime 3D finger tracking information;unrealistic mixed reality;Leap Motion;smart phone;auxiliary infrared stereo camera;seamless visualisation;depth knowledge;augmented object;MR;seamless bare-hand interaction;Cameras;Three-dimensional displays;Virtual reality;Transforms;Solid modeling;Tracking;Skin;Human Computer Interaction;Mixed Reality;Hand Interaction;Leap Motion},
	Month = {Oct},
	Pages = {198-203},
	Title = {Seamless Bare-Hand Interaction in Mixed Reality},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2018.00066}}

@inproceedings{8699187,
	Abstract = {Voice-activated Intelligent Virtual Assistants (IVAs) such as Amazon Alexa offer a natural and realistic form of interaction that pursues the level of social interaction among real humans. The user experience with such technologies depends to a large degree on the perceived trust in and reliability of the IVA. In this poster, we explore the effects of a three-dimensional embodied representation of Amazon Alexa in Augmented Reality (AR) on the user's perceived trust in her being able to control Internet of Things (IoT) devices in a smart home environment. We present a preliminary study and discuss the potential of positive effects in perceived trust due to the embodied representation compared to a voice-only condition.},
	Author = {S. {Haesler} and K. {Kim} and G. {Bruder} and G. {Welch}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2018.00067},
	Keywords = {augmented reality;home computing;human computer interaction;Internet of Things;speech-based user interfaces;trusted computing;user experience;perceived trust;visually embodied Alexa;Augmented Reality;Voice-activated Intelligent Virtual Assistants;IVA;social interaction;user experience;three-dimensional embodied representation;Amazon Alexa;Internet of Things devices;IoT devices;smart home environment;Task analysis;Visualization;Augmented reality;Internet of Things;Resists;Market research;Personal digital assistants;Augmented reality;intelligent virtual assistants;social interaction;perceived trust and reliability;Internet of Things;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems---Artificial, Augmented, and Virtual Realities},
	Month = {Oct},
	Pages = {204-205},
	Title = {Seeing is Believing: Improving the Perceived Trust in Visually Embodied Alexa in Augmented Reality},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2018.00067}}

@inproceedings{8699332,
	Abstract = {This paper presents an approach to semantic segmentation and structural modeling from dense 3D point clouds. The core contribution is an efficient method for fitting of geometric primitives based on machine learning. First, the dense 3D point cloud is acquired together with RGB images on a mobile handheld device. Then, RANSAC is used to estimate the presence of geometric primitives, followed by an evaluation of their fit based on classification of the fitting parameters. Finally, the approach iterates over successive frames to optimize the fitting parameters or replace a detected primitive by a better fitting one. As a result, we obtain a semantic model of the scene consisting of a set of geometric primitives. We evaluate the approach on an extensive set of scenarios and show its plausibility in augmented reality applications.},
	Author = {A. {Stanescu} and P. {Fleck} and D. {Schmalstieg} and C. {Arth}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2018.00068},
	Keywords = {augmented reality;image colour analysis;image segmentation;learning (artificial intelligence);mobile computing;solid modelling;geometric primitives;dense 3D point cloud;semantic segmentation;fitting parameters;semantic model;structural modeling;machine learning;RGB images;mobile handheld device;RANSAC;augmented reality applications;Three-dimensional displays;Semantics;Support vector machines;Augmented reality;Solid modeling;Shape;Real-time systems;Computing methodologies---Computer Vision---Computer vision tasks---Scene Understanding;Computing methodologies---Mixed / Augmented Reality;Computing methodologies---Machine learning approaches---Kernel methods---Support vector machines},
	Month = {Oct},
	Pages = {206-211},
	Title = {Semantic Segmentation of Geometric Primitives in Dense 3D Point Clouds},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2018.00068}}

@inproceedings{8699165,
	Abstract = {We are proud to welcome you to the 2018 edition of ISMAR, the 17th International Symposium on Mixed and Augmented Reality, the major conference for mixed and augmented reality science and technology. ISMAR is organized and supported by the IEEE Computer Society, IEEE VGTC, ACM SIGCHI and a large community of science and industry leaders all over the world. Posters at ISMAR have always played a major role in the maturing of novel ideas into scientific and technological contributions for the MR/AR community. They represent the unique opportunity to share early ideas and preliminary results, to promote discussions, elicit opinions and spark new ideas as well as the methods to bring them to fruition. This is the second year posters were submitted after paper decisions were issued. This allowed science and technology papers to be added as posters when appropriate.},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2018.00007},
	Month = {Oct},
	Pages = {xxi-xxi},
	Title = {Message from the ISMAR 2018 Poster Chairs},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2018.00007}}

@inproceedings{8699245,
	Abstract = {We present a concept of emotion sharing and augmentation for collaborative mixed-reality. To depict the ideal use case of such system, we give two example scenarios. We describe our prototype system for capturing and augmenting emotion through facial expression, eye-gaze, voice, physiological data and share them through their virtual representation, and discuss on future research directions with potential applications.},
	Author = {J. D. {Hart} and T. {Piumsomboon} and G. {Lee} and M. {Billinghurst}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2018.00069},
	Keywords = {augmented reality;emotion recognition;face recognition;gaze tracking;groupware;voice activity detection;prototype system;collaborative mixed reality;emotion sharing;emotion augmentation;facial expression;eye-gaze;voice;physiological data;virtual representation;Augmented reality;Mixed reality;avatars;emotion sharing;emotion augmentation;facial expression;remote collaboration;H.5.1 Information interfaces and presentation (e.g., HCI): Multimedia Information Systems---Artificial, augmented, and virtual realities},
	Month = {Oct},
	Pages = {212-213},
	Title = {Sharing and Augmenting Emotion in Collaborative Mixed Reality},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2018.00069}}

@inproceedings{8699334,
	Abstract = {Museums and exhibitions often present physical artefacts which may contain rich histories or deep meaning associated with them. These additional contents are often installed physically as informational panels shown on the wall. However, these may sometimes be challenging to deploy due to space constraints. In order to address this challenge, we introduce the use of mixed reality. Mixed reality offers an immersive and interactive experience through the use of head mounted displays and in-air gestures. Visitors can discover additional content virtually, without changing the physical space. For a small-scale exhibition at a cafe, we developed a Microsoft HoloLens application to create an interactive experience on top of a collection of historic physical items. Through public experiences at the caf{\'e}, we received positive feedback of our system. In this paper, we discuss the design and implications of our system, survey results, as well as challenges that were encountered in deploying our mixed reality experience in a public setting.},
	Author = {K. {Cheng} and I. {Furusawa}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2018.00070},
	Keywords = {augmented reality;exhibitions;helmet mounted displays;history;museums;immersive experience;interactive experience;physical space;small-scale exhibition;historic physical items;mixed reality experience;exhibitions;physical artefacts;informational panels;space constraints;museums;head mounted displays;in-air gestures;Microsoft HoloLens;Sports;Organizations;Visualization;Augmented reality;History;Videos;Mixed reality;exhibition;public deployment;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems---Artificial, Augmented, Virtual, and Mixed Realities},
	Month = {Oct},
	Pages = {214-215},
	Title = {The Deployment of a Mixed Reality Experience for a Small-Scale Exhibition in the Wild},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2018.00070}}

@inproceedings{8699229,
	Abstract = {In this paper, we explore how Augmented Reality (AR) and anthropomorphism can be used to assign emotions to common physical objects based on their needs. We developed a novel emotional interaction model among personified physical objects so that they could react to other objects by changing virtual facial expressions. To explore the effect of such an emotional interface, we conducted a user study comparing three types of virtual cues shown on the real objects: (1) information only, (2) emotion only and (3) both information and emotional cues. A significant difference was found in task completion time and the quality of work when adding emotional cues to an informational AR-based guiding system. This implies that adding emotion feedback to informational cues may produce better task results than using informational cues alone.},
	Author = {L. {Zhang} and W. {Ha} and X. {Bai} and Y. {Chen} and M. {Billinghurst}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2018.00071},
	Keywords = {augmented reality;human computer interaction;user interfaces;emotional cues;emotion feedback;informational cues;personified physical objects;manual operation;anthropomorphism;emotional interaction model;virtual facial expressions;emotional interface;virtual cues;AR based emotional interaction;Augmented reality;Emotional Interaction;physical object personification;Manual Operation;Augmented Reality;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems---Artificial, augmented, and virtual realities},
	Month = {Oct},
	Pages = {216-221},
	Title = {The Effect of AR Based Emotional Interaction Among Personified Physical Objects in Manual Operation},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2018.00071}}

@inproceedings{8699257,
	Abstract = {In conventional virtual reality systems, users usually do not perceive and recognize the experience as reality. For example, users of a virtual disaster simulator know, consciously or unconsciously, that the presented disaster is not real, which inherently limits the training effect. To make the virtual experience more believable, we propose a novel real-virtual transiton technique that preserves the sense of ``conviction about reality'' in a virtual environment. This is realized by spatio-temporal smooth transition from the real environment to the virtual environment with omnidirectional video captured in advance at the user's position. Our technique requires less preparation cost and presents a more believable experience compared to existing transition techniques using a handmade 3D replica of the real environment. In this article, reported are the concept of our technique, a prototype system and a preliminary user study that has shown the effectiveness of the technique.},
	Author = {S. {Okeda} and H. {Takehara} and N. {Kawai} and N. {Sakata} and T. {Sato} and T. {Tanaka} and K. {Kiyokawa}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2018.00072},
	Keywords = {video signal processing;virtual reality;omnidirectional video;virtual experience;real-virtual transiton technique;virtual environment;spatio-temporal smooth transition;virtual reality systems;VR;Virtual environments;Three-dimensional displays;Resists;Prototypes;Visual effects;Solid modeling;Cameras;Human-centered computing---Human computer interaction (HCI)---Interaction paradigms---virtual reality;Computing methodologies---Artificial intelligence---Computer vision---Image and video acquisition},
	Month = {Oct},
	Pages = {222-225},
	Title = {Toward More Believable VR by Smooth Transition Between Real and Virtual Environments via Omnidirectional Video},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2018.00072}}

@inproceedings{8699270,
	Abstract = {We present a diminished reality application running live on consumer mobile devices. In our pre-observation-based approach, the clean 3D scene, free of undesired objects, is scanned beforehand and reconstructed as a high resolution textured 3D model. At runtime, objects added in a region of interest are efficiently removed by projecting the previously captured background. Differences of illumination conditions between scan time and run-time are compensated to obtain seamless results. The proposed approach requires no segmentation or manual input other than the definition of the 3D region of interest to be diminished, and is not based on any particular assumption on the background geometry. We show the potential of our approach by processing a variety of challenging unknown 3D scenes including textured backgrounds, dynamic illumination conditions and foreground objects partially occluding the diminished region. We provide details on our compute shader implementation to make as easy as possible the reimplementation by the community.},
	Author = {G. {Queguiner} and M. {Fradet} and M. {Rouhani}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2018.00073},
	Keywords = {computer graphics;geometry;image reconstruction;image segmentation;image texture;solid modelling;video signal processing;scan time;segmentation;background geometry;unknown 3D scenes;textured backgrounds;dynamic illumination conditions;foreground objects;diminished reality application;consumer mobile devices;pre-observation-based approach;clean 3D scene;mobile diminished reality;Three-dimensional displays;Cameras;Image color analysis;Lighting;Solid modeling;Mobile handsets;Streaming media;Diminished reality;mobile device;real-time;object removal;H.5.1 [Information Interfaces and Presentation]: Multimedia Information System---Artificial, augmented, and virtual realities},
	Month = {Oct},
	Pages = {226-231},
	Title = {Towards Mobile Diminished Reality},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2018.00073}}

@inproceedings{8699205,
	Abstract = {We propose a method that is effective in tracking 3D hand poses occluded by a real object. Since existing model-based tracking methods rely only on observed images to estimate hand joints, tracking generally fails when the hand joints are largely invisible. This problem becomes more prevalent when the tracked hand is grabbing an object, as occlusion by the object makes it harder to find a proper correspondence between the hand model and observation. The proposed method utilizes the occluded part of the hand as additional information for model-based tracking. The occluded depth information is reconstructed according to the geometric of the object and model-based tracking is employed based on particle swarm optimization (PSO). We demonstrate that the reconstructed depth information improves the performance of tracking an object-grabbing hand.},
	Author = {W. {Cho} and G. {Park} and W. {Woo}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2018.00074},
	Keywords = {gesture recognition;image classification;image reconstruction;image representation;motion estimation;object tracking;particle swarm optimisation;pose estimation;stereo image processing;reconstructed depth information;object-grabbing hand;occluded depth reconstruction;model-based tracking methods;hand joints;tracked hand;hand model;3D hand poses;particle swarm optimization;Image reconstruction;Image segmentation;Three-dimensional displays;Solid modeling;Pose estimation;Linear programming;Image color analysis;Computing methodologies;Computer vision problems;Tracking},
	Month = {Oct},
	Pages = {232-235},
	Title = {Tracking an Object-Grabbing Hand Using Occluded Depth Reconstruction},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2018.00074}}

@inproceedings{8699284,
	Abstract = {This paper presents a novel evaluation of an industry-ready HMD for delivering AR work instructions in a real-life, industrial procedure for novice users. A user study was performed to examine the potential benefits and limitations of a dynamic 3D virtual model and AR text instructions, delivered through an optical see through HMD, for training users in a new industry procedure (i.e., Yaw Motor Servicing of a wind turbine). Measures of task accuracy and completion time were used to evaluate the performance of one group of mechanical engineering students performing this procedure for the first time guided by AR compared to a second group performing it using a tablet-delivered instruction manual. Results showed AR improved accuracy but not speed of task completion. AR significantly increased accuracy on one specific task-step in the procedure, namely measurement of a thin air gap (see figure 1, left panel), but also showed limitations with other task-steps not benefitting or even being slowed down by AR (see figure 1, right panel). Findings speak to the importance of incorporating an analysis at the level of individual task steps in order to fully evaluate AR work instructions.},
	Author = {A. {Princle} and A. G. {Campbell} and S. {Hutka} and A. {Torrasso} and C. {Couper} and F. {Strunden} and J. {Bajana} and K. {Jastz{\k a}b} and R. {Croly} and R. {Quigley} and R. {McKiernan} and P. {Sweeney} and M. T. {Keane}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2018.00075},
	Keywords = {helmet mounted displays;human computer interaction;human factors;maintenance engineering;mechanical engineering computing;virtual reality;wind turbines;completion time;mechanical engineering students;tablet-delivered instruction manual;AR improved accuracy;task completion;task-steps;individual task steps;AR work instructions;maintenance task;AR benefits performance;industry-ready HMD;industrial procedure;novice users;user study;training users;industry procedure;wind turbine;task accuracy;yaw motor servicing;Task analysis;Resists;Three-dimensional displays;Industries;Maintenance engineering;Solid modeling;Manuals;Augmented Reality;providing instructions;maintenance;workpiece;head-mounted displays;H.5.2 [Information interfaces and Presentation]: Multimedia Information Systems-Artificial, augmented and virtual realities;H 5.2 [Information interfaces and Presentation]: User Interfaces-Training, help and documentation},
	Month = {Oct},
	Pages = {236-241},
	Title = {[Poster] Using an Industry-Ready AR HMD on a Real Maintenance Task: AR Benefits Performance on Certain Task Steps More Than Others},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2018.00075}}

@inproceedings{8699283,
	Abstract = {Virtual Reality (VR) has the potential to transform the way we work, rest and play. This promise comes with new challenges. One challenge stems from the interactive nature of immersive Virtual Environments (VEs). Placement of contextual information in VEs can be critical to the user experience. This poster describes the use of eye-tracking to alleviate usability issues surrounding information presentation in immersive VEs. Results from our experiments show that integrating eye tracking into a VE to dictate where and when textual information is presented can improve performance when searching for contextual information. In summary, the results show improvement in task performance when the new direct method is employed to reveal information in target regions based on gaze. This seems to hold true independent of the VE or the type of information questioned.},
	Author = {A. {McNamara} and K. {Boyd} and D. {Oh} and R. {Sharpe} and A. {Suther}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2018.00076},
	Keywords = {gaze tracking;information retrieval;user experience;virtual reality;eye tracking;contextual information;user experience;information presentation;immersive VEs;textual information;information retrieval;virtual reality;immersive virtual environments;Visualization;Clutter;Task analysis;Augmented reality;Gaze tracking;Information retrieval;Human-centered computing---Visualization---Visualization techniques---Treemaps;Human-centered computing---Visualization---Visualization design and evaluation methods},
	Month = {Oct},
	Pages = {242-243},
	Title = {Using Eye Tracking to Improve Information Retrieval in Virtual Reality},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2018.00076}}

@inproceedings{8699261,
	Abstract = {As head mounted displays (HMDs) become everyday consumer items, the potential of immersive Virtual Reality (VR) as a design space becomes ever more promising. However, their usage is impeded by human factors inherent to the technology itself, such as visually induced motion sickness (VIMS), caused by the disconnect between what is visually and physically perceived. Previous work on VIMS reduction has explored techniques targeting HMDs, while others explored techniques that target the multimedia content itself through visual optimization. The latter are often studied individually and cannot be applied to certain VR content such as 360$\,^{\circ}$ video. Consequently, this paper describes an exploratory study comparing and combining such techniques (independent visual background and restricted field of view) in 360$\,^{\circ}$ video. The work provides constructive insights for VR designers, while also exploring how analytics of VR content and user experience can be used for VIMS prevention and evaluation.},
	Author = {P. {Bala} and D. {Dion{\'\i}sio} and V. {Nisi} and N. {Nunes}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2018.00077},
	Keywords = {data visualisation;helmet mounted displays;human factors;virtual reality;visually induced motion sickness;360$\,^{\circ}$ videos;VIMS reduction;visual optimization;immersive virtual reality;head mounted displays;human factors;Visualization;Optical flow;Optimization;Prototypes;Streaming media;User experience;Resists;VR sickness;cybersickness;360$\,^{\circ}$ video;cinematic VR;field of view;content analysis;optical flow;H.5.1 [Information Interfaces and Presentation (e.g. HCI)]: Multimedia Information Systems},
	Month = {Oct},
	Pages = {244-249},
	Title = {Visually Induced Motion Sickness in 360$\,^{\circ}$ Videos: Comparing and Combining Visual Optimization Techniques},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2018.00077}}

@inproceedings{8699259,
	Abstract = {Due to advances in tracking technology, wireless technology and battery capacity for Head Mounted Displays (HMD), user's are capable not only of overlooking a virtual space while sitting on a chair, but also to play in Virtual Reality (VR) more dynamically while walking around a room or even outdoors. There is possibility that a user could collide with surrounding obstacles and be injured because of walking around in real space without removing the HMD. In this paper we propose a walking support system that displays a route to the user's destination in virtual space using a Social Force Model simulating how pedestrians move in the presence of obstacles and other pedestrians, in order to allow a user to reach a real destination while in VR.},
	Author = {K. {Kanamori} and N. {Sakata} and T. {Tominaga} and Y. {Hijikata} and K. {Harada} and K. {Kiyokawa}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2018.00078},
	Keywords = {computer games;helmet mounted displays;pedestrians;virtual reality;immersive HMD;wireless technology;battery capacity;virtual space;obstacles;walking support system;social force model;head mounted displays;virtual reality;pedestrians;route display;Augmented reality;Virtual Reality;Social Force Model;walking support;H.5.m. Information interfaces and presentation (e.g., HCI): Miscellaneous},
	Month = {Oct},
	Pages = {250-253},
	Title = {Walking Support in Real Space Using Social Force Model When Wearing Immersive HMD},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2018.00078}}

@inproceedings{8699271,
	Abstract = {We are very pleased to introduce the workshops and tutorials of the 17th IEEE International Symposium on Mixed and Augmented Reality. This year, we proudly host six workshops and four tutorials, covering a broad range of topics from Augmented Reality, Virtual Reality, and Mixed Reality with engaging presentation formats.},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2018.00008},
	Month = {Oct},
	Pages = {xxii-xxii},
	Title = {Message from the ISMAR 2018 Workshop and Tutorial Chairs},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2018.00008}}

@inproceedings{8699289,
	Abstract = {Many techniques have been proposed for navigation using head-mounted-displays (HMDs) in virtual reality (VR). A walking-in-place (WIP) interface for virtual locomotion provides a high presence and an immersive experience in a virtual environment (VE). However, most of the WIP techniques can only navigate users in the direction of their gaze. Other WIP methods considering virtual locomotion direction have complex configurations or are only feasible when the tracking spaces are not limited. This paper proposes a WIP interface independent of gaze direction based on the data analysis of waist-mounted inertial sensors. Our method can navigate in the locomotion direction by calculating the orientation of the pelvis. We experimentally compared two WIP methods using a navigation task that required participants to periodically observe the surrounding VE: (1) Conventional WIP (gaze-based direction) (2) Proposed WIP (pelvis-based direction). While there was no difference in learnability or cybersickness between the two methods, the proposed method had shorter task time and higher efficiency.},
	Author = {C. {Park} and K. {Jang} and J. {Lee}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2018.00079},
	Keywords = {helmet mounted displays;inertial navigation;sensors;user interfaces;virtual reality;gaze direction;waist-worn inertial measurement unit;head-mounted-displays;virtual reality;walking-in-place interface;immersive experience;virtual environment;WIP techniques;WIP methods;virtual locomotion direction;complex configurations;WIP interface;waist-mounted inertial sensors;navigation task;gaze-based direction;VR navigation;data analysis;pelvis orientation;pelvis-based direction;cybersickness;learnability;Task analysis;Navigation;Legged locomotion;Acceleration;Sensors;Tracking;Foot;Computing methodologies---Computer graphics---Graphics systems and interfaces---Virtual reality;Human-centered computing---Human computer interaction (HCI)---Interaction techniques},
	Month = {Oct},
	Pages = {254-257},
	Title = {Walking-in-Place for VR Navigation Independent of Gaze Direction Using a Waist-Worn Inertial Measurement Unit},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2018.00079}}

@inproceedings{8699246,
	Abstract = {Camera relocalization is a common problem in several applications such as augmented reality or robot navigation. Especially, augmented reality requires fast, accurate and robust camera localization. However, it is still challenging to have a both real-time and accurate method. In this paper, we present our hybrid method combing machine learning approach and geometric approach for real-time camera relocalization from a single RGB image. We propose a light Convolutional Neural Network (CNN) called xyzNet to efficiently and robustly regress 3D world coordinates of key-points in an image. Then, the geometric information about 2D-3D correspondences allows the removal of ambiguous predictions and the calculation of more accurate camera pose. Moreover, we show favorable results compared to previous machine learning based approaches about the accuracy and the performance of our method on different datasets as well as the capacity to address challenges concerning dynamic scene.},
	Author = {N. {Duong} and A. {Kacete} and C. {Sodalie} and P. {Richard} and J. {Royan}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2018.00080},
	Keywords = {cameras;computational geometry;convolutional neural nets;image colour analysis;image sensors;learning (artificial intelligence);pose estimation;regression analysis;augmented reality;robot navigation;accurate camera localization;robust camera localization;accurate method;geometric approach;real-time camera relocalization;single RGB image;geometric information;dynamic scene;hybrid method;light convolutional neural network;3D world coordinates;xyzNet;machine learning camera relocalization;scene coordinate prediction network;2D-3D correspondences;Augmented reality;Real-time RGB Camera Relocalization;Deep Learning Regression},
	Month = {Oct},
	Pages = {258-263},
	Title = {xyzNet: Towards Machine Learning Camera Relocalization by Using a Scene Coordinate Prediction Network},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2018.00080}}

@inproceedings{8699320,
	Abstract = {This workshop focuses on the functional requirements for enterprise AR components. Enterprise AR customers have requirements that differ substantially from those of consumers. Having functional requirements directly benefits enterprise customers: products and services will have interoperability, customer RFPs will be easier to create and respond to, and research as well as development communities will have more clear understanding of the requirements of enterprise AR buyers. Those ISMAR attendees conducting research about enterprise AR and providers of AR components and solutions will have clear definitions of customer needs. This will lead to the highest value research and greater enterprise AR project success which can then be used to influence research agendas, development roadmaps and future products. A preliminary set of enterprise AR requirements was created in 2016 through a collaboration between UI LABS (DMDII) and the AREA and delivered through a project led by Lockheed Martin, Caterpillar and Procter & Gamble. In 2017 and 2018, through several additional cycles of input by stakeholders, these requirements have since been refined. This workshop will shed new light on the requirements' current status, and provide valuable inputs to the further refinement and applications of the enterprise AR requirement documents.},
	Author = {M. {Rygol} and C. {Perey}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2018.00081},
	Keywords = {augmented reality;business data processing;customer relationship management;open systems;systems analysis;benefits enterprise customers;enterprise AR buyers;enterprise AR requirement documents;functional requirements;enterprise AR customers;customer RFP;augmented reality;Conferences;Augmented reality;Interoperability;Collaboration;Stakeholders},
	Month = {Oct},
	Pages = {264-264},
	Title = {Enterprise AR Functional Requirements Workshop},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2018.00081}}

@inproceedings{8699190,
	Abstract = {Despite recent progress in display technology, we are still far from the ultimate goal of creating new virtual environments and augmentations of existing ones that feel and react similarly as their real counterparts. Many challenges and open research questions remain - mostly in the area of multimodality and interaction. For example, current setups predominantly focus on visual and auditory senses, neglecting other modalities such as touch and smell that are an integral part of how we experience the real world around us. Likewise, it is still an open question how to best interact and communicate with a virtual world or virtual objects in AR.},
	Author = {W. {H{\"u}rst} and D. {Iwai} and K. {\v C}. {Pucihar} and M. {Kljun}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2018.00082},
	Keywords = {Conferences;Australia;Augmented reality;Virtual environments;Visualization;Art;Urban areas;Multimodality;interactivity;AR;VR},
	Month = {Oct},
	Pages = {265-265},
	Title = {Multimodal Virtual Augmented Reality - Editorial to the MVAR Workshop at ISMAR 2018},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2018.00082}}

@inproceedings{8699326,
	Abstract = {This paper presents a study examining interaction methods for manipulating objects in augmented reality (AR) environments using Google Glass (Glass). We compared five interaction methods; three of them were implemented on Glass (virtual buttons, swipe pad of Glass, remote control via the touchscreen of a smartwatch) and two on a smartphone (virtual buttons and the touch interaction). 32 participants were asked to scale and rotate a virtual 3D object created from a physical sculpture of the Museum G{\"u}nter Grass-Haus in Luebeck using the AR-App InfoGrid4Glass. We studied the interaction methods by measuring effectiveness, efficiency, and satisfaction of the users. The results of the study showed that smartphone interaction is superior to any Google Glass interaction methods. Of the interaction methods implemented for Glass, a combination of Glass with a smartwatch shows the highest usability. Our findings suggest that if users have a smartwatch available, it offers them a higher usability for interacting with virtual objects rather than using the touch pad of Glass or virtual buttons on Glass.},
	Author = {A. {Ohlei} and T. {Winkler} and D. {Wessel} and M. {Herczeg}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2018.00083},
	Keywords = {augmented reality;human computer interaction;smart phones;touch sensitive screens;direct manipulation methods;augmented reality environments;touch interaction;virtual 3D object;AR-App InfoGrid4Glass;smartphone interaction;Google Glass interaction methods;Museum G{\"u}nter Grass-Haus;smartwatch;Augmented reality;Augmented Reality;Cross-Device Interaction;User study;Google Glass;Smartwatch;*Human-centered computing â†’ Mixed / augmented reality},
	Month = {Oct},
	Pages = {266-269},
	Title = {Evaluation of Direct Manipulation Methods in Augmented Reality Environments Using Google Glass},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2018.00083}}

@inproceedings{8699230,
	Abstract = {A problem of most handheld applications is that the digital content is not rendered from the user's viewpoint, but rather from an arbitrary perspective. In this work, we propose an easy-to-use user-perspective rendering algorithm which can be applied to general handheld mobile devices. We present a smartphone App that can enhance the sense of presence when perceiving the virtual objects, by adjusting the 2D projections, lighting conditions, and spatial sound effects according to the user's viewpoint in real time.},
	Author = {J. {Yang} and S. {Wang} and G. {S{\"o}r{\"o}s}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2018.00084},
	Keywords = {augmented reality;mobile computing;rendering (computer graphics);smart phones;user experience;general handheld mobile devices;user-perspective rendering;handheld applications;digital content;users viewpoint;Cameras;Face;Rendering (computer graphics);Calibration;Two dimensional displays;Three-dimensional displays;User-perspective rendering;spatial sound;multimodal output;handheld applications;Human-centered computing---Sound-based input/output;Computing methodologies---Rendering},
	Month = {Oct},
	Pages = {270-274},
	Title = {User-Perspective Rendering for Handheld Applications},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2018.00084}}

@inproceedings{8699215,
	Abstract = {Realistic full-body avatar representation inside Virtual Reality is a big shortcoming of state-of-the-art VR systems. It remains a technically challenging task to capture human motion precisely without marker-based full-body tracking systems, which are expensive and impractical. Trying to tackle this challenge, we propose a simple yet efficient approach for avatar motion reconstruction. VIRTOOAIR (VIrtual Reality TOOlbox for Avatar Intelligent Reconstruction) combines Deep Learning for upper body reconstruction and most recent methods for single camera based pose recovery for the lower body parts. Our preliminary results demonstrate the advantages of our system's avatar pose reconstruction. This is mainly determined by the use of a powerful learning system, which offers significantly better results than existing heuristic solutions for inverse kinematics. Our system supports the paradigm shift towards learning systems capable to track full-body avatars inside Virtual Reality without the need of expensive external tracking hardware.},
	Author = {A. {Becher} and C. {Axenie} and T. {Grauschopf}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2018.00085},
	Keywords = {avatars;cameras;computer animation;image motion analysis;image reconstruction;learning (artificial intelligence);pose estimation;virtual reality;VIRTOOAIR;realistic full-body avatar representation;human motion;marker-based full-body tracking systems;avatar motion reconstruction;upper body reconstruction;full-body avatars;expensive external tracking hardware;VR systems;virtual reality toolbox;avatar intelligent reconstruction;Avatars;Cameras;Image reconstruction;Kinematics;Tracking;Skeleton;Quaternions;Machine Learning---Supervised learning by regression---;---Virtual Reality---Motion capture---},
	Month = {Oct},
	Pages = {275-279},
	Title = {VIRTOOAIR: Virtual Reality TOOlbox for Avatar Intelligent Reconstruction},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2018.00085}}

@inproceedings{8699174,
	Abstract = {It is our great pleasure to present the demonstration program for the 17th IEEE International Symposium on Mixed and Augmented Reality. ISMAR demonstrations provide the opportunity to discuss research results through a hands-on interactive session. The demonstration session is an open forum where new collaborations can be formed.},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2018.00009},
	Month = {Oct},
	Pages = {xxiii-xxiii},
	Title = {Message from the ISMAR 2018 Demonstration Chairs},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2018.00009}}

@inproceedings{8699181,
	Abstract = {Presents the title page of the proceedings record.},
	Author = {K. {Avgerinakis} and F. {Bellotti} and M. {Vergauwen} and L. {Wanner} and S. {Vrochidis}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2018.00086},
	Month = {Oct},
	Pages = {280-280},
	Title = {1st International Workshop on Multimedia Analysis for Architecture, Design and Virtual Reality Games (MADVR 2018)},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2018.00086}}

@inproceedings{8699281,
	Abstract = {Art, science and technology were during centuries associated to create experiences and to engage people in a different reality. Currently, the development of VR technologies is opening a dimension where contemporary art practice could contribute to create the ``experience society''. An Artist is a curious people that is always experimenting and having a critic view of our current society and systems. Their contribution is not only creating new aesthetics, more is rethinking all our conventional ideas: why orthogonal/perspective perception? What about dematerialization? Body experience/reality is involving all the senses, not only visual, collective creativity versus genius creativity, post-internet concepts, etc. This disruption aptitude will open for VR technology an enormous source of innovation and promote the social wellbeing, making real an ``experience society'', where people try to transform their lives into experiential personal projects. VR does offer real potential for multidisciplinary productions, knowledge transfers and engagement beyond academic slogans. This potential is too great to be left to the market. (1).},
	Author = {A. M. {Naranjo} and H. {Sprengel}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2018.00087},
	Keywords = {art;Internet;user interfaces;virtual reality;VR technology;experience society;contemporary art practice;curious people;current society;visual creativity versus genius creativity;collective creativity versus genius creativity;art;academic slogans;knowledge transfers;multidisciplinary productions;body experience-reality;Art;Games;Media;Technological innovation;Production;Virtual reality;Aerospace electronics;Art;VR;experience;creative;curiosity;critical thinking;digital life;levels of reality;virtuality;dematerialization;knowledge;non orthogonal;post-internet;anti-disciplinary},
	Month = {Oct},
	Pages = {281-286},
	Title = {Art and VR Technology, Creating the ``Experience Society''},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2018.00087}}

@inproceedings{8699242,
	Abstract = {Virtual reality games demand the use of highly realistic 3D models to provide an immersive environment to the users. Therefor, modeling of real world structures is a common process within the industry. In recent years the photogrammetric tools have become available to aid in this process. However, obtaining high quality imagery is still an issue w.r.t. budget and time constraints. We present a simple, effective method for the reuse of existing video and photo material towards the generation of 3D models suitable for a virtual reality gaming environment. An abundance of existing content may be available in on-line repositories (YouTube, Flickr, Google images,..) and can be exploited. A complete video-to-VR pipeline is presented and a use-case of the Berlin Gendarmenmarkt square is tested, wherein the use of both commercial and open source software is evaluated. Our proposed method improves existing workflows used by VR game designers in both structural accuracy and creation time of real world objects.},
	Author = {J. {Derdaele} and Y. {Shekhawat} and M. {Vergauwen}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2018.00088},
	Keywords = {computer games;public domain software;solid modelling;virtual reality;VR reconstruction;virtual reality games;highly realistic 3D models;immersive environment;photogrammetric tools;high quality imagery;virtual reality gaming environment;video-to-VR pipeline;Berlin Gendarmenmarkt square;open source software;Image reconstruction;Pipelines;Three-dimensional displays;Solid modeling;Virtual reality;Games;Cameras},
	Month = {Oct},
	Pages = {287-292},
	Title = {Exploring Past and Present: VR Reconstruction of the Berlin Gendarmenmarkt},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2018.00088}}

@inproceedings{8699291,
	Abstract = {The governors tombs located at Dayr al-Barsha are ranked among the most important heritage sites of the Egyptian Middle Kingdom. Unfortunately, the site underwent transformations and was affected by ancient quarrying, looting, and natural disasters such as earthquakes, thus bringing this archaeological site on the edge of ruin. For this reason, its digital documentation is important and valuable for the preservation and further study of this elite cemetery. This paper describes the work-flow towards the 3D digital recording and full immerse visualization of the monuments that compose this ancient site. These stages include the physical recording using Terrestrial Laser Scanners (TLS), the creation of realistic 3D models from the acquired data, and the creation of a Virtual Reality (VR) world of the most paramount burial monument of this archaeological area. The obtained digital representation of the site will not only serve as a basis for geoarchaeological surveying and analysis, but also as a digital tool for public disclosure of the archaeological remains.},
	Author = {R. {de Lima} and M. {Vergauwen}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2018.00089},
	Keywords = {archaeology;data visualisation;history;solid modelling;virtual reality;3D digital recording;immerse visualization;physical recording;realistic 3D models;paramount burial monument;archaeological area;digital representation;digital tool;archaeological remains;VR environment;Dayr al-Barsha;governors tombs;Egyptian Middle Kingdom;ancient quarrying;natural disasters;archaeological site;digital documentation;elite cemetery;ancient site;terrestrial laser scanners;virtual reality world;Egypt;heritage sites;digital preservation;immerse monument visualization;3D models;Three-dimensional displays;Solid modeling;Shafts;Documentation;Data visualization;Computational modeling;Tools;Heritage Documentation;Remote Sensing;Virtual Reality;Photogrammetry},
	Month = {Oct},
	Pages = {293-298},
	Title = {From TLS Recoding to VR Environment for Documentation of the Governor's Tombs in Dayr al-Barsha, Egypt},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2018.00089}}

@inproceedings{8699317,
	Abstract = {Amid the most recent years, gamification has gotten expanding consideration focusing on an assortment of individuals including children, students, youngsters and employers. Likewise, great advances have been also seen in the Internet-of-Things (IoT) triggering various researchers' interest. In this paper, the core integration architecture of the InLife ecosystem that combines IoT with Serious Games is presented, as well as the user portal that can be used from third-party developers to create their own Serious Games. Specifically, the proposed platform focuses on an innovative gamification framework targeting both typical as well as special education and social inclusion activities based on Serious Games. The core concept leverages on the potential of the IoT paradigm to link closely actions, decisions and events happening in real-life with in-game educational progress and modern gaming technologies. This bridge strengthens the infusion of gamification into non-leisure contexts, boosting at the same time the creation of new educational methodologies as well as new business opportunities.},
	Author = {P. {Kosmides} and K. {Demestichas} and E. {Adamopoulou} and N. {Koutsouris} and I. {Loumiotis} and V. {Ortega} and L. {Mureddu}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2018.00090},
	Keywords = {age issues;computer aided instruction;Internet of Things;mobile learning;portals;serious games (computing);core integration architecture;InLife ecosystem;innovative gamification framework;social inclusion activities;in-game educational progress;modern gaming technologies;IoT features;IoT;serious games creation;special education activities;Internet-of-Things;user portal;Games;Ecosystems;Sensors;Portals;Internet of Things;Wireless sensor networks;Education;Gamification;Serious Games;Internet - of - Things;sensors;platform},
	Month = {Oct},
	Pages = {299-304},
	Title = {InLife Ecosystem: Creating Serious Games with IoT Features},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2018.00090}}

@inproceedings{8699213,
	Abstract = {Architecture, Design and Virtual Reality game creation companies are in great need of acquiring, re-using and re-purposing visual and textual data to recreate, renovate or produce a novel target space, building or element. This come in align with the abrupt increase, which is lately observed, in the use of immersive VR environments and the great technological advance that can be found in the acquisition and manipulation of digital models. V4Design is a new project that takes into account these needs and intends to build a complete and robust design solution that will help the targeted industries to improve their creation process.},
	Author = {K. {Avgerinakis} and G. {Meditskos} and J. {Derdaele} and S. {Mille} and Y. {Shekhawat} and L. {Fraguada} and E. {Lopez} and J. {Wuyts} and A. {Tellios} and S. {Riegas} and J. {Wachtmeister} and K. {Doczy} and V. {Vos} and N. {Heise} and J. {Piesk} and M. {Vergauwen} and L. {Wanner} and S. {Vrochidis} and I. {Kompatsiaris}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2018.00091},
	Keywords = {computer games;multimedia computing;virtual reality;V4Design;enhancing architecture;video game creation;textual data;target space;building;immersive VR environments;complete design solution;robust design solution;creation process;visual data;renovation;digital models;multimedia content;Games;Semantics;Three-dimensional displays;Buildings;Solid modeling;Painting;Architecture;Computer vision;3D-reconstruction;VR in architecture;VR in video gamestext summarization},
	Month = {Oct},
	Pages = {305-309},
	Title = {V4Design for Enhancing Architecture and Video Game Creation},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2018.00091}}

@inproceedings{8699202,
	Abstract = {The Antikythera Mechanism is not only an ancient artifact. It is the first known geared computer that spurs interest of archaeologists, astronomers, mechanical engineers, and software developers. The high degree of corrosion, caused by its long burial in the shipwreck of the Antikythera, makes it a hypersensitive artifact. A pioneering technique has been applied, which generates accurate 3D models for the fragments of the artifact. The embodiment of these assets into an interactive and fully immersive virtual environment of a game engine, is the subject of this presentation.},
	Author = {E. {Anastasovitis} and M. {Roumeliotis}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2018.00092},
	Keywords = {archaeology;museums;virtual reality;Antikythera Mechanism;ancient artifact;hypersensitive artifact;interactive environment;fully immersive virtual environment;virtual museum;immersive cultural exhibition;game engine;Three-dimensional displays;Games;Virtual environments;Image reconstruction;Cultural differences;Solid modeling;Antikythera Mechanism;virtual reality;virtual museum;3D reconstruction;cultural heritage},
	Month = {Oct},
	Pages = {310-313},
	Title = {Virtual Museum for the Antikythera Mechanism: Designing an Immersive Cultural Exhibition},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2018.00092}}

@inproceedings{8699272,
	Abstract = {Although developments in devices and software are maturing towards novel Mixed Reality systems, there is too little connection to the design field. Especially if AR is combined with other ``smart'' technologies (internet of things), perspectives shift from merely technical characteristics and quantifiable human factors to more complex UX scenarios. Although there are other special interest groups/conferences that in part cover this theme (CHI, IUI, UIST), we would think that ISMAR is a better venue in connecting the graphics/tracking community with design researchers. We also would like to address the lack of software engineering skills with design students/professionals. How can we bridge these disciplines and silos of innovation? This workshop invites both industrial and academic participants to contribute to this debate, first of all by submitting extended abstracts that cover case studies, best practices and challenges in design for/with AR. To cater for a design debate, we strongly encourage submissions of annotated artworks/3D scenes/pictures/floorplans as well as more traditional papers. Position papers should be 2-6 pages long, submitted in PDF format and formatted using the ISMAR 2018 paper template available from https://ismar2018.org/guidelines_submission/index.html. Papers will be peer-reviewed and after acceptance be published in the adjunct proceedings or ISMAR. During the workshop papers will be presented/demonstrations as short presentations. Submissions should not be anonymized and the author names and affiliations should be displayed on the first page. At least one author of each accepted paper must attend the workshop and register for at least one day of the conference. All accepted papers workshop's papers will be published in the ISMAR 2018 Adjunct Proceedings.},
	Author = {J. {Verlinden} and D. {Aschenbrenner} and S. {Lukosh}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2018.00093},
	Keywords = {augmented reality;computer graphics;human factors;Internet;Internet of Things;software engineering;user interfaces;creativity;smart technologies;design researchers;software engineering skills;mixed reality systems;human factors;tracking community;augmented reality;graphics community;Internet of things;Conferences;Augmented reality;Creativity;Software;Internet of Things;Human factors},
	Month = {Oct},
	Pages = {314-314},
	Title = {Workshop on Creativity in Designing with for Mixed Reality},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2018.00093}}

@inproceedings{8699175,
	Abstract = {Mixed reality applications currently make their way into diverse academic and industrial domains. Due to the evolving character of this new technology, early adopters face a variety of challenges for example in soft- and hardware availability. Another deficit is the lack of content. Opening up the production life cycle to involve users early on can help to improve the acceptance and target new possibilities. In this article we present a methodology and tool support for co-designing mixed reality applications by combining requirements engineering and gamification. This enables both creating a continuous innovation life cycle and achieving long-term motivation. Our prototype was evaluated in the context of technology-enhanced learning at a medical school. We are confident that our results are transferable to other application areas, as all our components are freely available as open source resources.},
	Author = {I. {Koren} and B. {Hensen} and R. {Klamma}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2018.00094},
	Keywords = {computer aided instruction;innovation management;serious games (computing);virtual reality;gamified mixed reality applications;evolving character;hardware availability;production life cycle;tool support;continuous innovation life cycle;academic domains;technology-enhanced learning;medical school;open source resources;Virtual reality;Tools;Three-dimensional displays;Solid modeling;Hardware;Training},
	Month = {Oct},
	Pages = {315-317},
	Title = {Co-Design of Gamified Mixed Reality Applications},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2018.00094}}
