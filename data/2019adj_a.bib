%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for Rafael Radkowski at 2021-03-13 08:56:50 -0600 


%% Saved with string encoding Unicode (UTF-8) 



@inproceedings{8951944,
	Abstract = {Since the introduction of augmented reality (AR) technology, in-situ instructions for manual tasks have been a central use case for a large body of previous work. However, most implementations provide identical sets of instructions to each user disregarding the user's current mental load. This is a major issue since previous work has shown the importance and potential of an adapted instruction fidelity for manual tasks such as playing an instrument. To implement a low-cost mental load adaptation for AR instructions, we evaluated a mobile off-the-shelf electroencephalographic (EEG) device for its suitability and feasibility to measure mental load while wearing a video see-through AR head-mounted display (HMD). In a first user experiment (n=12), data of EEG power band values and proprietary performance metrics of the manufacturer were collected and analysed regarding their validity to estimate the user's mental load. Our results indicate that our setup successfully induced different levels of mental effort. The proprietary performance metrics, however, only partially reflected the participants' current mental effort and require further analysis.},
	Author = {D. {Wolf} and T. {Wagner} and E. {Rukzio}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00015},
	Keywords = {augmented reality;computer aided instruction;electroencephalography;helmet mounted displays;human computer interaction;medical signal processing;neurophysiology;mental effort;proprietary performance metrics;low-cost real-time mental load adaptation;augmented reality instructions;in-situ instructions;manual tasks;central use case;current mental load;adapted instruction fidelity;low-cost mental load adaptation;AR instructions;off-the-shelf electroencephalographic device;user experiment;Task analysis;Electroencephalography;Resists;Real-time systems;Augmented reality;Human computer interaction;eeg;augmented reality;in situ instructions;user adaptation},
	Month = {Oct},
	Pages = {1-3},
	Title = {Low-Cost Real-Time Mental Load Adaptation for Augmented Reality Instructions - A Feasibility Study},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00015}}

@inproceedings{8951956,
	Abstract = {Order picking is one of the most important tasks in modern warehouses, while augmented reality (AR) is a friendly alternative by conveying manual picking information into visual guidance to improve the picking performance. Nevertheless, there is still a lack of scalable and systematic consensus to deploy the pick-by-AR method associated with actual warehouse workplaces. This article describes a comprehensive methodology for the use of wearable AR in the order picking process. To enable a robust AR assisted manual order picking system in a warehouse floor, the marker-based global map about the actual shelves is established automatically, empowering a scalable, low-cost and long-term tracking capacity for pick-by-AR within the warehouse floor and allowing the operator to move freely in the actual workplace. The system is evaluated in the automobile assembly line, and experimental results illustrate that the proposed work can provide a systematic solution for pick-by-AR applications in actual warehouse fields.},
	Author = {W. {Fang} and S. {Zheng} and Z. {Liu}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00016},
	Keywords = {assembling;augmented reality;automobile industry;order picking;production engineering computing;long-term tracking capacity;warehouse floor;augmented reality;pick-by-AR method;order picking process;marker-based global map;automobile assembly line;Augmented reality;Order picking, augmented reality, global navigation},
	Month = {Oct},
	Pages = {4-7},
	Title = {A Scalable and Long-Term Wearable Augmented Reality System for Order Picking},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00016}}

@inproceedings{8951955,
	Abstract = {In this paper, we investigate the effectiveness of augmented reality-based peephole interaction on a smartphone which fixes a virtual screen on a flat surface in real space and that enables a user to change the viewing area rapidly by moving or tilting the device. Since the virtual screen is superimposed in real space, the user can easily grasp the sense of distance to the virtual screen and move the device to a proper position rapidly. In addition, it is expected that the user can remember the positions of target objects easily by associating the objects with real space information. We conducted an experiment to compare the proposed augmented reality-based peephole interface with a conventional non-augmented reality-based peephole interface, and the conventional touchscreen interface. Although there was no significant difference in task completion time, the results of the questionnaire showed that the proposed interface and the conventional peephole interface were preferred to the conventional touchscreen interface and that it could be possible to show the effectiveness of the proposed interface in further investigations.},
	Author = {M. {Miyazaki} and T. {Komuro}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00017},
	Keywords = {augmented reality;human computer interaction;touch sensitive screens;user interfaces;virtual screen;real space information;augmented reality-based peephole interface;nonaugmented reality-based peephole interface;touchscreen interface;conventional peephole interface;augmented reality-based peephole interaction;Augmented reality;mobile-device;peephole-interaction;augmented-reality},
	Month = {Oct},
	Pages = {8-11},
	Title = {Augmented Reality-Based Peephole Interaction using Real Space Information},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00017}}

@inproceedings{8951995,
	Abstract = {Technology in education is rapidly changing the way that students learn. This allows for the creation of learning tools that provide better interaction, creative engagement and adaptability to a learner. Augmented Reality (AR) is one of these emerging technologies which can facilitate the development of new learning tools. AR has successfully been proven to allow new types of learning pedagogies by providing human-centered learning environments. In particular, the pedagogical approach of Kinesthetic learning or ''Learning by Doing'' has not been explored in great detail in combination with Augmented Reality. This approach is to physically act out an activity to aid in the learning process and has been previously proven as one of the most successful approaches. For a successful application of this pedagogy, the student must get precise feedback and be guided through a process, thus some form of intelligent guide needs to be actively monitoring the learning environment. This paper presents the exploration of this concept through the presentation of an initial prototype system that was developed and implemented based on an adaptive learning methodology within an AR application, with the prospect that in the future will use intelligent agents.},
	Author = {M. Z. {Iqbal} and E. {Mangina} and A. G. {Campbell}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00018},
	Keywords = {augmented reality;computer aided instruction;human computer interaction;multi-agent systems;virtual reality;Augmented Reality;learning process;adaptive learning methodology;intelligent agents;kinesthetic learning;intelligent virtual embodied agent;learning tools;pedagogical approach;human-centered learning environments;student learning;Human-centered-computing;Interaction-paradigms---Mixed-/-Augmented-reality;Applied-computing---E-learning;Applied-computing---Interactive-learning-environments},
	Month = {Oct},
	Pages = {12-16},
	Title = {Exploring the Use of Augmented Reality in a Kinesthetic Learning Application Integrated with an Intelligent Virtual Embodied Agent},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00018}}

@inproceedings{8951777,
	Abstract = {This work describes a system and a user study for hiding and showing parts of the shared surrounding spaces in social AR applications for wearable devices. It extends previous work on the Social AR Continuum by exploring how sharing the surrounding environment can vary based on the social proximity between contacts. We built a prototype system for sharing the surrounding environment between two HoloLens devices. We found that Remove is the preferred hiding mechanism for the sharer in terms of social presence. We discuss the research findings and outline future directions for research in sharing surrounding spaces on social AR applications for wearable devices.},
	Author = {A. {Nassani} and G. {Lee} and M. {Billinghurst} and R. W. {Lindeman}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00019},
	Keywords = {augmented reality;behavioural sciences computing;helmet mounted displays;wearable devices;filtering mechanisms;shared surrounding spaces;social AR applications;Social AR Continuum;social proximity;prototype system;HoloLens devices;hiding mechanism;social presence;Three-dimensional displays;Prototypes;Collaboration;Avatars;Social networking (online);Privacy;Semantics;Augmented Reality;Virtual Avatars;Social Computing},
	Month = {Oct},
	Pages = {17-19},
	Title = {Filtering Mechanisms of Shared Social Surrounding Environments in AR},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00019}}

@inproceedings{8951972,
	Abstract = {Based on the fact that most systems that support piano learning only focus on self-study and ignore the piano class learning situation, we propose an AR-based system to support live group piano learning. The system has two modes: formal learning mode (FLM) and group competition mode (GCM). The first mode, FLM, is designed to allow students to observe teacher's key pressing and finger movement directly from their HMDs so as to enhance mutual understanding between the two parties during the learning process. The second mode, GCM, is developed to further ignite students' passion on learning piano via competition among students. Our goal is to exploit AR technologies for enhancing the learning fun and enthusiasm of beginner-level students on a group piano learning situation, and thus ultimately provide students with a great learning experience.},
	Author = {M. {Cai} and M. A. {Amrizal} and T. {Abe} and T. {Suganuma}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00020},
	Keywords = {augmented reality;computer aided instruction;helmet mounted displays;music;musical instruments;FLM;learning process;GCM;piano class learning situation;live group piano learning;group competition mode;AR-based system;formal learning mode;HMD;AR technologies;beginner-level student;group piano learning situation;learning experience;Pressing;Keyboards;Resists;Bars;Color;Education;Augmented reality;Augmented Reality;HMD;piano learning;group learning},
	Month = {Oct},
	Pages = {20-21},
	Title = {Design of an AR-Based System for Group Piano Learning},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00020}}

@inproceedings{8951978,
	Abstract = {We propose a new technique for Mixed Reality (MR) remote collaboration that uses live and static 360 panoramas inside a 3D scene. Prior work on MR remote collaboration demonstrated benefits of using a 360 panorama, or using a 3D scene. However, collaboration using 360 panorama offers limited spatial information. Our prototype system allows viewing and manipulating of 360 panoramas and visual cues inside a 3D scene, adding spatial information for enhanced contextual understanding between the remote collaborators. We designed a system that combines Augmented Reality (AR) and Virtual Reality (VR) to bridge the visual cues and information relative to the 3D space to help enhance the user experience and interaction in a larger scale MR remote collaboration. We describe our implementation in detail with a discussion of the design implications and potential benefits that could help improve MR remote collaboration.},
	Author = {T. {Teo} and G. A. {Lee} and M. {Billinghurst} and M. {Adcock}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00021},
	Keywords = {augmented reality;groupware;static 360 panoramas;3D scene;mixed reality remote collaboration;spatial information;visual cues;augmented reality;virtual reality;larger scale MR remote collaboration;live 360 panoramas;Three-dimensional displays;Collaboration;Visualization;Prototypes;Avatars;Cameras;Mixed Reality;Remote Collaboration;360 Panorama;3D Reconstruction},
	Month = {Oct},
	Pages = {22-25},
	Title = {Merging Live and Static 360 Panoramas Inside a 3D Scene for Mixed Reality Remote Collaboration},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00021}}

@inproceedings{8951780,
	Abstract = {We propose a concept in this poster paper, the Kuroko Paradigm, which is able to enhance user engagement during interaction with an augmented reality (AR) avatar by adding a physical object to the interaction with the avatar. With the development of AR and VR, interactions between users and AR avatars have been realized with different approaches. However, most of such interactions and experiences are passive, from which users do not expect a high level of engagement. We hypothesize that by introducing a reality actuator, such as a robot or a drone, to handle a physical object triggered by the user without being noticed, and rendering AR avatars as interacting with the physical object at the same time, user engagement during the experience will be enhanced. To prove this concept, we conducted an experiment emulating a classic game of catch. In the experiment, a user will try to throw a ball to an AR avatar, and the ball will be caught by a reality actuator. From the user's perspective, the ball is caught by the AR avatar. In the future, we plan to extend the experiment by adding control groups with differing conditions.},
	Author = {T. {Gao} and Y. {Itoh}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00022},
	Keywords = {augmented reality;avatars;human computer interaction;rendering (computer graphics);physical interaction;AR avatar;user engagement;augmented reality avatar;physical object;reality actuator;Kuroko paradigm;AR avatars;rendering;Avatars;Actuators;Augmented reality;Resists;Manipulators;Computing methodologies;Computer graphics;Mixed / augmented reality;Graphics systemsand interface},
	Month = {Oct},
	Pages = {26-27},
	Title = {The Kuroko Paradigm: The Implications of Augmenting Physical Interaction with AR Avatars},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00022}}

@inproceedings{8951899,
	Abstract = {Systems for remote collaboration on physical tasks generally use AR/VR technologies to create a shared visual space for collaborators to perform tasks together. The shared space often comes from a single camera view. Prior research has not reported on the benefits of using multiple cameras for remote collaboration. On the contrary, there seems to be some usability issues, which must be addressed, when designing remote collaboration systems that use multiple cameras to capture different areas and perspectives of a task space. To be usable, a multi-camera remote collaboration system must indicate to the local user which camera the remote user is looking at and vice versa, the system must make it fast and easy for the remote user to obtain the right camera view for a given collaborative task. We present SceneCam, an AR prototype with which we explore different techniques for improving the usability of multi-camera remote collaboration by making camera selection easier and faster. Specifically, SceneCam implements two camera selection techniques. The first technique nudges the remote user to manually select an optimal camera view of the local user's actions. The second technique automatically selects an optimal camera view of the local user and shows it to the remote user. Additionally, SceneCam implements two focus-in-context views (exocentric and egocentric views) that provide the remote user with a spatial overview of the local user's whereabouts in relation to the multiple task space areas and direct visual access to the camera views of said areas. Camera selection techniques (manual point-and-click, nudging, automatic), and focus-in-context views (no focus-in-context view, exocentric, egocentric) make up the two dimensions in a design space for multi-camera remote collaboration. We describe how SceneCam spans this design space. Lastly, as part of future work we discuss some hypotheses regarding the effects of the proposed camera selection techniques, focus-in-context views and combinations hereof on the usability of multi-camera remote collaboration.},
	Author = {T. A. {Rasmussen} and W. {Huang}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00023},
	Keywords = {augmented reality;cameras;data visualisation;groupware;SceneCam;single camera view;multiple cameras;multicamera remote collaboration system;camera selection techniques;optimal camera view;focus-in-context view;multiple task space areas;multicamera remote collaboration;augmented reality;AR/VR technologies;exocentric view;egocentric view;Cameras;Task analysis;Collaboration;Two dimensional displays;Visualization;Usability;Three-dimensional displays;Remote collaboration;Augmented Reality;Multiple cameras;Usability},
	Month = {Oct},
	Pages = {28-33},
	Title = {SceneCam: Improving Multi-camera Remote Collaboration using Augmented Reality},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00023}}

@inproceedings{8951933,
	Abstract = {This research investigates applying Augmented Reality (AR) visualisation of spatial cues in first-person view task instruction videos. Instructional videos are becoming popular, and are not only used in formal education and training, but even in everyday life as more people seek for how-to videos when they need help with instructions. However, video clips are 2D visualisation of the task space, sometimes making it hard for the viewer to follow and match the objects in the video to those in the real-world task space. We propose augmenting task instruction videos with 3D visualisation of spatial cues to overcome this problem, focusing on creating and viewing first-person view instruction videos. As a proof of concept, we designed and implemented a prototype system, called AR Tips, which allows users to capture and watch first-person view instructional videos on a wearable AR device, augmented with 3D visual cues shown in-situ at the task environment. Initial feedback from potential end users indicate that the prototype system is very easy to use and could be applied to various scenarios.},
	Author = {G. {Lee} and S. {Ahn} and W. {Hoff} and M. {Billinghurst}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00024},
	Keywords = {augmented reality;data visualisation;augmented reality visualisation;spatial cues;formal education;video clips;real-world task space;creating viewing first-person view instruction videos;first-person view instructional videos;3D visual cues;task environment;augmented first-person view task instruction videos;Videos;Task analysis;Visualization;Prototypes;Augmented reality;Three-dimensional displays;Annotations;Augmented task guidance;instructional video;spatial cue},
	Month = {Oct},
	Pages = {34-36},
	Title = {AR Tips: Augmented First-Person View Task Instruction Videos},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00024}}

@inproceedings{8951896,
	Abstract = {In contrast to indoor tracking using computer vision, which has reached a good amount of maturity, outdoor tracking still suffers from comparably poor localization on a global scale. Smartphones and other commodity devices contain consumer-grade sensors for GPS, compass and inertial measurements, which are not accurate enough for augmented reality (AR) in most situations. This restricts what AR can offer to application areas such as surveying or building constructions. We present a self-contained localization device which connects wirelessly to any AR device, such as a smartphone or headset. The device gives centimeter-level accuracy and can be built out of commercial-of-the-shelf components for less than 500 EUR. We demonstrate the performance of the localization device using a variety of position and orientation sensing benchmarks.},
	Author = {M. {Stranner} and C. {Arth} and D. {Schmalstieg} and P. {Fleck}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00025},
	Keywords = {augmented reality;computer vision;Global Positioning System;smart phones;self-contained localization device;AR device;smartphone;high-precision localization device;outdoor augmented reality;indoor tracking;computer vision;outdoor tracking;consumer-grade sensors;inertial measurements;GPS;Global Positioning System;Receivers;Simultaneous localization and mapping;Smart phones;Hardware;Three-dimensional displays;Wireless fidelity;Augmented Reality;Outdoor;Localization},
	Month = {Oct},
	Pages = {37-41},
	Title = {A High-Precision Localization Device for Outdoor Augmented Reality},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00025}}

@inproceedings{8951910,
	Abstract = {Haptic interaction is an important way to enhance human-computer interaction experience in the Augmented Reality (AR). Passive haptic realizes the perception and interaction of virtual objects using a physical prop without active actuators. However, when a single physical prop represents multiple different virtual objects, we might assist active haptic to enhance the haptic perception of virtual objects. This paper described a hybrid technique that combined passive haptic and active haptic and proposed a low-cost vibrotactile feedback prototype, Smart Haproxy, for haptic interaction in an AR assembly task. Smart Haproxy was integrated with a cuboid-shaped physical prop installed with vibration motors. We hope to enhance the haptic perception of virtual objects through active vibrotactile feedback while providing passive haptic sensation. Therefore, we designed a desktop jigsaw-like task to explore the effects of the combination of passive haptic and active haptic on haptic perception. The results found that our prototype could reduce the mismatch of haptic and vision between the haptic proxy and different virtual objects and significantly enhance haptic perception and interaction experience in the AR environment.},
	Author = {M. {Sun} and W. {He} and L. {Zhang} and P. {Wang}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00026},
	Keywords = {augmented reality;haptic interfaces;human computer interaction;active vibrotactile feedback;passive haptic sensation;haptic perception;haptic vision;haptic proxy;Smart Haproxy;active haptic;AR interaction;haptic interaction;human-computer interaction experience;active actuators;single physical prop;multiple different virtual objects;low-cost vibrotactile feedback prototype;cuboid-shaped physical prop;novel vibrotactile feedback prototype;augmented reality;virtual object interaction;hybrid technique;vibration motors;desktop jigsaw-like task;AR assembly task;Haptic interfaces;Task analysis;Shape;Prototypes;Vibrations;User experience;Actuators;Passive-haptic;active-haptic;vibrotactile-feedback},
	Month = {Oct},
	Pages = {42-46},
	Title = {Smart Haproxy: A Novel Vibrotactile Feedback Prototype Combining Passive and Active Haptic in AR Interaction},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00026}}

@inproceedings{8951783,
	Abstract = {Virtual reality (VR) is an important new technology that is fundamentally changing the way people experience entertainment and education content. Due to the fact that most currently available VR products are one-size-fits-all, the user experience of the content interface and user interaction for children is not well understood compared to that for adults. In this study, we seek to explore user experience of locomotion in VR between healthy adults and healthy minors along both objective and subjective dimensions. We design the experiment where subjects complete a task of moving the body and touching underwater animals using VR controllers. The locomotion in VR is implemented using one of four different modalities, as well as using real-world walking without wearing the VR headset as the baseline. Our results show that physical body movement that mirrors real-world movement exclusively is the least preferred by both adults and minors. However, within the different modalities of controller assisted locomotion, there are variations between adults and minors for preference and challenge levels.},
	Author = {Y. {Zhang} and Z. {Huang} and K. {Quigley} and R. {Sankar} and A. {Yang}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00027},
	Keywords = {age issues;design of experiments;human computer interaction;human factors;user experience;virtual reality;content interface;user interaction;VR controllers;controller assisted locomotion;user experience;locomotion design;virtual reality;adult users;minor users;human-centered computing;visualization techniques;physical body movement;VR interactions;Augmented reality;Human centered computing;Virtual reality;Locomotion experience;Visualization design and evaluation},
	Month = {Oct},
	Pages = {47-51},
	Title = {A User Experience Study of Locomotion Design in Virtual Reality Between Adult and Minor Users},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00027}}

@inproceedings{8951975,
	Abstract = {In order to solve the problem that the boundary of gingiva is wrinkled after deformation in orthodontic simulation, we designed a simulation method in a wrapping manner for virtual gingiva based on the traditional Mass-spring model. Firstly, the transition boundary point between teeth and gums was selected as the active point in the deformation process, and the position of the active point was determined by wrapping it on the tooth surface after the correction operation. Then, in order to solve the problem of missing attachment point when the position of the active point is determined, we added a segment of tooth root based on the method of tooth contour curve fitting. Finally, we used the sum of surface curvature values to compare the experimental results. The experimental results show that the proposed method can effectively alleviate the problem of boundary wrinkles after gingival deformation in the case of large correction range, and improve the authenticity of the virtual orthodontic system.},
	Author = {T. {Ma} and Y. {Li} and J. {Li} and Y. {Li}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00028},
	Keywords = {biomechanics;curve fitting;deformation;dentistry;medical computing;orthotics;springs (mechanical);transition boundary point;teeth;gums;tooth surface;tooth root;tooth contour curve;surface curvature values;boundary wrinkles;gingival deformation;virtual orthodontic system;deformation method;virtual gingiva;orthodontic simulation;mass-spring model;Strain;Deformable models;Biological tissues;Springs;Force;Mathematical model;Wrapping;virtual-orthodontics;soft-tissue-deformation;Mass-spring-model;wrapping-deformation},
	Month = {Oct},
	Pages = {52-57},
	Title = {A Deformation Method in a Wrapping Manner for Virtual Gingiva Based on Mass-Spring Model},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00028}}

@inproceedings{8951925,
	Abstract = {Head mounted display (HMD) is an important part of the virtual reality (VR) system and the main interface of virtual world. During the process of using HMD, there is a certain time gap between the user's movement and the triggered change of display, which is referred to herein as a " Motion-to- Photon " latency. The latency may cause cybersickness which has an adverse effect on user experience and even physical health, so it is necessary to evaluate the latency of an HMD. We propose an automated and universal HMD latency measurement system. By simulating a user's motion, we are able to measure "Motion-to-Photon" latency using the generated periodic signals in HMD, with accuracy and reliability confirmed by our experiments. The system is fully automatic and easy to use, and suitable for the HMD with inside-out or outside-in positioning methods. It helps to improve both the detection level of existing HMDs and the user experience. It is of guiding significance for the design and manufacture of new HMD.},
	Author = {H. {Xun} and Y. {Wang} and D. {Weng}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00029},
	Keywords = {helmet mounted displays;human computer interaction;human factors;user experience;virtual reality;motion-to-photon latency;virtual reality head mounted display;user experience;automated HMD latency measurement system;cybersickness;human-centered computing;human computer interaction;Augmented reality;Measurement;head-mounted-display;virtual-reality;motion-to-photon-latency},
	Month = {Oct},
	Pages = {58-62},
	Title = {New System to Measure Motion Motion-to-Photon Latency of Virtual Reality Head Mounted Display},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00029}}

@inproceedings{8951998,
	Abstract = {Many real world applications for Microsoft HoloLens*-based applications suffer the problem of reliably recognizing and identifying movable objects within an environment. While the HoloLens is perfectly able to discern already known rooms, it still has troubles with reflecting surfaces or identically shaped objects. Using dedicated recognition libraries for each task poses the issue of shared resource access in the rather controlled HoloLens environment-In this poster we present a solution for scenario with hard to track objects and similarly shaped objects for an electrical cabinet assembly task, where the reflective cabinet is tagged with a marker and the prefabricated cables are differentiated by text-based labels.},
	Author = {S. {Knopp} and P. {Klimant} and R. {Schaffrath} and E. {Voigt} and R. {Fritzsche} and C. {Allmacher}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00030},
	Keywords = {augmented reality;computer-generated holography;image recognition;object tracking;text analysis;hololens AR;Vuforia-based marker;text recognition;assembly scenario;Microsoft HoloLens;movable objects;electrical cabinet assembly task;reflective cabinet;text-based labels;recognition libraries;HoloLens;Augmented Reality;Vuforia;Text Recognition;Application showcase},
	Month = {Oct},
	Pages = {63-64},
	Title = {Hololens AR - Using Vuforia-Based Marker Tracking Together with Text Recognition in an Assembly Scenario},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00030}}

@inproceedings{8951957,
	Abstract = {Cinematic Virtual Reality allows viewers to watch films without the limitation of screen edges, whilst controlling their own viewpoint. The loss of screen edges and camera control leads to the problem that filmmakers cannot precisely edit each shot using traditional techniques, as the viewer's direction and field of view is dynamic. As such, many established film making methods should be reconsidered. In this paper, we present our initial exploration for the implementation of montage in Virtual Reality (VR) films, focusing on the investigation of transition effects. A pilot study is presented, which compared three transition effects. Two popular existing transition effects (cut and fade) were applied, along with a third method (VR portal). The VR Portal was designed and selected specifically to meet the requirements for montage in VR films. We present the preliminary results and our insights, concluding with future plans.},
	Author = {R. {Cao} and J. {Walsh} and A. {Cunningham} and C. {Reichherze} and S. {Dey} and B. {Thomas}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00031},
	Keywords = {cameras;cinematography;portals;virtual reality;montage transitions;cinematic Virtual Reality;screen edges;camera control;Virtual Reality films;VR portal;VR films;preliminary exploration;film making methods;Portals;Videos;Visualization;Cameras;Media;Pediatrics;Virtual reality;Cinematic Virtual Reality;montage;transition effects},
	Month = {Oct},
	Pages = {65-70},
	Title = {A Preliminary Exploration of Montage Transitions in Cinematic Virtual Reality},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00031}}

@inproceedings{8951936,
	Abstract = {Ubiquitous Tracking (UT) for Augmented Reality (AR) applications shows great potential in becoming a key driving force in the wide scale adoption of AR. However, the implementation and integration of a global sensor network necessary for this technology may seem too daunting of a task to entrust in one central body. We propose an architecture designed to allow both individual and corporate bodies to equally contribute to the construction of a sensor network, providing a platform in which the growth of the AR environment is in the hands of its users. We propose a distributed blockchain based model capable of storing tracking entity pose data. The model allows users to contribute tracking entities to the blockchain and to share this data through peer-to-peer connections. We conclude with a discussion surrounding beneficial contributions and necessary future works.},
	Author = {A. {Sosin} and Y. {Itoh}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00032},
	Keywords = {augmented reality;cryptography;distributed databases;peer-to-peer computing;software architecture;ubiquitous computing;contributional tracking architecture;UT architecture;augmented reality information systems;data tracking;data sharing;data storage;architecture design;peer-to-peer connections;distributed blockchain based model;global sensor network;ubiquitous tracking;augmented reality;WARP;Blockchain;Augmented reality;Data structures;Peer-to-peer computing;Data models;Computational modeling;Indexes;Ubiquitous Tracking, Blockchain, Sensor Network, Contributional Tracking, Augmented Reality},
	Month = {Oct},
	Pages = {71-72},
	Title = {WARP: Contributional Tracking Architecture Towards a Worldwide Augmented Reality Platform},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00032}}

@inproceedings{8951928,
	Abstract = {We examine the research agenda proposed for Augmented Reality Television (AR-TV) through the perspective of the findings of an online study, where 172 respondents shared their understanding of Augmented Reality (AR) and their preferences for AR-TV scenarios.},
	Author = {I. {Popovici} and R. {Vatavu}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00033},
	Keywords = {augmented reality;human computer interaction;human factors;research agenda;Augmented Reality Television;potential end-users;AR-TV;human computer interaction;Augmented reality;Visualization;Smart TV;Smart devices;Tools;Smart phones;augmented reality, interactive TV, study, questionnaire, research agenda},
	Month = {Oct},
	Pages = {73-74},
	Title = {Consolidating the Research Agenda of Augmented Reality Television with Insights from Potential End-Users},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00033}}

@inproceedings{8951941,
	Abstract = {This paper proposes a feasible and low-cost solution for designing optical-inertial hybrid motion capture systems by integrating two subsystems: the inertial subsystem and the optical subsystem. We calibrate the relative pose of the optical sensor module and the human joint to establish the transformation between the joint parameters and the optical sensor data, so that the data fusion can be performed under the same reference frame. To construct the above motion capture system, an optical-inertial data fusion scheme is proposed, which leverages the optical data of key nodes of the human body to correct the inertial data with accumulated errors. The proposed system can support long-term drift-free operation with a low-cost hardware setup, avoiding the drift problem of the inertial motion capture system. Simulation experiments have shown that optical data can reduce the drift error to the centimeter level. The practical experiment shows that the virtual character can complete the loop detection after the inertial position is corrected by the optical data. Our motion capture system outputs standard motion capture data, which can be directly imported into engines such as Unity3D and Unreal.},
	Author = {Y. {Li} and D. {Weng} and D. {Li} and Y. {Wang}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00034},
	Keywords = {image fusion;image motion analysis;image sensors;object detection;object tracking;optical sensors;pose estimation;virtual reality;optical data;inertial position;standard motion capture data;low-cost drift-free optical-inertial hybrid motion capture system;high-precision human pose detection;optical-inertial hybrid motion capture systems;inertial subsystem;optical subsystem;optical sensor module;human joint;optical sensor data;optical-inertial data fusion scheme;low-cost hardware setup;drift error reduction;centimeter level;virtual character;Adaptive optics;Tracking;Optical sensors;Joints;Optical design;Optical films;Base stations;Motion capture;data fusion;drift correction},
	Month = {Oct},
	Pages = {75-80},
	Title = {A Low-Cost Drift-Free Optical-Inertial Hybrid Motion Capture System for High-Precision Human Pose Detection},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00034}}

@inproceedings{8951778,
	Abstract = {This paper presents an obstacle alert system for the users using AR applications while walking. The system analyzes the input camera image to detect the obstacles. With this obstacle detector, two experiments were made to investigate the obstacle alert interfaces and the orientation guide interfaces. Then, the best interfaces identified from the two experiments were combined into an AR application named SafeAR.},
	Author = {H. {Kang} and G. {Lee} and J. {Han}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00035},
	Keywords = {augmented reality;cameras;object detection;pedestrians;road safety;AR application;input camera image;obstacle detector;obstacle alert interfaces;orientation guide interfaces;SafeAR;obstacle alert system;AR alert system;obstacle avoidance;pedestrians;Visualization;Legged locomotion;Augmented reality;Cameras;Detectors;Games;Augmented Reality;Pedestrian Safety;Human computer Interaction;Interface},
	Month = {Oct},
	Pages = {81-82},
	Title = {SafeAR: AR Alert System Assisting Obstacle Avoidance for Pedestrians},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00035}}

@inproceedings{8951990,
	Abstract = {This paper proposes a novel easy extrinsic calibration algorithm for an off-the-shelf VR system and a multi-camera based marker-less motion capture system. To realize interactions between 3D user motions and virtual objects reconstructed from multi-view videos in a common 3D space, the extrinsic calibration of the VR system and the multiple cameras must be conducted beforehand. This calibration, which involves estimating the pose and position of each coordinate system, is a key technology for handling 3D information in a system with various type of input sources. In general, extrinsic calibration is carried out by identifying and utilizing some common 3D points. However, since most of off-the-shelf VR systems do not include any imaging device, it is difficult to apply conventional automatic calibration approaches as there are no common points shared with the cameras. Against this problem, this paper introduces an easy calibration algorithm by generating corresponding points from the trajectories of the user's motion with VR devices and 3D human pose reconstructed from multi-view videos. Our study provides the following two contributions; (1) our method does not need to introduce additional devices, such as a chessboard and (2) our method does not need manual processes as the extrinsic parameters are automatically estimated. We demonstrate the performance of the proposed method in a practical scenario.},
	Author = {K. {Takahashi} and D. {Mikami} and M. {Isogawa} and S. {Sun} and Y. {Kusachi}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00036},
	Keywords = {cameras;image capture;image motion analysis;image reconstruction;pose estimation;stereo image processing;video signal processing;virtual reality;VR devices;multiview videos;extrinsic calibration algorithm;off-the-shelf VR system;3D user motions;common 3D space;multiple cameras;common 3D points;automatic calibration approaches;multicamera based markerless motion capture system;virtual objects reconstruction;3D human pose reconstruction;calibration;motion;multi view},
	Month = {Oct},
	Pages = {83-88},
	Title = {Easy Extrinsic Calibration of VR System and Multi-camera Based Marker-Less Motion Capture System},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00036}}

@inproceedings{8951906,
	Abstract = {Recording medical surgery operations is important for sharing the various operating techniques. In most operating rooms, fixed surgery cameras are already installed, but it is almost impossible to capture the surgical field because of occlusion by the surgeon's head and body. In order to capture the surgical field in real surgery operations, we proposed the installation of multiple cameras in a surgical lighting system, so that at least one camera can capture the target surgical field even when the surgeon's head and body occlude other cameras. In this paper, we present a method for automatic viewpoint selection from multi-view surgical videos, so that the surgical field can always be recorded in the output video. We employ a method for learning-based object detection from videos for automatic evaluation of the surgical field area from multiple input videos. By selecting the viewpoint with the largest area of the surgical field, we can virtually reduce the area of the surgeon's head and hands. In general, frequent camera switching degrades the video quality of view (QoV). Therefore, we apply the Dijkstra method widely used in the shortest path problem as a combinatorial optimization method for this problem. Our camera scheduling method is that the camera switching is not performed for a certain period of time, and the surgical field observed in the entire video is maximized.},
	Author = {T. {Shimizu} and K. {Oishi} and H. {Saito} and H. {Kajita} and Y. {Takatsume}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00037},
	Keywords = {cameras;combinatorial mathematics;learning (artificial intelligence);medical image processing;object detection;optimisation;surgery;video recording;video signal processing;automatic viewpoint switching;multiview surgical videos;medical surgery operations;fixed surgery cameras;surgical lighting system;automatic viewpoint selection;camera scheduling method;camera switching;video quality of view;Dijkstra method;shortest path problem;combinatorial optimization method;learning-based object detection;Augmented reality},
	Month = {Oct},
	Pages = {89-90},
	Title = {Automatic Viewpoint Switching for Multi-view Surgical Videos},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00037}}

@inproceedings{8951982,
	Abstract = {In this paper, we describe a new Mixed Reality (MR) remote collaborative platform making use of 3D CAD models for training in the manufacturing industry. It enables a remote expert in Virtual Reality (VR) to train a local worker in a physical assembly task. For the local site, we use Spatial Augmented Reality (SAR) to enable the local worker see virtual cues without wearing any AR devices, leaving their user hands free to easily manipulate the physical parts. For the remote expert, we construct a 3D virtual environment using virtual replicas of the physical parts. We also report on the results of a usability study of the prototype.},
	Author = {P. {Wang} and X. {Bai} and M. {Billinghurst} and S. {Zhang} and D. {Han} and H. {Lv} and W. {He} and Y. {Yan} and X. {Zhang} and H. {Min}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00038},
	Keywords = {augmented reality;CAD;computer based training;groupware;manufacturing industries;production engineering computing;solid modelling;MR remote collaborative platform;3D CAD models;manufacturing industry;remote expert;Virtual Reality;physical assembly task;Spatial Augmented Reality;3D virtual environment;Mixed Reality remote collaborative platform;training in industry;Three-dimensional displays;Training;Collaboration;Solid modeling;Task analysis;Virtual reality;Usability;Remote collaboration;3D CAD models;training in the industry;Augmented Reality;Mixed reality},
	Month = {Oct},
	Pages = {91-92},
	Title = {An MR Remote Collaborative Platform Based on 3D CAD Models for Training in Industry},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00038}}

@inproceedings{8951987,
	Abstract = {Nowadays unbalanced crop yield and population growth have led to people worry about a potential future ``Agricultural Crisis''. To increase the arable land use efficiency, this research has focused on the development of Augmented Reality based Agricultural information visualization and fieldwork navigation tool. This tool has been developed to provide professional aids for farmers' daily fieldwork. To provide continuous fieldwork assistance, this system integrates non-distraction field navigation, the overall visualization of fields and subfields in the form of overlaid polygons, and the detail sensor data visualization for pertinent subfields in the form of Point of Interests. Due to the limitation of current commercially available AR headsets, we first validate this test bench on Virtual Reality video see-through headsets and AR tablets. At this stage, this test bench has only been validated with simulated data. In future, Situated Analytics concepts are planned to be integrated into the system to in-situ visualize real-time datachunks acquired from local sensors, so as to provide timely-accurate information and decision support even when the whole dataset is in a high volume.},
	Author = {M. {Zheng} and A. G. {Campbell}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00039},
	Keywords = {agriculture;augmented reality;crops;data visualisation;decision support systems;land use;nondistraction field navigation;detail sensor data visualization;AR headsets;Virtual Reality video;decision support;location-based Augmented Reality;in-situ visualization;Agricultural fieldwork navigation;crop yield;population growth;Agricultural information visualization;farmers;continuous fieldwork assistance;agricultural crisis;land use},
	Month = {Oct},
	Pages = {93-97},
	Title = {Location-Based Augmented Reality In-situ Visualization Applied for Agricultural Fieldwork Navigation},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00039}}

@inproceedings{8951949,
	Abstract = {This user-centered design research project aimed to investigate visual and interaction principles for augmented reality (AR) in the context of environmental and nutritional food labelling. Nutritional information on existing food labels is often misunderstood and environmental information is seldom depicted, despite consumer demand. The project explored the potential of AR in this context with a two-phase process. Phase 1 aimed to engage large audiences in public spaces using a stand-alone AR device showing environmental information only. This allowed design strategies to be tested. Phase 2 addressed personalised information, combining nutritional and environmental data with smartphone AR. Here we integrated design learnings from Phase 1 and then focused on assessing the benefits of AR. A between-subjects study with 84 participants compared two-versions of the smartphone application; one version showed the information with AR and the other showed the same information with a static page. Results showed that participants using the AR version learned more about food products than those using the static version. In addition, the AR version matched the high scores of the static version with regards to usability (SUS score of 86) and aesthetics (VisAWI score of 5.9), despite technical limitations of AR. This work reveals that AR can be a credible medium in the food industry and provides visual and interaction design learnings to inform designers in the industry.},
	Author = {A. {Sonderegger} and D. {Ribes} and N. {Henchoz} and E. {Groves}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00040},
	Keywords = {augmented reality;food products;health care;learning (artificial intelligence);smart phones;user centred design;user interfaces;food talks;environmental food information;nutritional food information;augmented reality;user-centered design research project;environmental food labelling;nutritional food labelling;nutritional information;food labels;environmental information;nutritional data;environmental data;AR version;food products;food industry;visual principles;interaction design learnings;visual design learnings;interaction principles;smartphone application;Visualization;Usability;Augmented reality;Labeling;Atmospheric measurements;Particle measurements;Food products;augmented reality;food labelling;user experience;interaction design;design research},
	Month = {Oct},
	Pages = {98-103},
	Title = {Food Talks: Visual and Interaction Principles for Representing Environmental and Nutritional Food Information in Augmented Reality},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00040}}

@inproceedings{8951981,
	Abstract = {In many complex tasks, a remote expert may need to assist a local user or to guide his or her actions in the local user's environment. Existing solutions also allow multiple users to collaborate remotely using high-end Augmented Reality (AR) and Virtual Reality (VR) head-mounted displays (HMD). In this paper, we propose a portable remote collaboration approach, with the integration of AR and VR devices, both running on mobile platforms, to tackle the challenges of existing approaches. The AR mobile platform processes the live video and measures the 3D geometry of the local environment of a local user. The 3D scene is then transited and rendered in the remote side on a mobile VR device, along with a simple and effective user interface, which allows a remote expert to easily manipulate the 3D scene on the VR platform and to guide the local user to complete tasks in the local environment.},
	Author = {J. {Venerella} and T. {Franklin} and L. {Sherpa} and H. {Tang} and Z. {Zhu}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00041},
	Keywords = {augmented reality;groupware;helmet mounted displays;mobile computing;rendering (computer graphics);solid modelling;user interfaces;local user environment;mobile VR device;user interface;VR platform;mobile remote collaboration;virtual reality head-mounted displays;AR mobile platform;portable remote collaboration;high-end augmented reality HMD;multiple user collaboration;live video;local environment 3D geometry;rendering;Three-dimensional displays;Annotations;Collaboration;Task analysis;Solid modeling;Google;Cloud computing;augmented reality, virtual reality, remote collaboration, 3D mesh},
	Month = {Oct},
	Pages = {104-108},
	Title = {Integrating AR and VR for Mobile Remote Collaboration},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00041}}

@inproceedings{8952001,
	Abstract = {A serious trouble of virtual bicycle ride has to be faced currently, that is the visual information of scene motion perceived by riders doesn't synchronize with the proprioceptive information of actual ride. This phenomenon makes riders deeply depressing in immersion and interaction. This paper proposes three different interpolation algorithms of Catmull-Rom curve to evaluate the consistency of visual and proprioceptive perceptions for virtual bicycle ride. Experimental results show that the realtime interpolation algorithm of Catmull-Rom curve shall help to match visual and proprioceptive perceptions, and improve the immersion and interaction of virtual bicycle ride.},
	Author = {X. {Wu} and Q. {Zhou} and X. {Li} and W. {Yang} and Z. {Pan}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-69},
	Keywords = {bicycles;human computer interaction;interpolation;mechanoception;virtual reality;visual perception;visual evaluation;proprioceptive evaluation;virtual bicycle ride;visual information;proprioceptive information;Catmull-Rom curve;visual perceptions;proprioceptive perceptions;scene motion;interpolation algorithms;Virtual-ride;Proprioceptive-perception;Interpolation-algorithm;HCI},
	Month = {Oct},
	Pages = {109-113},
	Title = {Visual and Proprioceptive Evaluation for Virtual Bicycle Ride},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-69}}

@inproceedings{8951921,
	Abstract = {Augmented reality is a technology which extends measureless contents in the virtual space and makes actual objects not limited in the physical properties. As the most popular cultural media among the campus, the poster have its shortcoming of offering restricted information. In this paper, we introduce a responsive poster-reading system which generates diverse contents step by step with body-based and hand-based interactions, and a design guideline which applies for corresponding posters. The proposed method is able to provide users with a user-friendly, immersive and intuitive experience.},
	Author = {S. {Liu} and S. {Jang} and W. {Woo}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-68},
	Keywords = {augmented reality;human computer interaction;PostAR;campus augmented posters;augmented reality;measureless contents;virtual space;actual objects;physical properties;cultural media;restricted information;responsive poster-reading system;diverse contents step;hand-based interactions;design guideline;body-based interactions;user-friendly experience;immersive experience;intuitive experience;Augmented reality;Three-dimensional displays;TV;Media;Avatars;Two dimensional displays;MAR;augmented poster;visualization;interaction;guidance},
	Month = {Oct},
	Pages = {114-117},
	Title = {PostAR: Design a Responsive Reading System with Multiple Interactions for Campus Augmented Posters},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-68}}

@inproceedings{8951939,
	Abstract = {The use of augmented reality technology is becoming a necessity of many cultural heritage sites to stay competitive and attractive to tourists. In this paper, we present an outdoor augmented reality guidance system for ancient rock painting aiming to enhance tourist experience and introduce the culture and historical background. We propose a two-stage approach that combines GPS and feature-based recognition methods. In the first stage, we build a navigation map in advance, which consists of the distribution information of major scenic spots, then visit these spots guided by GPS data on the standard smartphone. In the second stage, we adopt a feature-based recognition and tracking method to identify rock painting accurately and augment them with interactive contextual annotations like 3D textual labels, images, and videos. The reliability of this system is verified through on-site demonstrations - RockArt of the Helan Mountain, which is one of National Key Historical and Cultural Sites in China.},
	Author = {Q. {Zhang} and X. {Zhu} and H. {Yu} and Y. {Jiang}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-67},
	Keywords = {augmented reality;Global Positioning System;history;human computer interaction;image annotation;image recognition;interactive systems;object tracking;travel industry;augmented reality technology;cultural heritage sites;outdoor augmented reality guidance system;ancient rock painting;historical background;two-stage approach;feature-based recognition methods;navigation map;distribution information;scenic spots;GPS data;tracking method;Cultural Sites;on-site demonstrations;rock painting tour experience enhancement;standard smartphone;interactive contextual annotations;3D textual labels;RockArt;Helan mountain;Painting;Rocks;Feature extraction;Cultural differences;Navigation;Augmented reality;ugmented reality;historic sites;outdoor tour guiding},
	Month = {Oct},
	Pages = {118-121},
	Title = {Enhancing Rock Painting Tour Experience with Outdoor Augmented Reality},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-67}}

@inproceedings{8951926,
	Abstract = {Interactive educational environments are one of the prime applications for the use of Augmented Reality (AR). A large variety of such systems has been proposed in the past for various areas of education. However, in most cases the number of users these AR systems can support is limited. Only few systems have been developed that support a large number of co-located users to jointly collaborate in a dynamic and interactive learning environment. Multi-user AR collaboration presents a unique setting with distinct challenges and requirements for user interaction and information sharing. In this paper, we present VesARlius, a novel AR system for collaborative and interactive anatomy learning in a large group of co-located users. Our system employs a set of multi-user collaboration paradigms allowing users to engage in an interactive AR learning environment. We evaluated the collaborative features of our system in a user study with 16 medical students. Results demonstrate the potential of the VesARlius system to be used effectively for large-group AR anatomy learning. From our lessons learned, we provide a set of design guidelines for developing similar AR systems to enable large-group collaboration in other application domains.},
	Author = {F. {Bork} and A. {Lehner} and D. {Kugelmann} and U. {Eck} and J. {Waschke} and N. {Navab}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-66},
	Keywords = {augmented reality;biology computing;computer aided instruction;groupware;interactive systems;medical computing;multiuser AR collaboration;user interaction;information sharing;AR system;collaborative anatomy;interactive anatomy;multiuser collaboration paradigms;interactive AR learning environment;collaborative features;VesARlius system;augmented reality system;interactive educational environments;dynamic learning environment;interactive learning environment;co-located anatomy learning;Collaboration;Synchronization;Pins;Three-dimensional displays;Solid modeling;Augmented reality},
	Month = {Oct},
	Pages = {122-123},
	Title = {VesARlius: An Augmented Reality System for Large-Group Co-located Anatomy Learning},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-66}}

@inproceedings{8951979,
	Abstract = {Mental fatigue can affect people's performance, which is an important human factor in various application fields. In the past, the research on mental fatigue caused by long-term tasks was based on real environments. This study assessed the mental fatigue of performing long-term tasks in a virtual environment (VE) and compared it with the mental fatigue in the real environment (RE). We designed a comparative 8-hour experiment, allowing the participants to perform four simple office tasks in the VE and the RE. Psychomotor Vigilance Task (PVT) was carried out at the six stages of the experiment. The changes in mental fatigue of the subjects were evaluated by lapses and mean reaction times. According to the results, it can be concluded that the mental fatigue caused by long-term office work in VE was higher. Short breaks can suppress the growth of lapses, but its effect on the mean reaction times was not obvious.},
	Author = {R. {Shen} and D. {Weng} and S. {Chen} and J. {Guo} and H. {Fang}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-65},
	Keywords = {ergonomics;fatigue;human computer interaction;human factors;medical computing;virtual reality;mental fatigue;virtual environment;long-term office tasks;psychomotor vigilance task;Task analysis;Fatigue;Virtual environments;Resists;Microsoft Windows;Human factors;Virtual reality, User studies, Psychology},
	Month = {Oct},
	Pages = {124-127},
	Title = {Mental Fatigue of Long-Term Office Tasks in Virtual Environment},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-65}}

@inproceedings{8951970,
	Abstract = {The military training systems based on augmented reality(AR) present challenges for cooperation training and confrontation training. In this paper, we propose a multi-vehicle cooperative military training simulation system based on video see-through AR. Our system combines robust tracking algorithm, realistic AR display and network module to overlay virtual forces accurately on each soldier's real-world view. We present our tracking algorithm using a monocular camera in combination with an inertial measurement unit(IMU) and GPS in a loosely coupled Extended Kalman Filter(EKF)-based framework to obtain robust and real-time pose estimation. Our AR display provides real-time dynamic painting of virtual forces, battlefield situational awareness, and visual feedback for a variety of training activities. The inserted virtual forces can move in a pre-defined trajectory and shoot real vehicles in the specified range. The network module is used to support multi-vehicle real-time cooperative training. We evaluate the accuracy of the tracking algorithm by comparing with DGPS. The consistency of rendering on different soldiers' AR displays is analyzed. In addition, we test our distinctive multi-vehicle cooperative confrontation training system online in a small team.},
	Author = {L. {Fan} and J. {Chen} and Y. {Miao} and J. {Ren} and Y. {Wang}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-64},
	Keywords = {augmented reality;computer based training;computer simulation;Global Positioning System;Kalman filters;military computing;nonlinear filters;pose estimation;robust tracking algorithm;virtual forces;confrontation training system;military training systems;multivehicle cooperative military training simulation system;augmented reality;video see-through AR;AR display;monocular camera;inertial measurement unit;extended Kalman filter;real-time pose estimation;battlefield situational awareness;visual feedback;cooperation training;Augmented reality;Augmented reality, Military simulation, Multi-sensor fusion, SLAM},
	Month = {Oct},
	Pages = {128-133},
	Title = {Multi-vehicle Cooperative Military Training Simulation System Based on Augmented Reality},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-64}}

@inproceedings{8951907,
	Abstract = {For the development of industrial Augmented Reality (AR) applications beyond the scope of a marketing showcase for trade shows, the practical usability of researched technologies needs to be considered. In this paper we show the design process for a Microsoft HoloLens* application, with the goal of demonstrating the viability of AR-guidance in industrial business. The design is consistently focussing on the benefit of the application as well as usability in an industrial environment. This includes specifically aspects of image and object recognition, which are impaired in a dirty environment or with objects frequently subjected to wear and tear.},
	Author = {S. {Knopp} and P. {Klimant} and C. {Allmacher}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-63},
	Keywords = {augmented reality;groupware;image recognition;object recognition;production engineering computing;wear;collaborative support;industrial augmented reality;design process;Microsoft HoloLens* application;AR-guidance;industrial business;industrial environment;modular mold disassembly;modular mold assembly;industrial use case;object recognition;image recognition;Streaming media;Maintenance engineering;Task analysis;Augmented reality;Cameras;Conferences;HoloLens;AR-Guidance;Industrial Use Case;Live streaming},
	Month = {Oct},
	Pages = {134-135},
	Title = {Industrial Use Case - AR Guidance using Hololens for Assembly and Disassembly of a Modular Mold, with Live Streaming for Collaborative Support},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-63}}

@inproceedings{8951966,
	Abstract = {Architectural walkthrough of 3D buildings usually requires the user to manipulate the position and orientation of a camera for exploration, measurement, and annotation. In this paper, we propose a novel two-point interaction method by interacting with the corresponding 2D floor plans to navigate in 3D buildings. Our method is based on the observation that, naturally, people tend to relate standing tall to seeing far.},
	Author = {K. {Chen} and E. {Lee}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-62},
	Keywords = {architecture;buildings (structures);floors;interactive systems;solid modelling;user interfaces;two-point map-based interface;architectural walkthrough;two-point interaction method;2D floor plans;3D buildings;Three-dimensional displays;Two dimensional displays;Cameras;Buildings;Navigation;Prototypes;touch interface;architectural walkthrough},
	Month = {Oct},
	Pages = {136-137},
	Title = {A Two-Point Map-Based Interface for Architectural Walkthrough},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-62}}

@inproceedings{8951889,
	Abstract = {Previous research has shown the potential of Augmented Reality (AR) in education, however, its use is still not widespread in the classroom setting. This study aims to understand what is the current maturity level regarding AR use in schools, as well as what is preventing schools to reach higher levels of maturity. We aim to discuss the current use of AR in schools and reflect on ways AR technology can evolve and adapt to support more meaningful and effective learning practices. 106 teachers answered an online survey in order to help us understand those issues. Results have shown that lack of infrastructure and authoring tools are the two biggest problems hindering AR use in classrooms. Evidence suggests the need to focus on authoring tools that support collaboration and creativity in the educational settings, thus, enabling schools to use AR technology in more effective ways and achieving higher levels of maturity. We conclude by listing features for designing AR applications in relation to the maturity levels identified.},
	Author = {M. M. {Oliveira da Silva} and R. {Alves Roberto} and I. {Radu} and P. {Smith Cavalcante} and V. {Teichrieb}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-61},
	Keywords = {augmented reality;computer aided instruction;educational institutions;educational settings;augmented reality;classroom setting;learning practices;authoring tools;AR technology;schools;Education;Technological innovation;Tools;Collaboration;Augmented reality;Problem-solving;Adaptation models;Augmented Reality;Education;Technology Maturity},
	Month = {Oct},
	Pages = {138-143},
	Title = {Why Don't We See More of Augmented Reality in Schools?},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-61}}

@inproceedings{8951891,
	Abstract = {The traditional way of learning geometry cannot provide a great support for novice students since the geometric figures are 2D on the blackboard or the book. In consideration that Augmented Reality(AR) provides an intuitive way to learn geometry, an interactive AR system that enables students to naturally and directly manipulating 3D objects through hand gesture-based interactions and intuitively explore the spatial relationship between spheres and polyhedrons is proposed in this paper. The proposed gesture-based interaction enables the user manipulate AR objects in the real 3D space instead of 2D space. We design three levels of study to enable students to learn the geometric concepts as well as an experiment to evaluate the effectiveness of the AR system. Analysis of experimental results showed that the proposed system is easy to use, attractive, and helpful for students.},
	Author = {R. {Cao} and Y. {Liu}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-60},
	Keywords = {augmented reality;computer aided instruction;geometry;gesture recognition;mathematics computing;geometric figures;3D objects manipulation;hand gesture-based interactions;geometric concepts;interactive AR system;augmented reality application;novice students;hand controlAR;3D geometry learning;AR objects manipulation;Three-dimensional displays;Geometry;Education;Solid modeling;Two dimensional displays;Shape;Tracking;Augmented Reality;Hand Gesture Interaction;User Defined Targets;3D Objects Manipulation;Geometry Education},
	Month = {Oct},
	Pages = {144-149},
	Title = {Hand ControlAR: An Augmented Reality Application for Learning 3D Geometry},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-60}}

@inproceedings{8951900,
	Abstract = {Vocabulary is an essential part in second language learning. The traditional way of learning vocabulary is dull and boring in class with thin context. Nevertheless, a rich context for memorizing words is necessary according to the situated learning theory. Thanks to virtual reality (VR) technology, such a rich context can be achieved by creating a virtual simulation scenario. "Words In Kitchen" is a VR system designed as an educational tool for second language learners to learn vocabulary. With the help of the proposed system, a user can interact with virtual objects in the virtual scene to learn the spelling and pronunciation of specific vocabularies. We aim to make full use of immersion and interaction of VR and propose a prototype of vocabulary learning in VR, so that the system can promote learners' common vocabulary learning as a supplement to class and textbooks. A pilot study was conducted to identify its usability and enjoyment and the result indicated that the system could help remember words and stimulate interests.},
	Author = {T. {Jia} and Y. {Liu}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-59},
	Keywords = {computer aided instruction;linguistics;virtual reality;vocabulary;pronunciation;spelling;educational tool;words memorization;Words in Kitchen;second language learning;virtual reality technology;vocabulary learning;virtual scene;virtual objects;language learners;VR system;virtual simulation scenario;situated learning theory;Vocabulary;Task analysis;Education;Games;Augmented reality;Tools;Language learning, education, virtual reality},
	Month = {Oct},
	Pages = {150-155},
	Title = {Words in Kitchen: An Instance of Leveraging Virtual Reality Technology to Learn Vocabulary},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-59}}

@inproceedings{8951893,
	Abstract = {We present an idea that uses a tracked graphics tablet attached a virtual or physical object to grasp in VR for direct 3D sketching on virtual objects, giving users the feeling of holding a physical object and drawing on it. We developed a prototype system and evaluated the effect of different tablet manipulation modes on user performance and experience when drawing strokes on virtual objects in VR. Results suggested that having a physical handle or object to grasp enhanced the participants' sense of realism, compared to conditions where there was no tablet or a simple tablet interface.},
	Author = {S. {Wang} and W. {He} and B. {Zheng} and S. {Feng} and S. {Wang} and X. {Bai} and M. {Billinghurst}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-58},
	Keywords = {graphical user interfaces;human computer interaction;solid modelling;virtual reality;virtual reality;tablet manipulation modes;tangible 3D sketching;tablet interface;physical object;virtual object;tracked graphics tablet;Augmented reality;3D sketching, virtual reality, tangible interface},
	Month = {Oct},
	Pages = {156-157},
	Title = {Holding Virtual Objects Using a Tablet for Tangible 3D Sketching in VR},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-58}}

@inproceedings{8952000,
	Abstract = {We introduce a unique wearable device for safe navigation that supports a stop function. People have many opportunities to stop when they are walking and running, especially in the outdoor environment. They have to stop when a traffic light turns red or when an accident suddenly occurs in front of them. However, conventional navigation applications do not take these situations into account, so they do not support stop functions. In contrast, our approach provides two functions: navigation with vibrating motors and a stop function that squeezes a user's neck. In this paper, we introduce our concept and prototype implementation.},
	Author = {Y. {Yue} and H. {Tobita}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-57},
	Keywords = {navigation;road traffic;wearable computers;tie-brake;brake function;traffic light;navigation applications;tie-based wearable device;vibrating motors;Wearable device;brake function;physical feedback;computational tie},
	Month = {Oct},
	Pages = {158-159},
	Title = {Tie-Brake: Tie-Based Wearable Device for Navigation with Brake Function},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-57}}

@inproceedings{8951969,
	Abstract = {The severity and development of psoriasis is difficult to describe and discuss between patients and doctors. Such phenomenon mainly caused by traditional medical communication methods (language and pictures) cannot visually show the disease status and the comprehension largely depend on doctors' (variable) skills to describe the anomaly. In this paper, we propose a solution based on augmented reality (AR). Patients can see psoriasis develop on their face by making accurate and reliable psoriasis maps. Heuristic analysis and pilot testing of 24 participants demonstrated that the solution can significantly increase comprehension and promote the patient's willingness to receive medical support. We believe that AR can be an effective tool to assist the treatment of psoriasis.},
	Author = {Y. {Jiang} and D. {Weng} and R. {Ju}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-56},
	Keywords = {augmented reality;biomedical communication;decision making;diseases;health care;medical computing;medical image processing;medical information systems;patient treatment;skin;telemedicine;psoriasis-patient doctor-dialogue;psoriasis pathology;doctors;traditional medical communication methods;language;disease status;augmented reality;accurate psoriasis maps;reliable psoriasis maps;pilot testing;Face;Diseases;Mobile handsets;Skin;Pathology;Lesions;Augmented-reality,-psoriasis,-augmented-faces.},
	Month = {Oct},
	Pages = {160-164},
	Title = {Augmenting a Psoriasis-patient Doctor-dialogue through Intergrating Real Face and Maps of Psoriasis Pathology},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-56}}

@inproceedings{8951935,
	Abstract = {When controlling robots, users often face the issue of an operating area that is occluded by an element in the environment or the robot's body. To gain an unobstructed view of the scene, users have to either adjust the pose of the robot or their own viewpoint. This presents a problem, especially for users who rely on assistive robots as they can't easily change their point of view. We introduce InvisibleRobot, a diminished reality-based approach that overlays background information onto the robot in the user's view through an Optical See-Through Head-Mounted Display. We consider two visualization modes for InvisibleRobot: removing the robot body from the user's view entirely, or removing the interior of the robot while maintaining its outline. In a preliminary user study, we compare InvisibleRobot with traditional robot manipulation under different occlusion conditions. Our results suggest that InvisibleRobot can support manipulation in occluded conditions and could be an efficient method to simplify control in assistive robotics.},
	Author = {A. {Plopski} and A. V. {Taylor} and E. J. {Carter} and H. {Admoni}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-55},
	Keywords = {data visualisation;helmet mounted displays;image reconstruction;manipulators;position control;robot vision;telerobotics;virtual reality;InvisibleRobot;robot control;operating area;unobstructed view;assistive robots;diminished reality-based approach;head-mounted display;robot body;robot manipulation;assistive robotics;robo pose adjustment;background information;optical see-through head-mounted display;visualization modes;occlusion conditions;occluded conditions;Robot kinematics;Visualization;Manipulators;Mobile robots;Augmented reality;Head-mounted displays;Diminished Reality;Robot Manipulation;Optical See Through Head-Mounted Display},
	Month = {Oct},
	Pages = {165-166},
	Title = {InvisibleRobot: Facilitating Robot Manipulation Through Diminished Reality},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-55}}

@inproceedings{8951985,
	Abstract = {We present an augmented reality (AR) system for drones to improve people's comfort by camouflaging the appearance of the drones via optical see-through AR visualization. Given the growing social use of autonomous robots such as self-driving vehicles and delivery drones, a comfortable coexistence of human and robot is becoming an increasingly important topic. In this work, we explore how AR technology could improve human-robot comfort. Specifically, we demonstrated an AR system and conducted a user study (N=14) on a proof-of-concept AR system consisting of a HoloLens, OptiTrack, and a toy drone. While showing AR avatars with seemingly positive or negative appearances, we collected both objective and subjective measures of the subjects' comfort. Our results show that subjects were more comfortable when the drone was camouflaged with a positive avatar and less comfortable for the negative avatar.},
	Author = {A. {Mori} and Y. {Itoh}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-54},
	Keywords = {augmented reality;autonomous aerial vehicles;avatars;data visualisation;human factors;human-robot interaction;mobile robots;augmented reality system;autonomous robots;delivery drones;human-robot comfort;AR system;toy drone;negative appearances;positive appearances;human-drone comfort;DroneCamo;optical see-through AR visualization;self-driving vehicles;HoloLens;OptiTrack;AR avatars;positive avatar;negative avatar;Drones;Avatars;Augmented reality;Autonomous robots;Visualization;Current measurement;Computingmethodologies;Computergraphics;Graphics systems and interfaces;Mixed / augmented reality},
	Month = {Oct},
	Pages = {167-168},
	Title = {DroneCamo: Modifying Human-Drone Comfort via Augmented Reality},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-54}}

@inproceedings{8951929,
	Abstract = {In recent years, the availability of affordable mobile Virtual Reality (VR) viewers has resulted in strong interests to incorporate Immersive Virtual Reality (IVR) within classrooms. However, studies on the effect of IVR on primary schoolers' learning are few, and they have often used equipment and settings far removed from everyday classroom instruction. We explored the role of interactivity and immersion in learning in a primary school classroom with 36 children aged 11-13 years, using commercially available devices that are ready-to-scale. We co-created content with different levels of immersion and interactivity together with teachers and investigated student engagement and learning. We present and discuss the use of multiple data sources (performance, physiological responses, observations and self-report) in a real-world classroom evaluation.},
	Author = {Y. {Chua} and P. K. {Sridhar} and H. {Zhang} and V. {Dissanayake} and S. {Nanayakkara}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-53},
	Keywords = {computer aided instruction;mobile learning;virtual reality;primary schoolers;everyday classroom instruction;primary school classroom;student engagement;learning;real-world classroom evaluation;IVR;mobile virtual reality viewers;immersive virtual reality;time 11.0 year to 13.0 year;Bridges;Rivers;Physiology;History;Google;Virtual reality;Tablet computers;Virtual Reality, Interactivity, Immersion, Education, K 12},
	Month = {Oct},
	Pages = {169-174},
	Title = {Evaluating IVR in Primary School Classrooms},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-53}}

@inproceedings{8951937,
	Abstract = {We present the 3D User Interaction Toolkit (3DUITK), an open-source toolkit consisting of 25 Virtual Reality (VR) interaction techniques proposed from thirty years of prior research. Although interaction has been a topic heavily explored in previous papers, much of the research is yet to be re-implemented into an easy to use toolkit. Existing open-source VR interaction solutions provide basic functionality with few available techniques to choose from, and there has yet to be an open-source immersive user interaction toolkit to date. In this paper, we provide an overview of 3DUITK which includes the features, related works, design decisions, example applications, limitations, and future directions. While the focus of this work is on the VR context, 3DUITK has been employed in spatial augmented reality and mobile augmented reality applications.},
	Author = {K. {May} and I. {Hanan} and A. {Cunningham} and B. {Thomas}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-52},
	Keywords = {augmented reality;user interfaces;virtual reality;opensource Toolkit;three-dimensional Interaction research;3DUITK;open-source toolkit;virtual reality interaction techniques;open-source VR interaction solutions;open-source immersive user interaction toolkit;3D user interaction toolkit;spatial augmented reality;mobile augmented reality application;Three-dimensional displays;Task analysis;Open source software;Hardware;Games;Grasping;Australia;3D User Interfaces;Interaction Technique;Toolkit;Virtual Reality;Augmented Reality;Human Computer Interaction;HTC Vive;Oculus Rift;Unity},
	Month = {Oct},
	Pages = {175-180},
	Title = {3DUITK: An Opensource Toolkit for Thirty Years of Three-Dimensional Interaction Research},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-52}}

@inproceedings{8951784,
	Abstract = {Virtual trainings are probably the most popular application of virtual reality (VR) in industry. Such applications are usually hand crafted by computer graphics experts, so generating new trainings is timeintensive and expensive. Even when a training application has been built, any subsequent changes require the inclusion of Software Developers and 3D modeling experts. In this paper, we propose a solution that separates these concerns and makes the generation of AR or VR/AR Training applications easier and cheaper. Our approach is based on a Business Process Model (BPM) of the task to be trained. For technical experts it is common to use such process models to describe tasks. Numerous BPM tools are available, and often models of tasks to be trained are already available. We thus present a compiler that directly translates such process models and a corresponding CAD Model of a machine to a ready-to-use VR/AR machine Training application. By feeding different process models to the compiler, different VR/AR Training applications can be generated quickly, without programming or 3D modeling expertise. We also show how knowledge about the possible interactions is exploitable within a particular training to optimize rendering and simulation performance, which allows us to generate VR/AR Training applications even for machines of high geometric complexity.},
	Author = {L. {Thies} and C. {Strohmeyer} and J. {Ebert} and M. {Stamminger} and F. {Bauer}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-51},
	Keywords = {business data processing;CAD;computer based training;rendering (computer graphics);virtual reality;CAD Model;3D modeling expertise;compiler;BPM tools;business process model;3D modeling experts;software developers;training application;computer graphics experts;virtual reality;virtual trainings;Training;Solid modeling;Task analysis;Tools;Software;Games;Computing methodologies;Computer graphics;Graphics systems and interfaces;Virtual reality;Human centered computing;Human computer interaction (HCI);Interactive systems and tools},
	Month = {Oct},
	Pages = {181-186},
	Title = {Compiling VR/AR Trainings from Business Process Models},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-51}}

@inproceedings{8951953,
	Abstract = {Superquadrics are one of the ideal shape representations for adapting various kinds of primitive shapes with a single equation. This paper revisits the task of representing a 3D human body with multiple superquadrics. As a single superquadric surface can only represent symmetric primitive shapes, we present a method that segments the human body into body parts to estimate their superquadric parameters. Moreover, we propose a novel initial parameter estimation method by using 3D skeleton joints. The results show that superquadric parameters are estimated, which represent human body parts volumetrically.},
	Author = {R. {Hachiuma} and H. {Saito}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-50},
	Keywords = {image representation;parameter estimation;symmetric primitive shapes;single superquadric surface;ideal shape representations;volumetric representation;human body parts;superquadric parameters;3D skeleton joints;initial parameter estimation method;Three-dimensional displays;Shape;Skeleton;Image segmentation;Parameter estimation;Semantics;Minimization;Superquadrics;Semantic segmentation},
	Month = {Oct},
	Pages = {187-188},
	Title = {Volumetric Representation of Human Body Parts Using Superquadrics},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-50}}

@inproceedings{8951958,
	Abstract = {Visual coherence is a basic requirement for providing a realistic scene in Augmented Reality. As one of the most important factors, illumination consistency is usually achieved through two steps: real lighting estimation and virtual object rendering. In this paper, we propose an end-to-end method to adjust the luminance and shadow of the virtual object in the composite image using a deep neural network. It learns the varying appearances of the virtual object under different illumination and viewpoints from a large amount of synthesized images, and transforms the illumination of the composite image into coherence according to the appearance of the real world. The effectiveness of our method is demonstrated through extensive experiments.},
	Author = {X. {Wang} and K. {Wang} and S. {Lian}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-49},
	Keywords = {augmented reality;feature extraction;image enhancement;image reconstruction;lighting;neural nets;rendering (computer graphics);deep neural network;synthesized images;consistent illumination;augmented reality;visual coherence;illumination consistency;virtual object rendering;virtual object luminance;feature extractor;Lighting;Cameras;Feature extraction;Probes;Training data;Generators;Light sources;Mixed/augmented Reality;Neural networks},
	Month = {Oct},
	Pages = {189-194},
	Title = {Deep Consistent Illumination in Augmented Reality},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-49}}

@inproceedings{8951988,
	Abstract = {CAVE system gives us access to interact with an immersive virtual environment in the first-person. However, several crucial obstacles have hindered its application in practical engineering, and one of which is the full body motion tracking. This paper addresses the problem of full body motion tracking in a CAVE environment and proposes a novel hybrid tracking system which integrates both the optical and inertial tracking system to overcome the occlusion and interference problem. The proposed approach is evaluated by comparing with the most advanced Xsens full body motion tracking suit in a real CAVE environment, which proves its great improvement of stability and precision. Furthermore, a real case is tested as well to show its applicability in practical engineering applications.},
	Author = {Y. {Lyu} and S. {Xu} and W. {Fang} and C. {Wu} and T. {Cheng}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-48},
	Keywords = {interactive systems;virtual reality;first-person interaction;immersive CAVE environment;CAVE system;immersive virtual environment;body motion tracking;hybrid tracking system;optical tracking system;inertial tracking system;occlusion;interference problem;practical engineering applications;Xsens full body motion;First-person interaction, optical-inertial fusion, immersive environment},
	Month = {Oct},
	Pages = {195-199},
	Title = {Improving Hybrid Tracking System for First-Person Interaction in Immersive CAVE Environment},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-48}}

@inproceedings{8951785,
	Abstract = {This paper presents a generative adversarial learning-based human upper body video synthesis approach to generate an upper body video of target person that is consistent with the body motion, face expression, and pose of the person in source video. We use upper body keypoints, facial action units and poses as intermediate representations between source video and target video. Instead of directly transferring the source video to the target video, we firstly map the source person's facial action units and poses into the target person's facial landmarks, then combine the normalized upper body keypoints and generated facial landmarks with spatio-temporal smoothing to generate the corresponding target video's image. Experimental results demonstrated the effectiveness of our method.},
	Author = {Z. {Liu} and H. {Hu} and Z. {Wang} and K. {Wang} and J. {Bai} and S. {Lian}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-47},
	Keywords = {face recognition;image motion analysis;learning (artificial intelligence);neural nets;pose estimation;smoothing methods;video signal processing;realistic face;generative adversarial learning;target person;body motion;face expression;source video;normalized upper body keypoints;target video;human upper body video synthesis;source person facial action;spatio-temporal smoothing;Face;Computer vision;Training;Pattern recognition;Gallium nitride;Pipelines;Conferences;video synthesisi;body reenactment;face reenactment;GAN},
	Month = {Oct},
	Pages = {200-202},
	Title = {Video Synthesis of Human Upper Body with Realistic Face},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-47}}

@inproceedings{8951904,
	Abstract = {Image inpainting aims to restore texture of missing regions in scene from an RGB image. In this paper, we aim to restore not only the texture but also the geometry of the missing regions in scene from a pair of RGB and depth images. Inspired by the recent development of generative adversarial network, we employ an encoder-decoderbased generative adversarial network with the input of RGB and depth image. The experimental results show that our method restores the missing region of both RGB and depth image.},
	Author = {R. {Fujii} and R. {Hachiuma} and H. {Saito}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-46},
	Keywords = {decoding;image coding;image colour analysis;image fusion;image restoration;image texture;depth image inpainting;RGB image fusion approach;image restoration;image texture;encoder-decoder based generative adversarial network;Image restoration;Geometry;Generative adversarial networks;Task analysis;Cameras;Three-dimensional displays;Convolution;Artificial-intelligence;Computer-vision;Computer-vision-tasks;Scene-understanding;Computer-graphics;Graphics-systems-and-interfaces;Mixed-/-augmented-reality},
	Month = {Oct},
	Pages = {203-204},
	Title = {Joint Inpainting of RGB and Depth Images by Generative Adversarial Network with a Late Fusion Approach},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-46}}

@inproceedings{8951943,
	Abstract = {We present a novel strategy for image-level interaction that is applicable to a single image without any prior structural knowledge, such as object status or reconstructed 3D models. By training sets of input image and interaction pairs using a target image, our model can generate result images by applying the desired interaction to new unseen images. The proposed method is differentiated from previous approaches for changing poses, which requires absolute statuses for training images or estimated 3D model with reconstruction errors. Based on the conceptual analysis of encoder-decoder networks, we propose a novel generator network architecture containing a feature converter network, which is suitable for applying interactions to images. We also implement a discriminator network for training, which is a well-known technique for generative adversarial networks. Experimental results demonstrate that the proposed method successfully generates result images with applied interactions without any prior knowledge. We expect that our method will provide insights into novel interaction schemes for augmented reality by reflecting interactions onto real scenes and providing more realistic user experiences.},
	Author = {M. {Son} and H. S. {Chang}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-45},
	Keywords = {augmented reality;human computer interaction;image reconstruction;image segmentation;learning (artificial intelligence);neural nets;solid modelling;user experience;training images;encoder-decoder networks;feature converter network;discriminator network;generative adversarial networks;image-level interaction;InteractionGAN;generator network architecture;3D model estimation;user experiences;augmented reality;Training;Three-dimensional displays;Decoding;Feature extraction;Network architecture;Generators;Image reconstruction;image level interaction;convolutional neural networks;encoder decoder structure;generative adversarial networks;interaction manifold},
	Month = {Oct},
	Pages = {205-210},
	Title = {InteractionGAN: Image-Level Interaction using Generative Adversarial Networks},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-45}}

@inproceedings{8951924,
	Abstract = {With the recent developments of Mixed Reality (MR) devices and advances in 3D scene understanding, MR applications on mobile devices are becoming available to a large part of the society. These applications allow users to mix virtual content into the surrounding environment. However the ability to mediate (i.e., modify or alter) the surrounding environment remains a difficult and unsolved problem that limits the degree of immersion of current MR applications on mobile devices. In this paper, we present a method to mediate 2D views of a real environment using a single consumer-grade RGB-D camera and without the need of pre-scanning the scene. Our proposed method creates in real-time a dense and detailed keyframe-based 3D map of the real scene and takes advantage of a semantic instance segmentation to isolate target objects. We show that our proposed method allows to remove target objects in the environment and to replace them by their virtual counterpart, which are built on-the-fly. Such an approach is well suited for creating mobile Mediated Reality applications.},
	Author = {Y. {Xue} and D. {Thomas} and F. {Rayar} and H. {Uchiyama} and R. {Taniguchi} and B. {Yin}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-44},
	Keywords = {cameras;image colour analysis;image segmentation;mobile computing;virtual reality;target object removal;semantic instance segmentation;virtual content;3D scene understanding;mixed reality devices;mobile mediated reality applications;dense keyframe-based 3D map;single consumer-grade RGB-D camera;mediate 2D views;mobile devices;MR applications;blended-keyframes;Three-dimensional displays;Cameras;Image reconstruction;Semantics;Image segmentation;Virtual reality;Solid modeling;Human centered computing, Human computer interaction (HCI), Interaction paradigms, Mixed / augmented reality},
	Month = {Oct},
	Pages = {211-216},
	Title = {Blended-Keyframes for Mobile Mediated Reality Applications},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-44}}

@inproceedings{8951964,
	Abstract = {Previous studies have suggested that different types of human-computer interactions (HCI) have different impact on use's emotional responses in virtual environment. In this study, we investigat the effects of two different types of HCI, including controller-based interaction and speech-based interaction, on user's emotion in a virtual counseling environment. To achieve this end, we first develop a virtual counseling environment which enable each participant acted as a counselor to help a depressed virtual woman. We then assess the emotional intensity of each participant after they experienced 20-30 minutes' consultation in either controller-based or speech-based interaction condition. Our results show that, although there is no significant difference in emotional intensity between two different interaction conditions, participants in speech-based condition show relatively stronger emotion than these in controller-based condition.},
	Author = {Z. {Tu} and D. {Weng} and D. {Cheng} and R. {Shen} and H. {Fang} and Y. {Bao}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-43},
	Keywords = {biomedical education;computer based training;emotion recognition;human computer interaction;interactive systems;medical computing;psychology;speech recognition;virtual reality;human-computer interactions;training apparatus;user emotional responses;controller-based condition;speech-based condition;interaction conditions;emotional intensity;depressed virtual woman;speech-based interaction;controller-based interaction;virtual environment;HCI;virtual counseling environment;time 20.0 min to 30.0 min;Employee welfare;Speech recognition;Training;Human computer interaction;Resists;Emotional responses;Virtual environments;Virtual reality;Sound based input;User studies},
	Month = {Oct},
	Pages = {217-221},
	Title = {The Effect of Two Different Types of Human-Computer Interactions on User's Emotion in Virtual Counseling Environment},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-43}}

@inproceedings{8951930,
	Abstract = {Neural network machine learning approaches are widely used for object classification or detection problems with significant success. A similar problem with specific constraints and challenges is object state estimation, dealing with objects that consist of several removable or adjustable parts. A system that can detect the current state of such objects from camera images can be of great importance for Augmented Reality (AR) or robotic assembly and maintenance applications. In this work, we present a CNN that is able to detect and regress the pose of an object in multiple states. We then show how the output of this network can be used in an automatically generated AR scenario that provides step-by-step guidance to the user in assembling an object consisting of multiple components.},
	Author = {Y. {Su} and J. {Rambach} and N. {Minaskan} and P. {Lesur} and A. {Pagani} and D. {Stricker}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-42},
	Keywords = {augmented reality;convolutional neural nets;image classification;learning (artificial intelligence);object detection;pose estimation;automatically generated AR scenario;CNN;object detection problems;deep multistate object pose estimation;augmented reality assembly;object classification;neural network machine learning approaches;multiple states;maintenance applications;robotic assembly;object state estimation;Feature extraction;Pose estimation;Augmented reality;Task analysis;Neural networks;Maintenance engineering;Solid modeling;Neural Networks;Computer Vision;Mixed/ Augmented Reality},
	Month = {Oct},
	Pages = {222-227},
	Title = {Deep Multi-state Object Pose Estimation for Augmented Reality Assembly},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-42}}

@inproceedings{8951989,
	Abstract = {Hand gesture recognition is a challenging problem for natural human-computer interaction(HCI). We address this problem by introducing a real-time human-mobile interaction interface with a depth sensor. Our interface consists of two components, 3D hand pose estimation and hand skeleton state based gesture description. Firstly, we propose a 3D hand pose estimation method that combines learning based pose initialization and physical based model fitting, which can estimate the per-frame's hand pose that appears in the depth camera's field of view. Afterwards, we map the estimated pose to gesture, e.g. open or close, through a hand skeleton state based method. With the tracked hand gesture, we can stably and smoothly implement common operations such as 'Touch', 'Grasp' and 'Hold' with mid-air interface. Our main contribution is combine 3D hand pose estimation and hand gesture tracking, and implementing an interaction application system with the details.},
	Author = {Y. {Che} and Y. {Qi} and Y. {Song}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-41},
	Keywords = {computer vision;gesture recognition;human computer interaction;learning (artificial intelligence);mobile computing;pose estimation;real-time systems;stereo image processing;hand gesture recognition;real-time human-mobile interaction interface;hand skeleton state based gesture description;hand gesture tracking;interaction application system;real-time 3D hand gesture;3D hand pose estimation;learning based pose initialization;physical based model fitting;computer vision;3D Hand pose estimation;Hand gesture recognition;human-mobile interaction;Augmented Reality;Interaction interface},
	Month = {Oct},
	Pages = {228-232},
	Title = {Real-Time 3D Hand Gesture Based Mobile Interaction Interface},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-41}}

@inproceedings{8951918,
	Abstract = {This paper presents a novel framework to generate realistic face video of an anchor, who is reading certain news. This task is also known as Virtual Anchor. Given some paragraphs of words, we first utilize a pretrained Word2Vec model to embed each word into a vector; then we utilize a Seq2Seq-based model to translate these word embeddings into action units and head poses of the target anchor; these action units and head poses will be concatenated with facial landmarks as well as the former n synthesized frames, and the concatenation serves as input of a Pix2PixHD-based model to synthesize realistic facial images for the virtual anchor. The experimental results demonstrate our framework is feasible for the synthesis of virtual anchor.},
	Author = {Z. {Wang} and Z. {Liu} and Z. {Chen} and H. {Hu} and S. {Lian}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-40},
	Keywords = {face recognition;neural nets;vectors;virtualisation;realistic face video;Word2Vec model;Seq2Seq-based model;word embeddings;head poses;target anchor;synthesized frames;Pix2PixHD-based model;facial images;neural virtual anchor synthesizer;Seq2Seq models;GAN models;realistic facial images;Face;Task analysis;Computational modeling;Gold;Training;Computer vision;Generative adversarial networks;virtual anchor;Seq2Seq;GAN},
	Month = {Oct},
	Pages = {233-236},
	Title = {A Neural Virtual Anchor Synthesizer Based on Seq2Seq and GAN Models},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-40}}

@inproceedings{8951897,
	Abstract = {The objective of this contribution is to introduce setforge, a set of software tools for synthetic data generation for convolutional neural network (CNN) training. Our focus is on CNNs for 6-degree-of-freedom pose estimation using RGB-D data. To determine the pose of physical objects in 6-degree-of-freedom is an essential task for many augmented reality applications. The recent years have shown the advent of trainable methods such as CNNs and others. However, those approaches require training data. The tools, this paper introduces, allow one to generate training data from 3D models. They come with plenty of features for random data generation and augmentation, adapting colors, hue, and noise. We contribute these tools as open-source software available on Github. A prototype CNN demonstrates how one can utilize it. An augmented reality demo application also shows its real-time pose estimation performance.},
	Author = {So. {Zhang} and C. {Song} and R. {Radkowski}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Date-Modified = {2021-03-12 02:36:17 +0000},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-39},
	Keywords = {augmented reality;convolutional neural nets;feature extraction;image colour analysis;pose estimation;software tools;CNN-based pose estimation;software tools;convolutional neural network training;RGB-D data;random data generation;Setforge;synthetic RGB-D training data generation;augmented reality;data augmentation;open-source software;Rendering (computer graphics);Tools;Image color analysis;Cameras;Training;Training data;Augmented reality;Augmented Reality;6 DoF Pose Estimation;Convolutional Neural Network;Synthetic data;Synthetic data training;open-source software},
	Month = {Oct},
	Pages = {237-242},
	Title = {Setforge - Synthetic RGB-D Training Data Generation to Support CNN-Based Pose Estimation for Augmented Reality},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-39}}

@inproceedings{8951983,
	Abstract = {Color Vision Deficiency (CVD) is often characterized by the inability to distinguish color due to a defective or missing cone in the eye. Although it is possible to modify the observed color to make it easier for users to distinguish, this can lead to color confusion with unaffected colors. To address this problem we investigate how flicker can assist distinguishing colors for CVD patients. In preliminary study, we evaluated the efficiency of color and brightness modulation with 4 participants with normal vision. Our findings suggests that while brightness modulation was ineffective, color modulation can help users distinguish between different colors.},
	Author = {S. {Hasana} and Y. {Fujimoto} and A. {Plopski} and M. {Kanbara} and H. {Kato}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-38},
	Keywords = {colour vision;handicapped aids;image colour analysis;color modulation;normal vision;brightness modulation;CVD patients;color confusion;temporal-domain modulation;color vision deficiency;color discrimination;Image color analysis;Modulation;Brightness;Task analysis;Integrated optics;Optical imaging;Visualization;Color blind, Color Vision Deficiency, CVD, Flicker},
	Month = {Oct},
	Pages = {243-244},
	Title = {Improving Color Discrimination for Color Vision Deficiency (CVD) with Temporal-Domain Modulation},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-38}}

@inproceedings{8951962,
	Abstract = {This paper discusses the design of a wearable display in the form of compact eyeglasses, supporting a fair field of view, correct focus cue, and optical see-through capacity. Based on integral imaging, our proposal comprises a discrete transparent microdisplay array as the image source and a discrete lenslet array as the spatial light modulator, without the need for a pre-imaging system or special prism. We designed an annular aperture array to eliminate stray light, controlled within an imperceptible limit. Through a stray light simulation and an imaging simulation, the system was proved to provide a good image quality for both virtual and real information.},
	Author = {C. {Yao} and Y. {Liu} and D. {Cheng} and Y. {Wang}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-37},
	Keywords = {augmented reality;helmet mounted displays;lenses;microdisplays;optical arrays;optical design techniques;spatial light modulators;stray light;three-dimensional displays;compact light field augmented reality display;eliminated stray light;discrete structures;wearable display;compact eyeglasses;correct focus cue;integral imaging;discrete transparent microdisplay array;image source;discrete lenslet array;spatial light modulator;pre-imaging system;annular aperture array;stray light simulation;imaging simulation;Apertures;Stray light;Microdisplays;Optical imaging;Adaptive optics;Cameras;augmented reality, light-field display, micro-LED display},
	Month = {Oct},
	Pages = {245-250},
	Title = {Compact Light Field Augmented Reality Display with Eliminated Stray Light using Discrete Structures},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-37}}

@inproceedings{8951951,
	Abstract = {Head-mounted-displays (HMDs) provide immersive experiences of virtual content. While being flexible, HMDs could be a hindrance for Virtual Reality (VR) applications such as VR teleconference where facial components and expressions of the user are partially occluded thus cannot be seen by others. We present an automatic face image completion solution that treats the occluded region as a hole and completes the hole with the help of an occlusion-free reference image of the same person. Given the occluded input image and an occlusion-free reference image, our method first computes head pose features from estimated facial landmarks. The head pose features, as well as images, are then fed into a generative adversarial network (GAN) to synthesize the output image. Our method can generate faithful results from various input cases and outperforms other face completion methods. It provides a light-weighted solution to HMD occlusion removal and has the potential to benefit VR applications.},
	Author = {M. {Wang} and X. {Wen} and S. {Hu}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-36},
	Keywords = {face recognition;feature extraction;helmet mounted displays;neural nets;pose estimation;virtual reality;HMD occlusion removal;head-mounted-displays;immersive experiences;virtual content;virtual reality applications;VR teleconference;facial components;occluded region;occlusion-free reference image;occluded input image;VR applications;facial landmarks;automatic face image completion;head pose features;generative adversarial network;Face;Resists;Generative adversarial networks;Generators;Hardware;Gallium nitride;Face Completion;Virtual Reality;Generative Adversarial Networks},
	Month = {Oct},
	Pages = {251-256},
	Title = {Faithful Face Image Completion for HMD Occlusion Removal},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-36}}

@inproceedings{8951986,
	Abstract = {In this paper, a novel Deep HDR Merger network, which is called MergeNet, is proposed to reconstruct a HDR image from a single filtered LDR image. Filtered images are adopted as input since they contain more dynamic range than traditional ones. By learning the correlation between filtered LDR images and HDR images, the MergeNet successfully achieves HDR reconstruction of filtered images. We used five evaluation methods to make qualitative and quantitative comparisons to show that our method produced excellent results. Experimental results show that the proposed method performs favorably against state-of-the-art HDR image reconstruction methods.},
	Author = {B. {Liang} and D. {Weng} and Y. {Bao} and Z. {Tu} and L. {Luo}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-35},
	Keywords = {image filtering;image reconstruction;merging;filtered LDR images;deep HDR merger network;HDR image reconstruction methods;MergeNet;Image reconstruction;Dynamic range;Cameras;Corporate acquisitions;Optical filters;Graphics;Tensors},
	Month = {Oct},
	Pages = {257-258},
	Title = {Reconstructing HDR Image from a Single Filtered LDR Image Base on a Deep HDR Merger Network},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-35}}

@inproceedings{8951967,
	Abstract = {We propose a spatial calibration method for Optical See-Through Head-Mounted Displays (OST-HMDs) having complex optical distortion such as wide field-of-view (FoV) designs. Viewpoint-dependent non-linear optical distortion makes existing spatial calibration methods either impossible to handle or difficult to compensate without intensive computation. To overcome this issue, we propose OSTNet, a non-parametric data-driven calibration method that creates a generative 2D distortion model for a given six-degree-of-freedom viewpoint pose.},
	Author = {K. {Someya} and Y. {Hiroi} and M. {Yamada} and Y. {Itoh}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-34},
	Keywords = {calibration;helmet mounted displays;optical distortion;nonparametric distortion map generation;spatial calibration method;complex optical distortion;field-of-view designs;viewpoint-dependent nonlinear optical distortion;nonparametric data-driven calibration method;generative 2D distortion model;OSTNet;Optical distortion;Calibration;Cameras;Nonlinear distortion;Two dimensional displays;Decoding;optical see through head mounted display;calibration;Variational Autoencoder},
	Month = {Oct},
	Pages = {259-260},
	Title = {OSTNet: Calibration Method for Optical See-Through Head-Mounted Displays via Non-Parametric Distortion Map Generation},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-34}}

@inproceedings{8951787,
	Abstract = {Projector-camera systems are commonly used for projector calibration. Conventional methods usually use stationary cameras and temporal coded structured light (SL). In this paper, we propose a projector calibration method using a mobile camera and spatial coded SL. Our method allows the users to use a handheld camera to carry out projector calibration and therefore reduce the effort and time required for camera setup. Although the decoding of temporal coded SL can be error-prone in a real-world situation, our method can achieve robust calibration results by taking advantage of the multi-view observations of the projection thanks to the mobility of the camera. Experiments show that the result of our method is comparable with that of a checkerboard-based approach.},
	Author = {C. {Xie} and H. {Shishido} and Y. {Kameda} and I. {Kitahara}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-33},
	Keywords = {calibration;cameras;decoding;image coding;optical projectors;handheld camera;projector calibration method;mobile camera;projection mapping system;projector-camera systems;stationary cameras;temporal coded structured light;spatial coded SL;temporal coded SL decoding;checkerboard-based approach;Cameras;Calibration;Diamond;Three-dimensional displays;Feature extraction;Image reconstruction;Surface reconstruction;projector-camera system;projector calibration;projection mapping;augmented reality},
	Month = {Oct},
	Pages = {261-262},
	Title = {A Projector Calibration Method Using a Mobile Camera for Projection Mapping System},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-33}}

@inproceedings{8951916,
	Abstract = {Reliable connectivity is a mandatory requirement for modern augmented reality (AR) glasses. For some applications, current radio frequency-based wireless communication technologies fail to meet sophisticated data rate, latency and interference requirements. This paper presents a proof of concept of using Light-Fidelity (Li-Fi) technology for AR glasses to address these issues. In particular, the paper discusses basic Li-Fi architectures and derives a concept for highspeed, bi-directional Li-Fi connectivity. In order to provide spatially aware content, a network-based localization method is introduced. This approach employs the simple network management protocol (SNMP) and is independent from the Li-Fi transceiver, as long as it has a media access control (MAC) address. The practicability of the developed Li-Fi transceiver is verified by data rate and latency measurements. Data rates of up to 536MBit/s are demonstrated over short distances and 216MBit/s over 5m with a sufficient field of view. The complete setup including our localization algorithm is verified by spatial tracking of the AR glasses. Thereby, we report a login time into the Li-Fi domain of 2.04s on average.},
	Author = {R. {Kirrbach} and M. {Faulwa{\ss}er} and B. {Jakob} and T. {Schneider} and A. {Noack}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-32},
	Keywords = {access protocols;augmented reality;computer network management;free-space optical communication;mobility management (mobile radio);optical transceivers;mandatory requirement;modern augmented reality glasses;interference requirements;light-fidelity technology;AR glasses;bi-directional Li-Fi connectivity;spatially aware content;network-based localization method;network management protocol;media access control address;Li-Fi transceiver;Li-Fi domain;Li-Fi architectures;radio frequency-based wireless communication;size 5.0 m;time 2.04 s;bit rate 536 Mbit/s;bit rate 216 Mbit/s;Augmented-Reality;Li-Fi;Localization;OFDM;P2MP;Optical-wireless-communications;spatially-aware-content},
	Month = {Oct},
	Pages = {263-268},
	Title = {Li-Fi for Augmented Reality Glasses: A Proof of Concept},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-32}}

@inproceedings{8951959,
	Abstract = {This work presents a hand model estimation method designed specifically with augmented reality (AR) glasses and 3D AR interface in mind. The proposed work is capable of estimating the 3D positions of all ten finger from a single depth image. By leveraging a low-dimensional hand model and exploiting hand geometries from an ego-centric view, we build a lightweight algorithm that is accurate, environment agnostic, and runs in real time on mobile hardware. One major consideration in our design for AR is that the user's hand is likely to interact with planar surfaces since they serve as ideal "touchscreens". As a result, our method will not fail to detect the hand even when the hand is in physical contact with a surface such as a table, wall, or even another palm. Our experiment shows using the CVAR database that the accuracy with clear background at 98% and with cluttered background at around 85%.},
	Author = {B. {Zhou} and A. {Yu} and J. {Menke} and A. {Yang}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-31},
	Keywords = {augmented reality;human computer interaction;palmprint recognition;depth images;wearable augmented reality glasses;hand model estimation method;single depth image;low-dimensional hand model;ego-centric view;real-time hand model estimation;hand geometries;3D AR interface;3D position estimation;mobile hardware;planar surfaces;touch screens;CVAR database;Three-dimensional displays;Augmented reality;Cameras;Support vector machines;Hardware;Pipelines;Estimation;Human-centered computing;human computer interaction (HCI);interaction paradigms;Mixed/augmented reality;computing methologies;artificial intelligence;computer vision;computer vision problems},
	Month = {Oct},
	Pages = {269-273},
	Title = {Real-Time Hand Model Estimation from Depth Images for Wearable Augmented Reality Glasses},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-31}}

@inproceedings{8951971,
	Abstract = {Online hand gesture recognition (HGR) techniques are essential in augmented reality (AR) applications for enabling natural human-to-computer interaction and communication. In recent years, the consumer market for low-cost AR devices has been rapidly growing, while the technology maturity in this domain is still limited. Those devices are typical of low prices, limited memory, and resource-constrained computational units, which makes online HGR a challenging problem. To tackle this problem, we propose a lightweight and computationally efficient HGR framework, namely LE-HGR, to enable real-time gesture recognition on embedded devices with low computing power. We also show that the proposed method is of high accuracy and robustness, which is able to reach high-end performance in a variety of complicated interaction environments. To achieve our goal, we first propose a cascaded multi-task convolutional neural network (CNN) to simultaneously predict probabilities of hand detection and regress hand keypoint locations online. We show that, with the proposed cascaded architecture design, false-positive estimates can be largely eliminated. Additionally, an associated mapping approach is introduced to track the hand trace via the predicted locations, which addresses the interference of multi-handedness. Subsequently, we propose a trace sequence neural network (TraceSeqNN) to recognize the hand gesture by exploiting the motion features of the tracked trace. Finally, we provide a variety of experimental results to show that the proposed framework is able to achieve state-of-the-art accuracy with significantly reduced computational cost, which are the key properties for enabling real-time applications in low-cost commercial devices such as mobile devices and AR/VR headsets.},
	Author = {H. {Xie} and J. {Wang} and B. {Shao} and J. {Gu} and M. {Li}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-30},
	Keywords = {augmented reality;convolutional neural nets;gesture recognition;human computer interaction;image colour analysis;object detection;real-time applications;low-cost commercial devices;mobile devices;LE-HGR;online hand gesture recognition techniques;augmented reality;human-to-computer interaction;online HGR;real-time gesture recognition;embedded devices;cascaded multitask convolutional neural network;hand detection;regress hand keypoint locations;cascaded architecture design;hand trace;trace sequence neural network;RGB-based online gesture recognition network;embedded AR devices;low-cost AR devices;Gesture recognition;Detectors;Real-time systems;Shape;Three-dimensional displays;Tracking;Visualization;augmented reality;hand gesture recognition;multi task cascaded network;trace sequence neural network},
	Month = {Oct},
	Pages = {274-279},
	Title = {LE-HGR: A Lightweight and Efficient RGB-Based Online Gesture Recognition Network for Embedded AR Devices},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-30}}

@inproceedings{8951999,
	Abstract = {We propose a virtual reality interaction method based on multitouch screen, which provides a human-computer interaction method for gesture touch control for manipulating 3D virtual objects in the virtual world. The method combines the real touch screen and the touch gestures, and the user can directly interact with the real touch screen like a daily tablet to control objects in the virtual environment. The interaction mode enables the user to obtain the real dynamic feedback information while interacting, and meets the daily operation habits of the user, so the operation of the object with multiple degrees of freedom is more accurate and convenient. Since the virtual object can be operated by translating, rotating, scaling, etc., this interaction can be applied to various display systems, and the user can obtain a more realistic and intuitive feeling through multi-angle observation. In the interactive system we designed, the hands and touch screens in the real world will be mapped in the virtual scene to achieve virtual and real registration, providing visual tracking of the contact between the hand and the touch screen, improving user immersion and realism.},
	Author = {J. {Pan} and D. {Weng} and J. {Mo}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-29},
	Keywords = {human computer interaction;image registration;interactive systems;object tracking;touch sensitive screens;user interfaces;virtual reality;virtual reality interaction method;multitouch screen;human-computer interaction method;gesture touch control;3D virtual object manipulation;virtual world;touch screen;touch gestures;virtual environment;interaction mode;interactive system;touch screens;virtual scene;virtual registration;daily tablet;real dynamic feedback information;multiple degrees of freedom;display systems;multiangle observation;visual tracking;touch-screen,-man-machine-interaction,-virtual-reality},
	Month = {Oct},
	Pages = {280-284},
	Title = {Object Manipulation: Interaction for Virtual Reality on Multi-touch Screen},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-29}}

@inproceedings{8951973,
	Abstract = {Despite recent technological advanced in Augmented Reality (AR), the majority of state-of-the-art head-mounted displays (HMDs) still suffers from a limited field of view. Hence, there is a high probability that virtual objects are located outside of the user's view. Several approaches to mitigate the problem of out-of-view objects have been proposed in the past by using visualization techniques to guide the user's attention towards such objects. However, none of these techniques provides any context about the real environment. Current state-of-the-art HMDs are equipped with various sensors to create a 3D map of the environment. In this work, we leverage such 3D reconstructions and developed a set of different 3D minimap visualizations for conveying information about out-of-view objects in AR. We propose a novel kind of minimap using stereographic fisheye projection and compare it to more traditional, bird's-eye view minimaps. Preliminary results show that our stereographic fisheye minimap offers a set of distinct advantages over bird's-eye view minimaps that facilitate the localization of virtual objects in AR environments.},
	Author = {F. {Bork} and U. {Eck} and N. {Navab}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-28},
	Keywords = {augmented reality;data visualisation;helmet mounted displays;augmented reality;head-mounted displays;visualization techniques;HMDs;3D reconstructions;stereographic fisheye minimap;3D minimap visualizations;out-of-view object visualization;3D map;stereographic fisheye projection;bird-eye view minimaps;AR environments;virtual object localization;Visualization;Three-dimensional displays;Cameras;Augmented reality;Games;Head-mounted displays;Sensors},
	Month = {Oct},
	Pages = {285-286},
	Title = {Birds vs. Fish: Visualizing Out-of-View Objects in Augmented Reality using 3D Minimaps},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-28}}

@inproceedings{8951950,
	Abstract = {Water on road is a potential hazard for road vehicles. In complex traffic, it is difficult for drivers and autonomous navigation systems to safely share the road with other vehicles and pedestrians while detecting and avoiding water puddles and pot holes. Detection of water puddles needs to be both accurate and fast enough for realtime assistance or planning. We present a new fast deep-learning based water detection network that exploits temporal information to achieve an accuracy comparable to the state-of-the-art accuracy while requiring less than half memory and CUDA cores. We also show how the detection results are visualised in 3D for AR/VR applications.},
	Author = {J. {Li} and C. {Nguyen}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-27},
	Keywords = {augmented reality;data visualisation;driver information systems;hazards;learning (artificial intelligence);navigation;parallel architectures;pedestrians;road vehicles;realtime water-hazard detection;advanced driving assistance;potential hazard;road vehicles;autonomous navigation systems;pedestrians;water puddles;pot holes;deep-learning based water detection network;AR/VR applications;Graphics processing units;Three-dimensional displays;Hazards;Cameras;Visualization;Roads;Automation;Water-puddle-detection;Road-hazard-detection;Deep-learning;Temporal-FCN},
	Month = {Oct},
	Pages = {287-288},
	Title = {Realtime Water-Hazard Detection and Visualisation for Autonomous Navigation and Advanced Driving Assistance},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-27}}

@inproceedings{8951954,
	Abstract = {In order to avoid driving distraction caused by traditional buttons or touch screens, gesture recognition technology has begun to be applied to the field of human-car interaction. Online gesture recognition system designed for vehicle needs powerful enough to satisfy the requirements of high classification accuracy, fast response time and low graphics memory consumption. To solve the above challenges, we propose an online gesture recognition algorithm based on RGB camera to identify motion, hand and gesture in sequence. We use the frame difference as a motion detection modality and apply the hand detection neural network to determine whether to activate the gesture classifier. In the gesture classifier, the frame difference is fused with the RGB image at data level based on Efficient Convolutional Network. We combined gesture recognition and Heads-Up Display to create a simulated driving system that allows users control auxiliary information through gestures, which used for usability analysis and user evaluation. For the purpose of finding the gestures that best match the various interactive tasks, we use the entropy weight method to analyze the usability of the gestures in the JESTER dataset and derive seven best gestures. The offline gesture classification accuracy on the JESTER dataset is 95.96% and online recognition algorithm runs on average at 306 fps when there is no motion and 164 fps in the presence of hand. According to the questionnaire results after the subjects used our system, more than 86.25% of the subjects expressed satisfaction with our gesture recognition system.},
	Author = {J. {Wang} and J. {Chen} and Y. {Qiao} and J. {Zhou} and Y. {Wang}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-26},
	Keywords = {cameras;convolutional neural nets;gesture recognition;head-up displays;human computer interaction;image classification;image sensors;JESTER dataset;heads-up display;convolutional network;offline gesture classification;simulated driving system;hand detection neural network;motion detection modality;frame difference;low graphics memory consumption;online gesture recognition system;human-car interaction;gesture recognition technology;driving distraction;HUD based smart driving system;online gesture recognition;derive seven best gestures;Detectors;Gesture recognition;Image segmentation;Streaming media;Training;Task analysis;Real-time systems;Online Gesture Recognition, Frame Difference, Neural Networks, Human Car Interaction, Heads Up Display},
	Month = {Oct},
	Pages = {289-294},
	Title = {Online Gesture Recognition Algorithm Applied to HUD Based Smart Driving System},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-26}}

@inproceedings{8951997,
	Abstract = {For objected detection, the availability of color cues strongly influences detection rates and is even a prerequisite for many methods. However, when training on synthetic CAD data, this information is not available. We therefore present a method for generating a texture-map from image sequences in real-time. The method relies on 6 degree-of-freedom poses and a 3D-model being available. In contrast to previous works this allows interleaving detection and texturing for upgrading the detector on-the-fly. Our evaluation shows that the acquired texture-map significantly improves detection rates using the LINEMOD detector on RGB images only. Additionally, we use the texture-map to differentiate instances of the same object by surface color.},
	Author = {P. {Rojtberg} and A. {Kuijper}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-25},
	Keywords = {CAD;image colour analysis;image sequences;image texture;object detection;pose estimation;solid modelling;LINEMOD detector;detector on-the-fly;3D-model;6D object instance detection;real-time texturing;surface color;differentiate instances;acquired texture-map;degree-of-freedom poses;image sequences;synthetic CAD data;RGB images;texturing;rendering},
	Month = {Oct},
	Pages = {295-300},
	Title = {Real-Time Texturing for 6D Object Instance Detection from RGB Images},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-25}}

@inproceedings{8951931,
	Abstract = {Virtual simulation is very important for engineering application. Traditional simulation based on DELMIA platform has some shortcomings, such as limited capability of detailed observation, low precision and time-consuming. VR system simulation can effectively solve these problems. However, in engineering applications, the large model simulation in VR system cannot meet the real-time requirements, and collision detection is often slow or even collapsed. To solve this problem, this paper proposes a dual-model approach for engineering collision detection in the CAVE environment: a display model and a collision detection model are established respectively. The display model contains a large model, and the collision detection model contains a manikin and the hidden large model. For visual rendering, the display model uses caching strategy, and the collision detection model uses real-time strategy. The two sets of models are combined and presented in CAVE. Because the dual-model approach separates the visual rendering of the large model from the collision detection calculation, the large model in the collision detection model can be lightweight, or the area of interest can be selected for the collision detection calculation. Experiments show that the dual-model approach can effectively guarantee the real-time performance of visual display and collision detection, and provides a complete solution for engineering collision detection using the CAVE system, which can be applied in the fields of industrial virtual maintenance, assembly, and pre-design evaluation.},
	Author = {Y. {Xue} and S. {Xu} and L. {Wang} and C. {Dai} and Y. {Wu}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-24},
	Keywords = {rendering (computer graphics);virtual reality;dual-model approach;engineering collision detection;display model;CAVE environment;virtual simulation;DELMIA platform;VR system simulation;visual rendering;Collision avoidance;Solid modeling;Real-time systems;Rendering (computer graphics);Visualization;Computational modeling;Load modeling;CAVE;dual-model;collision detection;virtual reality},
	Month = {Oct},
	Pages = {301-305},
	Title = {Dual-Model Approach for Engineering Collision Detection in the CAVE Environment},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-24}}

@inproceedings{8951948,
	Abstract = {Barrier detection and tracking is a key technique in augmented reality(AR). By adding up the sense of objects, users are able to safely observe or avoid moving objects in the view. With the development of 3D Lidar technology, the acquisition of 3D points in the scene is getting more efficient. In comparison with image data, the 3D points from Lidar contain reliable depth information in a large range. However, processing unstructured point cloud is less efficient for augmented reality applications. By noticing the structure of the 3D points captured from Lidar, we propose to parameterize the data from Lidar. In this way, we are able to reuse detection and tracking methods from images for a simple solution in barrier detection and tracking from Lidar data. We test our method by using the Lidar data captured from an unmanned ship. The results show that our method can quickly detect the barriers with bounding boxes, indicating the distance, direction and size of the barrier in the scene.},
	Author = {W. {Xing} and L. {Zhu} and A. {Song}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-23},
	Keywords = {augmented reality;image motion analysis;object detection;object tracking;optical radar;unstructured point cloud;augmented reality applications;barrier detection;parameterized Lidar data;image data;reliable depth information;barrier tracking methods;3D point acquisition;3D Lidar technology;unstructured point cloud processing;unmanned ship;bounding boxes;Three-dimensional displays;Laser radar;Augmented reality;Cameras;Two dimensional displays;Reliability;Target tracking;Barrier Detection;Lidar;Tracking},
	Month = {Oct},
	Pages = {306-310},
	Title = {Barrier Detection and Tracking from Parameterized Lidar Data},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-23}}

@inproceedings{8951946,
	Abstract = {Accurate indoor positioning has attracted a lot of attention for a variety of indoor location-based applications, with the rapid development of mobile devices and their onboard sensors. A hybrid indoor localization method is proposed based on single off-the-shelf smartphone, which takes advantage of its various onboard sensors, including camera, gyroscope and accelerometer. The proposed approach integrates three components: visual-inertial odometry (VIO), point-based area mapping, and plane-based area mapping. A simplified RANSAC strategy is employed in plane matching for the sake of processing time. Since Apple's augmented reality platform ARKit has many powerful high-level APIs on world tracking, plane detection and 3D modeling, a practical smartphone app for indoor localization is developed on an iPhone that can run ARKit. Experimental results demonstrate that our plane-based method can achieve an accuracy of about 0.3 meter, which is based on a much more lightweight model, but achieves more accurate results than the point-based model by directly using ARKit's area mapping. The size of the plane-based model is less than 2KB for a closed-loop corridor area of about 45m*15m, comparing to about 10MB of the point-based model.},
	Author = {L. {Chen} and Y. {Zou} and Y. {Chang} and J. {Liu} and B. {Lin} and Z. {Zhu}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-22},
	Keywords = {augmented reality;indoor navigation;location based services;mobile computing;sensor fusion;smart phones;solid modelling;point-based area mapping;plane-based area mapping;RANSAC strategy;plane matching;Apple's augmented reality platform ARKit;smartphone app;smartphone-based indoor localization;indoor positioning;mobile devices;multilevel scene modeling;multilevel scene matching;camera;gyroscope;accelerometer;visual-inertial odometry;iPhone;smartphone sensors;Sensors;Three-dimensional displays;Indoor environment;Visualization;Computational modeling;Solid modeling;Indoor localization, lightweight model, multi-level mapping, region segmentation, plane matching},
	Month = {Oct},
	Pages = {311-316},
	Title = {Multi-level Scene Modeling and Matching for Smartphone-Based Indoor Localization},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-22}}

@inproceedings{8951976,
	Abstract = {This paper presents a novel method of reconstructing indoor scenes, both structures and objects, by a single panorama photo. The method combines room structure estimation, furniture detection, models selection, as well as 3D positions reasoning. Compare with others, our preliminary results show this method could get almost the same performance with a simpler procedure.},
	Author = {C. {Luo} and B. {Zou} and X. {Lyu} and H. {Xie}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-21},
	Keywords = {CAD;image reconstruction;solid modelling;panorama images;CAD models;room structure estimation;furniture detection;model selection;indoor scene reconstruction;3D position reasoning;Solid modeling;Image reconstruction;Three-dimensional displays;Object detection;Image segmentation;Cognition;Computational modeling;Indoor scene reconstruction;CAD model;panorama images},
	Month = {Oct},
	Pages = {317-320},
	Title = {Indoor Scene Reconstruction: From Panorama Images to CAD Models},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-21}}

@inproceedings{8951934,
	Abstract = {Large-scale 3D scene has an important application prospect in virtual reality and augmented reality. However, due to the doubling of the amount of data in the 3D reconstruction of large-scale outdoor scene, the index of time becomes a great challenge under the condition of maintaining a certain degree of accuracy. In this paper, we provide a set of methods of aerial photography data acquisition and cluster-based 3D model reconstruction for large-scale scene. Firstly, for the data acquisition end, we adopt two strategies of track pre-planning and feedback-based trajectory planning to meet the requirements of efficient data acquisition. Secondly, we have designed and implemented a distributed 3D reconstruction system to process a large number of aerial photographs, which can quickly and robustly reconstruct large-scale 3D models. Finally, we simulate the crowd behavior based on the PEM model in the reconstructed 3D scene, which has a useful guiding significance for people's daily activities and emergency problems.},
	Author = {Y. {Li} and Y. {Xie} and X. {Wang} and X. {Luo} and Y. {Qi}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-20},
	Keywords = {augmented reality;data acquisition;distributed processing;image reconstruction;pattern clustering;photography;solid modelling;stereo image processing;aerial photography data acquisition;distributed 3D reconstruction system;virtual reality;augmented reality;cluster-based 3D model reconstruction;feedback-based trajectory planning;Three-dimensional displays;Image reconstruction;Solid modeling;Computational modeling;Data models;Data acquisition;Drones;Large-Scale;Distributed modeling;Path planning;Crowd simulation},
	Month = {Oct},
	Pages = {321-325},
	Title = {A Fast Method for Large-Scale Scene Data Acquisition and 3D Reconstruction},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-20}}

@inproceedings{8952003,
	Abstract = {We present an indoor RGB-D SLAM optimization algorithm capable of reducing poses drift based on plane geometrical constraints and reconstructing plane structural model incrementally. Our approach extracts planes from keyframes in backend, merges over-segmented planes, establishes observation constraints between keyframes and global landmark planes, and optimizes poses of keyframes and global landmark planes in a general framework for graph optimization (g2o). Moreover, in order to prevent structural constraints between global landmark planes from being destroyed in optimization process, plane angle structural constraints between global landmark planes observed by the same keyframe are added into optimization graph. We test our optimization algorithm on standard RGB-D benchmarks containing rich plane features, demonstrating that our approach can reduce poses drift and the reconstructed plane structural model covers the most part of planar regions of environment. Furthermore, the application feasibility of augmented reality (AR) is tested using reconstructed plane structural model, demonstrating that plane structural model reconstructed by our approach is suitable for AR application.},
	Author = {N. {Huang} and J. {Chen} and Y. {Miao}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-19},
	Keywords = {augmented reality;feature extraction;graph theory;image reconstruction;optimisation;pose estimation;SLAM (robots);augmented reality;observation constraints;over-segmented planes;plane geometrical constraints;indoor RGB-D SLAM optimization algorithm;reconstructed plane structural model;rich plane features;plane angle structural constraints;optimization process;graph optimization;global landmark planes;RGB-D SLAM;plane geometrical constraints;optimization;plane structural model;augmented reality},
	Month = {Oct},
	Pages = {326-331},
	Title = {Optimization for RGB-D SLAM Based on Plane Geometrical Constraint},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-19}}

@inproceedings{8951914,
	Abstract = {We propose a finite element method (FEM) based approach for surface stitching which can be integrated into existing SLAM and NRSfM pipelines for AR applications. Given individual reconstructions and camera poses at different time stamps, our stitching method incrementally completes the surface with a smooth transition between the hidden and the observed parts, so that all the observed parts can be stitched into a single surface. Thanks to the physical modelling, deformations from the observed parts are propagated to the hidden parts enabling an overall high-fidelity and realistic estimate. To keep the computational time in bounds, deformations near the observed parts are computed with FEM, and the remaining region is approximated by Laplacian deformation. We assume that no force is applied to the hidden parts. To evaluate the algorithm, we generate a synthetic dataset with ground truth. In our dataset, the camera observes only a part of the target surface in each frame and moves until the whole target surface is covered. The dataset which will be made publicly available includes the ground truth camera poses and geometries of the whole surface at each time frame. An experimental evaluation of the stitching method with accuracy metrics rounds out the draft.},
	Author = {Y. {Su} and V. {Golyanik} and N. {Minaskan} and S. A. {Ali} and D. {Stricker}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-18},
	Keywords = {cameras;finite element analysis;image motion analysis;image reconstruction;image sequences;pose estimation;robot vision;SLAM (robots);FEM;surface stitching;time stamps;stitching method;observed parts;hidden parts;target surface;shape completion component;nonrigid SLAM;finite element method;SLAM pipelines;NRSfM pipelines;AR applications;high-fidelity estimate;synthetic dataset;ground truth camera;Strain;Surface reconstruction;Cameras;Image reconstruction;Shape;Finite element analysis;Deformable models;non rigid SLAM;FEM;shape completion;monocular surface stitching},
	Month = {Oct},
	Pages = {332-337},
	Title = {A Shape Completion Component for Monocular Non-Rigid SLAM},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-18}}

@inproceedings{8951947,
	Abstract = {Inter-brain connectivity between pairs of people was explored during a finger tracking task in the real-world and in Virtual Reality (VR). This was facilitated by the use of a dual EEG set-up that allowed us to use hyperscanning to simultaneously record the neural activity of both participants. We found that similar levels of inter-brain synchrony can be elicited in the real-world and VR for the same task. This is the first time that hyperscanning has been used to compare brain activity for the same task performed in real and virtual environments.},
	Author = {A. {Barde} and N. {Saffaryazdi} and P. {Withana} and N. {Patel} and P. {Sasikumar} and M. {Billinghurst}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-17},
	Keywords = {electroencephalography;medical signal processing;neurophysiology;virtual reality;hyperscanning;brain activity;virtual environments;inter-brain connectivity;finger tracking task;virtual reality;VR;inter-brain synchrony;dual EEG set-up;neural activity;Task analysis;Electroencephalography;Virtual environments;Collaboration;Brain;Tracking;Data visualization;Hyperscanning, EEG, AR, VR, BCI, Brain Computer Interface},
	Month = {Oct},
	Pages = {338-339},
	Title = {Inter-Brain Connectivity: Comparisons between Real and Virtual Environments using Hyperscanning},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-17}}

@inproceedings{8951923,
	Abstract = {Spatialised auditory and visual cues were delivered via a wearable interface and a Bone Conduction Headset (BCH) to aid a search task. A study was conducted to determine which cues - auditory, visual or a combination - would lead a user to a target in the shortest time with a minimal attention demand. The static visual cue performed the best. The static auditory cue displayed a good level of usability and intuitiveness, especially with no visual cue.},
	Author = {A. {Barde} and M. {Ward} and R. W. {Lindeman} and M. {Billinghurst}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-16},
	Keywords = {augmented reality;bone;handicapped aids;hearing;medical computing;visual perception;static visual cue;static auditory cue;spatialised auditory;bone conduction headset;Visualization;Task analysis;Headphones;Glass;Augmented reality;Acoustics;Bones;Audio augmented reality, auditory cues, visual cues},
	Month = {Oct},
	Pages = {340-341},
	Title = {Less is More: Using Spatialized Auditory and Visual Cues for Target Acquisition in a Real-World Search Task},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-16}}

@inproceedings{8951922,
	Abstract = {We present FragmentFusion, a real-time dense reconstruction pipeline that combines sparse camera tracking with image-space volumetric fusion. The tracking is based on ORB-SLAM, which constructs and optimizes a sparse global map of 3D points and keyframes. We transform each of these keyframes into decimated meshes and render them from the estimated viewpoint. Fusion is performed in the pixel shader by exploiting atomic operations on a packed data structure. This eliminates the need of a 3D voxel grid making FragmentFusion very flexible at large scenes and varying scales. Moreover, since all keyframes are fused on-the-fly, we can use bundle adjustment and loop-closure without expensive volumetric re-integration. FragmentFusion is lightweight in terms of compute power and memory consumption. It can easily fuse several hundreds of keyframes in real time, in a quality comparable to other approaches. We achieve real-time frame rates on a notebook at around 20% CPU and GPU utilization and low memory consumption.},
	Author = {D. {R{\"u}ckert} and M. {Innmann} and M. {Stamminger}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-15},
	Keywords = {cameras;data structures;data visualisation;graphics processing units;image fusion;image reconstruction;mesh generation;power aware computing;rendering (computer graphics);robot vision;SLAM (robots);storage management;light-weight SLAM pipeline;real-time dense reconstruction pipeline;sparse camera tracking;image-space volumetric fusion;ORB-SLAM;3D points;decimated meshes;pixel shader;power consumption;memory consumption;FragmentFusion;3D voxel grid;packed data structure;Three-dimensional displays;Image reconstruction;Cameras;Rendering (computer graphics);Real-time systems;Memory management;Simultaneous localization and mapping;Camera Tracking;Reconstruction;Image Based Rendering;Volumetric Integration},
	Month = {Oct},
	Pages = {342-347},
	Title = {FragmentFusion: A Light-Weight SLAM Pipeline for Dense Reconstruction},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-15}}

@inproceedings{8951913,
	Abstract = {We present a prototype demonstrator that integrates three technologies: mixed reality head mounted displays, wearable biosensors, and mid-air haptic projectors to deliver an interactive tactile experience with a bio-hologram. Users of this prototype are able to see, touch and feel a hologram of a heart that is beating at the same rhythm as their own. The demo uses an Ultrahaptics device, a Magic Leap One Mixed Reality headset, and an Apple Watch that measures the wearer's heart rate, all synchronized and networked together such that updates from the wristband dynamically change the haptic feedback and the animation speed of the beating heart thus creating a more personalised experience.},
	Author = {T. {Romanus} and S. {Frish} and M. {Maksymenko} and W. {Frier} and L. {Corenthy} and O. {Georgiou}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-14},
	Keywords = {augmented reality;biosensors;cardiology;computer-generated holography;haptic interfaces;helmet mounted displays;medical computing;mid-air haptic bio-holograms;mixed reality;wearable biosensors;mid-air haptic projectors;interactive tactile experience;bio-hologram;heart rate;haptic feedback;beating heart;head mounted displays;ultrahaptics device;Magic Leap One mixed reality headset;Apple Watch;Mid-air Haptics;Sensor fusion;3D Hologram;Mixed Reality},
	Month = {Oct},
	Pages = {348-352},
	Title = {Mid-Air Haptic Bio-Holograms in Mixed Reality},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-14}}

@inproceedings{8951992,
	Abstract = {The following topics are dealt with: augmented reality; virtual reality; human computer interaction; helmet mounted displays; pose estimation; solid modelling; image reconstruction; cameras; data visualisation; image colour analysis.},
	Author = {C. {Cao} and J. {Sun}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-13},
	Keywords = {augmented reality;helmet mounted displays;human computer interaction;pose estimation;virtual reality;augmented reality;virtual reality;human computer interaction;helmet mounted displays;pose estimation;solid modelling;image reconstruction;cameras;data visualisation;image colour analysis;Mixed reality;HCI design;Environment reconstruction},
	Month = {Oct},
	Pages = {353-357},
	Title = {Perceptual MR Space: Interactive Toolkit for Efficient Environment Reconstruction in Mobile Mixed Reality},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-13}}

@inproceedings{8951901,
	Abstract = {We address in this paper software architecture design to enable peripheral interactions in Augmented Reality applications. To this end, we rely on SAPIENS, a recently introduced software architecture for engineering peripheral interactions in smart environments, which we reuse to our purpose. Our approach can be readily tested using the online simulator available from the SAPIENS home page.},
	Author = {O. {Schipor} and R. {Vatavu} and W. {Wu}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-12},
	Keywords = {augmented reality;human computer interaction;software architecture;peripheral interaction;augmented reality;software architecture design;SAPIENS software architecture;smart environments;Augmented reality;Software architecture;Task analysis;Computer architecture;Smart glasses;Visualization;Software;Human centered computing;Human computer interaction;Interaction paradigms;Mixed / augmented reality},
	Month = {Oct},
	Pages = {358-359},
	Title = {Integrating Peripheral Interaction Into Augmented Reality Applications},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-12}}

@inproceedings{8951932,
	Abstract = {The six degree-of-freedom (6DoF) pose estimation is an important task in Augmented Reality, especially for initializing or recovering from the failure of 3D tracking for the textureless object, since it still encounters the insufficient accuracy problems because of cluttered backgrounds, occasionally quick movements, and other factors. We propose a simple but effective method to cutout the interested textureless object in a single RGB image with clear contour, which can be employed to directly estimate 6Dof poses with relatively high precision, and then help to remove most of the disturbing edges of clutter background for further refinement of pose estimation. To achieve this task, we propose a novel convolutional neural network, similar to an autoencoder, to reconstruct arbitrary scenes containing the object of interest, and extract the object area. We evaluated our method on objects from the LINEMOD dataset, and the results show that our approach is superior to the baseline as well as some advanced methods.},
	Author = {X. {Liu} and J. {Zhang} and X. {He} and X. {Song} and X. {Qin}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-11},
	Keywords = {augmented reality;clutter;convolutional neural nets;edge detection;image colour analysis;image reconstruction;image texture;object tracking;pose estimation;Augmented Reality;interested textureless object;single RGB image;clutter background;object area;6DoF pose estimation;deep autoencoder;degree-of-freedom;convolutional neural network;LINEMOD dataset;Three-dimensional displays;Pose estimation;Image reconstruction;Solid modeling;Two dimensional displays;Task analysis;Tracking},
	Month = {Oct},
	Pages = {360-365},
	Title = {6DoF Pose Estimation with Object Cutout based on a Deep Autoencoder},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-11}}
