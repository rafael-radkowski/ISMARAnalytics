%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for Rafael Radkowski at 2021-03-13 08:57:59 -0600 


%% Saved with string encoding Unicode (UTF-8) 



@inproceedings{8613745,
	Abstract = {Real-time capable simultaneous localization and mapping (SLAM) approaches applying consumer hardware have been extensively researched in recent years. Their 3D reconstruction typically applies voxel volumes stored in regular grid hierarchies, sparse voxel octrees or voxel hash tables. They represent the model implicitly in the form of a truncated signed distance function (TSDF). Data integration is usually achieved by stepping through the reconstruction hierarchy from top to bottom and checking voxel grids against the new input data or by rasterizing input data to find associated voxels. For hierarchical representations, a major challenge remains the efficient determination of relevant portions of the reconstruction to be modified by new input data. We present a novel approach efficiently rasterizing input point clouds into intermediate volumes by the GPU. Our technique performs a simple preprocessing step on the input data to properly account for the TSDF representation, allowing for an accurate and hole-free reconstruction. We show that our approach is well suited for a fast integration of new input data into the hierarchical 3D reconstruction, allowing for real-time performance while only slightly increasing memory consumption.},
	Author = {C. {Kunert} and T. {Schwandt} and W. {Broll}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2018.00023},
	Issn = {1554-7868},
	Keywords = {graphics processing units;image reconstruction;octrees;pose estimation;rendering (computer graphics);SLAM (robots);solid modelling;virtual reality;input data;accurate hole-free reconstruction;real-time performance;efficient point cloud rasterization;time volumetric integration;mixed reality applications;voxel volumes;regular grid hierarchies;sparse voxel octrees;truncated signed distance function;data integration;reconstruction hierarchy;voxel grids;GPU;TSDF representation;hierarchical 3D reconstruction;memory consumption;SLAM;simultaneous localization and mapping;Three-dimensional displays;Simultaneous localization and mapping;Cameras;Real-time systems;Surface reconstruction;Hardware;Pipelines;Simultaneous localization and mapping;3D reconstruction;Voxelization;real time;volumetric surface integration},
	Month = {Oct},
	Pages = {1-9},
	Title = {Efficient Point Cloud Rasterization for Real Time Volumetric Integration in Mixed Reality Applications},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2018.00023}}

@inproceedings{8613746,
	Abstract = {We present MaskFusion, a real-time, object-aware, semantic and dynamic RGB-D SLAM system that goes beyond traditional systems which output a purely geometric map of a static scene. MaskFusion recognizes, segments and assigns semantic class labels to different objects in the scene, while tracking and reconstructing them even when they move independently from the camera. As an RGB-D camera scans a cluttered scene, image-based instance-level semantic segmentation creates semantic object masks that enable realtime object recognition and the creation of an object-level representation for the world map. Unlike previous recognition-based SLAM systems, MaskFusion does not require known models of the objects it can recognize, and can deal with multiple independent motions. MaskFusion takes full advantage of using instance-level semantic segmentation to enable semantic labels to be fused into an object-aware map, unlike recent semantics enabled SLAM systems that perform voxel-level semantic segmentation. We show augmented-reality applications that demonstrate the unique features of the map output by MaskFusion: instance-aware, semantic and dynamic. Code will be made available.},
	Author = {M. {Runz} and M. {Buffier} and L. {Agapito}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2018.00024},
	Issn = {1554-7868},
	Keywords = {augmented reality;image colour analysis;image fusion;image motion analysis;image recognition;image reconstruction;image representation;image segmentation;object detection;object recognition;object tracking;SLAM (robots);assigns semantic class labels;image-based instance-level semantic segmentation;semantic object masks;object-level representation;multiple independent motions;object-aware map;voxel-level semantic segmentation;multiple moving objects;object-aware RGB-D SLAM system;semantic RGB-D SLAM system;dynamic RGB-D SLAM system;recognition-based SLAM systems;geometric map;object recognition;Semantics;Simultaneous localization and mapping;Real-time systems;Three-dimensional displays;Image reconstruction;Image segmentation;Cameras},
	Month = {Oct},
	Pages = {10-20},
	Title = {MaskFusion: Real-Time Recognition, Tracking and Reconstruction of Multiple Moving Objects},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2018.00024}}

@inproceedings{8613747,
	Abstract = {The use of Optical See-Through (OST) technology for presenting Augmented Reality (AR) experiences is becoming more common. However, OST-AR displays require a calibration procedure, in order to determine the location of the users eyes. Currently, the predominantly cited manual calibration technique is the Single Point Active Alignment Method (SPAAM). However, with the SPAAM technique, there remains uncertainty about the causes of poor calibration results. This paper reports an experiment which examined the influence of two factors on SPAAM accuracy and precision: alignment point distribution, and user posture. Alignment point distribution is examined at user-centered reaching distances, 0.15 to 0.3 meters, as well as environment-centered room-scale distances, 0.5 to 2.0 meters. User posture likely contributes to misalignment error, and is examined at the levels of sitting and standing. In addition, a control condition replaces the user with a rigidly-mounted camera, and mounts the OST display on a precisely-adjustable tripod. The experiment finds that user-centric distributions are more accurate than environment-centric distributions, and, somewhat surprisingly, that the users posture has no effect. The control condition replicates these findings. The implication is that alignment point distribution is the predominant mode for induction of calibration error for SPAAM calibration procedures.},
	Author = {K. R. {Moser} and M. S. {Arefin} and J. E. {Swan}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2018.00025},
	Issn = {1554-7868},
	Keywords = {augmented reality;calibration;cameras;helmet mounted displays;alignment point distance;head-mounted displays;Optical See-Through technology;Augmented Reality experiences;OST-AR displays;calibration procedure;users eyes;predominantly cited manual calibration technique;Single Point Active Alignment Method;SPAAM technique;precision;alignment point distribution;user posture;user-centered reaching distances;environment-centered room-scale distances;OST display;user-centric distributions;environment-centric distributions;calibration error;SPAAM calibration procedures;Calibration;Cameras;Resists;Adaptive optics;Manuals;Optical imaging},
	Month = {Oct},
	Pages = {21-30},
	Title = {Impact of Alignment Point Distance and Posture on SPAAM Calibration of Optical See-Through Head-Mounted Displays},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2018.00025}}

@inproceedings{8613748,
	Abstract = {The choice of poses for camera calibration with planar patterns is only rarely considered - yet the calibration precision heavily depends on it. This work presents a pose selection method that finds a compact and robust set of calibration poses and is suitable for interactive calibration. Consequently, singular poses that would lead to an unreliable solution are avoided explicitly, while poses reducing the uncertainty of the calibration are favoured. For this, we use uncertainty propagation. Our method takes advantage of a self-identifying calibration pattern to track the camera pose in real-time. This allows to iteratively guide the user to the target poses, until the desired quality level is reached. Therefore, only a sparse set of key-frames is needed for calibration. The method is evaluated on separate training and testing sets, as well as on synthetic data. Our approach performs better than comparable solutions while requiring 30% less calibration frames.},
	Author = {P. {Rojtberg} and A. {Kuijper}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2018.00026},
	Issn = {1554-7868},
	Keywords = {calibration;cameras;measurement uncertainty;position measurement;interactive camera calibration;planar patterns;uncertainty propagation;pose selection method;self-identifying calibration pattern;camera pose tracking;Calibration;Cameras;Distortion;Three-dimensional displays;Estimation;Image analysis;Uncertainty;interactive;calibration;vision},
	Month = {Oct},
	Pages = {31-36},
	Title = {Efficient Pose Selection for Interactive Camera Calibration},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2018.00026}}

@inproceedings{8613749,
	Abstract = {The initialization is one of the less reliable pieces of Visual-Inertial SLAM (VI-SLAM) and Odometry (VI-O). The estimation of the initial state (camera poses, IMU states and landmark positions) from the first data readings lacks the accuracy and robustness of other parts of the pipeline, and most algorithms have high failure rates and/or initialization delays up to tens of seconds. Such initialization is critical for AR systems, as the failures and delays of the current approaches can ruin the user experience or mandate impractical guided calibration. In this paper we address the state initialization problem using a monocular-inertial sensor setup, the most common in AR platforms. Our contributions are 1) a general linear formulation to obtain an initialization seed, and 2) a non-linear optimization scheme, including gravity, to refine the seed. Our experimental results, in a public dataset, show that our approach improves the accuracy and robustness of current VI state initialization schemes.},
	Author = {J. {Dom{\'\i}nguez-Conti} and J. {Yin} and Y. {Alami} and J. {Civera}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2018.00027},
	Issn = {1554-7868},
	Keywords = {calibration;mobile robots;optimisation;robot vision;SLAM (robots);VI state initialization schemes;visual-inertial SLAM initialization;nonlinear optimization scheme;initialization seed;monocular-inertial sensor setup;state initialization problem;impractical guided calibration;initialization delays;high failure rates;data readings;landmark positions;IMU states;VI-SLAM;reliable pieces;gravity-observing nonlinear optimization;general linear formulation;Cameras;Feature extraction;Simultaneous localization and mapping;Gravity;Mathematical model;Optimization;Robustness;Visual Inertial SLAM;Visual Inertial Localization;Visual Inertial Mapping;Visual Inertial Initialization;Sensor Fusion},
	Month = {Oct},
	Pages = {37-45},
	Title = {Visual-Inertial SLAM Initialization: A General Linear Formulation and a Gravity-Observing Non-Linear Optimization},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2018.00027}}

@inproceedings{8613750,
	Abstract = {Ubiquitous Augmented Reality requires robust localization in complex daily environments. The combination of camera and Inertial Mersurement Unit (IMU) has shown promising results for robust localization due to the complementary characteristics of the visual and inertial modalities. However, there exists many cases where the measurements from visual and inertial modalities do not provide a single consistent motion estimate thus causing disagreement on the estimated motion. Limited literature has addressed this problem associated with sensor fusion for localization. Since the disagreement is not a result of measurement noises, existing outlier rejection techniques are not suitable to address this problem. In this paper, we propose a novel approach to handle the disagreement as motion conflict with two key components. The first one is a generalized Hidden Markov Model (HMM) that formulates the tracking and management of the primary motion and the secondary motion as a single estimation problem. The second component is an epipolar constrained Deep Neural Network that generates a per-pixel motion conflict probability map. Experimental evaluations demonstrate significant improvement to the tracking accuracy in cases of strong motion conflict compared to previous state-of-the-art algorithms for localization. Moreover, as a consequence of motion tracking on the secondary maps, our solution enables augmentation of virtual content attached to secondary motions, which brings us one step closer to Ubiquitous Augmented Reality.},
	Author = {B. P. {Wisely Babu} and Z. {Yan} and M. {Ye} and L. {Ren}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2018.00028},
	Issn = {1554-7868},
	Keywords = {cameras;feature extraction;hidden Markov models;motion estimation;neural nets;object tracking;probability;sensor fusion;per-pixel motion conflicts;Ubiquitous Augmented Reality;Inertial Mersurement Unit;complementary characteristics;visual modalities;inertial modalities;single consistent motion estimate;measurement noises;outlier rejection techniques;generalized Hidden Markov Model;primary motion;secondary motion;motion conflict probability map;motion tracking;secondary maps;sensor fusion;IMU;hidden Markov model;HMM;epipolar constrained deep neural network;Tracking;Visualization;Dynamics;Simultaneous localization and mapping;Hidden Markov models;Cameras;Visual Inertial Odometry;Deep Neural Network;Camera Pose Tracking;Motion Conflict;Sensor Fusion;Augmented Reality},
	Month = {Oct},
	Pages = {46-56},
	Title = {On Exploiting Per-Pixel Motion Conflicts to Extract Secondary Motions},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2018.00028}}

@inproceedings{8613751,
	Abstract = {We propose a novel method for model-based 3D tracking of hand articulations that is effective even for fastmoving hand postures in depth images. A large number of augmented reality (AR) and virtual reality (VR) studies have used model-based approaches for estimating hand postures and tracking movements. However, these approaches exhibit limitations if the hand moves rapidly or into the camera's field of view. To overcome these problems, researchers attempted a hybrid strategy that uses multiple initializations for 3D tracking of articulations. However, this strategy also exhibits limitations. For example, in genetic optimization, the hypotheses generated from the previous solution may search for a solution in an incorrect search space in a fast-moving hand gesture. This problem also occurs if the search space selected from the results of a trained model does not cover the true solution although the tracked hand moves slowly. Our proposed method estimates the hand pose based on model-based tracking guided by classification and search space adaptation. From the classification by a convolutional neural network (CNN), a data-driven prior is included in the objective function and additional hypotheses are generated in particle swarm optimization (PSO). In addition, the search spaces of the two sets of the hypotheses, generated by the data-driven prior and the previous solution, are adaptively updated using the distribution of each set of the hypotheses. We demonstrated the effectiveness of the proposed method by applying it to an American Sign Language (ASL) dataset consisting of fast-moving hand postures. The experimental results demonstrate that the proposed algorithm exhibits more accurate tracking results compared to other state-of-the-art tracking algorithms.},
	Author = {G. {Park} and W. {Woo}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2018.00029},
	Issn = {1554-7868},
	Keywords = {convolutional neural nets;gesture recognition;image classification;particle swarm optimisation;hand articulations;search space adaptation;model-based 3D tracking;hand postures;virtual reality;tracking movements;hand gesture;trained model;state-of-the-art tracking algorithms;Adaptation models;Tracking;Solid modeling;Three-dimensional displays;Estimation;Optimization;Assistive technology;Computing methodologies;Computer vision problems;Tracking},
	Month = {Oct},
	Pages = {57-69},
	Title = {Hybrid 3D Hand Articulations Tracking Guided by Classification and Search Space Adaptation},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2018.00029}}

@inproceedings{8613752,
	Abstract = {Although the mobility and emerging technology of augmented reality (AR) have brought significant entertainment and convenience in everyday life, the use of AR is becoming a social problem as the accidents caused by a shortage of situation awareness due to an immersion of AR are increasing. In this paper, we address the trade-off between immersion and situation awareness as the fundamental factor of the AR-related accidents. As a solution against the trade-off, we propose a third-party component that prevents pedestrian-vehicle accidents in a traffic environment based on vehicle position estimation (VPE) and vehicle position visualization (VPV). From a RGB image sequence, VPE efficiently estimates the relative 3D position between a user and a car using generated convolutional neural network (CNN) model with a region-of-interest based scheme. VPV shows the estimated car position as a dot using an out-of-view object visualization method to alert the user from possible collisions. The VPE experiment with 16 combinations of parameters showed that the InceptionV3 model, fine-tuned on activated images yields the best performance with a root mean squared error of 0.34 m in 2.1 ms. The user study of VPV showed the inversely proportional relationship between the immersion controlled by the difficulty of the AR game and the frequency of situation awareness in both quantitatively and qualitatively. Additional VPV experiment assessing two out-of-view object visualization methods (EyeSee360 and Radar) showed no significant effect on the participants' activity, while EyeSee360 yielded faster responses and Radar engendered participants' preference on average. Our field study demonstrated an integration of VPE and VPV which has potentials for safety-ensured immersion when the proposed component is used for AR in daily uses. We expect that when the proposed component is developed enough to be used in real world, it will contribute to the safety-ensured AR, as well as to the population of AR.},
	Author = {J. {Jung} and H. {Lee} and J. {Choi} and A. {Nanda} and U. {Gruenefeld} and T. {Stratmann} and W. {Heuten}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2018.00032},
	Issn = {1554-7868},
	Keywords = {augmented reality;convolutional neural nets;data visualisation;driver information systems;image colour analysis;image sequences;pedestrians;road accidents;road safety;road traffic;traffic engineering computing;InceptionV3 model;VPV experiment;out-of-view object visualization methods;pedestrian-vehicle accidents;third-party component;AR-related accidents;situation awareness;social problem;augmented reality;safety-ensured AR;safety-ensured immersion;EyeSee360;activated images;VPE experiment;out-of-view object visualization method;estimated car position;region-of-interest based scheme;convolutional neural network model;relative 3D position;RGB image sequence;vehicle position visualization;Estimation;Accidents;Visualization;Three-dimensional displays;Safety;Automobiles;Two dimensional displays;augmented reality;safety;warning system;evaluation;user study},
	Month = {Oct},
	Pages = {70-79},
	Title = {Ensuring Safety in Augmented Reality from Trade-off Between Immersion and Situation Awareness},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2018.00032}}

@inproceedings{8613753,
	Abstract = {Typical Head-Mounted Displays (HMDs) that provide a highly immersive Virtual Reality (VR) experience make any interaction between a user and real space difficult by occluding the user's entire field of view. Video see-through type HMDs can solve this problem by superimposing real-space information on the VR environment. The existing method of supporting interactions with the real space is superimposition of boundary lines of the real space on the virtual space in the HMD. However, overlaying the boundary lines on the entire field of view may reduce the user's immersive feeling. In this paper, we propose two methods to support interactions with the real world while playing immersive VR games without reducing the user's immersive feeling as much as possible, even when the user wanders. The first method is to superimpose a 3D point cloud of real space around the user on the virtual space in the HMD. The second method is to deploy familiar objects (e.g., furniture in his/her room) in the virtual space in the HMD. The user traces the familiar objects as subgoals to reach the goal. We implement the two methods and conduct a user study to compare interaction performance. As a result of the user study, we find that the second method provides better spatial information about the real space without reducing the user's immersive feeling, compared to the existing method.},
	Author = {K. {Kanamori} and N. {Sakata} and T. {Tominaga} and Y. {Hijikata} and K. {Harada} and K. {Kiyokawa}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2018.00033},
	Issn = {1554-7868},
	Keywords = {collision avoidance;helmet mounted displays;virtual reality;VR environment;boundary lines;virtual space;HMD;immersive feeling;immersive VR games;user wanders;obstacle avoidance method;virtual reality immersion;real-space information;highly immersive virtual reality experience;typical head-mounted displays;Resists;Three-dimensional displays;Legged locomotion;Virtual environments;Switches;Image color analysis;Virtual Reality;immersive feeling;walking assistance},
	Month = {Oct},
	Pages = {80-89},
	Title = {Obstacle Avoidance Method in Real Space for Virtual Reality Immersion},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2018.00033}}

@inproceedings{8613754,
	Abstract = {Achieving a full 3D auditory experience with head-related transfer functions (HRTFs) is still one of the main challenges of spatial audio rendering. HRTFs capture the listener's acoustic effects and personal perception, allowing immersion in virtual reality (VR) applications. This paper aims to investigate the connection between listener sensitivity in vertical localization cues and experienced presence, spatial audio quality, and attention. Two VR experiments with head-mounted display (HMD) and animated visual avatar are proposed: (i) a screening test aiming to evaluate the participants' localization performance with HRTFs for a non-visible spatialized audio source, and (ii) a 2 minute free exploration of a VR scene with five audiovisual sources in a both non-spatialized (2D stereo panning) and spatialized (free-field HRTF rendering) listening conditions. The screening test allows a distinction between good and bad localizers. The second one shows that no biases are introduced in the quality of the experience (QoE) due to different audio rendering methods; more interestingly, good localizers perceive a lower audio latency and they are less involved in the visual aspects.},
	Author = {M. {Geronazzo} and E. {Sikstr{\"o}m} and J. {Kleimola} and F. {Avanzini} and A. {de G{\"o}tzen} and S. {Serafin}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2018.00034},
	Issn = {1554-7868},
	Keywords = {audio signal processing;audio-visual systems;avatars;hearing;helmet mounted displays;quality of experience;rendering (computer graphics);transfer functions;virtual reality;bad localizers;accurate vertical localization;immersive virtual reality scenarios;3D auditory experience;head-related transfer functions;spatial audio rendering;HRTFs capture;personal perception;virtual reality applications;listener sensitivity;vertical localization cues;spatial audio quality;VR experiments;head-mounted display;visual avatar;screening test;nonvisible spatialized audio source;VR scene;audiovisual sources;2D stereo panning;HRTF rendering;audio rendering methods;time 2.0 min;Rendering (computer graphics);Acoustics;Virtual environments;Visualization;Solid modeling;Ear;virtual reality;spatial audio rendering;head related transfer function;auditory vertical localization;personalization;quality of the experience},
	Month = {Oct},
	Pages = {90-97},
	Title = {The Impact of an Accurate Vertical Localization with HRTFs on Short Explorations of Immersive Virtual Reality Scenarios},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2018.00034}}

@inproceedings{8613755,
	Abstract = {In practical use of optical see-through head-mounted displays, users often have to adjust the brightness of virtual content to ensure that it is at the optimal level. Automatic adjustment is still a challenging problem, largely due to the bidirectional nature of the structure of the human eye, complexity of real world lighting, and user perception. Allowing the right amount of light to pass through to the retina requires a constant balance of incoming light from the real world, additional light from the virtual image, pupil contraction, and feedback from the user. While some automatic light adjustment methods exist, none have completely tackled this complex input-output system. As a step towards overcoming this issue, we introduce IntelliPupil, an approach that uses eye tracking to properly modulate augmentation lighting for a variety of lighting conditions and real scenes. We first take the data from a small form factor light sensor and changes in pupil diameter from an eye tracking camera as passive inputs. This data is coupled with user-controlled brightness selections, allowing us to fit a brightness model to user preference using a feed-forward neural network. Using a small amount of training data, both scene luminance and pupil size are used as inputs into the neural network, which can then automatically adjust to a user's personal brightness preferences in real time. Experiments in a high dynamic range AR scenario with varied lighting show that pupil size is just as important as environment light for optimizing brightness and that our system outperforms linear models.},
	Author = {C. {Liu} and A. {Plopski} and K. {Kiyokawa} and P. {Ratsamee} and J. {Orlosky}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2018.00037},
	Issn = {1554-7868},
	Keywords = {augmented reality;brightness;cameras;feedforward neural nets;gaze tracking;helmet mounted displays;image matching;lighting;optimisation;light adjustment methods;brightness preferences;scene luminance;AR scenario;linear models;retina;virtual image;user perception;human eye;optimal level;head-mounted displays;pupillometric light modulation;environment light;feed-forward neural network;eye tracking camera;form factor light sensor;augmentation lighting;complex input-output system;pupil contraction;Brightness;Lighting;Cameras;Gaze tracking;Dynamic range;Resists;Three-dimensional displays;Head mounted displays, optical see through, eye tracking, augmented reality, lighting adjustment},
	Month = {Oct},
	Pages = {98-104},
	Title = {IntelliPupil: Pupillometric Light Modulation for Optical See-Through Head-Mounted Displays},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2018.00037}}

@inproceedings{8613756,
	Abstract = {Intelligent Virtual Agents (IVAs) are becoming part of our everyday life, thanks to artificial intelligence technology and Internet of Things devices. For example, users can control their connected home appliances through natural voice commands to the IVA. However, most current-state commercial IVAs, such as Amazon Alexa, mainly focus on voice commands and voice feedback, and lack the ability to provide non-verbal cues which are an important part of social interaction. Augmented Reality (AR) has the potential to overcome this challenge by providing a visual embodiment of the IVA. In this paper we investigate how visual embodiment and social behaviors influence the perception of the IVA. We hypothesize that a user's confidence in an IVA's ability to perform tasks is improved when imbuing the agent with a human body and social behaviors compared to the agent solely depending on voice feedback. In other words, an agent's embodied gesture and locomotion behavior exhibiting awareness of the surrounding real world or exerting influence over the environment can improve the perceived social presence with and confidence in the agent. We present a human-subject study, in which we evaluated the hypothesis and compared different forms of IVAs with speech, gesturing, and locomotion behaviors in an interactive AR scenario. The results show support for the hypothesis with measures of confidence, trust, and social presence. We discuss implications for future developments in the field of IVAs.},
	Author = {K. {Kim} and L. {Boelling} and S. {Haesler} and J. {Bailenson} and G. {Bruder} and G. F. {Welch}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2018.00039},
	Issn = {1554-7868},
	Keywords = {augmented reality;software agents;intelligent virtual agents;social behaviors;IVA;AR;Amazon Alexa;human-subject study;embodied gesture;social interaction;voice feedback;voice commands;artificial intelligence technology;social behavior;visual embodiment;locomotion behaviors;perceived social presence;locomotion behavior;Visualization;Internet of Things;Robots;Avatars;Augmented reality;Face;Atmospheric measurements;Intelligent virtual agents;digital assistants;social interaction;presence;confidence;trust in technology;augmented reality},
	Month = {Oct},
	Pages = {105-114},
	Title = {Does a Digital Assistant Need a Body? The Influence of Visual Embodiment and Social Behavior on the Perception of Intelligent Virtual Agents in AR},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2018.00039}}

@inproceedings{8613757,
	Abstract = {Redirected walking (RDW) allows virtual reality (VR) users to walk infinitely while staying inside a finite physical space through subtle shifts (gains) of the scene to redirect them back inside the volume. All prior approaches measure the feasibility of RDW techniques based on if the user perceives the manipulation, leading to rather small applicable gains. However, we treat RDW as an interaction technique and therefore use visually perceivable gains instead of using the perception of manipulation. We revisited prior experiments with focus on applied gains and additionally tested higher gains on the basis of applicability in a user study. We found that users accept curvature gains up to 20$\,^{\circ}$/m, which reduces the necessary physical volume down to approximately 6x6m for virtually walking infinitely straight ahead. Our findings strife to rethink the usage of redirection from being unperceived to being applicable and natural.},
	Author = {M. {Rietzler} and J. {Gugenheimer} and T. {Hirzle} and M. {Deubzer} and E. {Langbehn} and E. {Rukzio}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2018.00041},
	Issn = {1554-7868},
	Keywords = {virtual reality;virtual reality;finite physical space;RDW techniques;interaction technique;visually perceivable gains;user study;curvature gains;redirected walking;perceptual limitations;subtle shifts;Legged locomotion;Measurement;Meters;Navigation;Virtual environments;Teleportation;Augmented reality},
	Month = {Oct},
	Pages = {115-122},
	Title = {Rethinking Redirected Walking: On the Use of Curvature Gains Beyond Perceptual Limitations and Revisiting Bending Gains},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2018.00041}}

@inproceedings{8613758,
	Abstract = {We investigate using augmented reality to extend the screen of a smartphone beyond its physical limits with a virtual surface that is co-planar with the phone and that follows as the phone is moved. We call this extension a VESAD, or Virtually Extended Screen-Aligned Display. We illustrate and describe several ways that a VESAD could be used to complement the physical screen of a phone, and describe two novel interaction techniques: one where the user performs a quick rotation of the phone to switch the information shown in the VESAD, and another called "slide-and-hang" whereby the user can detach a VESAD and leave it hanging in mid-air, using the phone to establish the initial position and orientation of the virtual window. We also report an experiment that compared three interfaces used for an abstract classification task: the first using only a smartphone, the second using the phone for input but with a VESAD for output, and the third where the user performed input in mid-air on the VESAD (as detected by a Leap Motion). The second user interface was found to be superior in time and selection count (a metric of mistakes committed by users) and was also subjectively preferred over the other two interfaces. This demonstrates the added value of a VESAD for output over a phone's physical screen, and also demonstrates that input on the phone's screen was better than input in mid-air in our experiment.},
	Author = {E. {Normand} and M. J. {McGuffin}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2018.00043},
	Issn = {1554-7868},
	Keywords = {augmented reality;smart phones;user interfaces;virtually extended screen-aligned display;AR;physical screen;virtual surface;handheld VESAD;smartphone;virtual window;Microsoft Windows;Navigation;Resists;Switches;Task analysis;Hardware;User interfaces;augmented reality;HMD;smartphone},
	Month = {Oct},
	Pages = {123-133},
	Title = {Enlarging a Smartphone with AR to Create a Handheld VESAD (Virtually Extended Screen-Aligned Display)},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2018.00043}}

@inproceedings{8613759,
	Abstract = {Collaborative Systems are in daily use by millions of people promising to improve everyone's life. Smartphones, smartwatches and tablets are everyday objects and life without these unimaginable. New assistive systems such as head-mounted displays (HMDs) are becoming increasingly important for various domains, especially for the industrial domain, because they claim to improve the efficiency and quality of procedural tasks. A range of scientific laboratory studies already demonstrated the potential of augmented reality (AR) technologies especially for training tasks. However, most researches are limited in terms of inadequate task complexity, measured variables and lacking comparisons. In this paper, we want to close this gap by introducing a novel multimodal HMD-based training application and compare it to paper-based learning for manual assembly tasks. We perform a user study with 30 participants measuring the training transfer of an engine assembly training task, the user satisfaction and perceived workload during the experiment. Established questionnaires such as the system usability scale (SUS), the user experience questionnaire (UEQ) and the Nasa Task Load Index (NASA-TLX) are used for the assessment. Results indicate significant differences between both learning approaches. Participants perform significantly faster and significantly worse using paper-based instructions. Furthermore, all trainees preferred HMD-based learning for future assembly trainings which was scientifically proven by the UEQ.},
	Author = {S. {Werrlich} and A. {Daniel} and A. {Ginger} and P. {Nguyen} and G. {Notni}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2018.00046},
	Issn = {1554-7868},
	Keywords = {augmented reality;computer based training;groupware;helmet mounted displays;human computer interaction;human factors;user interfaces;virtual reality;paper-based training;Collaborative Systems;smartwatches;head-mounted displays;industrial domain;scientific laboratory studies;augmented reality;training tasks;inadequate task complexity;training transfer;engine assembly training task;user satisfaction;system usability scale;user experience questionnaire;Nasa Task Load Index;paper-based instructions;assistive systems;multimodal HMD-based learning;Task analysis;Training;Engines;Software;Resists;Atmospheric measurements;Particle measurements;Augmented Reality;Evaluation;Head Mounted Displays;Training},
	Month = {Oct},
	Pages = {134-142},
	Title = {Comparing HMD-Based and Paper-Based Training},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2018.00046}}

@inproceedings{8613760,
	Abstract = {Interactive visualizations are external cognitive artifacts aimed at supporting users' exploratory and sense-making activities. In recent years, there has been an explosion of commercial virtual reality (VR) head-mounted displays (HMD). These VR devices are meant to offer high levels of engagement and improve users' analytical exploration of the displayed content. However, given their rapid market introduction, the possible influences and usefulness that VR could bring in terms of supporting users' exploration with interactive visualizations remain largely underexplored. We attempt to fill this gap and provide results of an empirical study of an interactive visualization tool that we have developed for a VR HMD system. This tool is aimed at facilitating exploratory and analytical reasoning activities with 3D shapes and their transformational processes. Overall, the results show that the tool is supportive of users' exploratory and analytical activities based on the significant improvement in their post-experiment test scores (when compared to their pre-experiment ones) and their engagement level measured via a user engagement questionnaire and participants' comments. The results shed a positive light on the use of visualizations in VR environments and can inform the design of these tools of domains beyond 3D transformational geometry.},
	Author = {F. {Lu} and D. {Yu} and H. {Liang} and W. {Chen} and K. {Papangelis} and N. M. {Ali}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2018.00050},
	Issn = {1554-7868},
	Keywords = {cognition;data visualisation;helmet mounted displays;interactive systems;virtual reality;engagement level;interactive visualizations;virtual reality environments;external cognitive artifacts;sense-making activities;commercial virtual reality head-mounted displays;VR devices;displayed content;rapid market introduction;interactive visualization tool;VR HMD system;analytical reasoning activities;analytical activities;user engagement questionnaire;VR environments;Tools;Visualization;Three-dimensional displays;Solids;Shape;Virtual reality;Geometry;Virtual reality;Interactive visualizations;User engagement;3D geometry;Analytical Reasoning;Sense making},
	Month = {Oct},
	Pages = {143-152},
	Title = {Evaluating Engagement Level and Analytical Support of Interactive Visualizations in Virtual Reality Environments},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2018.00050}}

@inproceedings{8613761,
	Abstract = {Sharing and watching live 360 panorama video is available on modern social networking platforms, yet the communication is often a passive one-directional experience. This research investigates how to further improve live 360 panorama based remote collaborative experiences by adding Mixed Reality (MR) cues. SharedSphere is a wearable MR remote collaboration system that enriches a live captured immersive panorama based collaboration through MR visualisation of non-verbal communication cues (e.g., view awareness and gestures cues). We describe the design and implementation details of the prototype system, and report on a user study investigating how MR live panorama sharing affects the user's collaborative experience. The results showed that providing view independence through sharing live panorama enhances co-presence in collaboration, and the MR cues help users understanding each other. Based on the study results we discuss design implications and future research direction.},
	Author = {G. A. {Lee} and T. {Teo} and S. {Kim} and M. {Billinghurst}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2018.00051},
	Issn = {1554-7868},
	Keywords = {data visualisation;gesture recognition;groupware;human computer interaction;social networking (online);user interfaces;video signal processing;virtual reality;prototype system;MR cues;live 360 panorama video;remote collaborative experiences;wearable MR remote collaboration system;nonverbal communication cues;view awareness;gestures cues;social networking platforms;immersive panorama based collaboration;mixed reality cues;MR live panorama sharing;MR visualisation;Collaboration;Streaming media;Virtual reality;Visualization;Cameras;Three-dimensional displays;Two dimensional displays;Mixed Reality;Augmented Reality;remote collaboration;live panorama sharing;view independence},
	Month = {Oct},
	Pages = {153-164},
	Title = {A User Study on MR Remote Collaboration Using Live 360 Video},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2018.00051}}

@inproceedings{8613762,
	Abstract = {Collaboration is an important application area for virtual reality (VR). However, unlike in the real world, collaboration in VR misses important empathetic cues that can make collaborators aware of each other's emotional states. Providing physiological feedback, such as heart rate or respiration rate, to users in VR has been shown to create a positive impact in single user environments. In this paper, through a rigorous mixed-factorial user experiment, we evaluated how providing heart rate feedback to collaborators influences their collaboration in three different environments requiring different kinds of collaboration. We have found that when provided with real-time heart rate feedback participants felt the presence of the collaborator more and felt that they understood their collaborator's emotional state more. Heart rate feedback also made participants feel more dominant when performing the task. We discuss the implication of this research for collaborative VR environments, provide design guidelines, and directions for future research.},
	Author = {A. {Dey} and H. {Chen} and C. {Zhuang} and M. {Billinghurst} and R. W. {Lindeman}},
	Booktitle = {2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
	Doi = {10.1109/ISMAR.2018.00052},
	Issn = {1554-7868},
	Keywords = {feedback;groupware;virtual reality;immersive collaborative virtual environments;real-time multisensory heart rate feedback;collaborative VR environments;real-time heart rate feedback participants;providing heart rate feedback;single user environments;providing physiological feedback;Heart rate;Collaboration;Real-time systems;Physiology;Avatars;Task analysis;Virtual environments},
	Month = {Oct},
	Pages = {165-173},
	Title = {Effects of Sharing Real-Time Multi-Sensory Heart Rate Feedback in Different Immersive Collaborative Virtual Environments},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2018.00052}}
