%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for Rafael Radkowski at 2021-03-13 08:55:40 -0600 


%% Saved with string encoding Unicode (UTF-8) 



@inproceedings{8951909,
	Abstract = {The existing datasets for evaluating Visual Inertial Odometry (VIO) have boosted the research of autonomous agents, but they don't meet the prosperous research of Augmented Reality (AR) or Mixed Reality (MR) given that they are not collected at real AR scenes and do not account for affecting factors of mobile devices. This paper presents the NEAR dataset, an AR oriented visual-inertial dataset collected with commodity handheld phones with ground truth. The dataset has a total of 110 sequences in 49 elaborately designed collection cases at two typical indoor scenes, i.e. the living area and the table area. It also covers plenty of setting adjustments for comparison, including the comparisons of different level textures, illuminations, motion patterns, camera settings and the difference between the rolling shutter and the global shutter. The full dataset along with the calibration data has been publicly available.},
	Author = {C. {Wang} and Y. {Zhao} and J. {Guo} and L. {Pei} and Y. {Wang} and H. {Liu}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00-10},
	Keywords = {augmented reality;distance measurement;mobile computing;smart phones;visual databases;handheld smartphones;mobile AR application;commodity handheld phones;mobile devices;mixed reality;augmented reality;autonomous agents;visual inertial odometry;NetEase AR oriented visual inertial dataset;NEAR dataset;Cameras;Calibration;Smart phones;Lighting;Robots;Dynamics;Mixed / Augmented Reality;Tracking;Visual Inertial Odometry / SLAM;Mobile Phone;Rolling Shutter;Time Synchronization;Dataset},
	Month = {Oct},
	Pages = {366-371},
	Title = {NEAR: The NetEase AR Oriented Visual Inertial Dataset},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00-10}}

@inproceedings{8951974,
	Abstract = {In high-paced applications like air traffic control, disaster management, and medical triage, information is often routed through a central location such as a dispatch or control center. In this work, we introduce a set of color-coded and gaze-aware visualizations designed to simultaneously manage both information priority and user attention. These notifications are unique in that they not only respond to whether or not they have been observed by the user, but they use color to represent the priority of the object to assist processing of other incoming information in the environment. We built both heads-up display (HUD)-based and in-situ information management paradigms and combined these with gaze and color visualizations. In an interactive virtual reality (VR) control center simulation, HUD visualizations slightly reduced unnecessary head movement without sacrificing performance, but were distracting to participants.},
	Author = {J. {Orlosky} and C. {Liu} and D. {Kalkofen} and K. {Kiyokawa}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.000-9},
	Keywords = {air traffic control;data visualisation;head-up displays;helmet mounted displays;information management;interactive systems;traffic engineering computing;virtual reality;information management paradigms;interactive virtual reality control center simulation;HUD visualizations;unnecessary head movement;visualization-guided attention direction;dynamic control tasks;high-paced applications;air traffic control;disaster management;medical triage;central location;gaze-aware visualizations;information priority;user attention;notifications;incoming information;heads-up display;Visualization;Task analysis;Augmented reality;Image color analysis;Monitoring;Gaze tracking;Navigation;Visualization;attention;virtual-reality;eye-tracking;guidance;periphery},
	Month = {Oct},
	Pages = {372-373},
	Title = {Visualization-Guided Attention Direction in Dynamic Control Tasks},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.000-9}}

@inproceedings{8951779,
	Abstract = {A large-scale optical tracking system is proposed in this paper. It uses synchronous laser coding to realize optical locating. Inertial sensors are integrated to assist the calculation of 6-DOF pose. The encoding of the base station in the system can be customized and we designed a specific layout of base stations to prevent signal interference. Based on the above layout scheme, the tracking range of the system can be expanded to hundreds of square meters. The tracking system proposed in this paper benefits promoting the development of large scale optical tracking equipment with low cost and high precision.},
	Author = {D. {Li} and D. {Weng} and Y. {Li} and H. {Xun}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.000-8},
	Keywords = {encoding;inertial systems;interference suppression;light interference;optical sensors;optical tracking;position measurement;large-scale optical tracking system;base station;inertial sensors;6-DOF pose calculation;synchronous laser encoding;signal interference prevention;Base stations;Encoding;Optical sensors;Robot sensing systems;Optical films;Adaptive optics;Layout;optical tracking, large-scale},
	Month = {Oct},
	Pages = {374-375},
	Title = {Large-Scale Optical Tracking System},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.000-8}}

@inproceedings{8951892,
	Abstract = {In this paper, we introduce a hand interaction guidance system (HIGS) which can automatically author video guidance from 'one-shot' expert demonstration recording, and decompose the step instructions and deliver feedback according to the users' operation. The system observes hand-object interactions with a RGB-D camera from an egocentric view. In the guidance recording stage, HIGS is able to detect the global positions and moments of hand-object interactions and recover the 3D-hand poses during the operation. In the guidance stage, the recorded guidance video is automatically progressed as the user performs the task and the steps are used to assess task progress and completion. Our system operates in real time for both authoring and for monitoring and providing guidance. We see this work as a step towards fully automated learning and guidance systems, in mixed reality settings.},
	Author = {Y. {Lu} and W. {Mayol-Cuevas}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.000-7},
	Keywords = {human computer interaction;image colour analysis;learning (artificial intelligence);pose estimation;user interfaces;video signal processing;virtual reality;author video guidance;one-shot expert demonstration recording;hand-object interactions;guidance recording stage;HIGS;3D-hand poses;recorded guidance video;hand interaction guidance system;mixed reality settings;pose estimation;Three-dimensional displays;Task analysis;Pose estimation;Cameras;Solid modeling;Two dimensional displays;Human-centered computing;Augmented Reality;HCI system design and evaluation methods},
	Month = {Oct},
	Pages = {376-381},
	Title = {HIGS: Hand Interaction Guidance System},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.000-7}}

@inproceedings{8951920,
	Abstract = {We present FrictionHaptics which is an encountered-type haptic device that emulates friction when a user touches a virtual object by using a rotating sphere as an end effector to a 3DOF robot arm. Our proposed device has two advantages. Firstly, the device creates a tangential friction sensation of a virtual object, even the size of the real object is limited. Secondly, compared to wearable-type or grip-type, our device does not limit the sensing part of a human. We conducted a perceptual experiment to determine how a human perceives friction generated from our proposed device. As a result, we found that our device renders a reliable illusion of tangential friction even the human perceive it using different sensing part.},
	Author = {R. {Meguro} and P. {Ratsamee} and T. {Mashita} and Y. {Uranishi} and H. {Takemura}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.000-6},
	Keywords = {end effectors;friction;haptic interfaces;human perception;grip-type;tangential friction sensation;3DOF robot arm;virtual object;tangential friction emulation;encountered-type haptic device;FrictionHaptics;Robot sensing systems;Friction;Manipulators;Haptic interfaces;Emulation;Haptics;Virtual Reality;Friction;Encounter Type;Pseudo Haptics;Robot Arm},
	Month = {Oct},
	Pages = {382-383},
	Title = {FrictionHaptics : Encountered-Type Haptic Device forTangential Friction Emulation},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.000-6}}

@inproceedings{8951908,
	Abstract = {Low-cost mobile mixed reality solutions for head tracking, as well as tracking of interaction targets, can be enhanced by multiple inertial measurement units (IMUs). Since microelectro-mechanical systems (MEMS) were introduced, IMUs have become smaller, they need less power and cost less. Yet, as any sensor, IMUs produce sensor error and mitigate the accuracy of the tracking system. This is particularly the case for low-cost IMUs. In this poster, we investigate whether the joint use of three IMUs can reduce the overall error. We present a solution fusing three independently calibrated low-cost IMUs on a planar and non-planar grid. We show that we can achieve the tracking quality of a single high-cost IMU this way. For the comparison, we offer several one Degree of Freedom measurement setups for mechanical rotation and translation movements. Afterwards, we discuss some concepts towards designing non-planar arrangements of Multi-IMUs in a Grid that may be suitable for HMD tracking.},
	Author = {A. {Jadid} and L. {Rudolph} and F. {Pankratz} and G. {Klinker}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.000-5},
	Keywords = {calibration;inertial systems;microsensors;mobile computing;object tracking;rotation measurement;sensor fusion;sensor placement;virtual reality;enhanced mixed reality tracking;low-cost mobile mixed reality solutions;head tracking;multiple inertial measurement units;microelectro-mechanical systems;sensor error;tracking system;low-cost IMU;tracking quality;HMD tracking;multiple calibrated IMU;interaction target tracking;MEMS;nonplanar grid;mechanical rotation movement;mechanical translation movement;Calibration;Kalman filters;Acceleration;Pipelines;Tracking;Sensor fusion;Accelerometers},
	Month = {Oct},
	Pages = {384-386},
	Title = {Utilizing Multiple Calibrated IMUs for Enhanced Mixed Reality Tracking},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.000-5}}

@inproceedings{8951915,
	Abstract = {Text entry is a challenge for Virtual Reality (VR) applications. In the context of immersive VR Head-Mounted Displays, text entry has been investigated for standard physical keyboards as well as for various hand representations. Specifically, prior work has indicated that minimalistic fingertip visualizations is an efficient hand representation. However, these representations typically require external tracking systems. Touch-sensitive physical keyboards allow for on-surface interaction, with sensing integrated into the keyboard itself. However, they have not been thoroughly investigated within VR. We close this gap by comparing text entry on a standard physical keyboard and a touch-sensitive physical keyboard in a controlled user study (n=26). Our results indicate that text entry using touch-sensitive physical keyboards can be as efficient as the fingertip visualization, but that results vary between experienced and inexperienced typists.},
	Author = {A. {Otte} and D. {Schneider} and T. {Menzner} and T. {Gesslein} and P. {Gagel} and J. {Grubert}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.000-4},
	Keywords = {helmet mounted displays;keyboards;text analysis;touch sensitive screens;virtual reality;touch-sensitive physical keyboard;virtual reality applications;immersive VR head-mounted displays;standard physical keyboard;efficient hand representation;text entry evaluation;on-surface interaction;fingertip visualization;Keyboards;Sensors;Visualization;Pins;Resists;Standards;Task analysis},
	Month = {Oct},
	Pages = {387-392},
	Title = {Evaluating Text Entry in Virtual Reality using a Touch-sensitive Physical Keyboard},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.000-4}}

@inproceedings{8951977,
	Abstract = {We present a wearable Mixed Reality (MR) remote collaboration system called Wearable RemoteFusion. The system supports spatial annotation and view frustum sharing, and enables natural non-verbal communication cues (eye gaze and hand gesture) for visual assistance in a stitched live dense scene. We describe the design and implementation details of the prototype system, and report on a pilot user study investigating how sharing natural gaze and gesture cues affects collaborative performance and the user experience. We found that by sharing augmented natural cues like the local eye gaze and remote hand gesture, participants had a stronger feeling of Co-presence, and the remote user could guide the local user to complete tasks with less physical workload. We discuss implications for collaborative interface design and directions for future research.},
	Author = {P. {Sasikumar} and L. {Gao} and H. {Bai} and M. {Billinghurst}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.000-3},
	Keywords = {augmented reality;gesture recognition;groupware;human computer interaction;spatial annotation;view frustum sharing;nonverbal communication cues;visual assistance;wearable mixed reality remote collaboration system;wearable RemoteFusion;remote hand gesture sharing;collaborative interface design;local eye gaze;augmented natural cues;user experience;collaborative performance;gesture cues;natural gaze;stitched live dense scene;Collaboration;Virtual reality;Task analysis;Three-dimensional displays;Annotations;Biomedical engineering;Mixed Reality, Augmented Reality, remote collaboration, eye gaze, hand gesture},
	Month = {Oct},
	Pages = {393-394},
	Title = {Wearable RemoteFusion: A Mixed Reality Remote Collaboration System with Local Eye Gaze and Remote Hand Gesture Sharing},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.000-3}}

@inproceedings{8951781,
	Abstract = {Virtual agents are being increasingly used in the areas of healthcare, treatments, and therapy. Virtual agents with emotional intelligence have shown the potential to be applicable to deliver mental health intervention and facilitate therapies for children on the autism spectrum. To build an emotionally intelligent agent, automatic recognition of emotions is an essential building block. For mental health therapies and treatments, detecting rapidly changing emotions of individuals can be useful to know whether the patients are showing appropriate emotional response or not. To this end, we present a new data-driven approach to identify the perceived emotions of individuals based on their walking styles. We extract an individual's walking gait in the form of a sequence of 3D poses given an RGB video of him/her walking. We leverage the gait features to classify the perceived emotional state of the individual into one of four categories: happy, sad, angry, or neutral. First, we use an LSTM network to extract deep features of the gait using labeled emotion datasets. Next, we compute the affective features of the gaits using posture and movement cues. We combine these affective features with the deep features and classify them using a Random Forest Classifier. We observe that this approach provides an accuracy of 80:07% in identifying the perceived emotions. Additionally, we present a new dataset consisting of videos of walking individuals, their extracted gaits, and perceived emotion labels associated with each gait. We refer to this dataset as the `EWalk (Emotion Walk)'' dataset.},
	Author = {T. {Randhavane} and U. {Bhattacharya} and K. {Kapsaskis} and K. {Gray} and A. {Bera} and D. {Manocha}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.000-2},
	Keywords = {emotion recognition;gait analysis;health care;image colour analysis;learning (artificial intelligence);medical image processing;patient treatment;video signal processing;virtualisation;deep features;mental health applications;virtual agents;emotional intelligence;emotionally intelligent agent;mental health therapies;walking gait;affective features;walking individuals;EWalk dataset;Emotion Walk;random forest classifier;LSTM network;Feature extraction;Legged locomotion;Foot;Emotion recognition;Three-dimensional displays;Psychology;Neck},
	Month = {Oct},
	Pages = {395-399},
	Title = {Learning Perceived Emotion Using Affective and Deep Features for Mental Health Applications},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.000-2}}

@inproceedings{8951890,
	Abstract = {Human-animal interaction has been studied in a variety of settings and for a range of populations, with some findings pointing towards its benefits for physical, mental and social human health. Technological advances opened up new opportunities for researchers to replicate human-animal interactions with robotic and graphical animals, and to investigate human-animal relationships for different applications such as mental health and education. Although graphical animals have been studied in the past in the physical health and education domains, most of the time, their realizations were bound to computer screens, limiting their full potential, especially in terms of companionship and the provision of support. In this work, we describe past research efforts investigating influences of human-animal interaction on mental health and different realization of such animals. We discuss the idea that augmented reality could offer potential for human-animal interaction in terms of mental and social health, and propose several aspects of augmented reality animals that warrant further research for such interactions.},
	Author = {N. {Norouzi} and G. {Bruder} and J. {Bailenson} and G. {Welch}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.000-1},
	Keywords = {augmented reality;biology computing;human computer interaction;zoology;human-animal interaction;physical human health;mental human health;social human health;graphical animals;human-animal relationships;mental health;physical health;education domains;social health;augmented reality animal investigation;robotic animals;Dogs;Robots;Sociology;Statistics;Medical treatment;Augmented reality;Animal Assisted Interventions;Augmented Reality;Companion Animals;Virtual Animals},
	Month = {Oct},
	Pages = {400-403},
	Title = {Investigating Augmented Reality Animals as Companions},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.000-1}}

@inproceedings{8951911,
	Abstract = {Mental health conditions pose a major challenge to healthcare providers and society at large. According to the Mental Health Foundation of New Zealand, one in five people will develop a serious mood disorder, including depression, at some time in their life [2]. Early intervention can have significant positive impact on a person's prognosis, particularly important in affecting outcomes for young people [38]. Co-designed solutions to improve resilience and well-being in young people have specifically been recognised as part of the National Suicide Prevention Strategy and the New Zealand Health Strategy. Innovative interventions that support long-term change for individuals are urgently needed [10]. Self-compassion/self-criticism constitutes a protective/risk factor with regard to developing and maintaining depression [3]; particularly in young people [4]. Self-criticism is one of the major psychological factors, defined as dominant response style of negative evaluation and judgement of self to perceived failure [5]. One effective method to increase self-compassion and reduction in depression may be to address self-criticism through compassion-focused therapy [6]. Virtual Reality (VR) in Health is an emerging field. It is becoming more commonplace with the advent of affordable consumer head mounted devices, and has significant potential for the understanding, assessment and treatment of mental health problems [7]. It can provide a non-threatening, zero risk environment which allows for free exploring of different strategies [16]. We propose to take this new technology and co-design Virtual Reality scenarios with young people, which focuses on real world situations that impact the sample group most and assists them to view these experiences with a self-compassionate lens. This is achieved by being taught compassionate manners of responding to a scenario and by switching perspective. We provide an overview of an initial proof-of-concept study, propose a study in different social settings and highlight key points for discussion pertaining to technology use, data safety, privacy, and considerations for addressing depressive symptoms necessary to advance this work.},
	Author = {N. {Baghaei} and S. {Hach} and I. {Khaliq} and L. {Stemmet} and J. {Krishnan} and J. {Naslund} and H. {Liang} and H. {Sharifzadeh}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00042},
	Keywords = {behavioural sciences computing;health care;medical information systems;patient treatment;psychology;virtual reality;young people;virtual reality;mental health conditions;depression;mental health problems;New Zealand health strategy;national suicide prevention strategy;mental health foundation;self-compassion;Depression;Mental health;Virtual reality;Medical treatment;Tools;virtual reality, mental health, self compassion},
	Month = {Oct},
	Pages = {404-407},
	Title = {Increasing Self-Compassion in Young People through Virtual Reality},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00042}}

@inproceedings{8951968,
	Abstract = {In this paper we present a Virtual Reality (VR) system (simulated kitchen; Sim:Kitchen) designed and developed for use in the rehabilitation of adults with neurogenic communication disorders. Communication disorders are commonly associated with neurological conditions such as stroke, brain injury, and Parkinson's disease and require long-term rehabilitation with a speech pathologist. Traditional speech pathology management involves therapy designed to address the communication impairment and to teach strategies to overcome barriers to successful communication. These treatments are delivered within a clinic setting with limited opportunity for practice in realworld situations. VR has the potential to provide a safe, supported lifelike environment in which people undergoing communication rehabilitation could practice therapy tasks and gain confidence before entering actual real-world communication contexts. The aim of this paper is to investigate the perspectives of speech pathologists about the use of VR and the simulated kitchen tool for people with communication disorders as a means to optimise rehabilitation engagement and outcomes.},
	Author = {A. {Vaezipour} and D. {Aldridge} and K. {Wall} and S. {Koenig} and D. {Theodoros}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00043},
	Keywords = {diseases;medical computing;medical disorders;neurophysiology;patient rehabilitation;virtual reality;stroke;speech pathology management;rehabilitation engagement;simulated kitchen tool;communication rehabilitation;communication impairment;long-term rehabilitation;Parkinson's disease;brain injury;neurological conditions;Sim:Kitchen;VR;virtual reality system;adult neurogenic communication disorders;Task analysis;Usability;Pathology;Virtual reality;Brain injuries;Medical treatment;Training;Virtual-reality;virtual-kitchen;virtual-environment;communication-disorder;neurorehabilitation;speech-pathology;human-computer-interaction},
	Month = {Oct},
	Pages = {408-411},
	Title = {Design and Development of a Virtual Reality System for the Management of Adult Neurogenic Communication Disorders},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00043}}

@inproceedings{8951980,
	Abstract = {Cystic fibrosis (CF) is a genetic disorder and chronic lung disease that affect between 70,000 and 100,000 people worldwide. Patients have to take medication and perform daily airway clearance exercises in order to mobilize sticky mucus from the lungs caused by CF. These exercises are rather repetitive and not very engaging causing patients to skip despite the health repercussions. In collaboration with a physiotherapist, we have developed a virtual reality diving game that diegetically incorporates the breathing exercise by translating it into movement through the underwater world. Initial user tests have shown that our combination of physiological input and virtual reality creates a compelling and immersive experience. However, it was also shown to be a task with widely differing perceived difficulty so that further steps need to be undertaken to adjust the game to various user groups.},
	Author = {R. {Wetzel} and T. {Kreienb{\"u}hl}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00044},
	Keywords = {computer games;diseases;genetics;lung;medical disorders;patient treatment;pneumodynamics;virtual reality;virtual reality game;cystic fibrosis;genetic disorder;chronic lung disease;medication;daily airway clearance exercises;sticky mucus;lungs;health repercussions;virtual reality diving game;breathing exercise;underwater world;initial user tests;Games;Virtual reality;Prototypes;Headphones;Lung;Medical treatment;Hardware;virtual-reality,-cystic-fibrosis,-serious-game,-breathing,-game-design,-health-game},
	Month = {Oct},
	Pages = {412-416},
	Title = {Breathe to Dive: Exploring a Virtual Reality Game for Treatment of Cystic Fibrosis},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00044}}

@inproceedings{8951888,
	Abstract = {The authors designed a kind of augmented reality annotation system based on network knowledge collaboration for primary science education to expand the cognitive effect. Various types of annotations such as text, images, videos, links, 3D models, etc. can be added to the corresponding position of the paper book by multi-user and multi-device through the system. And the annotation contents could be retrieved in AR mode by other users. The connotation and dimension of scientific knowledge could be expanded through concentrating diversified annotations. The system only records the relative position of the annotation on the page by a hand-aided registration, and uploads the location information to the server without infringing the copyright of the book. The system allows users to add links as annotations, through which users could interact with social media and knowledge communities. By using this system, users' ideas could be connected thus promote the flow of knowledge between different types of readers (such as students, parents, and teachers), readers and authors and it is conducive to the exchange and inspiration of ideas, promoting the integration of knowledge and the generation of group wisdom.},
	Author = {Y. {Zhang} and L. {Tao} and Y. {Lu} and Y. {Li}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00045},
	Keywords = {augmented reality;computer aided instruction;groupware;human computer interaction;natural sciences computing;social networking (online);scientific knowledge;social media;knowledge communities;paper book oriented augmented reality collaborative annotation system;augmented reality annotation system;network knowledge collaboration;primary science education;annotation content retrieval;Annotations;Collaboration;Databases;Augmented reality;Videos;Mobile handsets;Education;Collaborative-system,-knowledge-collaboration,-augmented-reality,-annotation,-science-education},
	Month = {Oct},
	Pages = {417-421},
	Title = {Design of Paper Book Oriented Augmented Reality Collaborative Annotation System for Science Education},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00045}}

@inproceedings{8951993,
	Abstract = {Extended reality has been used in various application areas creating societal impact including in health, education, and the arts. However, there has not been enough research to help alleviate the issues of refugees around the world using extended reality applications. In this paper, we focus on identifying issues that refugees face in Australia and how extended reality application can be designed to support in resolving those issues. We followed an ethnographic research approach that involved a semi-structured interview, participatory design, and speculative design approach where we recruited 30 people who arrived in Australia as refugees and currently finding ways to settle in the country. Our research identified three key issues that refugees face-post-traumatic stress disorder (PTSD), the information-seeking problem, and cultural adjustment in the host community. We discuss how extended reality applications can support the refugees in Australia and elsewhere in the world in each of the three issues.},
	Author = {A. {Almohamed} and A. {Dey} and J. {Zhang} and D. {Vyas}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00046},
	Keywords = {injuries;medical disorders;patient treatment;psychology;virtual reality;ethnographic research approach;speculative design approach;Australia;post-traumatic stress disorder;extended reality application;semi-structured interview;participatory design;information-seeking problem;cultural adjustment;refugees},
	Month = {Oct},
	Pages = {422-425},
	Title = {Extended Reality for Refugees: Pragmatic Ideas through Ethnographic Research with Refugees in Australia},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00046}}

@inproceedings{8951782,
	Abstract = {Over the last decade, there have been countless virtual reality rehabilitation systems make their way from the laboratories into the real world. For one reason or another, this same trend has not occurred for mixed-reality rehabilitation systems. Even less so, are either of these rehabilitation systems making their way to patients' homes for home rehabilitation. As mixed-reality hardware becomes more easily accessible, affordable and accepted; it is becoming more feasible for mixed reality rehabilitation systems to be placed in patients' homes. This brings researchers exciting new possibilities regarding patients' treatments but also new challenges regarding their design/implementation. This paper will discuss adapting one such mixed-reality system, The Augmented Reflection Technology System, in order to allow patients to carry out their clinician recommended rehabilitation at their own home. We present a demonstration system that can be used for immersive home-rehabilitation and discuss future possibilities in the field.},
	Author = {C. {Heinrich} and T. {Langlotz} and H. {Regenbrecht}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00047},
	Keywords = {augmented reality;medical computing;medical image processing;object tracking;patient rehabilitation;patient treatment;virtual reality;clinical mixed-reality rehabilitation System;patients;countless virtual reality rehabilitation systems;mixed-reality rehabilitation systems;home rehabilitation;mixed-reality hardware;mixed reality rehabilitation systems;mixed-reality system;demonstration system;immersive home-rehabilitation;augmented reflection technology system;home adapting;Subspace constraints;Mirrors;Medical treatment;Protocols;Virtual reality;Stroke (medical condition);Monitoring;Human-centered-computing---Mixed-/-augmented-reality;Human-centered-computing---Accessibility-system-and-tools},
	Month = {Oct},
	Pages = {426-430},
	Title = {Heading Home -- Adapting a Clinical Mixed-Reality Rehabilitation System for Patients' Home Use},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00047}}

@inproceedings{8951984,
	Abstract = {Deaf infants born to hearing parents are at risk of language deprivation, which may lead to life-long impact on linguistic, cognitive and socio-emotional development. It remains demanding for hearing parents to provide meaningful and linguistic-rich interaction with their deaf and hard of hearing (DHH) children, due to lack of sign language fluency and insufficient communication strategies. In this study, we present a proof-of-concept visual augmentation prototype utilizing the Augmented Reality (AR) lamp metaphor that aims to support context-aware and non-intrusive parent-child interaction using American Sign Language (ASL), with adaptation to joint-attention strategies that match with the child's communication modality. The proposed prototype enables future studies to collect in-depth design critiques and preliminary usability evaluation from domain experts, novice ASL learners, and hearing parents with DHH children.},
	Author = {A. {Tenesaca} and J. Y. {Oh} and C. {Lee} and W. {Hu} and Z. {Bai}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00048},
	Keywords = {augmented reality;handicapped aids;interactive systems;natural language processing;sign language recognition;proof-of-concept visual augmentation prototype;parent-child interaction;American Sign Language;hearing parents;DHH children;language deprivation;linguistic-rich interaction;augmented reality lamp metaphor;deaf and hard of hearing children;Assistive technology;Auditory system;Visualization;Gesture recognition;Linguistics;Prototypes;Videos;Multimedia-Information-Systems;Artificial,-augmented,-virtual-realities;Prototyping},
	Month = {Oct},
	Pages = {431-434},
	Title = {Augmenting Communication Between Hearing Parents and Deaf Children},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00048}}

@inproceedings{8951991,
	Abstract = {Facial expression is considered as the most intuitive and effective way of conveying one's emotion among other nonverbal interactions.However, people with autism have limited access to this rich communication channel due to their inability to read facial expressions. To help them be aware of others' emotions, we developed a CNN-based facial expression recognition system using Microsoft Hololens and explored three different modes for displaying facial expressions of a conversation partner varying the levels of explicitness. Subjective feedback from a preliminary study with 6 pilot participants suggests that each mode is worth investigating for serving people with various needs and preferences who wish to receive augmented visual hints on others' emotion.},
	Author = {S. {Chung} and U. {Oh}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00049},
	Keywords = {augmented reality;convolutional neural nets;emotion recognition;face recognition;autism;CNN-based facial expression recognition system;design space exploration;augmented display;Microsoft Hololens;Facial-expressions;emotion-recognition;autism;mixed-reality},
	Month = {Oct},
	Pages = {435-437},
	Title = {Exploring the Design Space of an Augmented Display for Conveying Facial Expressions for People with Autism},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00049}}

@inproceedings{8951961,
	Abstract = {Wearable computing devices are small enough that they can be worn on the body and are a constant companion to the user. While many wearable devices have been associated with monitoring health or managing diseases, head-mounted displays are traditionally linked to Augmented and Virtual Reality, and generally overlay 3D information that supports professionals or for edutainment. This is surprising as prescription glasses, their traditional siblings, are widely accepted as a standard device for managing focusing errors of the human eye. In this work, we want to make the case for Computational Glasses that utilise technologies from optical see-through head-mounted displays or computational optics to compensate visual impairments. We will introduce some of the seminal works in the field as well as introduce our own work in the field. We will also include some of the challenges for doing research on Computational Glasses as well as give an outlook for future developments.},
	Author = {J. {Sutton} and T. {Langlotz} and Y. {Itoh}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00050},
	Keywords = {augmented reality;handicapped aids;helmet mounted displays;wearable computers;Computational Glasses;vision augmentations;wearable computing devices;health monitoring;optical see-through head-mounted displays;prescription glasses;standard device;human eye;focusing error management;computational near-eye optics;disease management;visual impairment compensation;virtual reality;augmented reality;overlay 3D information;edutainment;Glass;Prototypes;Cameras;Adaptive optics;Visualization;Image color analysis;Computational-Glasses;Augmented-Human;OSTHMD;Near-Eye-Optics;Near-Eye-Display;Vision-Aid;Vision-Augmentation;Head-mounted-Displays},
	Month = {Oct},
	Pages = {438-442},
	Title = {Computational Glasses: Vision Augmentations Using Computational Near-Eye Optics and Displays},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00050}}

@inproceedings{8951894,
	Abstract = {E-learning systems are still not very accessible for persons with disabilities, particularly those with cognitive impairments. It's well known that the training deficit is one of the cause of lower employment rates. In the past, we have addressed this issue by working on the accessibility of MOOCs. We have developed Aiana, a MOOC player with accessibility features based on the fragmentation of information streams and enabling user interface self-configuration. We are starting a new research program focused on the accessibility of immersive Serious Games for persons with cognitive impairments by first transposing some of Aiana's design principles. We believe that immersive Serious Games can provide effective assistance to learning for PWDs and we want to demonstrate this rigorously through large field studies. More generally, we wonder about the questions raised by the accessibility of Mixed Reality tools in immersive e-learning systems.},
	Author = {P. {Guitton} and H. {Sauz{\'e}on} and P. {Cinquin}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00051},
	Keywords = {cognition;computer based training;educational courses;handicapped aids;human computer interaction;serious games (computing);virtual reality;immersive serious games;cognitive impairments;training deficit;employment rates;MOOC player;immersive e-learning systems;Aiana design principles;user interface self-configuration;person with cognitive disabilities;information stream fragmentation;PWDs;mixed reality tools;Augmented reality;accessibility;e-learning;serious-games;immersion},
	Month = {Oct},
	Pages = {443-447},
	Title = {Accessibility of Immersive Serious Games for Persons with Cognitive Disabilities},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00051}}

@inproceedings{8951905,
	Abstract = {Facial expressions are an important part of human communication. However, children with autism spectrum disorders (ASD) are often suffering from difficulties of understanding non-verbal cues and form appropriate responses. Traditional approaches including labeling formatted photographs of human facial expressions from a third person's perspective could help them learn and improve such skills. Yet such training systems are often in lack of real time feedback. As Optical See-Through (OST) Augmented Reality (AR) headsets possess the advantages of near-eyes display and better depth alignment between virtual renderings and the environment, we decided on an OST AR approach for the system. In this paper, we present a system designed for OST AR headsets that occludes the subject's facial expressions with an emotion-presenting 3D emoji model. We hope this system could help us understand how children with ASD perceive emotions through standard emotion presenting systems and help them enhance their skills of understanding facial expressions.},
	Author = {R. {Sun} and H. {Haraldsson} and Y. {Zhao} and S. {Belongie}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00052},
	Keywords = {augmented reality;emotion recognition;face recognition;handicapped aids;solid modelling;children with autism spectrum disorders;human communication;children with ASD;human facial expressions;OST AR headsets;anon-emoji;facial emotions;emotion-presentation 3D emoji model;optical see-through augmented reality headsets;Rendering (computer graphics);Headphones;Head;Autism;Image color analysis;Augmented reality;Three-dimensional displays;Augmented-Reality;Optical-see-through;Head-pose-tracking;Autism-spectrum-disorders;Emotion-understanding;Emoji},
	Month = {Oct},
	Pages = {448-450},
	Title = {Anon-Emoji: An Optical See-Through Augmented Reality System for Children with Autism Spectrum Disorders to promote Understanding of Facial Expressions and Emotions},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00052}}

@inproceedings{8951960,
	Abstract = {Too often, the accessibility of technology to people with disabilities is an afterthought (if it is considered at all); post-hoc or third-party patches to accessibility, while better than no solution, are less optimal than interface designs that consider ability-based concerns from the start [31]. Virtual Reality (VR) technologies are at a crucial point of near-maturity, with emerging, but not yet widespread, commercialization; as such, VR technologies have an opportunity to integrate accessibility as a fundamental, developing cross-industry standards and guidelines to ensure high-quality, inclusive experiences that could revolutionize the power and reach of this medium. In this position paper, we discuss the needs, opportunities, and challenges of creating accessible VR.},
	Author = {M. {Mott} and E. {Cutrell} and M. {Gonzalez Franco} and C. {Holz} and E. {Ofek} and R. {Stoakley} and M. {Ringel Morris}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00122},
	Keywords = {handicapped aids;user interfaces;virtual reality;third-party patches;interface designs;Virtual Reality technologies;VR technologies;post-hoc patches;cross-industry standards;cross-industry guidelines;people with disabilities;Three-dimensional displays;Standards;Hardware;Headphones;Virtual reality;Metadata;Rendering (computer graphics);Virtual-Reality,-Mixed-Reality,-accessibility},
	Month = {Oct},
	Pages = {451-454},
	Title = {Accessible by Design: An Opportunity for Virtual Reality},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00122}}

@inproceedings{8952002,
	Abstract = {This paper explores the junctional juncture between playable digital interventions and urban public spatial design. Ubiquitous mobile digital devices and their pervasiveness in daily life are creating a shift in the traditional uses of urban public spaces and redefining how play occurs in urban public spaces. Activating possibilities of play in public space encourages people to go on a spontaneous and amorphous way of discovering new things in daily life relying on awareness, taking one's time, reflecting, experimenting, exploring, etc. Utilising the framework of playable digital interaction and exploring urban spatial environment narrative literature, twenty-five existing and proposed playable digital interventions in urban public space are studied to develop an initial typology. This typology includes characteristics such as diverse interactive mechanism, experience, and spatial features. This classification of playful digital interventions in urban public spaces will enable urban planners, designers, decision-makers, game designers, and researchers to realise the great potential and conceptualize the design of future digital playable installations for public spaces.},
	Author = {K. {Chen} and K. {Gonsalves} and M. {Guaralda} and S. {Turkay} and J. {Kerr}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00123},
	Keywords = {augmented reality;computer games;human computer interaction;mobile computing;smart cities;town and country planning;playable digital interventions;urban public space;ubiquitous mobile digital devices;playable digital interaction;playable city;pervasive games;augmented reality interaction;Playable-digital-intervention;public-space;typology;playable-city},
	Month = {Oct},
	Pages = {455-459},
	Title = {Towards a Typology for Playable Digital Interventions in Urban Public},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00123}}

@inproceedings{8951917,
	Abstract = {In-lab driving simulators have been favored by human-vehicle interaction researchers and interface designers due to their capabilities in unfettered construction of surrounding environments. On-road driving simulators, meanwhile, were developed for experiments in a real-world context. We introduce new methods and tools for simulating driving via virtual vehicle in real-world environments. The proposed system fully exploits the benefits of vivid sensory information in real road environments without sacrificing the unlimited potential of environment construction for it. In this paper, we explain the system design of the platform, and we verify its capabilities as a driving simulator through a pilot study (N=10). The proposed platform is perceived as highly realistic and can easily be augmented to test new urban landscapes and futuristic interfaces before occupancies.},
	Author = {D. {Yeo} and G. {Kim} and S. {Kim}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00124},
	Keywords = {computer simulation;driver information systems;ergonomics;virtual reality;virtual vehicle;real-world environments;vivid sensory information;road environments;unlimited potential;environment construction;driving simulator;futuristic interfaces;MAXIM;mixed-reality automotive driving XIMulation;human-vehicle interaction researchers;interface designers;unfettered construction;surrounding environments;real-world context;on-road driving simulators;in-lab driving simulators;Autonomous vehicles;Automobiles;Solid modeling;Resists;Virtual reality;Cameras;Immersive-technology;Mixed-reality;autonomous-vehicles;on-road-simulation;methodology},
	Month = {Oct},
	Pages = {460-464},
	Title = {MAXIM: Mixed-reality Automotive Driving XIMulation},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00124}}

@inproceedings{8951927,
	Abstract = {This article shows an overview of an immersive VR application to explore, to analyze and to mark large-scale photogrammetric 3D models. The objective is to interrogate the effectiveness of the proposed navigation process and annotation tools for spatial understanding. Due to the amount of interactions, different kinds of menus were necessary: desktop coordination menu, radial menu attached to the controller and 3D spatial menu for creating markers. Besides the menus, the navigation tasks through different perception space scales required a great number of interactions metaphors, patterns and techniques displaying the complexity of the user experience in Virtual Reality for understanding and analyzing urban digital twins. Those interactions allowed by the user interface were then analysed and classifyed according to a theoretical background and were experimented in preliminary tests with end users. Although designed for particular needs of the army, the tools and interactions can be adapted for city models explorations and urban planning. For future steps of the research, a usability study is going to be performed to test the performance of the interface and to have more end users feedback.},
	Author = {M. {Lima Medeiros}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00125},
	Keywords = {graphical user interfaces;human computer interaction;interactive devices;photogrammetry;solid modelling;town and country planning;user interfaces;virtual reality;perception space scales;end users feedback;urban planning;city models explorations;user interface;urban digital twins;user experience;3D spatial menu;radial menu;desktop coordination menu;annotation tools;navigation process;large-scale photogrammetric 3D models;immersive VR application;virtual reality;multiple space scales;Solid modeling;Three-dimensional displays;Navigation;Tools;Annotations;Virtual reality;Visualization;Immersive-User-Interface,-Interaction-pattern,-Interaction-technique,-Digital-Urban-Twins},
	Month = {Oct},
	Pages = {465-469},
	Title = {Marking the City: Interactions in Multiple Space Scales in Virtual Reality},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00125}}

@inproceedings{8951963,
	Abstract = {With advances in information and communication technologies, cities are getting smarter to enhance the quality of human life. In smart cities, safety (including security) is an essential issue. In this paper, by reviewing several safe city projects, smart city facilities for the safety are presented. With considering the facilities, a design for a crime intelligence system is introduced. Then, concentrating on how to support police activities (i.e., emergency call reporting reception, patrol activity, investigation activity, and arrest activity) with immersive technologies in order to reduce a crime rate and to quickly respond to emergencies in the safe city, smart policing with augmented reality (AR) and virtual reality (VR) is explained.},
	Author = {J. {Bang} and Y. {Lee} and Y. {Lee} and W. {Park}},
	Booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct.2019.00126},
	Keywords = {augmented reality;domestic safety;police data processing;smart cities;town and country planning;immersive technologies;crime rate;smart policing;safe city projects;smart city facilities;crime intelligence system;police activities;information and communication technologies;augmented reality;virtual reality;Smart cities;Safety;Law enforcement;Security;Government;Cameras;Smart-Policing;Augmented-Reality;Virtual-Reality;Safe-City},
	Month = {Oct},
	Pages = {470-475},
	Title = {AR/VR Based Smart Policing For Fast Response to Crimes in Safe City},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct.2019.00126}}
