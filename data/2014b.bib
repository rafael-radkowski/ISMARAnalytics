@INPROCEEDINGS{6948486,
author={J. P. {Lima} and R. {Roberto} and J. M. {Teixeira} and V. {Teichrieb}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[DEMO] Device vs. user-perspective rendering in AR applications for monocular optical see-through head-mounted displays},
year={2014},
volume={},
number={},
pages={355-356},
abstract={This demonstration allows visitors to use AR applications for monocular optical see-through head-mounted displays with two forms of visualization. One is the device-perspective approach, in which the user sees the virtual content registered with the camera image at the display. The other is the user-perspective method, in which the display is used as a de facto optical see-through device and the virtual content is registered with the real world.},
keywords={device-perspective rendering;user-perspective rendering;optical see-through head-mounted displays;augmented reality},
doi={10.1109/ISMAR.2014.6948486},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948487,
author={K. {Matsumoto} and W. {Nakagawa} and F. {de Sorbier} and M. {Sugimoto} and H. {Saito} and S. {Senda} and T. {Shibata} and A. {Iketani}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[DEMO] RGB-D-T camera system for AR display of temperature change},
year={2014},
volume={},
number={},
pages={357-358},
abstract={The anomalies of power equipment can be founded using temperature changes compared to its normal state. In this paper we present a system for visualizing temperature changes in a scene using a thermal 3D model. Our approach is based on two precomputed 3D models of the target scene achieved with a RGB-D camera coupled with the thermal camera. The first model contains the RGB information, while the second one contains the thermal information. For comparing the status of the temperature between the model and the current time, we accurately estimate the pose of the camera by finding keypoint correspondences between the current view and the RGB 3D model. Knowing the pose of the camera, we are then able to compare the thermal 3D model with the current status of the temperature from any viewpoint.},
keywords={temperature;RGB-D camera;thermal camera. Viewpoint Generative Learning},
doi={10.1109/ISMAR.2014.6948487},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948488,
author={B. {Meden} and S. {Knödel} and S. {Bourgeois}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[DEMO] Markerless augmented reality solution for industrial manufacturing},
year={2014},
volume={},
number={},
pages={359-360},
abstract={We present a comprehensive augmented reality solution to efficiently perform different verification pocedures during industrial assembly-line manufacturing using CAD data from product lifecycle management (PLM) systems. The demonstration focuses on a variety of industrial use-cases that have to go through different control processes, assisted by augmented reality tools. Thus, the experience has to be precise, robust to natural movements and easy to realise for an assembly-line technician.},
keywords={},
doi={10.1109/ISMAR.2014.6948488},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948489,
author={D. {Molyneaux} and S. {BenHimane}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[DEMO] “It's a Pirate's Life” AR game},
year={2014},
volume={},
number={},
pages={361-362},
abstract={We present “It's a Pirate's Life” demonstration, an Augmented Reality (AR) game which makes use of real-time 3D reconstruction and tracking using an Intel® RealSense™ camera system embedded in a tablet to build a dynamic game world. Players can play as a pirate ship captain searching for gold on a virtual sea overlaid on the real-world. Real-world objects become part of the play space; islands in the tropical seas which you have to navigate your ships around while avoiding cannon balls to find the treasure. Players control the wind, and hence, direction of sail by moving the tablet around the play space to guide the virtual ship in the real and virtual environment to the pirate gold.},
keywords={Real-time 3D reconstruction;Marker-less tracking;Depth cameras;GPU;SLAM;Augmented Reality;Gaming},
doi={10.1109/ISMAR.2014.6948489},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948490,
author={H. {Park} and T. {Kim} and J. {Park}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[DEMO] QubeAR: Cube style QR code AR interaction},
year={2014},
volume={},
number={},
pages={363-364},
abstract={QR code, for its recognition robustness and data capacity, has been often used for Augmented Reality applications as well as for other commercial applications. However, it is difficult to enable tangible interactions through which users may change 3D models or animations. It is because QR codes are automatically generated by the rules, and are not easily modifiable. Our goal was to enable QR code based Augmented Reality interactions. By analysis and through experiments, we discovered that some parts of a QR code can be altered to change the text string that the QR code represents. In this demo, we introduced a prototype for QR code based Augmented Reality interactions, which allows for Rubik's cube style rolling interactions.},
keywords={QR code;Interaction;Augmented Reality},
doi={10.1109/ISMAR.2014.6948490},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948491,
author={T. {Piumsomboon} and A. {Clark} and M. {Billinghurst}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[DEMO] G-SIAR: Gesture-speech interface for augmented reality},
year={2014},
volume={},
number={},
pages={365-366},
abstract={We demonstrate an Augmented Reality (AR) system that utilizes a combination of direct free hand interaction and indirect multimodal gesture and speech interface. A three-dimensional (3D) design sandbox application, featuring online object creation, has been developed to illustrate the use case of our system that supports dual interaction techniques.},
keywords={Augmented reality;natural interaction;multimodal interface},
doi={10.1109/ISMAR.2014.6948491},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948492,
author={R. F. {Salas-Moreno} and B. {Glocker} and P. H. J. {Kelly} and A. J. {Davison}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[DEMO] Dense planar SLAM},
year={2014},
volume={},
number={},
pages={367-368},
abstract={Using higher-level entities during mapping has the potential to improve camera localisation performance and give substantial perception capabilities to real-time 3D SLAM systems. We present an efficient new real-time approach which densely maps an environment using bounded planes and surfels extracted from depth images (like those produced by RGB-D sensors or dense multi-view stereo reconstruction). Our method offers the every-pixel descriptive power of the latest dense SLAM approaches, but takes advantage directly of the planarity of many parts of real-world scenes via a data-driven process to directly regularize planar regions and represent their accurate extent efficiently using an occupancy approach with on-line compression. Large areas can be mapped efficiently and with useful semantic planar structure which enables intuitive and useful AR applications such as using any wall or other planar surface in a scene to display a user's content.},
keywords={Computing methodologies [Scene understanding];Computing methodologies [Reconstruction]. Computing methodologies [Image Processing and Computer Vision]: Segmentation. Information Systems [Information Interfaces and Presentation];Artificial;augmented;virtual realities},
doi={10.1109/ISMAR.2014.6948492},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948493,
author={D. {Schattel} and M. {Tönnis} and G. {Klinker} and G. {Schubert} and F. {Petzold}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Demo] On-site augmented collaborative architecture visualization},
year={2014},
volume={},
number={},
pages={369-370},
abstract={The early design phase for a new building is a crucial stage in the design process of architects. It has to be ensured that the building fits into the future environment. The Collaborative Design Platform targets this issue by integrating modern digital means with well known traditional concepts. Well-used styrofoam blocks are still cut by hand but are now tracked, placed and visualized in 3D by use of a tabletop platform and a TV screen showing an arbitrary view of the scenery. With this demonstration, we get one step further and provide an interactive visualization at the proposed building site, further enhancing collaboration between different audiences. Mobile phones and tablet devices are used to visualize marker-less registered virtual building structures and immediately show changes made to the models in the Collaborative Design laboratory. This way, architects can get a direct impression about how a building will integrate within the environment and residents can get an early impression about future plans.},
keywords={H.5.m [Information interfaces and presentation];Miscellaneous},
doi={10.1109/ISMAR.2014.6948493},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948494,
author={S. {Siltanen} and H. {Saraspää} and J. {Karvonen}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[DEMO] A complete interior design solution with diminished reality},
year={2014},
volume={},
number={},
pages={371-372},
abstract={We demonstrate an interior design solution with advanced features, most importantly, a diminished reality feature. The diminished reality functionality takes 3D indoor structures into account and adapts to the lighting of the environment. We present two demonstrations on tablet and laptop. The iPad version utilizes touch screen for selecting removed objects from the image. It allows users to build modular furniture and it casts shadows of the virtual furniture for realistic visualization result. On laptop PC we demonstrate real time diminished reality for indoor AR with several options. Our demonstrations are optimized for interior design and indoor AR.},
keywords={Augmented reality;AR;diminished reality;indoor AR;interior design;object removal;illumination adaptation;realtime;user interaction;tablet PC;iPad;touch screen},
doi={10.1109/ISMAR.2014.6948494},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948495,
author={D. {Stanimirovic} and D. {Kurz}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Demo] Smartwatch-aided handheld augmented reality},
year={2014},
volume={},
number={},
pages={373-374},
abstract={We demonstrate a novel method for interaction of humans with real objects in their surrounding combining Visual Search and Augmented Reality (AR). This method is based on utilizing a smart-watch tethered to a smartphone, and it is designed to provide a more user-friendly experience compared to approaches based only on a handheld device, such as a smartphone or a tablet computer. The smartwatch has a built-in camera, which enables scanning objects without the need to take the smartphone out of the pocket. An image captured by the watch is sent wirelessly to the phone that performs Visual Search and subsequently informs the smartwatch whether digital information related to the object is available or not. As described in our ISMAR 2014 poster [6], we distinguish between three cases. If no information is available or the object recognition failed, the user is notified accordingly. If there is digital information available that can be presented using the smartwatch display and/or audio output, it is presented there. The third case is that the recognized object has digital information related to it, which would be beneficial to see in an Augmented Reality view spatially registered with the object in real-time. Then the smartwatch informs the user that this option exists and encourages using the smartphone to experience the Augmented Reality view. Thereby, the user only needs to take the phone out of the pocket in case Augmented Reality content is available, and when the content is of interest for the user.},
keywords={},
doi={10.1109/ISMAR.2014.6948495},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948496,
author={T. {Stütz} and R. {Dinic} and M. {Domhardt} and S. {Ginzinger}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Demo] A mobile augmented reality system for portion estimation},
year={2014},
volume={},
number={},
pages={375-376},
abstract={Accurate assessment of nutrition information is an important part in the prevention and treatment of a multitude of diseases, but remains a challenging task. We present a novel mobile augmented reality application, which assists users in the nutrition assessment of their meals. The user sketches the 3D form of the food and selects the food type. The corresponding nutrition information is automatically computed.},
keywords={H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, augmented, virtual realities;I.4.9 [Image Processing and Computer Vision]: Applications;J.3 [Computer Applications];Life and medical Sciences — Health},
doi={10.1109/ISMAR.2014.6948496},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948497,
author={M. {Tönnis} and G. {Klinker}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Demo] Placing information near to the gaze of the user},
year={2014},
volume={},
number={},
pages={377-378},
abstract={Gaze tracking facilities have yet mainly been used in general for marketing or the disabled and, more specifically, in Augmented Reality, for interaction with control triggers, such as buttons. We go one step further and use the line of sight of the user to attach information. While any information may not conceal the view of the user, we displace the information by an angular degree and provide means for the user to capture the information by looking at it. With such an apporach we see a potential for faster resuming times of the original task for which a required information needs to be accessed. The demonstration shows a comparably complex primary task assisted by our gaze-mounted information and illustrates the inherent differences for information access w.r.t. conventional methods, such as listing action items at a fix position in space or on a screen.},
keywords={H.5.m [Information interfaces and presentation];Miscellaneous},
doi={10.1109/ISMAR.2014.6948497},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948498,
author={J. F. {Vigueras-Gomez}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[DEMO] Fast vision-based multiplanar scene modeling in unprepared environments},
year={2014},
volume={},
number={},
pages={379-380},
abstract={In this demonstration, we present a general purpose Augmented Reality (AR) system that allows to add easily 3D computer generated (CG) objects into real man-made environments. Our system goes to a very intuitive and easy in situ 3D structure recovery of planar piecewise scenes without using powerful hardware nor commodity sensors. The user simply has to move the camera (translation of the camera is mandatory) and take two different pictures of the scene, and our approach obtains a rough planar piecewise representation of the environment suitable to conduct multi-planar tracking for visual model-based augmented reality and to augment it with virtual objects coherently. Polyhedral representations of scenes are very convenient for manmade environments indoor (e.g., offices, rooms, classrooms) and outdoor (e.g., facades, floor), hence we focus the potential applications of our system to augment simple rooms or urban scenes with virtual imagery.},
keywords={Augmented reality;structure from motion;scene-based modeling;multiplanar structure;geometric coherence},
doi={10.1109/ISMAR.2014.6948498},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948499,
author={D. {Wagner} and G. {Reitmayr} and A. {Mulloni} and E. {Mendez} and S. {Diaz}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[DEMO] Mobile augmented reality — 3D object selection and reconstruction with an RGBD sensor and scene understanding},
year={2014},
volume={},
number={},
pages={381-381},
abstract={In this proposal we show case two 3D reconstruction systems running in real-time on a tablet equipped with a depth sensor. We believe that the proposed set of demonstrations will engage ISMAR attendees both in terms of tracking technology and user experience. Both demos show state-of-the art 3D reconstruction technology and give attendees a chance to try hands-on our tracking with simple and interactive user interfaces.},
keywords={Reconstruction;Scene understanding;Rendering},
doi={10.1109/ISMAR.2014.6948499},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948500,
author={D. {Wagner} and G. {Reitmayr} and A. {Mulloni} and E. {Mendez} and S. {Diaz}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[DEMO] Mobile augmented reality — Tracking, mapping and rendering},
year={2014},
volume={},
number={},
pages={383-383},
abstract={In this proposal we suggest a set of demonstrations to be presented at ISMAR 2014 around the topic of monocular tracking, mapping and rendering on mobile phones. We believe that the proposed set of demonstrations will engage ISMAR attendees both in terms of tracking technology and user experience. On the one hand, all demos show state-of-the art tracking and mapping technology and gives attendees a chance to try hands-on our tracking solutions and to discuss them with our researchers and developers. All demos also showcase compelling use cases enabled by our technology, encouraging discussions on topics such as integration of tracking technology with game engines and user experience research.},
keywords={Tracking;Reconstruction;Rendering},
doi={10.1109/ISMAR.2014.6948500},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948501,
author={F. {Wientapper} and T. {Engelke} and J. {Keil} and H. {Wuest} and J. {Mensik}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[DEMO] User friedly calibration and tracking for optical stereo see-through augmented reality},
year={2014},
volume={},
number={},
pages={385-386},
abstract={Optical see through head mounted displays (OST-HMD) are ever since the first days of Augmented Reality (AR) in focus of development and in nowadays first affordable and prototypes are spread out to markets. Despite common technical problems, such as having a proper field of view, weight, and other problems concerning the miniaturization of these systems, a crucial aspect for AR relies also in the calibration of such a device with respect to the individual user for proper alignment of augmentations. Our demonstrator shows a practical solution for this problem along with a fully featured example application for a typical maintenance use case based on a generalized framework for application creation. We depict the technical background and procedure of the calibration, the tracking approach considering the sensors of the device, user experience factors, and its implementation procedure in general. We present our demonstrator using an Epson Moverio BT-200 OST-HMD.},
keywords={H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, augmented, virtual realities;I.4.1 [Image Processing and Computer Vision]: Digitization and Image Capture — Camera calibration;I.4.8 [Image Processing and Computer Vision]: Scene Analysis — Tracking;G.1.6 [Numerical Analysis]: Optimization — Least squares methods},
doi={10.1109/ISMAR.2014.6948501},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948502,
author={H. {Yoshida} and T. {Okamoto} and H. {Saito}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[DEMO] Tablet system for visual, overlay of 3D virtual object onto real environment},
year={2014},
volume={},
number={},
pages={387-388},
abstract={We propose a novel system for visual overlay of 3D virtual object onto real environment observed by tablet PC with camera. This system allows us to visually simulate the layout of virtual 3D objects such as furniture in the real environment captured by the tablet PC. For estimating the pose and position of the tablet PC in the 3D structure of the target environment, we propose and implement the 2 procedures using the captured image and using the motion sensor in tablet PC. Those performances are presented in the demonstration.},
keywords={object preview;space recognition;plane arrangement;object translation},
doi={10.1109/ISMAR.2014.6948502},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948503,
author={Y. {Yoshida} and T. {Kawamoto}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[DEMO] Displaying free-viewpoint video with user controlable head mounted display DEMO},
year={2014},
volume={},
number={},
pages={389-390},
abstract={In this paper, we propose a method to experience a free-viewpoint video and image with a head mounted display (HMD) and a game controller that enable to operate it intuitively. The free-viewpoint video is generated by multiple 4K resolution cameras in sport games such as soccer and american football. This method can provide us a player's perspective. We adopt a billboard method to make a free-viewpoint video which is consisted of multiple textures accoding to user specified viewpoint. For implementing the program and displaying images effectively we used a game development system, a HMD and a game controller that user can operate their own views in the high degree of freedom. Experiment results show that the proposed method can obviously achive effective view.},
keywords={head mounted display;free-viewpoint video;billboard},
doi={10.1109/ISMAR.2014.6948503},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948504,
author={F. {Cutolo} and P. D. {Parchi} and V. {Ferrari}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Video see through AR head-mounted display for medical procedures},
year={2014},
volume={},
number={},
pages={393-396},
abstract={In the context of image-guided surgery (IGS), AR technology appears as a significant development in the field since it complements and integrates the concepts of surgical navigation based on virtual reality. The aim of the project is to optimize and validate an ergonomic, accurate and cheap video see-through AR system as an aid in various typologies of surgical procedures. The system will ideally have to be inexpensive and user-friendly to be successfully introduced in the clinical practice.},
keywords={Mixed / augmented reality;Image-guided surgery;Medical device validation;Interest point and salient region detections;Object detection;Tracking},
doi={10.1109/ISMAR.2014.6948504},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948505,
author={A. {Plopski} and K. {Kiyokawa} and H. {Takemura} and C. {Nitschke}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Corneal imaging in localization and HMD interaction},
year={2014},
volume={},
number={},
pages={397-400},
abstract={The human eyes perceive our surroundings and are one of, if not our most important sensory organs. Contrary to our other senses the eyes not only perceive but also provide information to a keen observer. However, thus far this has been mainly used to detect reflection of infrared light sources to estimate the user's gaze. The reflection of the visible spectrum on the other hand has rarely been utilized. In this dissertation we want to explore how the analysis of the corneal image can improve currently available eye-related solutions, such as calibration of optical see-through head-mounted devices or eye-gaze tracking and point of regard estimation in arbitrary environments. We also aim to study how corneal imaging can become an alternative for established augmented reality tasks such as tracking and localization.},
keywords={optical see-through head mounted display;augmented reality;corneal imaging},
doi={10.1109/ISMAR.2014.6948505},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948506,
author={D. {Rumiński} and K. {Walczak}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Semantic contextual augmented reality environments},
year={2014},
volume={},
number={},
pages={401-404},
abstract={The paper presents the concept of dynamic Contextual Augmented Reality Environments (CARE), in which augmentation presented to users is dynamically constructed based on four semantically described elements. The first element is the user's context (preferences, privileges, location, time, device's capabilities). The second element is a set of trackables — visual markers representing real world objects that can be augmented for a given user in a given context. The third element are content objects, representing interactive 2D and 3D multimedia content including video sequences and sounds to be presented on the trackables. The last one is a description of a user interface, which may be specific to a concrete device or application and which indicates the forms of information presentation and interaction available to a user.},
keywords={},
doi={10.1109/ISMAR.2014.6948506},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948507,
author={J. {Weigel} and S. {Viller} and M. {Schulz}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Designing support for collaboration around physical artefacts: Using augmented reality in learning environments},
year={2014},
volume={},
number={},
pages={405-408},
abstract={The aim of this thesis is to identify mechanisms for supporting collaboration around physical artefacts in co-located and remote settings. To explore the research question in the project, a Research through Design approach has been adopted. A technology probe — an evolutionary prototype of a remote collaboration system — will be used to fuel the research. The prototype will facilitate collaboration between small groups around physical artefacts in an augmented learning environment. The prototype will inform future collaborative augmented reality technology design.},
keywords={Collaboration;augmented reality;remote learning},
doi={10.1109/ISMAR.2014.6948507},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948508,
author={F. {Angermann} and M. {Krushwitz}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={AR development with the Metaio product suite: Demonstration of use cases in industry},
year={2014},
volume={},
number={},
pages={1-1},
abstract={This tutorial covers the AR creation process from idea to solution in an industrial context. Thereby employing the Metaio AR pipeline with practical demonstrations to show how AR projects can be quickly realized with readily available tools. To illustrate how AR can be employed to solve industrial problems, this tutorial will showcase several interesting industrial use cases implemented by Metaio. To conclude, the tutorial provides a brief outlook into new available tracking technologies and AR for wearable devices. Further, potential challenges for industrial integration of AR, for example with ERP systems, are pointed out. The attendees will acquire a solid understanding of developing an AR scenario from idea to solution. Further the tutorial provides a good insight into the Metaio software environment, enabling and hopefully encouraging attendees to implement their own AR projects in industry.},
keywords={Tutorials;Software;Industries;Pipelines;Abstracts;Context;Solids},
doi={10.1109/ISMAR.2014.6948508},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948509,
author={U. {Bockholt}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Fusing web technologies augmented reality},
year={2014},
volume={},
number={},
pages={1-1},
abstract={Within the German research project ARVIDA a large consortium of industrial Virtual and Augmented Reality users, of technology providing companies and research institutes cooperate on the establishment of highly flexible web-based reference architecture for Augmented Reality applications. The use of web technologies is motivated by modern web standards as WebGL or WebRTC supporting e.g. real time rendering of 3D-content of video streaming within Web-Browsers. Thereby, the use of Web technologies not only offers the possibility to develop applications platform and OS independent but it also facilitates the integration of Augmented Reality into industrial workflows or PDM environments. The developed reference architecture offers RESTful tracking, rendering and interaction services that foster the combination and exchange of different algorithms with the aim to ft the technology to the specific requirements of an AR-applications in an optimal way.},
keywords={Augmented reality;Educational institutions;Rendering (computer graphics);WebRTC;Tutorials;Context;Abstracts},
doi={10.1109/ISMAR.2014.6948509},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948510,
author={Y. {Oyamada}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={A ‘Look Into’ Medical augmented reality},
year={2014},
volume={},
number={},
pages={1-1},
abstract={The concept of augmented reality (AR) has been introduced to variety of felds in the last decade. Recent development of portable devices such as smart phone and tablet PC provides the community a lot of possible applications in AR systems. Even in the medical feld, various AR systems have recently been proposed: systems for education, pre-planning, and those in the operating room. The aim of this tutorial is to bridge the expertise between the researchers in ISMAR community and medical doctors so that researchers can contribute to the medical domain with their specialty more than one can do right now. This tutorial aims to make a bridge between researchers in augmented reality feld and medical doctors. We target an audience interested in medical augmented reality systems.},
keywords={Biomedical imaging;Augmented reality;Educational institutions;Tutorials;Bridges;Surgery},
doi={10.1109/ISMAR.2014.6948510},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948511,
author={M. {Melnykowycz}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Designing location-based experiences},
year={2014},
volume={},
number={},
pages={1-2},
abstract={The development of location based applications from the perspective of story structure and product design will be presented. We present the challenges with developing location based mobile products from a storytelling perspective and tools for integrating user experience into the development process to drive story structure of new products. Included is a casestudy focused on the Ghost of Venice mixedreality film project, which is centered on an augmented reality mobile application. Learning objectives of this tutorial are: Understand how communication patterns have evolved with new technologies to their present state and how this influences the way stories are told. • Understand the design intent behind different AR/MR location based games from the story and user experience design perspectives. • Gain an understanding for how to approach AR/MR projects, which may include distributed storylines over different media. • Understand the complexity of creating AR/MR location based applications and how to address them in app or story development. • Gain insight into how to work between writers and the app development (design and coding) team to efficiently translate story concepts into mobile apps. A workshop module is included at the end of the tutorial session, and at this point participants will be engaged to design a location based game experience. This will show in a project based learning environment, what the participants learned from the tuto­rial.},
keywords={Tutorials;Mobile communication;Games;Media;Conferences;Abstracts;Product design},
doi={10.1109/ISMAR.2014.6948511},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948512,
author={H. {Tamura}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Diminished reality as challenging extension of mixed and augmented reality},
year={2014},
volume={},
number={},
pages={1-1},
abstract={Diminished Reality (DR) has been considered as a sub-technology of Mixed and Augmented Reality (AR/MR). While AR/MR means technologies that add and/or overlay visual information onto images of real scene for providing users to enhance their visual experiences with the added/overlaid information, DR aims the similar enhanced visual experiences by deleting visual information from the images of real scene. Adding and deleting visual information might be considered as same technical issues, but they are actually totally different. In DR, visual information that is hidden by the deleted object should be recovered for filling into the deleted area. This recovery of the hidden area is not required for general adding/overlaying based AR/MR, but should be one of the typical issues for achieving DR. Camera pose estimation and tracking is a typical issue in AR/MR, but the condition of the scene and required performance for DR are not always the same as AR/MR. For example, the object to be diminished/removed should be detected and tracked while the camera is freely moving for DR. In this tutorial, challenging technical issues for DR are addressed, such as recovery of hidden area, detecting and tracking the object to be removed/diminished, tracking camera poses, illumination matching and re-lighting, etc. In addition to those technical issues for DR, a survey of applications of DR, expected futures with DR, and human factors of DR are also presented.},
keywords={Educational institutions;Visualization;Augmented reality;Cameras;Computer vision;Laboratories},
doi={10.1109/ISMAR.2014.6948512},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948513,
author={J. {Grubert}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Google glass, The META and Co. How to calibrate optical see-through head mounted displays},
year={2014},
volume={},
number={},
pages={1-2},
abstract={Head Mounted Displays such as Google Glass and the META have the potential to spur consumer-oriented Optical See-Through Augmented Reality applications. A correct spatial registration of those displays relative to a user's eye(s) is an essential problem for any HMD-based AR application. We provide an overview of established and novel approaches for the calibration of those displays including hands on experience in which participants will calibrate such head mounted displays. The following list provides a tentative list of topics covered during the tutorial. • Part 1: Introduction to OST calibration • Why OST Calibration is important? • Differences to Camera Calibration • Introduce camera calibration • Why is OST calibration hard? • The user in the loop — pointing accuracy • Slipping, the need for recalibration • Principal aspects of OST-HMD calibration • overview of data collection • confirmation methods • optimization • mono vs stereo • Details of OST calibration • Data collection methods: • SPAAM, Multi Point collection, stereo methods • Confirmation methods • Optimization approaches • Evaluation: perceptual measures vs. analytic measures • State of the art: Semi-, fully automatic calibration methodses • Part 2: Hands-on calibration • SPAAM-based calibration of Epson Moverio/ Google Glass with inside-out marker tracker.},
keywords={Calibration;Educational institutions;Google;Glass;Adaptive optics;Optical feedback;Augmented reality},
doi={10.1109/ISMAR.2014.6948513},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948514,
author={C. {Perey} and R. {Manson} and M. {Preda} and N. {Trevett} and M. {Lechner} and G. {Percivall} and T. {Engelke} and P. {Lefkin} and B. {Mahone} and M. L. {Nielsen}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Open and interoperable augmented reality},
year={2014},
volume={},
number={},
pages={1-3},
abstract={Today an experience developer must choose tools for authoring AR experiences based on many factors including ease of use, performance across a variety of platforms, reach and discoverability and cost. The commercially viable options are organized in closed technology silos (beginning with SDKs). A publisher of experiences must choose one or develop for multiple viewing applications, then promote one or more application to the largest possible audience. Developers of applications must then maintain the customized viewing application over time across multiple platforms or have the experience (and the application) expire at the end of a campaign.},
keywords={Augmented reality;Standards;Educational institutions;Tutorials;Geospatial analysis;Companies;Three-dimensional displays},
doi={10.1109/ISMAR.2014.6948514},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948515,
author={M. {Billinghurst}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={The glass class: Designing wearable interfaces},
year={2014},
volume={},
number={},
pages={1-2},
abstract={The course will teach how to create compelling user experiences for wearable computers focusing on design guidelines, prototyping tools, research directions, and a hands-on design experience. These topics will be presented using a number of platforms such as Google Glass, the Recon Jet and Vuzix M-100, although the material will be relevant to other wearable devices. The class will begin with an overview of almost 50 years of wearable computing, beginning with the casino computers of Ed Thorp, through the pioneering efforts of researchers at CMU and MIT, to the most recent commercial systems. The key technology components of a wearable system will be covered, as well as some of the theoretical underpinnings. Next, a set of design guidelines for developing wearable user interfaces will be presented. These include lessons learned from using wearables on a daily basis, design patterns from existing wearable interfaces, and relevant results from the research community. These will be presented in enough details that attendees will be able to use them in their own wearable designs. The third section of the course will introduce a number of tools that can be used for rapid prototyping of wearable interfaces. These include screen-building tools such as Glasssim, through to templating tools that support limited interactivity, and simple programming tools such as Processing. This will lead into a section that discusses the technology of wearable systems in more detail. For example, the different types of head mounted displays for wearables, tracking technology for wearable AR interfaces, input devices, etc. Finally, we will discuss active areas of research that will affect wearable interfaces over the next few years. This includes technologies such as new display hardware, input devices, body worn sensors, and connectivity. The course will have the following educational goals: • Provide an introduction to head mounted wearable computers • Give an understanding of current wearable computing technology • Describe key design principles/interface metaphors • Provide an overview of the relevant human perceptual principles • Explain how to use Processing for rapid prototyping • Show how to capturing and use sensor input • Outline active areas of research in wearable computing • Hands on demonstrations with Google Glass and other wearable computers.},
keywords={Wearable computers;Glass;Google;Educational institutions;Design methodology;Sensors},
doi={10.1109/ISMAR.2014.6948515},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948516,
author={J. {Howse}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Training detectors and recognizers in Python and OpenCV},
year={2014},
volume={},
number={},
pages={1-2},
abstract={Monty Python's Flying Circus had a “cat detector van” so, in this tutorial, we use Python and OpenCV to make our very own cat detector and recognizer. We also cover examples of human face detection and recognition. More generally, we cover a methodology that applies to training a detector (based on Haar cascades) for any class of object and a recognizer (based on LBPH, Fisherfaces, or Eigenfaces) for any unique objects. We build a small GUI app that enables an LBPH-based recognizer to learn new objects interactively in real time. Although this tutorial uses Python, the project could be ported to Android and iOS using OpenCV's Java and C++ bindings. Attendees will gain experience in using OpenCV to detect and recognize visual subjects, especially human and animal faces. GUI development will also be emphasized. Attendees will be guided toward additional information in books and online. There is no formal evaluation of attendees' work but attendees are invited to demonstrate their work and discuss the results they have achieved during the session by using different detectors and recognizers and different parameters.},
keywords={Training;Detectors;Tutorials;Graphical user interfaces;Face;Cats;Face recognition},
doi={10.1109/ISMAR.2014.6948516},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948517,
author={S. {Lukosch} and M. {Billinghurst} and K. {Kiyokawa} and L. {Alem}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Collaboration in mediated and augmented reality},
year={2014},
volume={},
number={},
pages={1-1},
abstract={In this half-day workshop we will explore how Augmented Reality (AR) and Mediated Reality (MR) can be used to develop radically new types of collaborative experiences that overcome some of the limitations of current conferencing systems. In combination, AR and MR technologies could be used to merge the shared perceived realities of different users as well as enriching their own individual experience in a collaborative task. The goal of the workshop is to bring together researchers who are interested developing collaborative systems using AR and MR technologies. They will build a picture of current and prior research on collaboration in AR and MR as well as set up a common research agenda for work going forward. Topics of the workshop will address open research issues and include but are not restricted to the following: •Case studies on using MR/AR for collaboration •Tools for building collaborative MR/AR systems •Effects of MR/AR on trust, presence, and coordination •Interaction models for collaboration in MR/AR •Tools for collaboration in MR/AR •Collaboration awareness in MR/AR.},
keywords={Collaboration;Educational institutions;Conferences;Augmented reality;Information systems;Computers},
doi={10.1109/ISMAR.2014.6948517},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948518,
author={C. {Perey} and F. {Wild} and K. {Helin} and M. {Janak} and P. {Davies} and P. {Ryan}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Advanced manufacturing with augmented reality},
year={2014},
volume={},
number={},
pages={1-1},
abstract={During this workshop, the participants will explore themes in three areas: • Augmented Reality and Technical Data Delivery. In this area we will hear from experts and engage in discussion to develop agreement among the workshop participants on what constitutes the state of the art for use of AR to deliver technical data in 2014. • Augmented Reality and the Shop Floor Environment. In this area we will engage in discussion to develop agreement among the workshop participants on what constitutes the state of the art for use of AR to map value streams and increase value, to avoid waste and increase sustainability, and to reduce risk and prevent human error on the shop floor in 2014. This will be documented in a brief, joint statement. Participants will then develop consensus on three major research areas on which there must be further investment in order for the technology to be widely implemented and adopted on the shop floor. • Augmented Reality and Quality Inspection. In this area we will engage in discussion to develop agreement among the workshop participants on what constitutes the state of the art for use of AR to inspect manufactured goods in 2014. This will be documented in a brief, joint statement. Participants will then develop consensus on three major research areas on which there must be further investment in order for the technology to be widely implemented and adopted. The three themes of the workshop are highly relevant to the ISMAR conference, not least since the region — Bavaria — strives in the use of AR in the manufacturing industry. This workshop will increase awareness of the current state of the art in industry and help further develop the research agenda for AR in manufacturing through the identification of common interests and grand challenges. This workshop will produce nine research topics that can become the basis for academic and public/private partnership-funded research projects.},
keywords={Augmented reality;Educational institutions;Manufacturing;Conferences;Communities;Joints;Investment},
doi={10.1109/ISMAR.2014.6948518},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948519,
author={J. {Ventura} and D. {Wagner} and D. {Kurz} and H. {Wuest} and S. {Benhimane}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Workshop on tracking methods applications},
year={2014},
volume={},
number={},
pages={1-2},
abstract={The focus of this workshop is on all issues related to tracking for mixed and augmented reality applications. Unlike the tracking sessions of the main conference, this workshop does not require pure novelty of the proposed methods; it rather encourages presentations that concentrate on complete systems and integrated approaches engineered to run in real-world scenarios. The research felds covered include self-localization using computer vision or other sensing modalities (such as depth cameras, GPS, inertial, etc.) and tracking systems issues (such as system design, calibration, estimation, fusion, etc.). This year's focus is also expanded to research on object detection and semantic scene understanding with relevance to augmented reality. Implementations on mobile devices and under real-time constraints are also part of the workshop focus. These are issues of core importance for practical augmented reality systems.},
keywords={Augmented reality;Educational institutions;Computer vision;Conferences;Cameras;Mobile handsets;Robot sensing systems},
doi={10.1109/ISMAR.2014.6948519},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6948520,
author={M. {Eder} and M. {Lechner} and T. {Stütz} and J. {Stadon}},
booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Hands free — Exploring AR glasses and their peculiarities},
year={2014},
volume={},
number={},
pages={1-1},
abstract={The workshop will focus on AR Glasses and how their new and unique interfaces change the way we use, develop, interact with and percept Augmented Reality. The main goal is to get a large amount of people interested in Augmented Reality Glasses to discuss particular topics of and issues with AR Glasses, and to understand what developers and users must change in the way they use AR on AR Glasses, compared to AR on smartphones, tablets and laptops.},
keywords={Glass;Augmented reality;Computer vision;Abstracts;Conferences;Smart phones;Portable computers},
doi={10.1109/ISMAR.2014.6948520},
ISSN={},
month={Sep.},}