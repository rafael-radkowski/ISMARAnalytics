@INPROCEEDINGS{5336523,
author={},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={[Title page]},
year={2009},
volume={},
number={},
pages={i-i},
abstract={The following topics are dealt with: mixed reality; user interfaces; rendering; human factors; tracking on mobiles and augmented reality.},
keywords={augmented reality;human factors;rendering (computer graphics);user interfaces;mixed reality;user interface;rendering;human factors;mobile tracking;augmented reality},
doi={10.1109/ISMAR.2009.5336523},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336524,
author={},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={[Copyright notice]},
year={2009},
volume={},
number={},
pages={ii-ii},
abstract={Presents the copyright information for the conference. May include reprint permission information.},
keywords={},
doi={10.1109/ISMAR.2009.5336524},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336520,
author={},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Contents},
year={2009},
volume={},
number={},
pages={iii-vi},
abstract={Presents the table of contents of the proceedings.},
keywords={},
doi={10.1109/ISMAR.2009.5336520},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336521,
author={V. {Vinge}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Mixed and augmented reality: “Scary and wondrous”},
year={2009},
volume={},
number={},
pages={vii-vii},
abstract={Summary form only given. Mixed and augmented reality is the nature of the supporting infrastructure.Its current incarnation is cloud computing versus microcontrollers distributed throughout the environment.Ubiquity has always been a catchword of the distributed processing enthusiasts, and each new generation has pushed the idea beyond the horizons of the previous generation. Imagine an environment where most physical objects know where they are, what they are, and can (in principle) network with any other object. With this infrastructure, reality becomes its own database.Multiple consensual virtual environments are possible, each oriented to the needs of its constituency. If we also have open standards, then bottom-up social networks and even bottom-up advertising become possible.Then the physical world becomes much more like a software construct. The possibilities are both scary and wondrous.},
keywords={augmented reality;distributed processing;microcontrollers;social networking (online);Web services;mixed reality;augmented reality;cloud computing;microcontroller;distributed processing;physical object;multiple consensual virtual environment;open standard;bottom-up social network;software construct;Augmented reality;Cloud computing;Microcontrollers;Distributed processing;Image databases;Object oriented databases;Virtual environment;Software standards;Social network services;Advertising},
doi={10.1109/ISMAR.2009.5336521},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336518,
author={},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Sponsors},
year={2009},
volume={},
number={},
pages={viii-xi},
abstract={The conference organizers greatly appreciate the support of the various corporate sponsors listed.},
keywords={},
doi={10.1109/ISMAR.2009.5336518},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336519,
author={J. P. {Rolland} and B. J. {Thompson} and C. {Stapleton}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={From the symposium general chairs},
year={2009},
volume={},
number={},
pages={xii-xii},
abstract={Mixed and Augmented Reality melts the boundaries between visceral physical reality and a dynamic virtual reality to enhance human performance and heighten experiences. It extends the power of our imagination to create and extends the efficacy of communication, and collaboration. Our symposium has expanded to represent and serve a large and diverse community. It is designed to address research challenges in science, business, professional trades, the arts, media and the humanities.},
keywords={},
doi={10.1109/ISMAR.2009.5336519},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336516,
author={G. {Klinker} and H. {Saito} and T. {Hollerer}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={From the Science and Technology program chairs},
year={2009},
volume={},
number={},
pages={xiii-xiv},
abstract={While ISMAR is expanding and reaching out to new communities, we proudly continue to present to you the best research papers in the field of Mixed and Augmented Reality. The technical program of ISMAR 2009, in the tradition of the proceedings of seven previous ISMAR, two ISAR, two ISMR, and two IWAR meetings, this year takes the form of a Science and Technology (S&T) track. This program is comprised of 24 papers, 28 posters, as well as an array of keynote talks, demonstrations, tutorials, workshops and the tracking competition. All of these elements of the program are the result of dedicated hard work by members of the conference committee and additional volunteers, and we would like to recognize their efforts.},
keywords={},
doi={10.1109/ISMAR.2009.5336516},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336517,
author={},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={IEEE Visualization and Graphics Technical Committee (VGTC)},
year={2009},
volume={},
number={},
pages={xv-xv},
abstract={Provides a listing of current committee members.},
keywords={Computer graphics;Computer science;Virtual reality;Educational institutions;Conferences;Laboratories;Data engineering;Data visualization;Technical Activities Board;Technical activities},
doi={10.1109/ISMAR.2009.5336517},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336514,
author={},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Task Force on Human Centered Computing (TFHCC)},
year={2009},
volume={},
number={},
pages={xvi-xvi},
abstract={},
keywords={Human computer interaction;Computer interfaces;Computer science;Physics computing;Cultural differences;Computer Society;Software systems;Executive Committee;Convergence;Sociology},
doi={10.1109/ISMAR.2009.5336514},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336515,
author={},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Conference committee},
year={2009},
volume={},
number={},
pages={xvii-xvii},
abstract={Provides a listing of current committee members.},
keywords={},
doi={10.1109/ISMAR.2009.5336515},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336512,
author={},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Science and technology program committee reviewers},
year={2009},
volume={},
number={},
pages={xviii-xviii},
abstract={The conference offers a note of thanks and lists its reviewers.},
keywords={},
doi={10.1109/ISMAR.2009.5336512},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336513,
author={M. {Mine}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Mixing reality and magic at Disney theme parks},
year={2009},
volume={},
number={},
pages={xix-xix},
abstract={Ever since Walt Disney first opened the doors to Disneyland in 1955, Imagineers have been using (and misusing) state-of the-art technology to immerse our guests in magical worlds. Combined with richly detailed environments, imaginative characters, and compelling stories, these tools have enabled our guests to dance with ghosts, sail with pirates, and fly to the furthest reaches of both inner and outer space. Today, advances in computing power, display technology, and sensing devices, along with ever accelerating trends of miniaturization and cost reduction are enabling exciting new ways to create our magical Disney worlds. Our environments are richer, our characters more interactive, and our storytelling more fluid, customizable, and reactive. In this talk, I will describe the new techniques we are developing to light, animate, and augment our environments, bringing the world of our animated features to life in ways never before possible. I will relate how we are using advanced sensing technology and better awareness of our guests to create smart reactive environments and new forms of entertainment. I will present advances in our characters that make them more responsive, aware, and engaging than ever before. I will show how we are working to break the confines of the conventional computer display in order to better surround and immerse our guests. I will discuss how all of these efforts are bound together by the common goal of bridging the everyday world of reality and the world of magic and imagination.},
keywords={computer displays;entertainment;interactive systems;Disney theme parks;Walt Disney;Disneyland;magical Disney worlds;display technology;advanced sensing technology;smart reactive environments;computer display;Space technology;Computer displays;Animation;Aerospace engineering;Acceleration;Costs;Design engineering;Computer graphics;Marine technology;Process design},
doi={10.1109/ISMAR.2009.5336513},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336510,
author={N. {Tsakos}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={UPWAKE: Art performance fusing dreams and technology},
year={2009},
volume={},
number={},
pages={xx-xx},
abstract={Playwright, conceptual director and performer Natasha Tsakos works in brave new form of theatre, where sound, computer generated images and the performer move meticulously in sync to create a dreamlike yet sharply real stage environment. Within this realm of total possibility, Tsakos muses on the deepest questions of the human soul.},
keywords={Art;Loudspeakers;Image generation;Humans},
doi={10.1109/ISMAR.2009.5336510},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336511,
author={P. {Maes}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={SixthSense: Integrating information and the real world},
year={2009},
volume={},
number={},
pages={xxi-xxi},
abstract={Pattie Maes is an associate professor in MIT's Program in Media Arts and Sciences and associate head of the Program in Media Arts and Sciences. She founded and directs the Media Lab's Fluid Interfaces research group ‹http://fluid. media.mit.edu/› which develops technologies for seamless integration of the digital world and physical world. Previously, she founded and ran the Software Agents group ‹http://fluid.media.mit.edu/›. Prior to joining the Media Lab, Maes was a visiting professor and a research scientist at the MIT Artificial Intelligence Lab. She holds bachelor's and PhD degrees in computer science from the Vrije Universiteit Brussel in Belgium. Her areas of expertise are human-computer interaction and intelligent user interfaces. Maes is the editor of three books, and is an editorial board member and reviewer for numerous professional journals and conferences. She has received several awards: /Newsweek/ magazine named her one of the “100 Americans to watch for” in the year 2000; /TIME/ Digital selected her as a member of the Cyber-Elite, the top 50 technological pioneers of the hightech world; the World Economic Forum honored her with the title “Global Leader for Tomorrow”; Ars Electronica awarded her the 1995 World Wide Web category prize; and in 2000 she was recognized with the “Lifetime Achievement Award” by the Massachusetts Interactive Media Council.},
keywords={Art;Artificial intelligence;Radio access networks;Software agents;Computer science;Intelligent agent;User interfaces;Watches;Web sites;Councils},
doi={10.1109/ISMAR.2009.5336511},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336508,
author={},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={2008 Awards},
year={2009},
volume={},
number={},
pages={xxii-xxiv},
abstract={},
keywords={Awards},
doi={10.1109/ISMAR.2009.5336508},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336509,
author={},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Papers},
year={2009},
volume={},
number={},
pages={1-1},
abstract={Start of the above-titled section of the conference proceedings record.},
keywords={},
doi={10.1109/ISMAR.2009.5336509},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336506,
author={},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={[Blank page]},
year={2009},
volume={},
number={},
pages={2-2},
abstract={This page or pages intentionally left blank.},
keywords={},
doi={10.1109/ISMAR.2009.5336506},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336522,
author={S. {Nilsson} and B. {Johansson} and A. {Jonsson}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Using AR to support cross-organisational collaboration in dynamic tasks},
year={2009},
volume={},
number={},
pages={3-12},
abstract={This paper presents a study where Augmented Reality (AR) technology has been used as a tool for supporting collaboration between the rescue services, the police and military personnel in a crisis management scenario. There are few studies on how AR systems should be designed to improve cooperation between actors from different organizations while at the same time support individual needs. In the present study an AR system was utilized for supporting joint planning tasks by providing organisation-specific views of a shared working. The study involved a simulated emergency event conducted in close to real settings with representatives from the organisations for which the system is developed. As a baseline, a series of trials without the AR system was carried out. Results show that the users were positive towards the AR system, and would like to use it in real work. They also experience some performance benefits of using the AR system compared to their traditional tools. Finally, the problem of designing for collaborative work as well as the benefits of using an iterative design processes is discussed.},
keywords={augmented reality;emergency services;groupware;cross-organisational collaboration;augmented reality technology;rescue services;police personnel;military personnel;crisis management;Collaboration;Collaborative work;Collaborative tools;Crisis management;Command and control systems;Augmented reality;Personnel;Terminology;Paper technology;Information science},
doi={10.1109/ISMAR.2009.5336522},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336507,
author={O. {Oda} and S. {Feiner}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Interference avoidance in multi-user hand-held augmented reality},
year={2009},
volume={},
number={},
pages={13-22},
abstract={In a multi-user augmented reality application for a shared physical environment, it is possible for users to interfere with each other. For example, in a multi-player game in which each player holds a display whose tracked position and orientation affect the outcome, one player may physically block another player's view or physically contact another player. We explore software techniques intended to avoid such interference. These techniques modify what a user sees or hears, and what interaction capabilities they have, when their display gets too close to another user's display. We present Redirected Motion, an effective, yet nondistracting, interference avoidance technique for hand-held AR, which transforms the 3D space in which the user moves their display, to direct the display away from other displays. We conducted a within-subject, formal user study to evaluate the effectiveness and distraction level of Redirected Motion compared to other interference avoidance techniques. The study is based on an instrumented, two-player, first-person-shooter, augmented reality game, in which each player holds a 6DOF-tracked ultra-mobile computer. Comparison conditions include an unmanipulated control condition and three other software techniques for avoiding interference: dimming the display, playing disturbing sounds, and disabling interaction capabilities. Subjective evaluation indicates that Redirected Motion was unnoticeable, and quantitative analysis shows that the mean distance between users during Redirected Motion was significantly larger than for the comparison conditions.},
keywords={augmented reality;computer displays;computer games;groupware;mobile computing;interference avoidance;multiuser hand-held augmented reality;shared physical environment;multiplayer game;position tracking;orientation tracking;software technique;interaction capability;user display;redirected motion;3D space;instrumented two-player first-person-shooter;augmented reality game;ultramobile computer;unmanipulated control condition;display dimming;disturbing sounds;Interference;Augmented reality;Collaborative work;Three dimensional displays;Computer displays;Solid modeling;Motion analysis;Computer interfaces;Visualization;Instruments;Collaborative/competitive augmented reality;computer games;interference avoidance;computer-supported cooperative play/work (CSCP/CSCW)},
doi={10.1109/ISMAR.2009.5336507},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336502,
author={N. {Petersen} and D. {Stricker}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Continuous natural user interface: Reducing the gap between real and digital world},
year={2009},
volume={},
number={},
pages={23-26},
abstract={Augmented reality (AR) presentation enables the creation of natural user interfaces that employ the whole user's environment as interaction device. Additionally, by using hand based 3D interaction with gestures that have a physical meaning like grabbing, dragging, and dropping this leads to a user experience that is intuitive, since close to the real world's behavior. We propose a novel approach to an AR-based natural user interface, that goes one step further by enabling the contents of the interface to switch domains from a virtual instance in AR to a physical instance in the real-world. All instances stay associated and changes made to the physical instance will be reflected on the virtual one. Because the behavior of our interface in AR is in key aspects consistent with the real-world, the gap between those domains is made less salient. To demonstrate our concept, we have implemented an exemplary industrial use case. Our main contribution is the methodology for an intuitive interface we call continuous natural user interface (CNUI). Additionaly, we conducted a user study to investigate the acceptance of this kind of interface. Results indicate an ergonomic ease and after a training period also an increased performance when using our system.},
keywords={augmented reality;gesture recognition;human computer interaction;human factors;continuous natural user interface;real-digital world;augmented reality;3D interaction device;user experience;virtual instance;ergonomic;User interfaces;Switches;Augmented reality;Ergonomics;Industrial training;Multimedia systems;Virtual reality;Computer graphics;Mice;Keyboards;H.5.2 [INFORMATION INTERFACES AND PRESENTATION]: User Interfaces;Input devices and strategies H.5.1 [INFORMATION INTERFACES AND PRESENTATION]:Multimedia Information Systems;Artificial, augmented, and virtual realities I.3.6 [COMPUTER GRAPHICS]: Methodology and Techniques;Interaction techniques},
doi={10.1109/ISMAR.2009.5336502},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336503,
author={P. {Lincoln} and G. {Welch} and A. {Nashel} and A. {Ilie} and A. {State} and H. {Fuchs}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Animatronic Shader Lamps Avatars},
year={2009},
volume={},
number={},
pages={27-33},
abstract={Applications such as telepresence and training involve the display of real or synthetic humans to multiple viewers. When attempting to render the humans with conventional displays, non-verbal cues such as head pose, gaze direction, body posture, and facial expression are difficult to convey correctly to all viewers. In addition, a framed image of a human conveys only a limited physical sense of presence - primarily through the display's location. While progress continues on articulated robots that mimic humans, the focus has been on the motion and behavior of the robots. We introduce a new approach for robotic avatars of real people: the use of cameras and projectors to capture and map the dynamic motion and appearance of a real person onto a humanoid animatronic model. We call these devices Animatronic Shader Lamps Avatars (SLA).We present a proof-of-concept prototype comprised of a camera, a tracking system, a digital projector, and a life-sized styrofoam head mounted on a pan-tilt unit. The system captures imagery of a moving, talking user and maps the appearance and motion onto the animatronic SLA, delivering a dynamic, real-time representation of the user to multiple viewers.},
keywords={avatars;computer vision;rendering (computer graphics);animatronic shader lamps avatars;articulated robots;robotic avatars;humanoid animatronic model;Animation;Lamps;Avatars;Humans;Displays;Robot sensing systems;Head;Robot vision systems;Cameras;Rendering (computer graphics);H.4.3 [Information Systems Applications]: Communications Applications;Computer conferencing, teleconferencing, and videoconferencing H.5.1 [Multimedia Information Systems]: Animations;Artificial, augmented, and virtual realities I.3.7 [Computer Graphics]: Three Dimensional Graphics and Realism;Virtual Reality; I.3.8 [Computer Graphics]: Applications},
doi={10.1109/ISMAR.2009.5336503},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336504,
author={},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={[Blank page]},
year={2009},
volume={},
number={},
pages={34-34},
abstract={This page or pages intentionally left blank.},
keywords={},
doi={10.1109/ISMAR.2009.5336504},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336505,
author={K. {Kim} and S. {Oh} and J. {Lee} and I. {Essa}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Augmenting Aerial Earth Maps with dynamic information},
year={2009},
volume={},
number={},
pages={35-38},
abstract={We introduce methods for augmenting aerial visualizations of Earth (from services like Google Earth or Microsoft Virtual Earth) with dynamic information obtained from videos. Our goal is to make Augmented Aerial Earth Maps that visualize an alive and dynamic scene within a city. We propose different approaches for analyzing videos of cities with pedestrians and cars, under differing conditions and then created augmented Aerial Earth Maps (AEMs) with live and dynamic information. We further extend our visualizations to include analysis of natural phenomenon (specifically clouds) and add this information to the AEMs adding to the visual reality.},
keywords={data visualisation;geography;aerial Earth Maps;dynamic information;aerial visualization;Google Earth;Microsoft Virtual Earth;Earth;Videos;Cities and towns;Cameras;Layout;Information analysis;Clouds;Data mining;Data visualization;Web and internet services},
doi={10.1109/ISMAR.2009.5336505},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336500,
author={S. {White} and D. {Feng} and S. {Feiner}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Interaction and presentation techniques for shake menus in tangible augmented reality},
year={2009},
volume={},
number={},
pages={39-48},
abstract={Menus play an important role in both information presentation and system control. We explore the design space of shake menus, which are intended for use in tangible augmented reality. Shake menus are radial menus displayed centered on a physical object and activated by shaking that object. One important aspect of their design space is the coordinate system used to present menu options. We conducted a within-subjects user study to compare the speed and efficacy of several alternative methods for presenting shake menus in augmented reality (world-referenced, display-referenced, and object-referenced), along with a baseline technique (a linear menu on a clipboard). Our findings suggest tradeoffs amongst speed, efficacy, and flexibility of interaction, and point towards the possible advantages of hybrid approaches that compose together transformations in different coordinate systems. We close by describing qualitative feedback from use and present several illustrative applications of the technique.},
keywords={augmented reality;graphical user interfaces;haptic interfaces;human computer interaction;3D interaction technique;information presentation technique;shake menu design space;tangible augmented reality;radial menu;physical object;coordinate system;world-referenced menu;display-referenced menu;object-referenced menu;linear menu;clipboard;qualitative feedback;3D user interface;Augmented reality;3D interactions;augmented reality;menus;shake menus;information display;authoring;selection;positioning},
doi={10.1109/ISMAR.2009.5336500},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336501,
author={B. {Knorlein} and M. {Di Luca} and M. {Harders}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Influence of visual and haptic delays on stiffness perception in augmented reality},
year={2009},
volume={},
number={},
pages={49-52},
abstract={Visual delays are unavoidable in augmented reality setups and occur in different steps of the rendering pipeline. In the context of haptic interaction with virtual objects, it has been shown that delayed force feedback can alter the perception of object stiffness. We hypothesize that delays in augmented reality systems can have similar consequences. To test this, we carried out a user study to investigate the effect of visual and haptic delays on the perception of stiffness. The experiment has been performed in an optimized visuo-haptic augmented reality setup, which allows to artificially manipulate delays during visual and haptic rendering. In line with previous results, delays for haptic feedback resulted in decreased perceived stiffness. In contrast, visual delays caused an increase in perceived stiffness. However, the simultaneous occurrence of delays in both sensory channels led to a partial compensation of these effects. This could potentially help to correct stiffness perception of virtual objects in visuo-haptic augmented reality systems.},
keywords={augmented reality;delays;force feedback;haptic interfaces;rendering (computer graphics);visual delay;haptic delay;rendering pipeline;haptic interaction;virtual object;force feedback;object stiffness perception;optimized visuo-haptic augmented reality setup;visual rendering;haptic rendering;sensory channel;Haptic interfaces;Augmented reality;Feedback;Delay effects;Displays;Cameras;Rendering (computer graphics);Synchronization;Computer vision;Laboratories;augmented reality;multimodal;user evaluation;delay;haptic},
doi={10.1109/ISMAR.2009.5336501},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336496,
author={M. A. {Livingston} and Z. {Ai} and J. W. {Decker}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={A user study towards understanding stereo perception in head-worn augmented reality displays},
year={2009},
volume={},
number={},
pages={53-56},
abstract={Properly perceived stereo display is often assumed to be vital in augmented reality (AR) displays used for close distances, echoing the general understanding from the perception literature. However, the accuracy of the perception of stereo in head-worn AR displays has not been studied greatly. We conducted a user study to elicit the precision of stereo perception in AR and its dependency on the size and contrast of the stimulus. We found a strong effect of contrast on the disparity users desired to make a virtual target verge at the distance of a real reference object. We also found that whether the target began behind or in front of the reference in a method of adjustments protocol made a significant difference. The mean disparity in the rendering that users preferred had a strong linear relationship with their IPD. We present our results and infer stereoacuity thresholds.},
keywords={augmented reality;helmet mounted displays;stereo perception understanding;head-worn augmented reality displays;Augmented reality;Displays;Virtual reality;Laboratories;Protocols;Rendering (computer graphics);Multimedia systems;Humans;Eyes;Stereo vision;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems;Artificial, augmented, and virtual realities H.5.2 [Information Interfaces and Presentation]: User Interfaces;Evaluation/Methodology; H.1.2 [Models and Principles]: User/Machine Systems;Human factors},
doi={10.1109/ISMAR.2009.5336496},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336497,
author={D. {Wagner} and D. {Schmalstieg} and H. {Bischof}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Multiple target detection and tracking with guaranteed framerates on mobile phones},
year={2009},
volume={},
number={},
pages={57-64},
abstract={In this paper we present a novel method for real-time pose estimation and tracking on low-end devices such as mobile phones. The presented system can track multiple known targets in real-time and simultaneously detect new targets for tracking. We present a method to automatically and dynamically balance the quality of detection and tracking to adapt to a variable time budget and ensure a constant frame rate. Results from real data of a mobile phone Augmented Reality system demonstrate the efficiency and robustness of the described approach. The system can track 6 planar targets on a mobile phone simultaneously at framerates of 23 fps.},
keywords={mobile computing;object detection;pose estimation;tracking;multiple target detection;mobile phone;real-time pose estimation;constant frame rate;augmented reality system;Object detection;Target tracking;Mobile handsets;Real time systems;Augmented reality;Robustness;Cameras;Optical filters;Optical mixing;Optical sensors;Pose estimation;6DOF;mobile phone;natural features},
doi={10.1109/ISMAR.2009.5336497},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336498,
author={N. {Hagbi} and O. {Bergig} and J. {El-Sana} and M. {Billinghurst}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Shape recognition and pose estimation for mobile augmented reality},
year={2009},
volume={},
number={},
pages={65-71},
abstract={In this paper we present Nestor, a system for real-time recognition and camera pose estimation from planar shapes. The system allows shapes that carry contextual meanings for humans to be used as augmented reality (AR) tracking fiducials. The user can teach the system new shapes at runtime by showing them to the camera. The learned shapes are then maintained by the system in a shape library. Nestor performs shape recognition by analyzing contour structures and generating projective invariant signatures from their concavities. The concavities are further used to extract features for pose estimation and tracking. Pose refinement is carried out by minimizing the reprojection error between sample points on each image contour and its library counterpart. Sample points are matched by evolving an active contour in real time. Our experiments show that the system provides stable and accurate registration, and runs at interactive frame rates on a Nokia N95 mobile phone.},
keywords={augmented reality;feature extraction;image registration;mobile handsets;pose estimation;shape recognition;shape recognition;pose estimation;mobile augmented reality;Nestor;contour structures;projective invariant signatures;concavities;feature extraction;pose refinement;reprojection error;image registration;Nokia N95 mobile phone;Shape;Augmented reality;Cameras;Libraries;Real time systems;Humans;Runtime;Performance analysis;Feature extraction;Active contours;In-Place Augmented Reality;handheld AR;shape recognition;geometric projective invariance;3D pose estimation;vision-based tracking;free-hand sketching;shape dual perception},
doi={10.1109/ISMAR.2009.5336498},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336499,
author={},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={[Blank page]},
year={2009},
volume={},
number={},
pages={72-72},
abstract={This page or pages intentionally left blank.},
keywords={},
doi={10.1109/ISMAR.2009.5336499},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336494,
author={C. {Arth} and D. {Wagner} and M. {Klopschitz} and A. {Irschara} and D. {Schmalstieg}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Wide area localization on mobile phones},
year={2009},
volume={},
number={},
pages={73-82},
abstract={We present a fast and memory efficient method for localizing a mobile user's 6DOF pose from a single camera image. Our approach registers a view with respect to a sparse 3D point reconstruction. The 3D point dataset is partitioned into pieces based on visibility constraints and occlusion culling, making it scalable and efficient to handle. Starting with a coarse guess, our system only considers features that can be seen from the user's position. Our method is resource efficient, usually requiring only a few megabytes of memory, thereby making it feasible to run on low-end devices such as mobile phones. At the same time it is fast enough to give instant results on this device class.},
keywords={image reconstruction;image registration;mobile radio;object detection;pose estimation;wide area localization;mobile phone;mobile user 6DOF pose localization;camera image;image registration;sparse 3D point reconstruction;3D point dataset;occlusion culling;pose estimation;Mobile handsets;Image reconstruction;Cameras;Computer vision;Data acquisition;Image segmentation;Layout;Image analysis;Image generation;Mobile computing;I.2.10 [Artificial Intelligence]: Vision and Scene Understanding;3D/stereo scene analysis I.4.8 [Image Processing And Computer Vision]: Scene Analysis;Tracking; I.5.4 [Pattern Recognition]: Applications;Computer Vision C.5.3 [Computer System Implementation]: Microcomputers;Portable devices (e.g., laptops, personal digital assistants)},
doi={10.1109/ISMAR.2009.5336494},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336495,
author={G. {Klein} and D. {Murray}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Parallel Tracking and Mapping on a camera phone},
year={2009},
volume={},
number={},
pages={83-86},
abstract={Camera phones are a promising platform for hand-held augmented reality. As their computational resources grow, they are becoming increasingly suitable for visual tracking tasks. At the same time, they still offer considerable challenges: Their cameras offer a narrow field-of-view not best suitable for robust tracking; images are often received at less than 15 Hz; long exposure times result in significant motion blur; and finally, a rolling shutter causes severe smearing effects. This paper describes an attempt to implement a keyframe-based SLAMsystem on a camera phone (specifically, the Apple iPhone 3 G). We describe a series of adaptations to the Parallel Tracking and Mapping system to mitigate the impact of the device's imaging deficiencies. Early results demonstrate a system capable of generating and augmenting small maps, albeit with reduced accuracy and robustness compared to SLAM on a PC.},
keywords={augmented reality;cameras;mobile computing;mobile handsets;parallel tracking;parallel mapping;camera phone;hand-held augmented reality;visual tracking;Apple iPhone 3G;Cameras;Robustness;Simultaneous localization and mapping;Augmented reality;Tracking;Layout;Computer displays;Acceleration;Laboratories;Books},
doi={10.1109/ISMAR.2009.5336495},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336490,
author={O. {Bergig} and N. {Hagbi} and J. {El-Sana} and M. {Billinghurst}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={In-place 3D sketching for authoring and augmenting mechanical systems},
year={2009},
volume={},
number={},
pages={87-94},
abstract={We present a framework for authoring three-dimensional virtual scenes for Augmented Reality (AR) which is based on hand sketching. Sketches consisting of multiple components are used to construct a 3D virtual scene augmented on top of the real drawing. Model structure and properties can be modified by editing the sketch itself and printed content can be combined with hand sketches to form a single scene. Authoring by sketching opens up new forms of interaction that have not been previously explored in Augmented Reality. To demonstrate the technology, we implemented an application that constructs 3D AR scenes of mechanical systems from freehand sketches, and animates the scenes using a physics engine. We provide examples of scenes composed from trihedral solid models, forces, and springs. Finally, we describe how sketch interaction can be used to author complicated physics experiments in a natural way.},
keywords={augmented reality;computational geometry;image reconstruction;visual languages;in-place 3D sketching;mechanical systems authoring;mechanical systems augmenting;three-dimensional virtual scenes;augmented reality;hand sketching;trihedral solid models;sketch interaction;Mechanical systems;Psychology;Breast;Educational institutions;Assembly;Visualization;Education;Object detection;Output feedback;Virtual reality;In-Place Augmented Reality;free hand sketching;Augmented Reality;3D content authoring;physical simulation;interaction by sketching;visual language;dual perception},
doi={10.1109/ISMAR.2009.5336490},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336491,
author={H. {Uchiyama} and H. {Saito}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Augmenting text document by on-line learning of local arrangement of keypoints},
year={2009},
volume={},
number={},
pages={95-98},
abstract={We propose a technique for text document tracking over a large range of viewpoints. Since the popular SIFT or SURF descriptors typically fail on such documents, our method considers instead local arrangement of keypoints. We extends locally likely arrangement hashing (LLAH), which is limited to fronto-parallel images: We handle a large range of viewpoints by learning the behavior of keypoint patterns when the camera viewpoint changes. Our method starts tracking a document from a nearly frontal view. Then, it undergoes motion, and new configurations of keypoints appear. The database is incrementally updated to reflect these new observations, allowing the system to detect the document under the new viewpoint. We demonstrate the performance and robustness of our method by comparing it with the original LLAH.},
keywords={augmented reality;text analysis;text document augmentation;online learning;locally likely arrangement hashing;pose estimation;paper-based augmented reality;Augmented reality;Nearest neighbor searches;Cameras;Image databases;Pattern matching;Robustness;Multimedia systems;Virtual reality;Image processing;Computer vision;LLAH;on-line learning;pose estimation;paper registration;paper based augmented reality},
doi={10.1109/ISMAR.2009.5336491},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336492,
author={F. I. {Cosco} and C. {Garre} and F. {Bruno} and M. {Muzzupappa} and M. A. {Otaduy}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Augmented touch without visual obtrusion},
year={2009},
volume={},
number={},
pages={99-102},
abstract={Visuo-haptic mixed reality consists of adding to a real scene the ability to see and touch virtual objects. It requires the use of see-through display technology for visually mixing real and virtual objects, and haptic devices for adding haptic interaction with the virtual objects. However, haptic devices tend to be bulky items that appear in the field of view of the user. In this work, we propose a novel mixed reality paradigm where it is possible to touch and see virtual objects in combination with a real scene, but without visual obtrusion produced by the haptic device. This mixed reality paradigm relies on the following three technical steps: tracking of the haptic device, visual deletion of the device from the real scene, and background completion using image-based models. We have developed a successful proof-of-concept implementation, where a user can touch virtual objects in the context of a real scene.},
keywords={augmented reality;data visualisation;haptic interfaces;rendering (computer graphics);augmented touch;visuo-haptic mixed reality;virtual object;see-through display technology;haptic device;haptic interaction;real scene;image-based rendering;Haptic interfaces;Virtual reality;Layout;Space technology;Actuators;Rendering (computer graphics);Optical devices;Displays;Multimedia systems;Medical services;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems;Artificial, Augmented, and Virtual Realities},
doi={10.1109/ISMAR.2009.5336492},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336493,
author={J. {Ventura} and T. {Hollerer}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Online environment model estimation for augmented reality},
year={2009},
volume={},
number={},
pages={103-106},
abstract={Augmented reality applications often rely on a detailed environment model to support features such as annotation and occlusion. Usually, such a model is constructed offline, which restricts the generality and mobility of the AR experience. In online SLAM approaches, the fidelity of the model stays at the level of landmark feature maps. In this work we introduce a system which constructs a textured geometric model of the user's environment as it is being explored. First, 3D feature tracks are organized into roughly planar surfaces. Then, image patches in keyframes are assigned to the planes in the scene using stereo analysis. The system runs as a background process and continually updates and improves the model over time. This environment model can then be rendered into new frames to aid in several common but difficult AR tasks such as accurate real-virtual occlusion and annotation placement.},
keywords={augmented reality;computer graphics;stereo image processing;online environment model estimation;augmented reality;annotation;occlusion;online SLAM approach;stereo analysis;real-virtual occlusion;Augmented reality;Layout;Image reconstruction;Simultaneous localization and mapping;Image analysis;Rendering (computer graphics);Cameras;Rough surfaces;Surface roughness;Computer graphics;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism;Augmented reality; I.4.8 [Image Processing and Computer Vision]: Scene Analysis;Stereo; I.4.8 [Image Processing and Computer Vision]: Scene Analysis;Color},
doi={10.1109/ISMAR.2009.5336493},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336482,
author={A. {van den Hengel} and R. {Hill} and B. {Ward} and A. {Dick}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={In situ image-based modeling},
year={2009},
volume={},
number={},
pages={107-110},
abstract={We present an interactive image-based modelling method for generating 3D models within an augmented reality system. Applying real time camera tracking, and high-level automated image analysis, enables more powerful modelling interactions than have previously been possible. The result is an immersive modelling process which generates accurate three dimensional models of real objects efficiently and effectively. In demonstrating the modelling process on a range of indoor and outdoor scenes, we show the flexibility it offers in enabling augmented reality applications in previously unseen environments.},
keywords={augmented reality;computer vision;in situ image-based modeling;interactive image-based modelling method;augmented reality system;real time camera tracking;high-level automated image analysis;immersive modelling process;Layout;Power system modeling;Cameras;Augmented reality;Geometry;Shape;Image analysis;Information analysis;Solid modeling;Parameter estimation;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems;Artificial, augmented, and virtual realities I.4.8 [IMAGE PROCESSING AND COMPUTER VISION]: Scene Analysis;Shape},
doi={10.1109/ISMAR.2009.5336482},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336483,
author={P. {Barnum} and Y. {Sheikh} and A. {Datta} and T. {Kanade}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Dynamic seethroughs: Synthesizing hidden views of moving objects},
year={2009},
volume={},
number={},
pages={111-114},
abstract={This paper presents a method to create an illusion of seeing moving objects through occluding surfaces in a video. This illusion is achieved by transferring information from a camera viewing the occluded area. In typical view interpolation approaches for 3D scenes, some form of correspondence across views is required. For occluded areas, establishing direct correspondence is impossible as information is missing in one of the views. Instead, we use a 2D projective invariant to capture information about occluded objects (which may be moving). Since invariants are quantities that do not change across views, a visually compelling rendering of hidden areas is achieved without the need for explicit correspondences. A piece-wise planar model of the scene allows the entire rendering process to take place without any 3D reconstruction, while still producing visual parallax. Because of the simplicity and robustness of the 2D invariant, we are able to transfer both static backgrounds and moving objects in real time. A complete working system has been implemented that runs live at 5Hz. Applications for this technology include the ability to look through corners at tight intersections for automobile safety, concurrent visualization of a surveillance camera network, and monitoring systems for patients/elderly/children.},
keywords={image motion analysis;piecewise constant techniques;rendering (computer graphics);video signal processing;dynamic seethrough;synthesizing hidden view;moving object;illusion creation method;camera viewing;occluded area;view interpolation approach;3D scene;2D projective invariant;information capture;piece wise planar model;rendering process;visual parallax production;2D invariant robustness;static background;automobile safety;surveillance camera networks concurrent visualization;monitoring systems;bandwidth 5 Hz;Cameras;Layout;Vehicle dynamics;Interpolation;Robustness;Automobiles;Vehicle safety;Visualization;Surveillance;Patient monitoring;personal MR/AR information systems;industrial and military MR/AR applications;real-time rendering;vision-based registration and tracking;object overlay and spatial layout techniques;performance issues [real-time approaches]},
doi={10.1109/ISMAR.2009.5336483},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336484,
author={B. {Schwerdtfeger} and R. {Reif} and W. A. {Gunthner} and G. {Klinker} and D. {Hamacher} and L. {Schega} and I. {Bockelmann} and F. {Doil} and J. {Tumler}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Pick-by-Vision: A first stress test},
year={2009},
volume={},
number={},
pages={115-124},
abstract={In this paper we report on our ongoing studies around the application of augmented reality methods to support the order picking process of logistics applications. Order picking is the gathering of goods out of a prepared range of items following some customer orders. We named the visual support of this order picking process using head-mounted displays ldquopick-by-visionrdquo. This work presents the case study of bringing our previously developed pick-by-vision system from the lab to an experimental factory hall to evaluate it under more realistic conditions. This includes the execution of two user studies. In the first one we compared our pick-by-vision system with and without tracking to picking using a paper list to check picking performance and quality in general. In a second test we had subjects using the pick-by-vision system continuously for two hours to gain in-depth insight into the longer use of our system, checking user strain besides the general performance. Furthermore, we report on the general obstacles of trying to use HMD-based AR in an industrial setup and discuss our observations of user behaviour.},
keywords={augmented reality;helmet mounted displays;logistics data processing;order picking;order processing;pick-by-vision system;stress test;AR;augmented reality;order picking process;logistics application;customer order;goods gathering;HMD;head-mounted display;experimental factory hall;Stress;Testing;Logistics;Augmented reality;Displays;User interfaces;Employee welfare;Visualization;Production systems;Usability;H.5.1 [ INFORMATION INTERFACES AND PRESENTATION]: Multimedia Information Systems;Artificial, augmented, and virtual realities; Evaluation/methodology; H.5.2 [ INFORMATION INTERFACES AND PRESENTATION]: User Interfaces;User-centered design},
doi={10.1109/ISMAR.2009.5336484},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336485,
author={A. {Kotranza} and D. {Scott Lind} and C. M. {Pugh} and B. {Lok}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Real-time in-situ visual feedback of task performance in mixed environments for learning joint psychomotor-cognitive tasks},
year={2009},
volume={},
number={},
pages={125-134},
abstract={This paper proposes an approach to mixed environment training of manual tasks requiring concurrent use of psychomotor and cognitive skills. To train concurrent use of both skill sets, the learner is provided real-time generated, in-situ presented visual feedback of her performance. This feedback provides reinforcement and correction of psychomotor skills concurrently with guidance in developing cognitive models of the task. The general approach is presented: 1) Sensors placed in the physical environment detect in real-time a learner's manipulation of physical objects. 2) Sensor data is input to models of task performance which output quantitative measures of the learner's performance. 3) Pre-defined rules are applied to transform the learner's performance data into visual feedback presented in realtime and in-situ with the physical objects being manipulated. With guidance from medical education experts, we have applied this approach to a mixed environment for learning clinical breast exams (CBEs). CBE belongs to a class of tasks that require learning multiple cognitive elements and task-specific psychomotor skills. Traditional approaches to learning CBEs and other joint psychomotor-cognitive tasks rely on extensive one-onone training with an expert providing subjective feedback. By integrating real-time visual feedback of learners' quantitatively measured CBE performance, a mixed environment for learning CBEs provides on-demand learning opportunities with more objective, detailed feedback than available with expert observation. The proposed approach applied to learning CBEs was informally evaluated by four expert medical educators and six novice medical students. This evaluation highlights that receiving real-time in-situ visual feedback of their performance provides students an advantage, over traditional approaches to learning CBEs, in developing correct psychomotor and cognitive skills.},
keywords={augmented reality;biomedical education;computer based training;feedback;medical computing;patient diagnosis;real-time in-situ visual feedback;task performance;learning;mixed environment training;psychomotor skills;cognitive skills;skills reinforcement;skills correction;learner manipulation;physical objects;output quantitative measure;medical education;clinical breast exam;detailed feedback;medical students;Psychology;Breast;Educational institutions;Assembly;Visualization;Education;Object detection;Output feedback;Virtual reality;Computer applications;mixed reality;information visualization},
doi={10.1109/ISMAR.2009.5336485},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336486,
author={S. J. {Henderson} and S. {Feiner}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Evaluating the benefits of augmented reality for task localization in maintenance of an armored personnel carrier turret},
year={2009},
volume={},
number={},
pages={135-144},
abstract={We present the design, implementation, and user testing of a prototype augmented reality application to support military mechanics conducting routine maintenance tasks inside an armored vehicle turret. Our prototype uses a tracked head-worn display to augment a mechanic's natural view with text, labels, arrows, and animated sequences designed to facilitate task comprehension, location, and execution. A within-subject controlled user study examined professional military mechanics using our system to complete 18 common tasks under field conditions. These tasks included installing and removing fasteners and indicator lights, and connecting cables, all within the cramped interior of an armored personnel carrier turret. An augmented reality condition was tested against two baseline conditions: an untracked headworn display with text and graphics and a fixed flat panel display representing an improved version of the laptop-based documentation currently employed in practice. The augmented reality condition allowed mechanics to locate tasks more quickly than when using either baseline, and in some instances, resulted in less overall head movement. A qualitative survey showed mechanics found the augmented reality condition intuitive and satisfying for the tested sequence of tasks.},
keywords={augmented reality;computer animation;flat panel displays;laptop computers;maintenance engineering;military computing;military equipment;military vehicles;armored personnel carrier turret maintenance;augmented reality;task localization;military mechanics;armored vehicle turret;tracked head-worn display;computer animated sequence;task comprehension;computer graphics;fixed flat panel display;laptop-based documentation;Augmented reality;Personnel;Testing;Prototypes;Flat panel displays;Maintenance;Vehicles;Animation;Control systems;Fasteners;maintenance;service;repair;attention;localization augmented reality},
doi={10.1109/ISMAR.2009.5336486},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336487,
author={S. {Lieberknecht} and S. {Benhimane} and P. {Meier} and N. {Navab}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={A dataset and evaluation methodology for template-based tracking algorithms},
year={2009},
volume={},
number={},
pages={145-151},
abstract={Unlike dense stereo, optical flow or multi-view stereo, template-based tracking lacks benchmark datasets allowing a fair comparison between state-of-the-art algorithms. Until now, in order to evaluate objectively and quantitatively the performance and the robustness of template-based tracking algorithms, mainly synthetically generated image sequences were used. The evaluation is therefore often intrinsically biased. In this paper, we describe the process we carried out to perform the acquisition of real scene image sequences with very precise and accurate ground truth poses using an industrial camera rigidly mounted on the end-effector of a high-precision robotic measurement arm. For the acquisition, we considered most of the critical parameters that influence the tracking results such as: the texture richness and the texture repeatability of the objects to be tracked, the camera motion and speed, and the changes of the object scale in the images and variations of the lighting conditions over time. We designed an evaluation scheme for object detection and interframe tracking algorithms and used the image sequences to apply this scheme to several state-of-the-art algorithms. The image sequences will be made freely available for testing, submitting and evaluating new template-based tracking algorithms, i.e. algorithms that detect or track a planar object in an image sequence given only one image of the object (called the template).},
keywords={augmented reality;image sequences;image texture;object detection;pose estimation;dataset;evaluation methodology;template-based tracking algorithms;real scene image sequences;ground truth poses;high-precision robotic measurement arm;texture richness;texture repeatability;camera motion;camera speed;object detection;interframe tracking algorithms;Image sequences;Robot vision systems;Cameras;Tracking;Object detection;Image motion analysis;Robustness;Image generation;Performance evaluation;Layout},
doi={10.1109/ISMAR.2009.5336487},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336488,
author={},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={[Blank page]},
year={2009},
volume={},
number={},
pages={152-152},
abstract={This page or pages intentionally left blank.},
keywords={},
doi={10.1109/ISMAR.2009.5336488},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336489,
author={G. {Schall} and D. {Wagner} and G. {Reitmayr} and E. {Taichmann} and M. {Wieser} and D. {Schmalstieg} and B. {Hofmann-Wellenhof}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Global pose estimation using multi-sensor fusion for outdoor Augmented Reality},
year={2009},
volume={},
number={},
pages={153-162},
abstract={Outdoor Augmented Reality typically requires tracking in unprepared environments. For global registration, Global Positioning System (GPS) is currently the best sensing technology, but its precision and update rate are not sufficient for high quality tracking. We present a system that uses Kalman filtering for fusion of Differential GPS (DGPS) or Real-Time Kinematic (RTK) based GPS with barometric heights and also for an inertial measurement unit with gyroscopes, magnetometers and accelerometers to improve the transient oscillation. Typically, inertial sensors are subjected to drift and magnetometer measurements are distorted by electro-magnetic fields in the environment. For compensation, we additionally apply a visual orientation tracker which is drift-free through online mapping of the unknown environment. This tracker allows for correction of distortions of the 3-axis magnetic compass, which increases the robustness and accuracy of the pose estimates. We present results of applying this approach in an industrial application scenario.},
keywords={accelerometers;augmented reality;electromagnetic fields;Global Positioning System;gyroscopes;Kalman filters;magnetometers;pose estimation;sensor fusion;tracking;global pose estimation;multi-sensor fusion;outdoor augmented reality;inertial tracking;global registration;Global Positioning System;GPS;Kalman filter;differential GPS;real-time kinematic;barometric heights;gyroscope;magnetometer;accelerometer;transient oscillation;electromagnetic fields;3-axis magnetic compass;visual tracking;Augmented reality;Global Positioning System;Magnetometers;Kalman filters;Magnetic separation;Filtering;Real time systems;Kinematics;Measurement units;Gyroscopes;handheld augmented reality;inertial tracking;visual tracking;kalman filter;sensor fusion},
doi={10.1109/ISMAR.2009.5336489},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336480,
author={Y. {Park} and V. {Lepetit} and W. {Woo}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={ESM-Blur: Handling rendering blur in 3D tracking and augmentation},
year={2009},
volume={},
number={},
pages={163-166},
abstract={The contribution of this paper is two-fold. First, we show how to extend the ESM algorithm to handle motion blur in 3D object tracking. ESM is a powerful algorithm for template matching-based tracking, but it can fail under motion blur. We introduce an image formation model that explicitly considers the possibility of blur, and show it results in a generalization of the original ESM algorithm. This allows to converge faster, more accurately and more robustly even under large amount of blur. Our second contribution is an efficient method for rendering the virtual objects under the estimated motion blur. It renders two images of the object under 3D perspective, and warps them to create many intermediate images. By fusing these images we obtain a final image for the virtual objects blurred consistently with the captured image. Because warping is much faster that 3D rendering, we can create realistically blurred images at a very low computational cost.},
keywords={augmented reality;image matching;motion estimation;object detection;rendering (computer graphics);tracking;ESM-blur;blur rendering;3D object tracking;augmentation;motion blur estimation;template matching-based tracking;image formation model;ESM algorithm;Tracking;Rendering (computer graphics);Robustness;Cameras;Electronic mail;Image converters;Motion estimation;Computational efficiency;Mobile computing;Detection algorithms},
doi={10.1109/ISMAR.2009.5336480},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336481,
author={},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Posters},
year={2009},
volume={},
number={},
pages={167-168},
abstract={Start of the above-titled section of the conference proceedings record.},
keywords={},
doi={10.1109/ISMAR.2009.5336481},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336478,
author={Z. {Ai} and M. A. {Livingston}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Integration of georegistered information on a virtual globe},
year={2009},
volume={},
number={},
pages={169-170},
abstract={In collaborative augmented reality (AR) missions, much georegistered information is collected and sent to a command and control center. This paper describes the concept and prototypical implementation of a mixed reality (MR) based system that integrates georegistered information from AR systems and other sources on a virtual globe. The application can be used for a command and control center to monitor the field operation where multiple AR users are engaging in a collaborative mission. Google Earth is used to demonstrate the system, which integrates georegistered icons, live video streams from field operators or surveillance cameras, 3D models, and satellite or aerial photos into one MR environment.},
keywords={augmented reality;geographic information systems;georegistered information integration;virtual globe;collaborative augmented reality;mixed reality based system;collaborative mission;Google Earth;georegistered icons;live video streams;surveillance cameras;3D models;Virtual reality;Collaboration;Command and control systems;Augmented reality;Virtual prototyping;Earth;Streaming media;Surveillance;Cameras;Satellites;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems;Artificial, augmented, and virtual realities; H.5.3 [Information Interfaces and Presentation]: Group and Organization Interfaces;Computer-supported cooperative work},
doi={10.1109/ISMAR.2009.5336478},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336479,
author={D. {Beaney} and B. {Mac Namee}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Forked! A demonstration of physics realism in augmented reality},
year={2009},
volume={},
number={},
pages={171-172},
abstract={In making fully immersive augmented reality (AR) applications, real and virtual objects will have to be seen to physically interact together in a realistic and believable way. This paper describes Forked! a system that has been developed to show how physical interactions between real and virtual objects can be simulated realistically and believably through appropriate use of a physics engine. The system allows users control a robotic forklift to manipulate virtual crates in an AR environment. The paper also describes a evaluation experiment in which it is shown that the physical interactions between the forklift and the virtual creates are realistic and believable enough to be comparable with the physical interactions between a forklift and real crates.},
keywords={augmented reality;manipulators;physics realism;augmented reality;Forked!;robotic forklift;virtual crates;Augmented reality;Robot kinematics;Engines;Robot control;Artificial intelligence;Physics computing;Control systems;Performance evaluation;Layout;Cameras},
doi={10.1109/ISMAR.2009.5336479},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336474,
author={C. {Bichlmeier} and S. {Holdstock} and S. M. {Heining} and S. {Weidert} and E. {Euler} and O. {Kutter} and N. {Navab}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Contextual in-situ visualization for port placement in keyhole surgery: Evaluation of three target applications by two surgeons and eighteen medical trainees},
year={2009},
volume={},
number={},
pages={173-174},
abstract={Port position in minimally invasive surgeries is chosen to minimize the lesion of tissue and maximize the movability for endoscopic instruments. In this study, we present an evaluation of the potential of a 3D contextual in-situ visualization of the anatomic target region to help surgeons for three different surgical procedures decide where best to create ports and incisions to enable the insertion of a specific set of instruments.},
keywords={augmented reality;data visualisation;medical computing;contextual in-situ visualization;port placement;keyhole surgery;medical trainees;minimally invasive surgeries;3D contextual in-situ visualization;Visualization;Minimally invasive surgery;Surgical instruments;Target tracking;Testing;Computer vision;Augmented reality;Lesions;Cameras;Imaging phantoms;K.3.1 [Computer Uses in Education];H.5.1 [Multimedia Information Systems]: Artificial, augmented, and virtual realities},
doi={10.1109/ISMAR.2009.5336474},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336475,
author={G. {Bleser} and G. {Hendeby}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Using optical flow as lightweight SLAM alternative},
year={2009},
volume={},
number={},
pages={175-176},
abstract={Visual simultaneous localisation and mapping (SLAM) is since the last decades an often addressed problem. Online mapping enables tracking in unknown environments. However, it also suffers from high computational complexity and potential drift. Moreover, in augmented reality applications the map itself is often not needed and the target environment is partially known, e.g. in a few 3D anchor or marker points. In this paper, rather than using SLAM, measurements based on optical flow are introduced. With these measurements, a modified visual-inertial tracking method is derived, which in Monte Carlo simulations reduces the need for 3D points and allows tracking for extended periods of time without any 3D point registrations.},
keywords={augmented reality;computational complexity;flow measurement;image sequences;Monte Carlo methods;optical tracking;optical variables measurement;optical flow;lightweight SLAM;simultaneous visual localisation and mapping;online mapping;computational complexity;visual-inertial tracking method;Monte Carlo simulations;augmented reality applications;Image motion analysis;Simultaneous localization and mapping;Optical sensors;Fluid flow measurement;Optical filters;Cameras;Optical variables control;Sensor fusion;Time measurement;Kinematics;augmented reality;camera tracking;visual SLAM;inertial sensors;sensor fusion;optical flow},
doi={10.1109/ISMAR.2009.5336475},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336476,
author={T. {Blum} and S. M. {Heining} and O. {Kutter} and N. {Navab}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Advanced training methods using an Augmented Reality ultrasound simulator},
year={2009},
volume={},
number={},
pages={177-178},
abstract={Ultrasound (US) is a medical imaging modality which is extremely difficult to learn as it is user-dependent, has low image quality and requires much knowledge about US physics and human anatomy. For training US we propose an Augmented Reality (AR) ultrasound simulator where the US slice is simulated from a CT volume. The location of the US slice inside the body is visualized using contextual in-situ techniques. We also propose advanced methods how to use an AR simulator for training.},
keywords={augmented reality;biomedical ultrasonics;computer based training;data visualisation;medical image processing;teaching;augmented reality ultrasound simulator;medical imaging;training method;image quality;US physics-and-human anatomy;CT volume;US slice;contextual in-situ visualization;teaching;Augmented reality;Ultrasonic imaging;Medical simulation;Biological system modeling;Biomedical imaging;Image quality;Physics;Human anatomy;Computed tomography;Visualization;H.5.1 [Information Interfaces and Presentation]: Artificial, augmented, and virtual realities;J.3 [Life and Medical Sciences]: —},
doi={10.1109/ISMAR.2009.5336476},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336477,
author={R. O. {Castle} and D. W. {Murray}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Object recognition and localization while tracking and mapping},
year={2009},
volume={},
number={},
pages={179-180},
abstract={This paper demonstrates how objects can be recognized, reconstructed, and localized within a 3D map, using observations and matching of SIFT features in keyframes. The keyframes arise as part of a frame-rate process of parallel camera tracking and mapping, in which the keyframe camera poses and 3D map points are refined using bundle adjustment. The object reconstruction process runs independently, and in parallel to, the tracking and mapping processes. Detected objects are automatically labelled on the user's display using predefined annotations. The annotations are also used to highlight areas of interest upon the objects to the user.},
keywords={computer vision;image recognition;object detection;object recognition;object-oriented databases;object recognition;Object localization;parallel camera mapping;3D map points;SIFT features;keyframes;frame-rate process;parallel camera tracking;bundle adjustment;object reconstruction process;Object recognition;Cameras;Object detection;Image databases;Image reconstruction;Spatial databases;Yarn;Laboratories;Displays;Multimedia systems;H.5.1 [Multimedia Information Systems]: Artificial, augmented, and virtual realities;I.4.8 [Scene Analysis]: Object Recognition;Tracking},
doi={10.1109/ISMAR.2009.5336477},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336472,
author={D. M. {Chen} and S. S. {Tsai} and R. {Vedantham} and R. {Grzeszczuk} and B. {Girod}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Streaming mobile augmented reality on mobile phones},
year={2009},
volume={},
number={},
pages={181-182},
abstract={Continuous recognition and tracking of objects in live video captured on a mobile device enables real-time user interaction. We demonstrate a streaming mobile augmented reality system with 1 second latency. User interest is automatically inferred from camera movements, so the user never has to press a button. Our system is used to identify and track book and CD covers in real time on a phone's viewfinder. Efficient motion estimation is performed at 30 frames per second on a phone, while fast search through a database of 20,000 images is performed on a server.},
keywords={augmented reality;mobile computing;motion estimation;multimedia systems;object recognition;video signal processing;video streaming;streaming mobile augmented reality;mobile phone;continuous object recognition;continuous object tracking;live video;real-time user interaction;camera movement;book cover identification;book cover tracking;CD cover tracking;phone viewfinder;motion estimation;Augmented reality;Mobile handsets;Streaming media;Delay;Cameras;Books;Image databases;Motion estimation;Real time systems;Layout},
doi={10.1109/ISMAR.2009.5336472},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336473,
author={S. {Engelhardt} and A. {Langs} and G. {Lochmann} and I. {Schmidt} and S. {Muller}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={MoleARlert - an augmented reality game based on Lemmings},
year={2009},
volume={},
number={},
pages={183-184},
abstract={In this poster, we present our outdoor AR game MoleARlert. The idea behind this research prototype was to exploit the enormous potential of AR in combination with classical game play based on the well-known game Lemmings by Psygnosis. Real players interact with virtual moles on a real playing field. The moles are guided through the hazardous environment to their final goal. The multiplayer game is observed through a stationary video see-through monitor. The team leader directs the real players around the field in order for them to steer the game with special markers and human gestures. Thus, the game requires a lot of action from players and is very entertaining.},
keywords={augmented reality;computer displays;computer games;gesture recognition;MoleARlert;augmented reality game;Lemmings;Psygnosis;virtual moles;real playing field;multiplayer game;human gestures;video see-through monitor;special markers;Augmented reality;Games;Tiles;Humans;Hazards;Cameras;Prototypes;Multimedia systems;Multimedia computing;Virtual reality;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems;Artificial, augmented, and virtual realities; K.8.0 [Personal Computing]: General;Games},
doi={10.1109/ISMAR.2009.5336473},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336467,
author={S. {Gauglitz} and T. {Hollerer} and P. {Krahwinkler} and J. {Rossmann}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={A setup for evaluating detectors and descriptors for visual tracking},
year={2009},
volume={},
number={},
pages={185-186},
abstract={In many cases, visual tracking is based on detecting, describing, and then matching local features. A variety of algorithms for these steps have been proposed and used in tracking systems, leading to an increased need for independent comparisons. However, existing evaluations are geared towards object recognition and image retrieval, and their results have limited validity for real-time visual tracking. We present a setup for evaluation of detectors and descriptors which is geared towards visual tracking in terms of testbed, candidate algorithms and performance criteria. Most notably, our testbed consists of video streams with several thousand frames naturally affected by noise and motion blur.},
keywords={feature extraction;image matching;image motion analysis;image retrieval;object detection;object recognition;real-time systems;tracking;video signal processing;video streaming;real-time visual tracking system;local feature matching;object recognition;image retrieval;candidate algorithm;video stream;motion blur;noise;descriptor;Detectors;Testing;Layout;Streaming media;Tracking;Cameras;Computer science;Man machine systems;Object recognition;Image retrieval},
doi={10.1109/ISMAR.2009.5336467},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336468,
author={P. {Georgel} and S. {Benhimane} and J. {Sotke} and N. {Navab}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Photo-based Industrial Augmented Reality application using a single keyframe registration procedure},
year={2009},
volume={},
number={},
pages={187-188},
abstract={In the recent years, many industrial augmented reality (IAR) applications are shifting from video to still images to create a mixed view. This new type of application is called photo-based augmented reality. In order to guarantee the success of these applications, a simple and efficient registration method is required. We present a new method to register an image to a CAD model using a single keyframe. This registration is based on sparse 3D information from the model linked to the keyframe during its offline registration. We demonstrate this method in our in-house IAR software for visual inspection and documentation: VID.},
keywords={augmented reality;CAD;image registration;industrial engineering;production engineering computing;solid modelling;photo-based industrial augmented reality;single keyframe registration procedure;still image;sparse 3D information;CAD model;Augmented reality;Matrix decomposition;Transmission line matrix methods;Cameras;Documentation;Registers;Inspection;Application software;Head;Displays},
doi={10.1109/ISMAR.2009.5336468},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336469,
author={L. {Gruber} and S. {Zollmann} and D. {Wagner} and D. {Schmalstieg}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Evaluating the trackability of natural feature-point sets},
year={2009},
volume={},
number={},
pages={189-190},
abstract={In this work we present a novel idea of evaluating natural feature-point based tracking targets. Our main objective is to evaluate the inherent characteristics of natural feature-point sets with respect to vision-based pose estimation algorithms. Our work attempts to break new ground by concentrating on evaluating complete tracking targets, rather than evaluating tracking methods or single features. This allows deriving indications on how to improve the trackability of natural feature point sets.},
keywords={augmented reality;pose estimation;trackability;natural feature-point sets;vision-based pose estimation algorithms;Target tracking;Computational modeling;Computer vision;Robustness;Object detection;Pipelines;Algorithm design and analysis;Image processing;Runtime;Karhunen-Loeve transforms;Natural Feature Tracking Target Design;Augmented Reality;Tracking Simulation},
doi={10.1109/ISMAR.2009.5336469},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336470,
author={W. {Guan} and L. {Wang} and J. {Mooser} and S. {You} and U. {Neumann}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Robust pose estimation in untextured environments for augmented reality applications},
year={2009},
volume={},
number={},
pages={191-192},
abstract={We present a robust camera pose estimation approach for stereo images captured in untextured environments. Unlike most of existing registration algorithms which are point-based and make use of intensities of pixels in the neighborhood, our approach imports line segments in registration process. With line segments as primitives, the proposed algorithm is capable to handle untextured images such as scenes captured in man-made environments, as well as the cases when there are large viewpoint changes or illumination changes. Furthermore, since the proposed algorithm is robust to large base-line stereos, there are improvements on the accuracy of 3D points reconstruction. With well-calculated camera pose and object positions in 3D space, we can embed virtual objects into existing scene with higher accuracy for realistic effects. In our experiments, 2D labels are embedded in the 3D scene space to achieve annotation effects as in AR.},
keywords={augmented reality;cameras;image registration;image segmentation;pose estimation;stereo image processing;robust camera pose estimation;untextured environments;augmented reality;stereo images;point-based algorithm;image registration;man-made environments;3D points reconstruction;annotation effect;line segments;Robustness;Augmented reality;Image segmentation;Cameras;Layout;Lighting;Application software;Image registration;Stereo vision;Computer graphics;augmented reality;pose estimation;image registration},
doi={10.1109/ISMAR.2009.5336470},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336471,
author={T. N. {Hoang} and B. H. {Thomas}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={In-situ refinement techniques for outdoor geo-referenced models using mobile AR},
year={2009},
volume={},
number={},
pages={193-194},
abstract={We present a set of techniques to perform in-situ refinements on simple geo-referenced models using mobile augmented reality systems. The refinements include affine transformations to the model and surface feature additions, including high detail concave and convex features. The techniques employ pinch gloves and a single-point laser rangefinder augmented with an orientation sensor as input devices. Finished models can be exported for use with other geospatial applications. The proposed techniques are intended to be an effective and elegant approach to enhancing outdoor models using mobile augmented reality.},
keywords={augmented reality;data gloves;data visualisation;laser ranging;mobile computing;solid modelling;in-situ refinement technique;outdoor geo-referenced model;mobile AR;mobile augmented reality system;affine transformation;surface concave feature;surface convex feature;pinch glove;single-point laser rangefinder;orientation sensor;geospatial application;3D modelling;data visualization;Augmented reality;Mobile computing;Wearable computers;Laser modes;Buildings;Earth;Visualization;Solids;Thumb;Clouds;3D modelling;geo-referenced models;outdoor AR},
doi={10.1109/ISMAR.2009.5336471},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336465,
author={M. {Huber} and M. {Schlegel} and G. {Klinker}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Temporal calibration in multisensor tracking setups},
year={2009},
volume={},
number={},
pages={195-196},
abstract={Spatial tracking is one of the most challenging parts of Augmented Reality. Many AR applications rely on the fusion of several tracking systems in order to optimize the overall performance. While the topic of sensor fusion has already seen considerable interest, most results only deal with the integration of particular setups. A crucial part of sensor fusion is the temporal alignment of the sensor signals, as sensors in general are not synchronized. We present a general method to calibrate the temporal offset between different sensors by applying the normalized cross correlation method.},
keywords={augmented reality;calibration;correlation methods;sensor fusion;tracking;temporal calibration;multisensor tracking setup;spatial tracking;augmented reality;sensor fusion;sensor signals temporal alignment;cross correlation method;Calibration;Synchronization;Sensor fusion;Hardware;Time measurement;Augmented reality;Clocks;Operating systems;Cameras;Signal processing;sensor fusion;calibration;tracking;ubiquitous tracking;synchronization},
doi={10.1109/ISMAR.2009.5336465},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336466,
author={Y. {Hwang} and S. {Lampotang} and N. {Gravenstein} and I. {Luria} and B. {Lok}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Integrating conversational virtual humans and mannequin patient simulators to present mixed reality clinical training experiences},
year={2009},
volume={},
number={},
pages={197-198},
abstract={We integrated two simulators, a Virtual Patient (VP) system and a physical Human Patient Simulator (HPS) to create a mixed reality conscious sedation training environment. The VP system simulates clinical environments where learners conduct patient interviews. The HPS is a mannequin that models and simulates a wide array of clinical signs. The VP+HPS combination provides new capabilities specially suited for conscious sedation training. Future work will evaluate the efficacy of this MR simulation.},
keywords={biomedical education;computer based training;digital simulation;human computer interaction;patient care;virtual reality;conversational virtual humans;mixed reality;virtual patient system;physical human patient simulator;conscious sedation training environment;mannequin patient simulator;Humans;Virtual reality;Medical simulation;History;Computational modeling;Drugs;Patient monitoring;Protocols;Computer simulation;Computer applications;virtual humans;mannequin patient simulator;mixed reality;conscious sedation;training;virtual patients},
doi={10.1109/ISMAR.2009.5336466},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336462,
author={P. {Keitler} and F. {Pankratz} and B. {Schwerdtfeger} and D. {Pustka} and W. {Rodiger} and G. {Klinker} and C. {Rauch} and A. {Chathoth} and J. {Collomosse} and Y. {Song}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Mobile augmented reality based 3D snapshots},
year={2009},
volume={},
number={},
pages={199-200},
abstract={We describe a mobile augmented reality application that is based on 3D snapshotting using multiple photographs. Optical square markers provide the anchor for reconstructed virtual objects in the scene. A novel approach based on pixel flow highly improves tracking performance. This dual tracking approach also allows for a new single-button user interface metaphor for moving virtual objects in the scene. The development of the AR viewer was accompanied by user studies confirming the chosen approach.},
keywords={augmented reality;image reconstruction;mobile computing;solid modelling;mobile augmented reality;3D snapshots;multiple photographs;optical square markers;virtual object reconstruction;pixel flow;tracking performance;dual tracking;single-button user interface;AR viewer;Augmented reality;Layout;Cameras;Mobile handsets;Tracking;Jitter;Algorithm design and analysis;Image reconstruction;Research and development;Usability;H.5.1 [Multimedia Information Systems]: Artificial, augmented, and virtual realities—;H.5.2 [Information Interfaces and Presentation]: User Interfaces;Interaction styles},
doi={10.1109/ISMAR.2009.5336462},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336463,
author={E. {Kwon} and G. J. {Kim} and S. {Lee}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Effects of sizes and shapes of props in tangible augmented reality},
year={2009},
volume={},
number={},
pages={201-202},
abstract={In this paper, we investigated the effect of relative difference between the virtual object and tangible prop in its size and shape in terms of usability. We have found the obvious fact that in general that manipulation is more efficient with equally sized/shaped prop and virtual objects. In addition, when decoupled, the size difference factor did not matter. While an additional experiment (in progress) is needed to confirm the true effect of shape difference, we posit that prop design should concentrate on representing the critical shape features for a given class of objects the prop is to represent. Humans are adept at recognizing and identifying objects even if there are shown at different scales and in different angles. This ability, called ldquoconstancyrdquo is weaker in the dimension of shapes, e.g. compared to sizes, which is another reason to suspect shape to be a more critical factor for effective prop design. Another significant factor, in the design of props (not treated in this paper) is the prop to virtual object alignment (which can affect the task performance in terms of finding and feeling for a stable grasp). For instance, possible choices for spatially registering a prop to a virtual object (or vice versa) can be about their respective center of gravity, about the chosen surface, etc. Future experiments and prop design method will have to take this into account as well.},
keywords={augmented reality;tangible augmented reality;tangible prop design;virtual object alignment;prop shape;prop size;Augmented reality;Usability;Testing;Shape control;Laboratories;USA Councils;Guidelines;Floppy disks;Haptic interfaces;Product design},
doi={10.1109/ISMAR.2009.5336463},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336464,
author={C. {Lee} and S. {Bonebrake} and T. {Hollerer} and D. A. {Bowman}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={A replication study testing the validity of AR simulation in VR for controlled experiments},
year={2009},
volume={},
number={},
pages={203-204},
abstract={It is extremely challenging to run controlled studies comparing multiple augmented reality (AR) systems. We use an ldquoAR simulationrdquo approach, in which a virtual reality (VR) system is used to simulate multiple AR systems. In order to validate this approach, we carefully replicated a well-known study by Ellis et al. using our simulator, obtaining comparable results.},
keywords={augmented reality;AR simulation system testing;virtual reality system;VR system;controlled experiment;multiple augmented reality system;Virtual reality;Delay;Augmented reality;Hardware;Performance analysis;System testing;Graphics;Control system synthesis;Displays;Computational modeling;I.3.7 [Three-Dimensional Graphics and Realism]: Virtual Reality;AR Simulation; I.3.6 [Methodology and Techniques]: Device independence;Replication},
doi={10.1109/ISMAR.2009.5336464},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336458,
author={M. R. {Marner} and B. H. {Thomas} and C. {Sandor}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Physical-virtual tools for spatial augmented reality user interfaces},
year={2009},
volume={},
number={},
pages={205-206},
abstract={This paper presents a new user interface methodology for Spatial Augmented Reality systems. The methodology is based on a set of physical tools that are overloaded with logical functions. Visual feedback presents the logical mode of the tool to the user by projecting graphics onto the physical tools. This approach makes the tools malleable in their functionality, with this change conveyed to the user by changing the projected information. Our prototype application implements a two handed technique allowing an industrial designer to digitally airbrush onto an augmented physical model, masking the paint using a virtualized stencil.},
keywords={augmented reality;graphical user interfaces;technical drawing;spatial augmented reality;user interfaces;physical-virtual tools;visual feedback;projected information;virtualized stencil;logical functions;graphics projection;Augmented reality;User interfaces;Paints;Feedback;Visualization;Large-scale systems;Shape;Lamps;Graphical user interfaces;Spraying;Spatial Augmented Reality;User Interfaces},
doi={10.1109/ISMAR.2009.5336458},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336459,
author={A. {Martin-Gonzalez} and S. {Heining} and N. {Navab}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Head-mounted virtual loupe with sight-based activation for surgical applications},
year={2009},
volume={},
number={},
pages={207-208},
abstract={This work presents the development of an augmented reality magnification system, termed virtual loupe, implemented in a head-mounted display for surgical applications. The system provides a magnified view with a novel control based on tracked sight orientation. The system was evaluated by measuring the completion time of a suturing task performed by surgeons. The magnifying approach implemented proved to be useful by providing global context of the operating field. The sight-based activation was widely accepted by surgeons as a useful functionality to control viewing modalities.},
keywords={augmented reality;helmet mounted displays;medical computing;surgery;head-mounted virtual loupe;sight-based activation;surgical application;augmented reality magnification system;Surgery;Cameras;Target tracking;Biomedical imaging;Augmented reality;Computer displays;Visualization;Microscopy;Application software;Medical control systems;Augmented reality;medical visualization;user interaction},
doi={10.1109/ISMAR.2009.5336459},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336460,
author={Q. {Pan} and G. {Reitmayr} and T. W. {Drummond}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Interactive model reconstruction with user guidance},
year={2009},
volume={},
number={},
pages={209-210},
abstract={ProFORMA, an on-line reconstruction system for textured objects rotated by a user's hand, can be coupled with augmented reality (AR) to allow users to rapidly generate textured 3D models. We demonstrate how the use of an overlaid mesh model and 3D arrow can be used to assist the user in view planning, guiding the user to collect new keyframes from desirable views. The method described is particularly suited for use with AR headsets, providing guidance with minimal user input and allowing in situ modelling using the head-mounted camera (ProFORMA does not require a completely stationary camera).},
keywords={augmented reality;image reconstruction;image texture;interactive devices;solid modelling;interactive model reconstruction;user guidance;ProFORMA;on-line reconstruction system;textured objects;user hand rotation;augmented reality;textured 3D model;overlaid mesh model;3D arrow;view planning;minimal user input;head-mounted camera;Cameras;Uncertainty;Visualization;Wire;Mice;Augmented reality;User interfaces;Usability;Yarn},
doi={10.1109/ISMAR.2009.5336460},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336461,
author={C. {Sandor} and A. {Cunningham} and U. {Eck} and D. {Urquhart} and G. {Jarvis} and A. {Dey} and S. {Barbier} and M. R. {Marner} and S. {Rhee}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Egocentric space-distorting visualizations for rapid environment exploration in mobile mixed reality},
year={2009},
volume={},
number={},
pages={211-212},
abstract={Throughout the last decade, mobile information browsing has become a widely-adopted practice. Most of today's mobile Internet devices contain facilities to display maps of the user's surroundings with points of interest embedded into the map. Other researchers have already explored complementary, egocentric visualizations of these points of interest using mobile mixed reality. However, it is challenging to display off-screen or occluded points of interest. We have designed and implemented space-distorting visualizations to address these situations. Based on the informal user feedback that we have gathered, we have performed several iterations on our visualizations. We hope that our initial results can inspire other researchers to also investigate space-distorting visualizations for mixed and augmented reality.},
keywords={augmented reality;data visualisation;Internet;iterative methods;mobile computing;egocentric space-distorting visualizations;rapid environment exploration;mobile mixed reality;mobile Internet devices;iterations;augmented reality;Visualization;Virtual reality;Cameras;Uncertainty;Wire;Mice;Augmented reality;User interfaces;Usability;Yarn;H.5.1. [Information Interfaces and Presentation]: Multimedia Information Systems;[Artificial, augmented and virtual realities] I.3.6 [Computer Graphics]: Methodology and Techniques;[Interaction Techniques]},
doi={10.1109/ISMAR.2009.5336461},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336455,
author={H. {Seichter} and R. {Grasset} and J. {Looser} and M. {Billinghurst}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Multitouch interaction for Tangible User Interfaces},
year={2009},
volume={},
number={},
pages={213-214},
abstract={We introduce a novel touch-based interaction technique for tangible user interfaces (TUIs) in Augmented Reality (AR) applications. The technique allows for direct access and manipulation of virtual content on a registered tracking target, is robust and lightweight, and can be applied in numerous tracking and interaction scenarios.},
keywords={augmented reality;target tracking;user interfaces;multitouch interaction;tangible user interface;augmented reality;target tracking;User interfaces;Target tracking;Fingers;Computer vision;Electronic mail;Robustness;Augmented reality;Computer interfaces;Cameras;Image sampling;H.5.1 [Information Systems]: Information Interfaces and Presentation;Artificial, augmented, and virtual realities; I.4.6 [Computing Methodologies]: Image Processing and Computer Vision;Edge and feature detection},
doi={10.1109/ISMAR.2009.5336455},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336456,
author={G. {Simon}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Immersive image-based modeling of polyhedral scenes},
year={2009},
volume={},
number={},
pages={215-216},
abstract={In this paper, we describe a purely image-based system that allows a user to interactively capture the 3D geometry of a polyhedral scene with the aid of its physical presence. A video camera is used as both an interaction and tracking device. The 3D user interface is intuitive to a non-expert and the mouseless control procedure makes the system particularly suitable for mobile devices such as PDAs and mobile phones. The efficiency and accuracy of the method are demonstrated on a polyhedral scene made of two house-like boxes.},
keywords={computer graphics;graphical user interfaces;video signal processing;immersive image-based modeling;polyhedral scenes;video camera;3D user interface;PDA;mobile phones;Layout;Cameras;Solid modeling;Geometry;Mice;User interfaces;Mobile handsets;Computer graphics;Turning;Head;Augmented Reality;Image-Based Modeling;Construction at a Distance;3D User Interfaces;Wearable Computers},
doi={10.1109/ISMAR.2009.5336456},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336457,
author={Y. {Uranishi} and A. {Ihara} and H. {Sasaki} and Y. {Manabe} and K. {Chihara}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Real-time representation of inter-reflection for cubic marker},
year={2009},
volume={},
number={},
pages={217-218},
abstract={This paper proposes a method for rendering an inter-reflection between a marker and a glossy floor in Augmented Reality (AR). At first, a reflectance ratio of the floor is estimated from the reflection on the floor and from the marker box directly. Then the roughness of the floor is estimated based on the sharpness of the reflected marker box image on the floor. Lastly, the marker box reflection is eliminated based on the surrounding colors of the marker box reflection. Rendered images show that natural inter-reflection can be achieved by using the proposed method.},
keywords={augmented reality;rendering (computer graphics);real-time representation;cubic marker box reflection;inter-reflection image rendering;augmented reality;reflectance ratio;glossy floor estimation;Optical reflection;Reflectivity;Rendering (computer graphics);Electronic mail;Cameras;Light sources;Rough surfaces;Surface roughness;Augmented reality;Mirrors;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems;Artificial, Augmented, and Virtual Realities},
doi={10.1109/ISMAR.2009.5336457},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336451,
author={S. {Uusitalo} and P. {Eskolin} and P. {Belimpasakis}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={A solution for navigating user-generated content},
year={2009},
volume={},
number={},
pages={219-220},
abstract={We are interested how to contextualize the digital content of an individual user with the help of content from other users, how to create a positive user experience from that, and what kind of interaction that can encourage between users. As a result the environment can be modelled. Creating a digital representation of the world from photographs is an exciting opportunity, for e.g. as a registering system for Augmented Reality (AR), forming one cornerstone for what Hollerer et al. call Anywhere Augmentation (AA). In this paper we present the user experience of the prototype and its key features. We have implemented a prototype solution for discovering shared digital media via a novel user interface, presenting the spatial relationships of the content.},
keywords={augmented reality;data mining;multimedia computing;user interfaces;navigating user generated content;discovering shared digital media via user interface;content spatial relationship;individual user digital content;world digital representation;User-generated content;Prototypes;Augmented reality;Mirrors;Photography;Satellite navigation systems;Virtual reality;Videos;Digital cameras;Application software},
doi={10.1109/ISMAR.2009.5336451},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336452,
author={C. A. L. {Waechter} and D. {Pustka} and G. J. {Klinker}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Vision based people tracking for ubiquitous Augmented Reality applications},
year={2009},
volume={},
number={},
pages={221-222},
abstract={The task of vision based people tracking is a major research problem in the context of surveillance applications or human behavior estimation, but it has had only minimal impact on (Ubiquitous) Augmented Reality applications thus far. Deploying stationary infrastructural cameras within indoor environments for the purpose of Augmented Reality could provide a users' devices with additional functionality that a small device and mobile sensors cannot provide to its user. Therefore people tracking could be expected to become an ubiquitously available infrastructural element in buildings since surveillance cameras are widely used. The use for scenarios indoors or close to buildings is obvious. We present and discuss several different ways where people tracking in real-time could influence the fields of Augmented Reality and further vision based applications.},
keywords={augmented reality;cameras;computer vision;sensor fusion;surveillance;tracking;ubiquitous computing;vision based people tracking;ubiquitous augmented reality;surveillance;human behavior estimation;stationary infrastructural cameras;mobile sensors;sensor fusion;Augmented reality;Cameras;Surveillance;Sensor fusion;Global Positioning System;Target tracking;Particle filters;Image analysis;Sensor systems;Fuses;Augmented Reality;People Tracking;Sensor Fusion},
doi={10.1109/ISMAR.2009.5336452},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336453,
author={R. C. {Yeoh} and S. Z. {Zhou}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Consistent real-time lighting for virtual objects in augmented reality},
year={2009},
volume={},
number={},
pages={223-224},
abstract={We present a technique for rendering realistic shadows of virtual objects in a mixed reality environment by recovering the light source distribution of a scene in real-time, through the segmentation and analysis of a known occluding object's shadows. A fiducial marker provides information about the position of the occluding object and the plane of the surface on which shadows are cast, and serves as the origin of a marker coordinate system. A new shadow segmentation approach is carried out on the shadow image and is able to recover geometrical information on multiple faint shadows. Using normalised iterative reinforcement, noise and artifacts can be suppressed in the final shadow map. The scene's light source distribution is then extrapolated using geometrical data from both the occluding object and its cast shadows. Virtual light sources in a game engine are used to mimic real light sources and achieve consistent illumination and increase the realism of the augmented reality scene.},
keywords={augmented reality;geometry;image segmentation;lighting;rendering (computer graphics);augmented reality;virtual object;consistent real-time lighting;realistic shadow rendering;light source distribution;shadow image segmentation approach;fiducial marker;marker coordinate system;geometrical information;normalised iterative reinforcement;game engine;Augmented reality;Layout;Light sources;Virtual reality;Lighting;Cameras;Probes;Image edge detection;Image segmentation;Engines;Radiosity;illumination;shadow;augmented reality;realistic rendering},
doi={10.1109/ISMAR.2009.5336453},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336454,
author={},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Tutorials workshops},
year={2009},
volume={},
number={},
pages={225-225},
abstract={Provides an abstract for each of the tutorial presentations and a brief professional biography of each presenter. The complete presentations were not made available for publication as part of the conference proceedings.},
keywords={},
doi={10.1109/ISMAR.2009.5336454},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336449,
author={C. {Hughes} and J. {Quarles}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Comprehensive tutorials},
year={2009},
volume={},
number={},
pages={226-226},
abstract={Provides an abstract for each of the tutorial presentations and a brief professional biography of each presenter. The complete presentations were not made available for publication as part of the conference proceedings.},
keywords={Tutorial;Augmented reality;Research and development;Displays;Human factors;Laboratories},
doi={10.1109/ISMAR.2009.5336449},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336450,
author={E. {Smith}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Experiential learning with Mixed and Augmented Reality},
year={2009},
volume={},
number={},
pages={227-227},
abstract={},
keywords={Augmented reality;Virtual reality;Space technology;Continuing education;Convergence;Laboratories;Facebook;Technological innovation;Medical simulation;Internet},
doi={10.1109/ISMAR.2009.5336450},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336447,
author={M. {Trier}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Transforming lives: High-performance, high-risk training with mixed reality},
year={2009},
volume={},
number={},
pages={227-227},
abstract={},
keywords={Virtual reality;Augmented reality;Space technology;Continuing education;Convergence;Laboratories;Facebook;Technological innovation;Medical simulation;Internet},
doi={10.1109/ISMAR.2009.5336447},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336448,
author={N. {Guldemond}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Industrial workshop: Manufacturing the future},
year={2009},
volume={},
number={},
pages={228-228},
abstract={},
keywords={},
doi={10.1109/ISMAR.2009.5336448},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336445,
author={C. {Perey}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Mobile Magic Wand: Augmented reality on mobile devices},
year={2009},
volume={},
number={},
pages={228-228},
abstract={},
keywords={Augmented reality;Commercialization;Manufacturing industries;Assembly;Packaging;Industrial training;Process planning;Manufacturing processes;Mobile handsets;Internet},
doi={10.1109/ISMAR.2009.5336445},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336446,
author={C. {Sandor} and I. {Kitahara} and G. {Reitmayr} and S. {Feiner} and Y. {Ohta}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Lets go out: Research in outdoor mixed and augmented reality},
year={2009},
volume={},
number={},
pages={229-229},
abstract={},
keywords={Augmented reality;Social network services;Cameras;Large-scale systems;Australia;Lighting;Displays;Physics computing;Cellular phones;Graphics},
doi={10.1109/ISMAR.2009.5336446},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336443,
author={T. {Hollerer} and D. {Schmalstieg} and M. {Billinghurst}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={AR 2.0: Social Augmented Reality - social computing meets Augmented Reality},
year={2009},
volume={},
number={},
pages={229-230},
abstract={},
keywords={Augmented reality;Social network services;Cameras;Large-scale systems;Australia;Lighting;Displays;Physics computing;Cellular phones;Graphics},
doi={10.1109/ISMAR.2009.5336443},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336444,
author={},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Panel},
year={2009},
volume={},
number={},
pages={231-232},
abstract={Start of the above-titled section of the conference proceedings record.},
keywords={},
doi={10.1109/ISMAR.2009.5336444},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336441,
author={H. {Tamura} and H. {Kato}},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Proposal of international voluntary activities on establishing benchmark test schemes for AR/MR geometric registration and tracking methods},
year={2009},
volume={},
number={},
pages={233-236},
abstract={This is a proposal to the ISMAR community from Japanese AR/MR researchers for the future progress of AR/MR technology. We hope to expand our activities over the ISMAR community and call for international participants who would take part in a number of voluntary works. At the same time, this paper presents a current view of the outcomes of these activities may have be in due course. We will focus here on the various tracking methods, one of the most active themes at the annual ISMAR symposiums. The main goal of our activities is to build a framework to comprehensively evaluate a variety of existing and future tracking methods. Strictly speaking, our targets should include all the geometric registration methods that merge the real and virtual world seamlessly. They can also be termed as real-time 3D matchmove. After initial registration (calibration) of two spaces is achieved, either object tracking or camera tracking is required when the subject or camera moves. In addition to methods suited to static registration, there exist also methods that focus solely on improving the performance of tracking, without calibration. In this paper, when we use the terms "registration and tracking" or simply "tracking" we will be referring to the general definition of tracking as given above.},
keywords={augmented reality;tracking;international voluntary activities;benchmark test schemes;augmented reality;mixed reality;geometric registration method;tracking method;ISMAR symposium;real-time 3D matchmove;object tracking;camera tracking;static registration;Proposals;Benchmark testing;Target tracking;Virtual reality;Calibration;Cameras;Software testing;Pipelines;Recruitment},
doi={10.1109/ISMAR.2009.5336441},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336442,
author={},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={Author index},
year={2009},
volume={},
number={},
pages={237-237},
abstract={The author index contains an entry for each author and coauthor included in the proceedings record.},
keywords={},
doi={10.1109/ISMAR.2009.5336442},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5336439,
author={},
booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, title={[Front and back covers]},
year={2009},
volume={},
number={},
pages={c1-c4},
abstract={Presents the front and back cover or splash screen of the proceedings record.},
keywords={},
doi={10.1109/ISMAR.2009.5336439},
ISSN={},
month={Oct},}