@INPROCEEDINGS{1544641,
author={},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Proceedings. International Symposium on Mixed and Augmented Reality},
year={2005},
volume={},
number={},
pages={},
abstract={The following topics are dealt with: image rendering; head-mounted displays; vision-based tracking; collaborative mixed reality; interaction techniques; augmented reality; calibration; authoring systems.},
keywords={augmented reality;rendering (computer graphics);image processing;helmet mounted displays;rendering;head-mounted displays;vision-based tracking;collaborative mixed reality;interaction techniques;augmented reality;calibration;authoring systems},
doi={10.1109/ISMAR.2005.34},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544642,
author={},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality - Title Page},
year={2005},
volume={},
number={},
pages={i-iii},
abstract={Conference proceedings title page.},
keywords={},
doi={10.1109/ISMAR.2005.36},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544643,
author={},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality - Copyright Page},
year={2005},
volume={},
number={},
pages={iv-iv},
abstract={Copyright and Reprint Permissions: Abstracting is permitted with credit to the source. Libraries may photocopy beyond the limits of US copyright law, for private use of patrons, those articles in this volume that carry a code at the bottom of the first page, provided that the per-copy fee indicated in the code is paid through the Copyright Clearance Center. The papers in this book comprise the proceedings of the meeting mentioned on the cover and title page. They reflect the authors' opinions and, in the interests of timely dissemination, are published as presented and without change. Their inclusion in this publication does not necessarily constitute endorsement by the editors or the Institute of Electrical and Electronics Engineers, Inc.},
keywords={},
doi={10.1109/ISMAR.2005.33},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544644,
author={},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality [Table of contents]},
year={2005},
volume={},
number={},
pages={v-viii},
abstract={Presents the table of contents of the proceedings.},
keywords={},
doi={10.1109/ISMAR.2005.35},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544645,
author={},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Message from the General Chairs},
year={2005},
volume={},
number={},
pages={ix-ix},
abstract={Presents the welcome message from the conference proceedings.},
keywords={},
doi={10.1109/ISMAR.2005.40},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544646,
author={},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Message from the Program Chairs},
year={2005},
volume={},
number={},
pages={x-x},
abstract={Presents the welcome message from the conference proceedings.},
keywords={},
doi={10.1109/ISMAR.2005.41},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544647,
author={},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Organizing Committee},
year={2005},
volume={},
number={},
pages={xi-xi},
abstract={Provides a listing of current committee members.},
keywords={},
doi={10.1109/ISMAR.2005.44},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544648,
author={},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Steering Committee and Area Chairs},
year={2005},
volume={},
number={},
pages={xii-xii},
abstract={Provides a listing of current committee members.},
keywords={},
doi={10.1109/ISMAR.2005.55},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544649,
author={},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Program Committee},
year={2005},
volume={},
number={},
pages={xiii-xiv},
abstract={Provides a listing of current committee members.},
keywords={},
doi={10.1109/ISMAR.2005.47},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544650,
author={},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Additional Reviewers},
year={2005},
volume={},
number={},
pages={xv-xv},
abstract={The publication offers a note of thanks and lists its reviewers.},
keywords={},
doi={10.1109/ISMAR.2005.9},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544651,
author={},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Support},
year={2005},
volume={},
number={},
pages={xvi-xvi},
abstract={The conference organizers greatly appreciate the support of the various corporate sponsors listed.},
keywords={},
doi={10.1109/ISMAR.2005.56},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544652,
author={},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Workshop [Industrial Augmented Reality]},
year={2005},
volume={},
number={},
pages={xvii-xviii},
abstract={Augmented Reality has matured from a pure research field into actual industrial applications. There are still many research questions to solve, and solutions are discussed every year at the ISMAR conference series. However, making systems work reliably in an industrial setting usually requires more effort, knowledge, and ideas than can be discussed by academia at a purely scientific conference. The goal of this workshop is to bring together people from industry and academia who use augmented reality technologies in real industrial settings aimed at producing a commercial benefit. The workshop provides a platform to jointly discuss the “devil in the detail” and to identify ways to make the leap from research-based demonstrations to fully integrated systems. It is organized as a series of talks intended for a broad audience interested in the current application of augmented reality technology in industrial settings.},
keywords={},
doi={10.1109/ISMAR.2005.62},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544653,
author={},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Panel - Hand-held Augmented Reality},
year={2005},
volume={},
number={},
pages={xix-xxi},
abstract={},
keywords={Augmented reality;Mobile handsets;Hardware;Computer graphics;Mobile communication;Cellular phones;Personal digital assistants;Cameras;Robustness;Application software},
doi={10.1109/ISMAR.2005.45},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544654,
author={E. {Badique}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Mixed Reality Research: The European Dimension},
year={2005},
volume={},
number={},
pages={xxii-xxii},
abstract={},
keywords={Virtual reality;Collaboration;Image processing;Proposals;Digital images;Bit rate;Video coding;Research and development},
doi={10.1109/ISMAR.2005.42},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544655,
author={C. {Stapleton}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={The Art of Technology and the Future of MR: Integrating an Artistic Approach to Transform the Next Generation of Mixed and Augmented Reality},
year={2005},
volume={},
number={},
pages={xxiii-xxiii},
abstract={},
keywords={Subspace constraints;Augmented reality;Virtual reality;Laboratories;Art;Technological innovation;Convergence;Commercialization;Cellular phones;Turning},
doi={10.1109/ISMAR.2005.59},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544656,
author={C. {Stapleton} and E. {Smith} and C. E. {Hughes}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={The art of nurturing citizen scientists through mixed reality},
year={2005},
volume={},
number={},
pages={2-11},
abstract={Modern society requires the public to have an increased knowledge of science beyond the basics, with average people needing to understand concepts in science, technology, engineering and mathematics that rapidly evolve throughout their lifetimes. Our future depends upon closing the gaps between citizen and scientist to create a "citizen scientist". Mixed reality innovations provide the magic to spark a lifetime of learning for community learning centers by continuously providing new content and reinventing the free-choice learning experience that helps citizens make informed choices in an increasingly complex world. This work explores a mixed reality experiential learning landscape that expands our ability to provide dynamic content structures for venues to engage the user's physical environment and interactive imagination by incorporating the conventions of story, play and game employed in competing leisure time activities. This is intended to reverse the standard decline of attendance by teenagers and young adults.},
keywords={virtual reality;computer aided instruction;humanities;citizen scientists;community learning centers;mixed reality experiential learning landscape;interactive imagination;Subspace constraints;Virtual reality;Mathematics;Environmental economics;Computational modeling;Computer simulation;Computer science;Laboratories;Knowledge engineering;Technological innovation},
doi={10.1109/ISMAR.2005.58},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544657,
author={O. {Bimber} and G. {Wetzstein} and A. {Emmerling} and C. {Nitschke}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Enabling view-dependent stereoscopic projection in real environments},
year={2005},
volume={},
number={},
pages={14-23},
abstract={We show how view-dependent image-based and geometric warping, radiometric compensation, and multi-focal projection enable a view-dependent stereoscopic visualization on ordinary (geometrically complex, colored and textured) surfaces within everyday environments. Special display configurations for immersive or semi-immersive AR/VR applications that require permanent and artificial projection canvases might become unnecessary. We demonstrate several ad-hoc visualization examples in a real architectural and museum application context.},
keywords={data visualisation;stereo image processing;computer displays;augmented reality;view-dependent stereoscopic projection;real environments;view-dependent image-based warping;geometric warping;radiometric compensation;multifocal projection;view-dependent stereoscopic visualization;museum;Visualization;Radiometry;Surface texture;Virtual reality;Augmented reality;Water resources;Two dimensional displays;Geometrical optics;Nonlinear optics;Optical distortion},
doi={10.1109/ISMAR.2005.27},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544658,
author={A. {Olwal} and C. {Lindfors} and J. {Gustafsson} and T. {Kjellberg} and L. {Mattsson}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={ASTOR: an autostereoscopic optical see-through augmented reality system},
year={2005},
volume={},
number={},
pages={24-27},
abstract={We present a novel autostereoscopic optical see-through system for augmented reality (AR). It uses a transparent holographic optical element (HOE) to separate the views produced by two, or more, digital projectors. It is a minimally intrusive AR system that does not require the user to wear special glasses or any other equipment, since the user would see different images depending on the point of view. The HOE itself is a thin glass plate or plastic film that can easily be incorporated into other surfaces, such as a window. The technology offers great flexibility, allowing the projectors to be placed where they are the least intrusive. ASTOR's capability of sporadic AR visualization is currently ideal for smaller physical workspaces, such as our prototype setup in an industrial environment.},
keywords={augmented reality;holographic optical elements;data visualisation;optical projectors;stereo image processing;visual perception;computer displays;ASTOR;autostereoscopic optical see-through augmented reality system;holographic optical element;intrusive AR system;sporadic AR visualization;Augmented reality;Holographic optical components;Holography;Displays;Optical films;Glass;Prototypes;Handheld computers;Mirrors;Numerical analysis;augmented reality;optical see-through;autostereoscopy;holographic optical element;system;projection-based},
doi={10.1109/ISMAR.2005.15},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544659,
author={A. {State} and K. P. {Keller} and H. {Fuchs}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Simulation-based design and rapid prototyping of a parallax-free, orthoscopic video see-through head-mounted display},
year={2005},
volume={},
number={},
pages={28-31},
abstract={We built a video see-through head-mounted display with zero eye offset from commercial components and a mount fabricated via rapid prototyping. The orthoscopic HMD's layout was created and optimized with a software simulator. We describe simulator and HMD design, we show the HMD in use and demonstrate zero parallax.},
keywords={software prototyping;helmet mounted displays;digital simulation;medical image processing;augmented reality;video cameras;surgery;simulation-based design;rapid prototyping;parallax-free orthoscopic video see-through head-mounted display;zero eye offset;orthoscopic HMD layout;software simulator;zero parallax;minimally invasive surgery;medical AR;augmented reality;Virtual prototyping;Displays;Cameras;Mirrors;Avatars;Biomedical optical imaging;Computational modeling;Optical devices;Eyes;Optical distortion},
doi={10.1109/ISMAR.2005.52},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544660,
author={H. {Hua} and C. {Gao}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={A polarized head-mounted projective display},
year={2005},
volume={},
number={},
pages={32-35},
abstract={The lack of image brightness is a common problem in optical see-through head-mounted displays (OST-HMD) where a beamsplitter is required to combine views from HMD image source and the direct-view of a real world scene. This problem is further aggregated in a head-mounted projective display (HMPD) due to the fact that light passes through the beamsplitter multiple times. We present a novel design of an ultra-bright polarized head-mounted projective display (p-HMPD). The image brightness observed by a viewer theoretically is four-times brighter than existing designs. We further demonstrate a design with currently available technology that leads to a display in which the observed image is significantly brighter than existing designs. Finally, experimental results from a bench setup are presented.},
keywords={helmet mounted displays;polarisation;brightness;optical beam splitters;image processing;image brightness;optical see-through head-mounted displays;beamsplitter;ultra-bright polarized head-mounted projective display;Layout;Optical polarization;Brightness;Optical attenuators;Biomedical optical imaging;Computer displays;Prototypes;Collaboration;Augmented reality;Optical computing},
doi={10.1109/ISMAR.2005.6},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544661,
author={T. {Sielhorst} and T. {Blum} and N. {Navab}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Synchronizing 3D movements for quantitative comparison and simultaneous visualization of actions},
year={2005},
volume={},
number={},
pages={38-47},
abstract={In our poster presentation at ISMAR '04, we proposed the idea of an AR training solution including capture and 3D replays of subtle movements. The crucial part missing for realizing such a training system was an appropriate way of synchronizing trajectories of similar movements with varying speed in order to simultaneously visualize the motion of experts and trainees, and to study trainees' performances quantitatively. We review the research from different communities on synchronization problems of similar complexity. We give a detailed description of the two most applicable algorithms. We then present results using our AR based forceps delivery training system and therefore evaluate both methods for synchronization of experts' and trainees' 3D movements. We also introduce the first concepts of an online synchronization system allowing the trainee to follow movements of an expert and the experts to annotate 3D trajectories for initiation of actions such as display of timely information. A video demonstration provides an overview of the work and a visual idea of what users of the proposed system could observe through their video-see through HMD.},
keywords={data visualisation;augmented reality;synchronisation;biomedical education;computer based training;helmet mounted displays;medical image processing;digital simulation;3D movement synchronization;quantitative comparison;AR based forceps delivery training system;online synchronization system;3D trajectories;video-see through HMD;medical education;AR birth simulator;virtual image;Visualization;Imaging phantoms;Target tracking;Medical simulation;Augmented reality;Head;Biomedical imaging;Instruments;Cameras;Three dimensional displays},
doi={10.1109/ISMAR.2005.57},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544662,
author={F. {Melchior} and T. {Laubach} and D. {de Vries}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Authoring and user interaction for the production of wave field synthesis content in an augmented reality system},
year={2005},
volume={},
number={},
pages={48-51},
abstract={Wave field synthesis (WFS) enables the accurate reproduction of a sound field for a large listening area with correct characteristics for each listener position. An exact perspective on the synthesized wave field is provided for every listener. Therefore, WFS-technology is ideally suited to be combined with augmented reality systems, where every user perceives his own visual perspective of a given scene. This work presents a concept for authoring and user interaction for the production of wave field synthesis content in an augmented reality system. Also, the implementation of a prototype WFS-AR system based on ARToolkit is explained.},
keywords={sound reproduction;audio user interfaces;augmented reality;authoring systems;visual perception;audio signal processing;signal synthesis;acoustic waves;acoustic field;user interaction;wave field synthesis content production;augmented reality system;sound field;visual perspective;ARToolkit;authoring systems;Production systems;Augmented reality;User interfaces;Layout;Integral equations;Surface waves;Hardware;Workstations;Prototypes;Circuits},
doi={10.1109/ISMAR.2005.20},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544663,
author={G. R. {King} and W. {Piekarski} and B. H. {Thomas}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={ARVino - outdoor augmented reality visualisation of viticulture GIS data},
year={2005},
volume={},
number={},
pages={52-55},
abstract={This work describes the combination of two technologies, augmented reality and GIS, to provide a new way to visualise viticulture GIS data using outdoor mobile computers. Viticulturists use GIS to assist with accurately understanding the parameters that affect their yields and quality of the grapes from different vineyards. The ability to view this data in the field digitally would be advantageous to the viticulturist. This work describes the ARVino system; an AR platform that was built for visualising 3D data outdoors using a movable tripod-based computer. We describe the user interface, some problems that were encountered, and how the visualisation and interface were evaluated through an expert review.},
keywords={agriculture;data visualisation;augmented reality;geographic information systems;mobile computing;user interfaces;agricultural products;ARVino;outdoor augmented reality visualisation;viticulture GIS data;outdoor mobile computers;grape quality;vineyards;3D data visualisation;movable tripod-based computer;user interface;Augmented reality;Data visualization;Geographic Information Systems;Wearable computers;Portable computers;Australia;Mobile computing;Pipelines;Virtual environment;Displays},
doi={10.1109/ISMAR.2005.14},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544664,
author={M. {Tonnis} and C. {Sandor} and G. {Klinker} and C. {Lange} and H. {Bubb}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Experimental evaluation of an augmented reality visualization for directing a car driver's attention},
year={2005},
volume={},
number={},
pages={56-59},
abstract={With recent advances of head-up display technology in cars, augmented reality becomes interesting in supporting the driving task to guide a driver's attention. We have set up an experiment to compare two different approaches to inform the driver about dangerous situations around the car. One approach used AR to visualize the source of danger in the driver's frame of reference while the other one presented information in an egocentric frame of reference. Both approaches were evaluated in user tests.},
keywords={driver information systems;augmented reality;data visualisation;automobiles;road safety;head-up displays;experimental evaluation;augmented reality visualization;car driver attention redirection;head-up display technology;Augmented reality;Visualization;Navigation;Displays;Testing;Automotive components;Roads;Eyes;Vehicles;Taxonomy},
doi={10.1109/ISMAR.2005.31},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544665,
author={H. {Wuest} and F. {Vial} and D. {Stricker}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Adaptive line tracking with multiple hypotheses for augmented reality},
year={2005},
volume={},
number={},
pages={62-69},
abstract={We present a real-time model-based line tracking approach with adaptive learning of image edge features that can handle partial occlusion and illumination changes. A CAD (VRML) model of the object to track is needed. First, the visible edges of the model with respect to the camera pose estimate are sorted out by a visibility test performed on standard graphics hardware. For every sample point of every projected visible 3D model line a search for gradient maxima in the image is then carried out in a direction perpendicular to that line. Multiple hypotheses of these maxima are considered as putative matches. The camera pose is updated by minimizing the distances between the projection of all sample points of the visible 3D model lines and the most likely matches found in the image. The state of every edge's visual properties is updated after each successful camera pose estimation. We evaluated the algorithm and showed the improvements compared to other tracking approaches.},
keywords={CAD;augmented reality;virtual reality languages;edge detection;feature extraction;image matching;image sampling;tracking;hidden feature removal;solid modelling;computer graphic equipment;video cameras;real-time visible 3D model-based line tracking approach;augmented reality;adaptive learning;image edge feature;partial occlusion;image illumination;CAD model;VRML model;camera pose estimate;graphics hardware;image gradient maxima search;image matching;edge visual property;Augmented reality;Cameras;Testing;Lighting;Performance evaluation;Hardware;Robustness;Graphics;State estimation;Gyroscopes},
doi={10.1109/ISMAR.2005.8},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544666,
author={ {Raghav Subbarao} and P. {Meer} and Y. {Gene}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={A balanced approach to 3D tracking from image streams},
year={2005},
volume={},
number={},
pages={70-78},
abstract={Estimation of camera pose is an integral part of augmented reality systems. Vision-based methods offer a flexible and accurate method for this estimation. Current vision based methods rely on markers to reduce the computation and increase robustness of the pose estimation. However, this limits the algorithm's applicability while being expensive since the markers also require maintenance. Alternatively, reconstructed scene features can be used for pose estimation but this can lead to a loss of accuracy. To avoid this we propose a two-stage balanced tracking method which does not require any visual markers in the scene. The first stage of our method is based on the sequential recovery of structure from motion which allows the system to learn the scene from a few frames in which the markers are visible. In the next stage, the learned features are used for camera tracking. The system ensures greater accuracy and reduces error drift due to its use of the HEIV estimator which is provably unbiased to the first degree. We also make use of a novel method for the detection and removal of outliers which are unavoidable in such systems. The experiments show the superiority of our method when compared to a nonlinear method based on Levenberg-Marquardt minimization.},
keywords={augmented reality;tracking;video streaming;image reconstruction;feature extraction;image motion analysis;video cameras;3D tracking;image stream;camera pose estimation;augmented reality system;vision-based method;scene feature reconstruction;two-stage balanced tracking method;visual marking;camera tracking;HEIV estimator;outlier detection;outlier removal;Levenberg-Marquardt minimization method;Streaming media;Layout;Cameras;Augmented reality;Real time systems;Robustness;Computer vision;Image reconstruction;Minimization methods;Rendering (computer graphics)},
doi={10.1109/ISMAR.2005.1},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544667,
author={A. {Henrysson} and M. {Billinghurst} and M. {Ollila}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Face to face collaborative AR on mobile phones},
year={2005},
volume={},
number={},
pages={80-89},
abstract={Mobile phones are an ideal platform for augmented reality. We describe how they also can be used to support face to face collaborative AR applications. We have created a custom port of the ARToolKit library to the Symbian mobile phone operating system and then developed a sample collaborative AR game based on this. We describe the game in detail and user feedback from people who have played it. We also provide general design guidelines that could be useful for others who are developing mobile phone collaborative AR applications.},
keywords={augmented reality;groupware;mobile handsets;mobile computing;graphical user interfaces;computer games;face to face collaborative AR;augmented reality;ARToolKit library;Symbian mobile phone operating system;collaborative AR game;user feedback;mobile phone collaborative AR application;Collaboration;Mobile handsets;Collaborative work;Application software;Collaborative software;Augmented reality;Guidelines;Libraries;User interfaces;Operating systems},
doi={10.1109/ISMAR.2005.32},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544668,
author={R. {Grasset} and P. {Lamb} and M. {Billinghurst}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Evaluation of mixed-space collaboration},
year={2005},
volume={},
number={},
pages={90-99},
abstract={Recently augmented reality (AR) technology has been used to develop the next generation collaborative interfaces. First results have shown the value of using AR for co-located tasks based on egocentric viewpoints. In contrast, virtual reality (VR) seems to offer interesting advantages for immersive collaborative experiences with egocentric viewpoints. We focus on a new area: a mixed collaboration between AR and VR environments. We present a new conceptual model of transitional interfaces that allow users to move between AR and VR viewpoints. We then describe the results of a quantitative evaluation with an AR exocentric viewpoint and a VR egocentric viewpoint for a navigational task. We also conducted a second experiment on the impact of the relationship between the interaction and visualization space in mixed collaboration. Results of these studies can provide a better understanding of how to design interfaces for multispace and transitional collaboration.},
keywords={augmented reality;groupware;data visualisation;graphical user interfaces;mixed-space collaboration;augmented reality technology;collaborative interface;egocentric viewpoint;virtual reality;visualization space;Collaboration;Virtual reality;Collaborative work;Augmented reality;Navigation;Books;Virtual environment;Switches;Space technology;Visualization},
doi={10.1109/ISMAR.2005.30},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544669,
author={K. {Tateno} and M. {Takemura} and Y. {Ohta}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Enhanced eyes for better gaze-awareness in collaborative mixed reality},
year={2005},
volume={},
number={},
pages={100-103},
abstract={The concept of "enhanced eyes" to restore gaze awareness in a collaborative mixed-reality space is proposed. Three "enhanced eyes" schemes are described: controlling the highlight in the eyes to aid awareness of eye contact, deforming the eyelids to enhance eye motion, and adjusting the rotation angle of the eyeballs to improve perception of gaze direction. The effectiveness of the schemes has been confirmed by subjective evaluations. The "enhanced eyes" emulate the natural appearance of the face and are effective not only to indicate gaze direction but also to create the feeling of gaze.},
keywords={virtual reality;groupware;eye;visual perception;collaborative mixed-reality space;gaze awareness restoration;enhanced eyes scheme;eye contact;eyelid deformation;eye motion;visual perception;gaze direction;Eyes;Collaboration;Virtual reality;Character generation;Space technology;Motion control;Eyelids;Communication effectiveness;Computer displays;Augmented reality},
doi={10.1109/ISMAR.2005.29},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544670,
author={ {Woohun Lee} and {Jun Park}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Augmented foam: a tangible augmented reality for product design},
year={2005},
volume={},
number={},
pages={106-109},
abstract={Computer aided design applications have become designers' inevitable tools for expressing and simulating innovative ideas and concepts. However, replacing traditional materials and mock-ups with 3D CAD systems, designers are faced with the intangibility problem, unable to physically interact with test products in early stages of design process. As a touchable and graspable interface based on 3D CAD data, we propose augmented foam, which applies augmented reality technologies to physical blue foams. Using augmented foam, a blue foam mock-up is overlaid with a 3D virtual object, which is rendered with the same CAD model used for mock-up production. We presented a method to correct occlusions of the virtual products by user's hand. Augmented foam was tested for a mug design and a cleaning robot design. Designers were able to inspect and evaluate the design alternatives interactively and efficiently.},
keywords={CAD/CAM;product design;virtual manufacturing;augmented reality;graphical user interfaces;rendering (computer graphics);hidden feature removal;foams;augmented foam;tangible augmented reality;product design;computer aided design;3D CAD system interface;augmented reality;blue foam mock-up;3D virtual object rendering;CAD model;mock-up production;occlusion;virtual product;mug design;cleaning robot design;Augmented reality;Product design;Design automation;Computer applications;Application software;Computational modeling;Computer simulation;Materials testing;System testing;Process design},
doi={10.1109/ISMAR.2005.16},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544671,
author={C. {Sandor} and A. {Olwal} and B. {Bell} and S. {Feiner}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Immersive mixed-reality configuration of hybrid user interfaces},
year={2005},
volume={},
number={},
pages={110-113},
abstract={Information in hybrid user interfaces can be spread over a variety of different, but complementary, displays, with which users interact through a potentially equally varied range of interaction devices. Since the exact configuration of these displays and devices may not be known in advance, it is desirable for users to be able to reconfigure at runtime the dataflow between interaction devices and objects on the displays. To make this possible, we present the design and implementation of a prototype mixed reality system that allows users to immersively reconfigure a running hybrid user interface.},
keywords={virtual reality;graphical user interfaces;data visualisation;interactive devices;display devices;mixed-reality configuration;hybrid user interface;interaction device;display object;dataflow visualization;Virtual reality;User interfaces;Computer displays;Control systems;Computer science;Runtime;Prototypes;Visualization;Augmented reality;Three dimensional displays},
doi={10.1109/ISMAR.2005.37},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544672,
author={M. {Gandy} and B. {MacIntyre} and P. {Presti} and S. {Dow} and J. {Bolter} and B. {Yarbrough} and N. {O'Rear}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={AR Karaoke: acting in your favorite scenes},
year={2005},
volume={},
number={},
pages={114-117},
abstract={We present a concept for augmented reality entertainment, called AR Karaoke, where users perform their favorite dramatic scenes with virtual actors. AR Karaoke is the acting equivalent of traditional Karaoke, where the goal is to facilitate an acting experience for the user that is entertaining for both the user and audience. Prototype implementations were created to evaluate various user interfaces and design approach reveal guidelines that are relevant to the design of mixed reality applications in the domains of gaming, performance, and entertainment.},
keywords={augmented reality;humanities;graphical user interfaces;entertainment;computer games;AR Karaoke;augmented reality entertainment;favorite dramatic scene;virtual actor;entertainment;user interface;mixed reality application design;gaming;Layout;Prototypes;Augmented reality;Virtual reality;Displays;Application software;Software prototyping;DVD;Educational institutions;User interfaces},
doi={10.1109/ISMAR.2005.11},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544673,
author={G. {Reitmayr} and E. {Eade} and T. {Drummond}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Localisation and interaction for augmented maps},
year={2005},
volume={},
number={},
pages={120-129},
abstract={Paper-based cartographic maps provide highly detailed information visualisation with unrivalled fidelity and information density. Moreover, the physical properties of paper afford simple interactions for browsing a map or focusing on individual details, managing concurrent access for multiple users and general malleability. However, printed maps are static displays and while computer-based map displays can support dynamic information, they lack the nice properties of real maps identified above. We address these shortcomings by presenting a system to augment printed maps with digital graphical information and user interface components. These augmentations complement the properties of the printed information in that they are dynamic, permit layer selection and provide complex computer mediated interactions with geographically embedded information and user interface controls. Two methods are presented which exploit the benefits of using tangible artifacts for such interactions.},
keywords={cartography;augmented reality;graphical user interfaces;data visualisation;augmented map interaction;paper-based cartographic map;information visualisation;map browsing;printed map augmentation system;computer-based map display;digital graphical information;user interface component;computer mediated interaction;geographically embedded information;user interface control;spatially augmented reality;projection display;optical tracking;User interfaces;Computer displays;Personal digital assistants;Computer interfaces;Augmented reality;Inspection;Embedded computing;Data visualization;Spatial resolution;Surfaces;Spatially Augmented Reality;Projection displays;Tangible User Interfaces;Optical Tracking},
doi={10.1109/ISMAR.2005.39},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544674,
author={B. {Streckel} and J. -. {Evers-Senne} and R. {Koch}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Lens model selection for a markerless AR tracking system},
year={2005},
volume={},
number={},
pages={130-133},
abstract={This work describes a visual markerless real-time tracking system for augmented reality applications. The system uses a firewire camera with a fisheye lens mounted at 10 fps. Visual tracking of 3D scene points is performed simultaneously with 3D camera pose estimation without any prior scene knowledge. All visual-geometric data is acquired using a structure-from-motion approach. The lens selection was driven by research results that show the superiority of a fisheye lens to a standard perspective lens for this approach. 2D features in the hemispherical image are tracked using a 2D point tracker. Based on the feature tracks, 3D camera ego-motion and 3D features are estimated.},
keywords={augmented reality;video cameras;feature extraction;tracking;real-time systems;photographic lenses;fisheye lens model selection;visual markerless real-time AR tracking system;augmented reality;firewire camera;visual 3D scene point tracking;3D camera pose estimation;visual-geometric data;structure-from-motion approach;2D hemispherical image feature;2D point tracker;3D camera ego-motion;3D feature estimation;Lenses;Cameras;Layout;Robot vision systems;Robustness;Real time systems;Augmented reality;Wearable computers;Firewire;Streaming media},
doi={10.1109/ISMAR.2005.38},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544675,
author={J. {Pilet} and V. {Lepetit} and P. {Fua}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Augmenting deformable objects in real-time},
year={2005},
volume={},
number={},
pages={134-137},
abstract={We present a real-time system that can draw virtual patterns or images on deforming real objects by estimating both the deformations and the shading parameters. We show that this is what is required to render the virtual elements so that they blend convincingly with the surrounding real textures. The whole process of uncompressing the video stream, measuring the deformations, estimating the lighting parameters, and realistically augmenting the input image takes about 100 ms on a 2.8 GHz PC. It is fully automated and does not require any manual initialization or engineering of the scene. It is also robust to large deformations, lighting changes, motion blur, specularities, and occlusions. It can therefore be demonstrated live on a simple laptop.},
keywords={augmented reality;video streaming;real-time systems;hidden feature removal;image texture;augmenting deformable object;real-time system;virtual element;video stream;Lighting;Layout;Rendering (computer graphics);Streaming media;Parameter estimation;Probes;Computer vision;Laboratories;Real time systems;Robustness},
doi={10.1109/ISMAR.2005.18},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544676,
author={D. {Kotake} and K. {Satoh} and S. {Uchiyama} and H. {Yamamoto}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={A hybrid and linear registration method utilizing inclination constraint},
year={2005},
volume={},
number={},
pages={140-149},
abstract={This work describes a new hybrid vision-based registration method with an inclination sensor. In the method, a camera is tracked by solving linear equations under inclination constraint. Linear operations are faster than the nonlinear optimization process, but the output does not usually satisfy the orthonormality constraint. On the contrary, the proposed method calculates the camera position and azimuth directly, thus the result satisfies the orthonormality constraint. Many hybrid approaches using inertia sensors have been proposed for AR/MR. However, such methods still depend on vision-based methods in initialization processes. On the other hand, the proposed method is totally hybrid in that the inclination measured by the sensor is always incorporated in pose calculation process as well as vision information. The method can be used in initialization process of conventional hybrid methods as well as it can be used as an independent registration method. The proposed method can be applied to not only inside-out-style camera tracking but also outside-in-style object tracking.},
keywords={image registration;image sensors;cameras;augmented reality;linear registration method;hybrid vision-based registration method;inclination sensor;nonlinear optimization process;orthonormality constraint;independent registration method;inside-out-style camera tracking;outside-in-style object tracking;Cameras;Robustness;Gyroscopes;Virtual reality;Azimuth;Position measurement;Image edge detection;Optimization methods;Humans;Laboratories},
doi={10.1109/ISMAR.2005.2},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544677,
author={L. {Naimark} and E. {Foxlin}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Encoded LED system for optical trackers},
year={2005},
volume={},
number={},
pages={150-153},
abstract={Since introducing a hybrid vision-inertial tracker using passive fiducial markers, we have encountered several applications in which the use of encoded LEDs would be preferable to paper fiducials. This includes small-scale applications requiring higher precision, and applications where the size of the target, or the range of viewing angles, or operation in the dark is important. We present a novel technique to encode LEDs without any need for synchronization between the LEDs and cameras. By using amplitude modulation codes instead of blinking binary codes, the LED is always on, and can therefore be tracked after it has been decoded at an arbitrary or even non-periodic frame rate with no missed data.},
keywords={light emitting diodes;optical tracking;cameras;encoding;binary codes;encoded LED system;optical tracker;hybrid vision-inertial tracker;passive fiducial marker;small-scale application;synchronization;cameras;blinking binary codes;Light emitting diodes;Pulse width modulation;Optical sensors;Decoding;Smart cameras;Wires;Batteries;Target tracking;Binary codes;Sensor fusion},
doi={10.1109/ISMAR.2005.28},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544678,
author={J. -. V. {Gomez} and G. {Simon} and M. -. {Berger}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Calibration errors in augmented reality: a practical study},
year={2005},
volume={},
number={},
pages={154-163},
abstract={This work confronts some theoretical camera models to reality and evaluates the suitability of these models for effective augmented reality (AR). It analyses what level of accuracy can be expected in real situations using a particular camera model and how robust the results are against realistic calibration errors. An experimental protocol is used that consists of taking images of a particular scene from different quality cameras mounted on a 4DOF micro-controlled device. The scene is made of a calibration target and three markers placed at different distances of the target. This protocol enables us to consider assessment criteria specific to AR as alignment error and visual impression, in addition to the classical camera positioning error.},
keywords={augmented reality;video cameras;calibration;image resolution;augmented reality;camera model;realistic calibration error;4DOF micro-controlled device;Calibration;Augmented reality;Cameras;Layout;Protocols;Target tracking;Volume measurement;Imaging phantoms;Robustness;Biomedical imaging},
doi={10.1109/ISMAR.2005.23},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544679,
author={T. {Ishikawa} and K. {Yamazawa} and N. {Yokoya}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Novel view generation from multiple omni-directional videos},
year={2005},
volume={},
number={},
pages={166-169},
abstract={Recently, generation of novel views from images acquired by multiple cameras has been investigated in the fields of virtual and mixed reality. Most conventional methods need some assumptions about the scene such as a static scene and limited positions of objects. We propose a new method for generating novel view images of a dynamic scene with a wide view, which does not depend on the scene. The images acquired from omni-directional cameras are first divided into static regions and dynamic regions. The novel view images are then generated by applying a morphing technique to static regions and by computing visual hulls for dynamic regions in realtime. In experiments, we show that a prototype system can generate novel view images in real-time from live video streams.},
keywords={image morphing;video streaming;real-time systems;multiple omni-directional videos;image morphing technique;visual hulls;real-time system;video streams;Videos;Layout;Cameras;Image generation;Prototypes;Real time systems;Streaming media;Image segmentation;Rendering (computer graphics);Information science},
doi={10.1109/ISMAR.2005.43},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544680,
author={X. {Wang} and A. {Kotranza} and J. {Quarles} and B. {Lok} and B. D. {Allen}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={A pipeline for rapidly incorporating real objects into a mixed environment},
year={2005},
volume={},
number={},
pages={170-173},
abstract={A method is presented to rapidly incorporate real objects into virtual environments using laser scanned 3D models with color-based marker tracking. Both the real objects and their geometric models are put into a mixed environment (ME). In the ME, users can manipulate the scanned, articulated real objects, such as tools, parts, and physical correlates to complex computer-aided design (CAD) models. Our aim is to allow engineering teams to effectively conduct hands-on assembly design verification. This task would be simulated at a high degree of fidelity, and would benefit from the natural interaction afforded by a ME with many specific real objects.},
keywords={virtual reality;CAD;image colour analysis;solid modelling;mixed environment;virtual environment;laser scanned 3D model;color-based marker tracking;computer-aided design;CAD models;hands-on assembly design verification;Pipelines;Laser modes;Design engineering;Assembly;NASA;Solid modeling;Design automation;Laser feedback;Laser noise;Augmented reality},
doi={10.1109/ISMAR.2005.5},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544681,
author={D. {Schmalstieg}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Augmented reality techniques in games},
year={2005},
volume={},
number={},
pages={176-177},
abstract={As a consequence of technical difficulties such as unreliable tracking, many AR applications get stuck in the "how to implement" phase rather than progressing to the "what to show" phase driven by information visualization needs rather than basic technology. In contrast, most of today's computer games are set in a fairly realistic 3D environment, and unlike many AR applications, game interfaces undergo extensive usability testing. This creates the interesting situation that games can be perfect simulators of AR applications, because they are able to show perfectly registered "simulated AR" overlays on top of a real-time environment. This work examines how some visualization and interaction techniques used in games can be useful for real AR applications.},
keywords={augmented reality;computer games;graphical user interfaces;real-time systems;augmented reality techniques;computer games;AR application;realistic 3D environment;game interfaces;usability testing;real-time environment;Augmented reality;Cameras;Games;Application software;Mice;Visualization;Keyboards;Displays;Computer interfaces;Usability},
doi={10.1109/ISMAR.2005.17},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544682,
author={K. {Dorfinuller-Ulhaas} and E. {Andre}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={The synthetic character Ritchie: first steps towards a virtual companion for mixed reality},
year={2005},
volume={},
number={},
pages={178-179},
abstract={Unlike most existing work on traversable interfaces, we focus on the use of synthetic characters to accompany the user in mixed reality (MR) applications. We examine virtual companions as a promising means to design smooth transitions between different worlds and to avoid orientation problems. We propose a taxonomy to describe the spatial relationship between character and user which has an important impact on the style of interaction. To flexibly transfer user and character into different spaces, we have created a platform that supports the design of interfaces derived from the proposed taxonomy as well as transitions between them.},
keywords={virtual reality;user interfaces;synthetic character Ritchie;virtual reality;mixed reality application;traversable interfaces;design interfaces;Virtual reality;Space technology;Taxonomy;Humans;User interfaces;Cities and towns;Displays;Switches;Geometry;Augmented reality},
doi={10.1109/ISMAR.2005.61},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544683,
author={M. {Fiala}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={The SQUASH 1000 tangible user interface system},
year={2005},
volume={},
number={},
pages={180-181},
abstract={A hand-held object whose pose can be determined automatically is useful for augmented reality (AR) and other applications needing human-computer interaction. Some existing such input devices use active (ex. LED lighting) or passive markers to be recognized in a video image by computer vision. Markers are typically mounted onto flat objects which are not ergonomic, or can only have limited number of sides due to the small library and inter-marker confusion rate of the marker system used. A system is presented based on the ARTag fiducial marker system where objects of arbitrary shape can be covered with many small markers, the pose of the object is recovered automatically and can be used as an input device. Other tangible user interface systems require specialized hardware, whereas this approach needs only a printer, a video camera or Webcam, and a large garden vegetable. The utility of this system both for measuring pose and for 3D modeling is shown.},
keywords={augmented reality;human computer interaction;video cameras;computer vision;SQUASH 1000 tangible user interface system;augmented reality;human-computer interaction;video image;computer vision;ARTag fiducial marker system;video camera;Webcam;3D modeling;User interfaces;Augmented reality;Application software;Light emitting diodes;LED lamps;Image recognition;Computer vision;Ergonomics;Libraries;Shape},
doi={10.1109/ISMAR.2005.60},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544684,
author={ {Xiaoming Hu} and {Yue Liu} and {Yongtian Wang} and {Yanling Hu} and {Dayuan Yan}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Autocalibration of an electronic compass for augmented reality},
year={2005},
volume={},
number={},
pages={182-183},
abstract={Electronic compass is often used to provide the absolute heading reference for tracking the user's head and hands in virtual reality (VR) and augmented reality (AR), especially for outdoor AR applications. However, compass is vulnerable to environment magnetism disturbance. Existing compass calibration methods require complex steps and true heading reference which is often impossible to be obtained in outdoor AR applications, and is useful only when compass is in horizontal plane. An autocalibration method without the need of heading reference and redundant sensors is proposed in This work. First the compass error model based on physical principle is presented, then the algorithm to calculate the compensation coefficients with a set of sample measurements of the sensors in the compass is described. Because the influence of the environmental disturbance has been effectively compensated, the calibrated compass can provided accurate heading even when it is under large tilt attitude.},
keywords={augmented reality;compasses;calibration;sensors;electronic compass autocalibration;augmented reality;absolute heading reference;virtual reality;sensors;Augmented reality;Calibration;Magnetic sensors;Magnetometers;Magnetic heads;Virtual reality;Control systems;Table lookup;Gravity;Accelerometers},
doi={10.1109/ISMAR.2005.21},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544685,
author={M. {Haller} and F. {Landerl}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={A mediated reality environment using a loose and sketchy rendering technique},
year={2005},
volume={},
number={},
pages={184-185},
abstract={We present sketchy-ar-us, a modified, realtime version of the Loose and Sketchy algorithm used to render graphics in an AR environment. The primary challenge was to modify the original algorithm to produce a NPR effect at interactive frame rate. Our algorithm renders moderately complex scenes at multiple frames per second. Equipped with a handheld visor, visitors can see the real environment overlaid with virtual objects with both the real and virtual content rendered in a non-photorealistic style.},
keywords={rendering (computer graphics);augmented reality;mediated reality environment;rendering technique;sketchy-ar-us realtime version;Loose and Sketchy algorithm;augmented reality;interactive frame rate;Rendering (computer graphics);Layout;Algorithm design and analysis;Augmented reality;Filters;Computer graphics;Filtering;Image segmentation;Cameras;Real time systems},
doi={10.1109/ISMAR.2005.4},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544686,
author={J. {Fischer} and D. {Bartz} and W. {Strasser}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Reality tooning: fast non-photorealism for augmented video streams},
year={2005},
volume={},
number={},
pages={186-187},
abstract={Recently, we have proposed a novel approach to generating augmented video streams. The output images are a non-photorealistic reproduction of the augmented environment. Special stylization methods are applied to both the background camera image and the virtual objects. This way, the graphical foreground and the real background images are rendered in a similar style, so that they are less distinguishable from each other. Here, we present a new algorithm for the cartoon-like stylization of augmented reality images, which uses a novel post-processing filter for cartoon-like color segmentation and high-contrast silhouettes. In order to make a fast post-processing of rendered images possible, the programmability of modern graphics hardware is exploited. The system is capable of generating a stylized augmented video stream of high visual quality at real-time frame rates.},
keywords={video streaming;augmented reality;rendering (computer graphics);image colour analysis;image segmentation;realistic images;computer graphic equipment;reality tooning;augmented video streams;nonphotorealistic reproduction;special stylization method;background camera image;virtual object;augmented reality images;cartoon-like color segmentation;rendered images;graphics hardware;Streaming media;Filters;Augmented reality;Color;Rendering (computer graphics);Graphics;Image edge detection;Cameras;Real time systems;Pipelines},
doi={10.1109/ISMAR.2005.50},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544687,
author={G. {Bianchi} and C. W. M. {Harders} and P. {Cattin} and G. {Szekely}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Camera-marker alignment framework and comparison with hand-eye calibration for augmented reality applications},
year={2005},
volume={},
number={},
pages={188-189},
abstract={An integral part of every augmented reality system is the calibration between camera and camera-mounted tracking markers. Accuracy and robustness of the AR overlay process is greatly influenced by the quality of this step. In order to meet the very high precision requirements of medical skill training applications, we have set up a calibration environment based on direct sensing of LED markers. A simulation framework has been developed to predict and study the achievable accuracy of the backprojection needed for the scene augmentation process. We demonstrate that the simulation is in good agreement with experimental results. Even if a slight improvement of the precision has been observed compared to well-known hand-eye calibration methods, the subpixel accuracy required by our application cannot be achieved even when using commercial tracking systems providing marker positions within very low error limits.},
keywords={augmented reality;calibration;cameras;light emitting diodes;biomedical education;medical computing;camera-marker alignment framework;hand-eye calibration;augmented reality;medical skill training application;LED marker;subpixel accuracy;Calibration;Augmented reality;Cameras;Computer vision;Application software;Medical simulation;Infrared detectors;Firewire;Laboratories;Robustness},
doi={10.1109/ISMAR.2005.24},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544688,
author={J. {Ehnes} and K. {Hirota} and M. {Hirose}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Projected augmentation II - A scalable architecture for multi projector based AR-systems based on 'Projected Applications'},
year={2005},
volume={},
number={},
pages={190-191},
abstract={Augmenting or annotating physical objects using steerable video projector and camera combinations works well in the range of the projection system. However, since these systems are usually mounted in fixed locations, users have to stay within a certain radius around that location. By using several of these AR-Projection systems, we can make the application roam these systems and thus follow the user beyond these limits. Using this method, we furthermore remove the need to hold the augmented objects in a suitable direction towards the projector and can reduce the possibility of shadowing the projection by the user. We describe an architecture based on our 'Projected Applications' which coordinates the systems used to project the augmentations and keeps the projected applications' states in sync between the projection systems.},
keywords={augmented reality;optical projectors;cameras;display devices;projected augmentation;multiprojector based AR-system;augmented reality;video projector;camera;Application software;Cameras;Control systems;Containers;Shadow mapping;Turning;Security;User interfaces;Computer interfaces;Current measurement},
doi={10.1109/ISMAR.2005.48},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544689,
author={R. {Tenmoku} and M. {Kanbara} and N. {Yokoya}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Annotating user-viewed objects for wearable AR systems},
year={2005},
volume={},
number={},
pages={192-193},
abstract={By realizing augmented reality on wearable computers, it is possible to overlay annotations on the real world based on the user's current position and orientation. However, it is difficult for the user to understand links between annotations and real objects intuitively when the scene is complicated or many annotations are overlaid at the same time. This work describes a view management method which emphasizes user-viewed real objects and their annotations using 3D models of the real scene. The proposed method highlights the objects viewed by the user. In addition, when the viewed object is occluded by other real objects, the object is complemented by using an image, which is made from 3D models, on the overlaid image.},
keywords={augmented reality;wearable computers;hidden feature removal;computational geometry;solid modelling;user-viewed objects;wearable AR system;augmented reality;wearable computer;view management method;3D model;occlusion;Layout;Wearable computers;Prototypes;Augmented reality;Image generation;Project management;Virtual reality;Buildings;Position measurement;Image sensors},
doi={10.1109/ISMAR.2005.10},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544690,
author={ {Hanhoon Park} and {Moon-Hyun Lee} and {Sang-Jun Kim} and {Jong-Il Park}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Specular reflection elimination for projection-based augmented reality},
year={2005},
volume={},
number={},
pages={194-195},
abstract={In projection-based augmented reality, specular reflection may distract the users. This work demonstrates that the specular reflection can be eliminated by redundantly illuminating the projection surface using multiple overlapping projectors mounted at different locations. Our initial system using two projectors is presented. The system automatically determines the pixels which include specular reflection in one projector, blank the light falling on the pixel and boost the other projector's output so that the added light is projected onto the pixel consistently.},
keywords={augmented reality;computational geometry;cameras;optical projectors;lighting;specular reflection elimination;projection-based augmented reality;projection surface;multiple overlapping projector;Augmented reality;Optical reflection;Geometry;Surface reconstruction;Calibration;Cameras;Visual effects;Virtual environment;Image reconstruction;Optimization methods},
doi={10.1109/ISMAR.2005.54},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544691,
author={P. {Fiala} and N. {Adamo-Villani}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={ARpm: an augmented reality interface for polygonal modeling},
year={2005},
volume={},
number={},
pages={196-197},
abstract={We present a prototype system called ARpm, (augmented reality for polygonal modeling). Created as a front-end to a commercially available 3D animation program (3D Studio Max), it does not require it's modification through inaccessible software code. Users look through a head mounted display (HMD) and the object being modeled appears as a 3D augmentation in the real world. This object can be manipulated using a tangible interface of marker panels that correspond to the modeling tools in 3D Studio Max, and a wireless pointer device. Unlike virtual reality which brings the user into the virtual world, this system places the 3D model in the user's world while allowing the use of a large range of complex 3D polygonal modeling tools provided by 3D Studio Max.},
keywords={augmented reality;solid modelling;graphical user interfaces;computer animation;helmet mounted displays;ARpm prototype system;augmented reality interface;polygonal modeling;3D animation program;3D Studio Max;head mounted display;3D augmentation;wireless pointer device;Augmented reality},
doi={10.1109/ISMAR.2005.13},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544692,
author={ {Midieum Shin} and {Byung-soo Kim} and {Jun Park}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={AR storyboard: an augmented reality based interactive storyboard authoring tool},
year={2005},
volume={},
number={},
pages={198-199},
abstract={In early stages of production, storyboards are used for visually describing the story and the script. In This work, an augmented reality based storyboard-authoring tool is introduced. Proposed tool is easy-to-use, and provides intuitive interface for scene composition and camera pose/motion control. Using AR Storyboard, non-experienced users may compose 3D scenes for a Storyboard using interfaces in his/her real environments.},
keywords={augmented reality;graphical user interfaces;authoring systems;cameras;AR storyboard;augmented reality;interactive storyboard authoring tool;graphical user interfaces;Augmented reality;Layout;Cameras;Production;Motion control;Animation;Computer science;Rendering (computer graphics);Aging;Pediatrics},
doi={10.1109/ISMAR.2005.12},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544693,
author={T. {Kakuta} and T. {Oishi} and K. {Ikeuchi}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Shading and shadowing of architecture in mixed reality},
year={2005},
volume={},
number={},
pages={200-201},
abstract={We propose a simple method to express shading and shadowing of virtual objects in mixed reality especially appropriate for static architecture models in outdoor scenes. We create the shadows of the virtual objects in a fast and efficient way using a set of pre-rendered basis images and shadowing planes. The proposed method is limited in interactivity but can operate in near real-time.},
keywords={virtual reality;rendering (computer graphics);solid modelling;architecture shading;mixed reality;virtual object;static architecture model;outdoor scene;pre-rendered basis image;shadowing plane;Shadow mapping;Virtual reality;Layout;Lighting;Geometry;Image generation;Jacobian matrices;Light sources;Assembly;Rendering (computer graphics)},
doi={10.1109/ISMAR.2005.51},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544694,
author={T. {Ogawa} and K. {Kiyokawa} and H. {Takemura}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={A hybrid image-based and model-based telepresence system using two-pass video projection onto a 3D scene model},
year={2005},
volume={},
number={},
pages={202-203},
abstract={A telepresence system is presented that has the advantage of both model-based and image-based approaches, namely, free viewpoint control and real-time color update with live video. A remote place is presented as a virtual environment by using live video projection captured by a head-worn camera onto the static 3D geometry. The observer can then observe the remote place in cooperation with the remote camera man, and give him a set of 3D instructions by a mouse.},
keywords={virtual reality;solid modelling;computational geometry;image texture;rendering (computer graphics);natural scenes;optical projectors;image-based telepresence;model-based telepresence system;two-pass live video projection;3D scene model;free viewpoint control;real-time color update;virtual environment;static 3D geometry;Layout;Cameras;Rendering (computer graphics);Virtual environment;Geometry;Buffer storage;Prototypes;Optical sensors;Real time systems;Mice},
doi={10.1109/ISMAR.2005.3},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544695,
author={A. {Candussi} and T. {Hollerer} and N. {Candussi}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Real-time rendering of realistic trees in mixed reality},
year={2005},
volume={},
number={},
pages={204-205},
abstract={Mixed reality applications put very high demands on both the visual realism and the rendering times of computer graphics elements that are to be perceived as part of the physical scene. This work presents novel techniques to render photorealistic trees in real-time mixed reality. Animation of the tree branches leads to a realistic effect of the tree swaying in the wind. To enhance the effect of blending the tree into a video texture, we present three levels of real-time filtering of the tree and its shadow, which has a great impact on the perceived realism.},
keywords={rendering (computer graphics);realistic images;virtual reality;computer animation;image texture;real-time rendering;realistic tree;mixed reality;visual realism;computer graphics;photorealistic tree animation;video texture;real-time filtering;tree blending;Virtual reality;Rendering (computer graphics);Tree graphs;Animation;Filtering;Augmented reality;Shadow mapping;Layout;Application software;Computer graphics},
doi={10.1109/ISMAR.2005.49},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544696,
author={R. {Grasset} and J. {Looser} and M. {Billinghurst}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={A step towards a multimodal AR interface: a new handheld device for 3D interaction},
year={2005},
volume={},
number={},
pages={206-207},
abstract={We describe the AR Mask, a novel handheld augmented reality (AR) input and display device that not only provides support for a full range of traditional interaction techniques, but also facilitates new metaphors. Unlike other AR display and input technologies, our device consolidates input and output within a single piece of hardware. We demonstrate how this design provides a closed control loop between the user's input modalities and natural sensory receptors.},
keywords={augmented reality;interactive devices;user interfaces;multimodal augmented reality interface;handheld device;3D interaction;AR Mask;user input modality;natural sensory receptor;Handheld computers;Displays;Hardware;Augmented reality;Collaboration;Switches;Grasping;Context awareness;Speech;Virtual reality},
doi={10.1109/ISMAR.2005.7},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544697,
author={B. {Reitinger} and P. {Werlberger} and A. {Bornik} and R. {Beichel} and D. {Schmalstieg}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Spatial measurements for medical augmented reality},
year={2005},
volume={},
number={},
pages={208-209},
abstract={This work presents a set of augmented reality (AR) based interaction techniques for spatial analysis of medical datasets. Computer-aided medical planning tools such as our virtual liver surgery planning system require precise and intuitive interaction for the quantitative inspection of anatomical and pathological structures. We argue that AR is a superior tool compared to desktop 2D or 3D visualization for performing such analysis, because it allows true direct manipulation of 3D virtual objects in space, while rendering the medical data in the familiar context of the user's own body.},
keywords={augmented reality;surgery;rendering (computer graphics);data visualisation;medical computing;spatial measurements;medical augmented reality;medical dataset rendering;computer-aided medical planning tool;virtual liver surgery planning system;3D virtual object;quantitative inspection;3D visualization;Augmented reality;Biomedical imaging;Volume measurement;Liver;Oncological surgery;Distance measurement;Data analysis;Path planning;Inspection;Performance analysis},
doi={10.1109/ISMAR.2005.53},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544698,
author={L. {Spassova} and R. {Wasinger} and J. {Baus} and A. {Kruger}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Product associated displays in a shopping scenario},
year={2005},
volume={},
number={},
pages={210-211},
abstract={We introduce the concept of product associated displays -PADs - as a way of providing visual feedback to users interacting with physical objects in an instrumented environment. PADs are projected public displays created at locations that can be intuitively associated with the objects they show information about. The concept is illustrated in a shopping scenario.},
keywords={computer displays;interactive devices;user interfaces;product associated display;shopping scenario;visual feedback;instrumented environment;projected public display;user interaction;Instruments;Feedback;Humans;Radiofrequency identification;Visualization;Computer displays;Cameras;Augmented reality},
doi={10.1109/ISMAR.2005.46},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544699,
author={M. {Norton} and B. {MacIntyre}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Butterfly effect: an augmented reality puzzle game},
year={2005},
volume={},
number={},
pages={212-213},
abstract={Butterfly effect is a 3D puzzle game using augmented reality. The key motivation was to create a game that leverages the structure of the physical world during gameplay without requiring the computer to have a detailed model of the space. The butterflies are virtual, but the space the player navigates is physical. The player travels her environment, collecting the butterflies. For butterflies that are out of reach, the player can rotate the virtual world in 90 degree chunks about an arbitrary axis to bring them to an accessible location.},
keywords={computer games;augmented reality;butterfly effect;augmented reality;3D puzzle game;gameplay;virtual world;Augmented reality;Games;Space technology;Physics computing;Space exploration;Navigation;Scattering;Humans;Laboratories;Telephone sets},
doi={10.1109/ISMAR.2005.22},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544700,
author={S. {Ono} and K. {Ogawara} and M. {Kagesawa} and H. {Kawasaki} and M. {Onuki} and K. {Honda} and K. {Ikeuchi}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Driving view simulation synthesizing virtual geometry and real images in an experimental mixed-reality traffic space},
year={2005},
volume={},
number={},
pages={214-215},
abstract={We propose an efficient and effective image generation system for an experimental mixed-reality traffic space. Our enhanced traffic/driving simulation system represents the view through a hybrid that combines virtual geometry with real images to realize high photo-reality with little human cost. Images for datasets are captured from the real world, and the view for the simulation system is created by synthesizing image datasets - with a conventional driving simulator.},
keywords={traffic engineering computing;data visualisation;virtual reality;realistic images;computational geometry;rendering (computer graphics);driving view simulation;virtual geometry;real image;mixed-reality traffic space;image generation system;image dataset synthesis;rendering;Solid modeling;Geometry;Virtual reality;Traffic control;Image reconstruction;Microscopy;Road vehicles;Cameras;Image generation;Humans},
doi={10.1109/ISMAR.2005.26},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544701,
author={D. {Reiter} and A. {Butz}},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Design and implementation of a widget set for steerable projector-camera units},
year={2005},
volume={},
number={},
pages={216-217},
abstract={We describe the design and implementation of graphical interaction widgets for use with a steerable projector-camera unit. The design of our widgets is adapted to provide the right visual cues when projected and they are controlled by the user's hand. The widgets' input regions are arranged in an ergonomic way and they use a simple but robust computer vision technique for interaction.},
keywords={interactive devices;computer vision;graphical user interfaces;optical projectors;cameras;steerable projector-camera unit;graphical interaction widget;visual cue;ergonomics;computer vision technique;Instruments;Cameras;Computer vision;Image resolution;Feedback;Computer science;Informatics;Ergonomics;Robustness;Ubiquitous computing},
doi={10.1109/ISMAR.2005.25},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1544702,
author={},
booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, title={Author index},
year={2005},
volume={},
number={},
pages={218-219},
abstract={The author index contains an entry for each author and coauthor included in the proceedings record.},
keywords={},
doi={10.1109/ISMAR.2005.19},
ISSN={},
month={Oct},}