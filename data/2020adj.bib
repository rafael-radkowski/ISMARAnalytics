%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for Rafael Radkowski at 2021-03-13 08:53:37 -0600 


%% Saved with string encoding Unicode (UTF-8) 



@inproceedings{9288412,
	Abstract = {A significant effort has been devoted to the creation of the enabling technology and in the proposal of novel methods to support remote collaboration using Augmented Reality (AR), given the novelty of the field. As the field progresses to focus on the nuances of supporting collaboration and with the growing number of prototypes mediated by AR, the characterization and evaluation of the collaborative process becomes an essential, but difficult endeavor. Evaluation is particularly challenging in this multifaceted context involving many aspects that may influence the way collaboration occurs. Therefore, it is essential the existence of holistic evaluation strategies that monitor the use and performance of the proposed solutions regarding the team, its members, and the technology, allowing adequate characterization and report of collaborative processes. As a contribute, we propose a conceptual model for multi-user data collection and analysis that monitors several collaboration aspects: individual and team performance, behaviour and level of collaboration, as well as contextual data in scenarios of remote collaboration using AR-based solutions.},
	Author = {B. {Marques} and A. {Teixeira} and S. {Silva} and J. {Alves} and P. {Dias} and B. {Sousa Santos}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00016},
	Keywords = {augmented reality;data acquisition;data analysis;groupware;conceptual model;AR-based remote collaboration evaluation;augmented reality;collaborative process;multiuser data collection;contextual data;data analysis;multifaceted context;team performance;Analytical models;Collaboration;Prototypes;Data collection;Data models;Monitoring;Augmented reality;Collaboration;Augmented Reality;Evaluation;Conceptual Model},
	Month = {Nov},
	Pages = {1-2},
	Title = {A Conceptual Model for Data Collection and Analysis for AR-based Remote Collaboration Evaluation},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00016}}

@inproceedings{9288390,
	Abstract = {In this study, we evaluated the performance of AR-assisted navigation in a real underground mine with good and limited illumination conditions as well as without the illumination considering possible search and rescue conditions. For this purpose, we utilized the Lumin SDK's embedded spatial mapping algorithm for mapping and navigating. We used the spatial mapping algorithm to create the mesh model of the escape route and to render it with the user input into the Magic Leap One. Then we compared the spatial mapping algorithm in three different scenarios for the evacuation of an underground mine in an emergency situation. The escape route has two junctions and 30 meters (100 feet). The baseline scenarios are (i) evacuation of the mine in a fully illuminated condition, (ii) evacuation with the headlamp and (iii) without any illumination. In the first scenario (fully illuminated route with the rendered meshes) the evacuation took 40 seconds. In the second scenario (illumination with the headlamp), the evacuation took 44 seconds. For the last scenario (no light source and hence in total darkness) the evacuation took 54 seconds. We found that AR-assisted navigation is effective for supporting search and rescue efforts in high attrition conditions of underground space.},
	Author = {D. C. {DEMIRKAN} and S. {Duzgun}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00017},
	Keywords = {augmented reality;emergency management;image segmentation;rendering (computer graphics);AR-assisted navigation;underground space;underground mine;rescue conditions;navigating;mesh model;escape route;evacuation route;baseline scenarios;fully illuminated condition;rendered meshes;embedded spatial mapping algorithm;Lumin SDK;Magic Leap One;time 44.0 s;time 54.0 s;size 100.0 feet;time 40.0 s;Meters;Measurement;Three-dimensional displays;Navigation;Lighting;Junctions;Light sources;Augmented Reality;Embedded Computing;Indoor Navigation;Indoor Location Tracking;Indoor Mapping;Lumin;Magic Leap},
	Month = {Nov},
	Pages = {1-2},
	Title = {An Evaluation of AR-Assisted Navigation for Search and Rescue in Underground Spaces},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00017}}

@inproceedings{9288379,
	Abstract = {Strabismus is a visual disorder characterized by eye misalignment. The effect of Panum's Fusional Area (PFA) compensates for small misalignments. However, prominent misalignments affect binocular vision and when present in childhood it may lead to amblyopia, a developmental disorder of the visual system. With the advent of Virtual Reality (VR) technology, possibilities for novel binocular treatments to amblyopia arise in which the measurement of strabismus is crucial to correctly compensate for it. Thus, VR yields great potential due to the ability of displaying content to each eye independently. Major research in VR addresses this topic using eye-tracking while there is a paucity of research on image-based assessment methods. In this work, we propose a VR application for measuring strabismus in nine lines of sight. We conducted a study with 14 healthy participants to evaluate the system under two conditions: no strabismus and an artificial deviation induced by prism lenses. Further, we evaluated the effect of PFA on the system by measuring its extent in horizontal and vertical lines of sight. Results show significant difference between the expected deviation induced by prism lenses and the measured deviation. The existing difference within the measurements can be explained with the recorded extent of the PFA.},
	Author = {W. A. {Mehringer} and M. {Gerhard Wirth} and S. {Gradl} and L. S. {Durner} and M. {Ring} and A. F. {Laudanski} and B. {Eskofier} and G. {Michelson}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00018},
	Keywords = {eye;gaze tracking;lenses;medical disorders;medical image processing;patient treatment;virtual reality;vision defects;eye-tracking;image-based assessment;lines of sight;artificial deviation;binocular treatments;virtual reality;visual system;developmental disorder;amblyopia;binocular vision;Panum's fusional area;eye misalignment;visual disorder;PFA;prism lenses;strabismus;VR application;Visualization;Atmospheric measurements;Particle measurements;Vision defects;Augmented reality;Lenses;Human-centered computing;Visualization;Strabismus},
	Month = {Nov},
	Pages = {5-12},
	Title = {An Image-Based Method for Measuring Strabismus in Virtual Reality},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00018}}

@inproceedings{9287826,
	Abstract = {We present AR Circuit Constructor (ARCC), an augmented reality application to explore and inspect electric circuits for use in educational settings. Learners use tangible electricity building blocks to construct a working electric circuit. Then, they can use a tablet device for exploring the circuit in an augmented reality visualization. Learners can switch between three distinct conceptual analogies: bicycle chain, water pipes, and waterfalls. Through experimentation with different circuit configurations, learners explore different properties of electricity to ultimately improve their understanding of it. We describe the development of our application, including a qualitative user study with a group of STEM teachers. The latter allowed us to gain insights into the qualities required for such an application before it can ultimately be deployed in a classroom setting.},
	Author = {T. {Kreienb{\"u}hl} and R. {Wetzel} and N. {Burgess} and A. {Maria Schmid} and D. {Brovelli}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00019},
	Keywords = {augmented reality;bicycles;computer aided instruction;data visualisation;electrical engineering computing;electrical engineering education;tangible electricity building blocks;working electric circuit;tablet device;augmented reality visualization;distinct conceptual analogies;circuit configurations;AR Circuit Constructor;augmented reality application;electric circuits;educational settings;bicycle chain;water pipes;waterfalls;STEM teachers;analogy-driven learning;Human computer interaction;Visualization;Switches;Bicycles;Augmented reality;Augmented reality;learning;electricity;electric circuits;tangible user interfaces;human-computer interaction},
	Month = {Nov},
	Pages = {13-18},
	Title = {AR Circuit Constructor: Combining Electricity Building Blocks and Augmented Reality for Analogy-Driven Learning and Experimentation},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00019}}

@inproceedings{9287827,
	Abstract = {We present an Augmented Reality (AR) application intended for use in supermarkets, with the primary goal to bring fun and digital engagement through AR mini-games to the customers while shopping. We believe that our approach can be extended and scaled up by integrating mini-games into existing shopping apps in the future.},
	Author = {U. {Riedlinger} and L. {Oppermann} and Y. {Uzun} and C. {Brosda}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00020},
	Keywords = {augmented reality;computer games;human computer interaction;interactive systems;mobile computing;retail data processing;AR mini-games;supermarkets;augmented reality application;digital engagement;shopping apps;Augmented reality;Mobile computing;Human-centered computing;Ubiquitous and mobile computing systems and tools;Human-centered computing;Mixed / augmented reality},
	Month = {Nov},
	Pages = {19-20},
	Title = {AR Mini-Games for Supermarkets},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00020}}

@inproceedings{9288441,
	Abstract = {Augmented Reality (AR) can be used in intra-logistics to optimize packing processes towards shipping less unused space by calculating and visualizing efficient pack schemas. This work describes the prototype of such a system using a Kinect v2. Based on the delivered RGB-D stream detection and tracking algorithm applied particle filters, tracks boxes and articles. Further, the tracking approach can check if an article is placed correctly in the box. A qualitative assessment of the prototype in a warehouse revealed that such a system only useful for unexperienced packers.},
	Author = {M. {Lorenz} and F. {Pfeiffer} and P. {Klimant}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00021},
	Keywords = {augmented reality;data visualisation;image colour analysis;logistics;object detection;object tracking;particle filtering (numerical methods);production engineering computing;video signal processing;warehousing;augmented reality;pack optimization;depth data;intra-logistics;Kinect v2;tracking algorithm;particle filters;video;packing processes;pack schemas;RGB-D stream detection;warehouse;visualization;Industries;Prototypes;Particle filters;Manufacturing;Augmented reality;Optimization;Monitoring;Augmented Reality;Heuristics;Packing;Warehouse;Particle filter},
	Month = {Nov},
	Pages = {21-23},
	Title = {Augmented Reality for Pack Optimization using Video and Depth Data},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00021}}

@inproceedings{9288399,
	Abstract = {The newly defined genre of Superhuman Sports provides several challenges, including the need to develop rich Augmented Reality (AR) game concepts with tangible interactions and augmentation. In this paper, we provide insights into a Superhuman Sports ball game, where players are able to interact in mid-air, rapidly and precisely with a smart, augmented and catchable drone ball. We describe our core concepts and a path towards a fully functional system with multiple and potentially different display solutions, ranging from smartphone-based AR to, eventually, HMD-based AR. For this AR game idea, a unique drone with a trackable cage based on LED pattern recognition has been developed. The player, as well as the drone will move swiftly during the game. To precisely estimate the 6DoF pose of the fast-moving drone in this dynamic scenario, we propose a suitable pipeline with a tracking algorithm. As foundation for the tracking, LEDs have been placed in a specific, spherical pattern on the drone cage. Furthermore, refinements based on the unique attributes of LEDs are considered.},
	Author = {C. {Eichhorn} and A. {Jadid} and D. A. {Plecher} and S. {Weber} and G. {Klinker} and Y. {Itoh}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00022},
	Keywords = {augmented reality;computer games;helmet mounted displays;remotely operated vehicles;smart phones;sport;tangible interactions;smart drone ball;augmented drone ball;catchable drone ball;display solutions;HMD-based AR;AR game;LED pattern recognition;drone cage;tangible augmented reality game;superhuman sports ball game;smartphone-based AR;6DoF pose;tracking algorithm;spherical pattern;Tracking;Heuristic algorithms;Games;Light emitting diodes;Augmented reality;Drones;Sports;Superhuman Sports;Trackable Drone;Tracking;Augmented Reality;Tangible AR;Game Concept},
	Month = {Nov},
	Pages = {24-29},
	Title = {Catching the Drone - A Tangible Augmented Reality Game in Superhuman Sports},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00022}}

@inproceedings{9288476,
	Abstract = {We present a user study comparing a pre-evaluated mapping approach with a state-of-the-art direct mapping method of facial expressions for emotion judgment in an immersive setting. At its heart, the pre-evaluated approach leverages semiotics, a theory used in linguistic. In doing so, we want to compare pre-evaluation with an approach that seeks to directly map real facial expressions onto their virtual counterparts. To evaluate both approaches, we conduct a controlled lab study with 22 participants. The results show that users are significantly more accurate in judging virtual facial expressions with pre-evaluated mapping. Additionally, participants were slightly more confident when deciding on a presented emotion. We could not find any differences regarding potential Uncanny Valley effects. However, the pre-evaluated mapping shows potential to be more convenient in a conversational scenario.},
	Author = {N. {Hube} and O. {Lenz} and L. {Engeln} and R. {Groh} and M. {Sedlmair}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00023},
	Keywords = {emotion recognition;face recognition;human computer interaction;virtual reality;enhance immersive collaboration;direct mapping;emotion judgment;virtual facial expressions;preevaluated mapping;facial expressions mapping;human centered computing;virtual faces;Visualization;Social computing;Pipelines;Collaboration;Tools;Linguistics;Semiotics;Human-centered computing [Visualization]: Visualization system and tools;Visualization design and evaluation methods;Human-centered computing [Collaborative and social computing]: Collaborative and social computing systems and tools},
	Month = {Nov},
	Pages = {30-35},
	Title = {Comparing Methods for Mapping Facial Expressions to Enhance Immersive Collaboration with Signs of Emotion},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00023}}

@inproceedings{9288443,
	Abstract = {For many VR applications where natural walking is necessary, the problem of a far smaller real movement space than in VR arises. Treadmills and redirected walking are established methods for this issue. However, both are limited to even surfaces and are unable to simulate different ground properties. Here a concept for a VR robot ground simulator is presented allowing to walk on steep ground or even staircase and which can simulate different undergrounds like sand, grass, or concrete. Starting from gait parameters, the technical requirements and implementation challenges for the realization of such a VR ground simulator are given.},
	Author = {M. {Lorenz} and S. {Knopp} and P. {Klimant} and J. {Quellmalz} and H. {Schlegel}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00024},
	Keywords = {computer simulation;mobile robots;virtual reality;gait parameters;treadmills;steep ground;VR robot ground simulator;redirected walking;natural walking;VR applications;virtual reality robot ground simulator;Legged locomotion;Technical requirements;Solid modeling;Computational modeling;Robots;Augmented reality;Virtual Reality;Robotics;Walking},
	Month = {Nov},
	Pages = {36-38},
	Title = {Concept for a Virtual Reality Robot Ground Simulator},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00024}}

@inproceedings{9288472,
	Abstract = {Many researchers and industry professionals believe Augmented Reality (AR) to be the next step in personal computing. However, the idea of an always-on context-aware AR device presents new and unique challenges to the way users organize multiple streams of information. What does multitasking look like and when should applications be tied to specific elements in the environment? In this exploratory study, we look at one such element: physical objects, and explore an object-centric approach to multitasking in AR. We developed 3 prototype applications that operate on a subset of objects in a simulated test environment. We performed a pilot study of our multitasking solution with a novice user, domain expert, and system expert to develop insights into the future of AR application design.},
	Author = {B. {Huynh} and J. {Orlosky} and T. {H{\"o}llerer}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00025},
	Keywords = {augmented reality;object-centric approach;AR application design;multitasking interface;augmented reality;personal computing;context-aware AR device;physical objects;object-aware AR applications;Industries;Prototypes;Multitasking;Augmented reality;Human-centered computing;Mixed / augmented reality;Human-centered computing;User interface design},
	Month = {Nov},
	Pages = {39-40},
	Title = {Designing a Multitasking Interface for Object-aware AR applications},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00025}}

@inproceedings{9288473,
	Abstract = {While many applications in AR will display embodied agents in scenes, there is little research examining the social influence of these AR renderings. In this experiment, we manipulated the behavioral and anthropomorphic realism of an embodied agent. Participants wore an AR headset and walked a path specified by four virtual cubes, designed to bring them close to either humans or objects rendered in AR. In addition there was a control condition with no virtual objects in the room. Participants were then asked to choose between two physical chairs to sit on-one with a virtual human or object on it, or one without any. We examined the interpersonal distance between participants and rendered objects, physical seat choice, body rotation direction while choosing a seat, and social presence ratings. For interpersonal distance, there was an effect of anthropomorphic realism but not behavioral realism-participants left more space for human-shaped objects than for non-human objects, regardless of how real the human behaved. There were no significant differences in seat choice and rotation direction. Social presence ratings were higher for agents high in both behavioral and anthropomorphic realism than for other conditions. We discuss implications for the social influence theory [5] and for the design of AR systems.},
	Author = {H. {Jun} and J. {Bailenson}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00026},
	Keywords = {human factors;rendering (computer graphics);virtual reality;object rendering;physical seat choice;social presence ratings;interpersonal distance;anthropomorphic realism;human-shaped objects;nonhuman objects;rotation direction;social influence theory;virtual human;AR renderings;virtual cubes;virtual objects;behavioral realism;AR headset;body rotation direction;Atmospheric measurements;Rendering (computer graphics);Particle measurements;Software;Augmented reality;Rotation measurement;Recruitment;Applied computing;Law, social and behavioral sciences;Psychology; Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality},
	Month = {Nov},
	Pages = {41-44},
	Title = {Effects of Behavioral and Anthropomorphic Realism on Social Influence with Virtual Humans in AR},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00026}}

@inproceedings{9288470,
	Abstract = {Many Augmented Reality (AR) applications require the alignment of virtual objects to the real world; this is particularly important in medical AR scenarios where medical imaging information may be displayed directly on a patient and is used to identify the exact locations of specific anatomical structures within the body. For optical see-through AR, alignment accuracy depends both on the optical parameters of the AR display as well as the visualization parameters of the virtual model. In this paper, we explore how different static visualization techniques influence users' ability to perform perception-based alignment in AR for breast reconstruction surgery, where surgeons must accurately identify the locations of several perforator blood vessels while planning the procedure. We conducted a pilot study in which four subjects used four different visualization techniques with varying degrees of opaqueness and brightness as well as outline contrast to align virtual replicas of the relevant anatomy to their 3D-printed counterparts. We collected quantitative scores on spatial alignment accuracy using an external tracking system and qualitative scores on user preference and perceived performance. Results indicate that the highest source of alignment error was along the depth dimension, with users consistently overestimating depth when aligning the virtual renderings. The majority of subjects preferred visualization techniques rendered with lower levels of opaqueness and brightness as well as higher outline contrast, which were also found to support more accurate alignment.},
	Author = {M. {Fischer} and C. {Leuze} and S. {Perkins} and J. {Rosenberg} and B. {Daniel} and A. {Martin-Gomez}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00027},
	Keywords = {augmented reality;blood vessels;data visualisation;image reconstruction;medical image processing;rendering (computer graphics);stereo image processing;surgery;perception-based alignment;augmented reality display;medical AR scenarios;medical imaging information;AR display;virtual model;static visualization techniques;breast reconstruction surgery;perforator blood vessels;opaqueness;brightness;virtual renderings;Visualization;Biomedical optical imaging;Brightness;Surgery;Resists;Optical imaging;Adaptive optics;XR;extended reality;AR;Augmented Reality;mixed reality;FDA;depth perception;arms-length interaction;near-field distance;perceptual accuracy;egocentric depth;Alignment;Visualization techniques;Occlusion;HoloLens;optical see-through HMD},
	Month = {Nov},
	Pages = {45-50},
	Title = {Evaluation of Different Visualization Techniques for Perception-Based Alignment in Medical AR},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00027}}

@inproceedings{9288373,
	Abstract = {Though virtual reality (VR) has been advanced to certain levels of maturity in recent years, the general public, especially the population of the blind and visually impaired (BVI), still cannot enjoy the benefit provided by VR. Current VR accessibility applications have been developed either on expensive head-mounted displays or with extra accessories and mechanisms, which are either not accessible or inconvenient for BVI individuals. In this paper, we present a mobile VR app that enables BVI users to access a virtual environment on an iPhone in order to build their skills of perception and recognition of the virtual environment and the virtual objects in the environment. The app uses the iPhone on a selfie stick to simulate a long cane in VR, and applies Augmented Reality (AR) techniques to track the iPhone's real-time poses in an empty space of the real world, which is then synchronized to the long cane in the VR environment. Due to the use of mixed reality (the integration of VR & AR), we call it the Mixed Reality cane (MR Cane), which provides BVI users auditory and vibrotactile feedback whenever the virtual cane comes in contact with objects in VR. Thus, the MR Cane allows BVI individuals to interact with the virtual objects and identify approximate sizes and locations of the objects in the virtual environment. We performed preliminary user studies with blind-folded participants to investigate the effectiveness of the proposed mobile approach and the results indicate that the proposed MR Cane could be effective to help BVI individuals in understanding the interaction with virtual objects and exploring 3D virtual environments. The MR Cane concept can be extended to new applications of navigation, training and entertainment for BVI individuals without more significant efforts.},
	Author = {L. {Zhang} and K. {Wu} and B. {Yang} and H. {Tang} and Z. {Zhu}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00028},
	Keywords = {augmented reality;handicapped aids;haptic interfaces;helmet mounted displays;mobile computing;3D virtual environment;blind and visually impaired;virtual reality;BVI;mobile VR app;virtual objects;augmented reality;VR environment;mixed reality cane;VR accessibility applications;MR cane;head-mounted displays;vibrotactile feedback;auditory feedback;Training;Visualization;Navigation;Virtual environments;Tools;Space exploration;Augmented reality;Virtual Reality;Mixed Reality;Visually Impaired;Spatial Exploration},
	Month = {Nov},
	Pages = {51-56},
	Title = {Exploring Virtual Environments by Visually Impaired Using a Mixed Reality Cane Without Visual Feedback},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00028}}

@inproceedings{9288453,
	Abstract = {Through the last decade, the creation of extended reality (XR) solutions has significantly increased due to the advent of cheaper, more advanced, and accessible instruments like smartphones, headsets, platforms, development kits, and engines. For instance, the number of GitHub repositories for XR related projects jumped from 51 in 2010 to over 15,000 in 2020. At the same time, the developer community approaches the creation of XR applications using inherited design processes and methods from past mainstream platforms such as web, mobile, or even product design. Unfortunately, those platforms do not consider the spatial aspects of these applications. In this paper, we present a revisited design process and a toolkit focused on the challenges innate to XR, that aims to help beginners and experienced teams in the creation of applications and interactions in Virtual, Augmented, and Mixed Reality. We also present a compendium of 113 techniques and 118 guidelines and a set of canvases that guides users through the process, preventing them from skipping important tasks and discoveries. At last, we present a pilot case where we accompany a team with developers and designers running our process and using our toolkit for the first time, showing the benefits of a process that strikes specific issues of XR apps.},
	Author = {A. {Gomes} and L. {Figueiredo} and W. {Correia} and V. {Teichrieb} and J. {Quintino} and F. Q. B. {da Silva} and A. {Santos} and H. {Pinho}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00029},
	Keywords = {augmented reality;spatial aspects;design process;toolkit;mixed reality;XR apps;XR experiences;extended reality;smartphones;headsets;development kits;GitHub repositories;product design;virtual reality;augmented reality;Instruments;Prototypes;Product design;X reality;Task analysis;Smart phones;Software development management;Human-centered computing;Interaction design;Interaction design process and methods},
	Month = {Nov},
	Pages = {57-62},
	Title = {Extended by Design: A Toolkit for Creation of XR Experiences},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00029}}

@inproceedings{9288432,
	Abstract = {Gestural interfaces in virtual reality (VR) expand the design space for user interaction, allowing spatial metaphors with the environment and more natural and immersive experiences. Typically, machine learning approaches recognize gestures with models that rely on a large number of samples for the training phase, which is an obstacle for rapidly prototyping gestural interactions. In this paper, we propose a solution designed for hi-fi prototyping of gestures within a virtual reality environment through a high-level Domain-Specific Language (DSL), as a subset of the natural language. The proposed DSL allows non-programmer users to intuitively describe a broad domain of poses and connect them for compound gestures. Our DSL was designed to be general enough for multiple input classes, such as body tracking, hand tracking, head movement, motion controllers, and buttons. We tested our solution for wands with VR designers and developers. Results showed that the tool gives non-programmers the ability to prototype gestures with ease and refine its recognition within a few minutes.},
	Author = {J. R. {Fonseca} and J. {Abreu} and L. {Figueredo} and J. G. {Neto} and V. {Teichrieb} and J. P. {Quintino} and F. Q. B. {da Silva} and A. L. M. {Santos} and H. {Pinho}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00030},
	Keywords = {gesture recognition;high level languages;human computer interaction;learning (artificial intelligence);motion control;object tracking;software prototyping;virtual reality;gestural interfaces;design space;user interaction;spatial metaphors;immersive experiences;machine learning;virtual reality environment;high-level domain-specific language;DSL;natural language;nonprogrammer users;compound gestures;prototype gestures;fast hi-fi prototyping;natural experience;gesture recognition;gestural interaction rapid prototyping;body tracking;hand tracking;head movement;motion controllers;buttons;Ginput;Training;Solid modeling;Tracking;Natural languages;Tools;DSL;Task analysis;Human-centered computing;Visualization;Visualization techniques;Treemaps; Human-centered computing;Visualization;Visualization design and evaluation methods},
	Month = {Nov},
	Pages = {63-64},
	Title = {Ginput: a tool for fast hi-fi prototyping of gestural interactions in virtual reality},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00030}}

@inproceedings{9288381,
	Abstract = {Image-based body tracking algorithms are useful in several scenarios, such as avatar animations and gesture interaction for VR applications. In the last few years, the best-ranked solutions presented on the state of the art of body tracking (according to the most popular datasets in the field) are intensively based on Convolutional Neural Networks (CNNs) algorithms and use large datasets for training and validation. Although these solutions achieve high precision scores while evaluated with some of these datasets, there are particular tracking challenges (for example, upside-down cases) that are not well-modeled and, therefore, not correctly tracked. Instead of lurking an all-in-one solution for all cases, we propose HuTrain, a framework for creating datasets quickly and easily. HuTrain comprises a series of steps, including automatic camera calibration, refined human pose estimation, and known dataset formats conversion. We show that, with our system, the user can generate human pose datasets, targeting specific tracking challenges for the desired application context, with no need to annotate human pose instances manually.},
	Author = {R. R. {Barioni} and W. L. {Costa} and J. A. C. {Neto} and L. S. {Figueiredo} and V. {Teichrieb} and J. P. {Quintino} and F. Q. B. {da Silva} and A. L. M. {Santos} and H. {Pinho}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00031},
	Keywords = {calibration;cameras;computer vision;convolutional neural nets;feature extraction;gesture recognition;image annotation;image colour analysis;image sampling;learning (artificial intelligence);object tracking;pose estimation;stereo image processing;HuTrain;avatar animations;gesture interaction;VR applications;convolutional neural networks;real human pose dataset creation;dataset format conversion;image-based body tracking algorithms;CNN;automatic camera calibration;refined human pose estimation;sample annotation;RGB cameras;human-based camera poses calibration algorithm;image feature extractor;feature descriptors;3D HPE datasets;Training;Target tracking;Pose estimation;Euclidean distance;Cameras;Calibration;Convolutional neural networks;Motion capture;Camera calibration;Reconstruction;Image processing},
	Month = {Nov},
	Pages = {65-66},
	Title = {HuTrain: a Framework for Fast Creation of Real Human Pose Datasets},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00031}}

@inproceedings{9288451,
	Abstract = {Maintenance departments of producing companies in most industrial countries are facing challenges originating from an aging workforce, increasing product variety, and the pressure to increase productivity. We present the concepts and the user interface (UI) designs for two Augmented Reality (AR) applications, which help to tackle these issues. An AR Guidance System will allow new and unexperienced staff to perform medium to highly complex maintenance tasks, which they currently incapable to. The AR Remote Service System enables technicians at the machine to establish a voice/video stream with an internal or external expert. The video stream can be augmented with 3D models and drawings so that problems can be solved remotely and more efficiently. A qualitative assessment with maintenance managers and technicians from three producing companies rated the AR application concept as beneficial and the UI designs as very usable.},
	Author = {J. {Kim} and M. {Lorenz} and S. {Knopp} and P. {Klimant}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00032},
	Keywords = {augmented reality;maintenance engineering;personnel;production engineering computing;solid modelling;user interfaces;industrial augmented reality;user interface designs;aging workforce;product variety;productivity;AR guidance system;UI designs;augmented reality maintenance worker support systems;AR remote service system;video stream;3D models;Productivity;Three-dimensional displays;Companies;Maintenance engineering;User interfaces;Task analysis;Augmented reality;Augmented Reality;User interface;Maintenance},
	Month = {Nov},
	Pages = {67-69},
	Title = {Industrial Augmented Reality: Concepts and User Interface Designs for Augmented Reality Maintenance Worker Support Systems},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00032}}

@inproceedings{9288398,
	Abstract = {Augmented reality (AR) has been incorporated into educational processes in various subjects to improve academic performance. One of these areas is the field of electronics since students often have difficulty understanding electricity. An interactive AR app on electrical circuits was developed. The app allows the manipulation of circuit elements, computes the voltage and amperage values using the loop method, and applies Kirchhoff's voltage law. This research aims to determine the intention of using the AR app by students. It also looks to determine if it is conditioned by how the survey is applied (online or face-to-face) or students' gender. The results show that the app is well evaluated on the intention of use by students. Regarding how the survey is applied, the attitude towards using does not present significant differences. In contrast, the students who carried out the online survey presented a higher behavioral intention to use than those who participated in the guided laboratory. Regarding gender, women showed a higher attitude toward using and behavioral intention to use this technology than men.},
	Author = {A. {{\'A}lvarez-Mar{\'\i}n} and J. {\'A}. {Vel{\'a}zquez-Iturbide} and M. {Castillo-Vergara}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00033},
	Keywords = {augmented reality;computer aided instruction;engineering education;engineering education;augmented reality;educational processes;academic performance;electricity;electrical circuits;circuit elements;amperage values;Kirchhoff voltage law;AR app;students;online survey;behavioral intention;Engineering education;Augmented reality;Augmented reality;technology acceptance;engineering;education},
	Month = {Nov},
	Pages = {70-73},
	Title = {Intention to use an interactive AR app for engineering education},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00033}}

@inproceedings{9288471,
	Abstract = {Target-based travel has become a common travel metaphor for virtual reality (VR) applications. Three of the most common target-based travel techniques include Point-and-Instant-Teleport (Teleport), Point-and-Walk-Motion (Motion), and Automatic- Walk-Motion (Automatic). We present a study that employed a dual-task methodology to investigate the user performance characteristics and cognitive loads of the three target-based travel techniques, in addition to several subjective measures. Our results indicate that the Teleport technique afforded the best user travel performances, but that the Automatic technique afforded the best cognitive load.},
	Author = {C. {Lai} and A. A. {Aiyaz} and R. P. {McMahan}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00034},
	Keywords = {cognition;human computer interaction;virtual reality;point-and-walk motion;automatic-walk motion;target-based travel;point-and-instant teleport;cognitive load;virtual reality;travel metaphor;cognitive trade-offs;locomotive trade-offs;Measurement;Augmented reality;Target-based travel;virtual reality;cognitive load},
	Month = {Nov},
	Pages = {74-75},
	Title = {Locomotive and Cognitive Trade-Offs for Target-based Travel},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00034}}

@inproceedings{9288469,
	Abstract = {Musicians face a number of issues when performing live, including organizing and annotating sheet music. This can be an unwieldy process, as musicians need to simultaneously read and manipulate sheet music and interact with the conductor and other musicians. Augmented Reality can provide a way to ease some of the more cumbersome aspects of live performance and practice. We present MiXR, a novel interactive system that combines an AR headset, a smartphone, and a tablet to allow performers to intuitively and efficiently manage and annotate virtual sheet music in their physical environment. We discuss our underlying motivation, the interaction techniques supported, and the system architecture.},
	Author = {S. {Kohen} and C. {Elvezio} and S. {Feiner}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00035},
	Keywords = {augmented reality;mobile computing;music;smart phones;user interfaces;live performance;musicians;MiXR;virtual sheet music;hybrid AR sheet music interface;augmented reality;AR headset;smartphone;tablet;Performance evaluation;Headphones;Interactive systems;Music;Systems architecture;Augmented reality;Faces;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Applied computing;Arts and Humanities;Sound and music computing;Human-centered computing;Ubiquitous and mobile computing;Ubiquitous and mobile devices;Smartphones;Tablet computers},
	Month = {Nov},
	Pages = {76-77},
	Title = {MiXR: A Hybrid AR Sheet Music Interface for Live Performance},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00035}}

@inproceedings{9288454,
	Abstract = {Virtual training environments (VTEs) using immersive technology have been able to successfully provide training for technical skills. Combined with recent advances in virtual social agent technologies and in affective computing, VTEs can now also support the training of social skills. Research looking at the effects of different immersive technologies on users' experience (UX) can provide important insights about their impact on user's engagement with the technology, sense presence and co-presence. However, current studies do not address whether emotions displayed by virtual agents provide the same level of UX across different virtual reality (VR) platforms. In this study, we considered a virtual classroom simulator built for desktop computer, and adapted for an immersive VR platform (CAVE). Users interact with virtual animated disruptive students able to display facial expressions, to help them practice their classroom behavior management skills. We assessed effects of the VR platforms and of the display of facial expressions on presence, co-presence, engagement, and believability. Results indicate that users were engaged, found the virtual students believable and felt presence and co-presence for both VR platforms. We also observed an interaction effects of facial expressions and VR platforms for co-presence (p = .018 <; .05).},
	Author = {A. {Delamarre} and C. {Lisetti} and C. {Buche}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00036},
	Keywords = {affective computing;computer based training;computer simulation;user experience;virtual reality;virtual reality;virtual classroom simulator;desktop computer;immersive VR platform;virtual animated disruptive students;facial expressions;classroom behavior management skills;virtual students;interaction effects;cross-platform virtual classroom study;virtual training environments;VTEs;technical skills;virtual social agent technologies;affective computing;social skills;sense presence;virtual agents;modeling emotions for training in immersive simulations;METIS;users experience;user engagement;CAVE;Training;Solid modeling;Affective computing;Resists;Augmented reality;Immersive Virtual Environment;Virtual Reality;Affective Computing;User Experience;Evaluation;Classroom simulations},
	Month = {Nov},
	Pages = {78-83},
	Title = {Modeling Emotions for Training in Immersive Simulations (METIS): A Cross-Platform Virtual Classroom Study},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00036}}

@inproceedings{9288370,
	Abstract = {Embodying users through avatars based on motion tracking and reconstruction is an ongoing challenge for VR application developers. High quality VR systems use full-body tracking or inverse kinematics to reconstruct the motion of the lower extremities and control the avatar animation. Mobile systems are limited to the motion sensing of head-mounted displays (HMDs) and typically cannot offer this.We propose an approach to reconstruct gait motions from a single head-mounted accelerometer. We train our models to map head motions to corresponding ground truth gait phases. To reconstruct leg motion, the models predict gait phases to trigger equivalent synthetic animations. We designed four models: a threshold-based, a correlation-based, a Support Vector Machine (SVM) -based and a bidirectional long-term short-term memory (BLSTM) -based model. Our experiments show that, while the BLSTM approach is the most accurate, only the correlation approach runs on a mobile VR system in real time with sufficient accuracy. Our user study with 21 test subjects examined the effects of our approach on simulator sickness and showed significantly less negative effects on disorientation.},
	Author = {T. {Feigl} and L. {Gruner} and C. {Mutschler} and D. {Roth}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00037},
	Keywords = {accelerometers;avatars;computer animation;gait analysis;helmet mounted displays;recurrent neural nets;support vector machines;mobile systems;motion sensing;gait motions;single head-mounted accelerometer;head motions;leg motion;synthetic animations;support vector machine;BLSTM;correlation approach;mobile VR system;virtual reality;single sensor;motion tracking;high quality VR systems;full-body tracking;inverse kinematics;lower extremities;avatar animation;real time gait reconstruction;bidirectional long-term short-term memory;ground truth gait phases;threshold-based model;correlation-based model;SVM;Support vector machines;Solid modeling;Tracking;Avatars;Animation;Real-time systems;Delays;Human-centered computing;Interaction paradigms;Virtual reality},
	Month = {Nov},
	Pages = {84-89},
	Title = {Real-Time Gait Reconstruction For Virtual Reality Using a Single Sensor},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00037}}

@inproceedings{9288414,
	Abstract = {Surgeons improve their skills through repetition of training tasks in order to operate on living patients, ideally receiving timely, useful, and objective performance feedback. However, objective performance measurement is currently difficult without 3D visualization, with effective surgical training apparatus being extremely expensive or limited in accessibility. This is problematic for medical students, especially in situations such as the COVID-19 pandemic in which they are needed by the community but have few ways of practicing without lab access. In this work, we propose and prototype a system for augmented reality (AR) visualization of laparoscopic training tasks using cheap and widely-compatible borescopes, which can track small objects typical of surgical training. We use forward kinematics for calibration and multi-threading to attempt synchronization in order to increase compatibility with consumer applications, resulting in an effective AR simulation with low-cost devices and consumer software, while also providing dynamic camera and marker tracking. We test the system with a typical peg transfer task on the HoloLens 1 and MagicLeap One.},
	Author = {N. {Rewkowski} and A. {State} and H. {Fuchs}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00038},
	Keywords = {augmented reality;computer based training;medical computing;medical image processing;object tracking;surgery;MagicLeap One;HoloLens 1;surgical training apparatus;peg transfer task;dynamic camera;consumer software;low-cost devices;AR simulation;consumer applications;widely-compatible borescopes;laparoscopic training tasks;augmented reality visualization;COVID-19 pandemic;medical students;objective performance measurement;objective performance feedback;augmented reality surgical training;movable consumer cameras;marker tracking;Training;Visualization;Three-dimensional displays;Surgery;Cameras;Task analysis;Augmented reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality; Computing methodologies;Artificial intelligence;Computer vision;Computer vision problems;Tracking},
	Month = {Nov},
	Pages = {90-95},
	Title = {Small Marker Tracking with Low-Cost, Unsynchronized, Movable Consumer Cameras For Augmented Reality Surgical Training},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00038}}

@inproceedings{9288475,
	Abstract = {The purpose of this study is to analyze the different kinds of ex- proprioceptive visual cues on an Augmented Reality (AR) system during gait exercise, on top of understanding which cues provide the best visualizations in stepping over obstacles. The main problem for users is to understand the position of virtual objects relative to themselves. Since visual exproprioception provides information about the body position in relation to the environment, it has the possibility to yield positive effects with regards to position control and gait biomechanics in the AR system. This research was born as part of the collaboration with the staff of Takanohara Central Hospital in Japan. Twenty-seven individuals were invited to take part in the user study to test three visual interfaces. The task of the mentioned user study involves making the subjects cross and avoid virtual obstacles of different heights that come from different directions. The AR application was implemented in the experiment by using the Head-Mounted Display (HMD) Microsoft HoloLens. Data obtained from the experiment revealed that the interface projected in front of the user from a third-person point of view resulted to improvements in terms of posture, visual stimuli, and safety.},
	Author = {A. {Luchetti} and E. {Parolin} and I. {Butaslac} and Y. {Fujimoto} and M. {Kanbara} and P. {Bosetti} and M. D. {Cecco} and H. {Kato}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00039},
	Keywords = {augmented reality;biomechanics;collision avoidance;control engineering computing;data visualisation;gait analysis;helmet mounted displays;medical computing;medical control systems;position control;visual exproprioception;proprioceptive visual cues;augmented reality system;gait exercise;position control;gait biomechanics;AR system;Takanohara Central Hospital;visual interfaces;virtual obstacles;visual stimuli;stepping over obstacles;Japan;head-mounted display Microsoft HoloLens;HMD Microsoft HoloLens;Visualization;Three-dimensional displays;Avatars;Two dimensional displays;Resists;Task analysis;Augmented reality;Augmented Reality;Exproprioception;HoloLens;Rehabilitation;Obstacle crossing;},
	Month = {Nov},
	Pages = {96-101},
	Title = {Stepping over Obstacles with Augmented Reality based on Visual Exproprioception},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00039}}

@inproceedings{9288397,
	Abstract = {Spatial steering is a common virtual reality (VR) travel metaphor that affords virtual locomotion and spatial understanding. Variations of spatial steering include Gaze-, Hand-, and Torso-directed steering. We present a study that employed a dual-task methodology to investigate the user performance characteristics and cognitive loads of the three spatial steering techniques, in addition to several subjective measures. Using the two one-sided tests (TOST) procedure for dependent means, we have found that Gaze- and Hand-directed steering were statistically equivalent for travel performance and cognitive load. However, we found that Gaze-directed steering induced significantly less simulator sickness than Hand-directed steering.},
	Author = {C. {Lai} and X. {Hu} and A. {Segismundo} and A. {Phadke} and R. P. {McMahan}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00040},
	Keywords = {cognition;gaze tracking;virtual reality;dual-task methodology;cognitive load;spatial steering;travel performance;virtual reality;travel metaphor;virtual locomotion;hand-directed steering;gaze-directed steering;two one-sided tests;torso-directed steering;Standards;Augmented reality;Bars;Spatial steering;travel;virtual reality;cognitive load},
	Month = {Nov},
	Pages = {102-103},
	Title = {The Comfort Benefits of Gaze-Directed Steering},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00040}}

@inproceedings{9288413,
	Abstract = {Accurate global camera registration is a key requirement for precise AR visualizations in large-scale outdoor AR applications. Existing approaches mostly use complex image-based registration methods requiring large pre-registered databases of geo-referenced images or point clouds that are hardly applicable to large-scale areas. In this paper, we present a simple yet effective user-aided registration method that utilizes common geospatial 3D data to globally register mobile devices. For this purpose, text-based 3D geospatial data including digital 3D terrain and city models is processed into small-scale 3D meshes and displayed in a live AR view. Via two common mobile touch gestures the generated virtual models can be aligned manually to match the actual perception of the real-world environment. Experimental results show that - combined with a robust local visual-inertial tracking system - this approach enables an efficient and accurate global registration of mobile devices in various environments determining the camera attitude with less than one degree deviation while achieving a high degree of immersion through realistic occlusion behavior.},
	Author = {S. {Burkard} and F. {Fuchs-Kittowski}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00041},
	Keywords = {augmented reality;cameras;data visualisation;geophysical image processing;image registration;mobile computing;solid modelling;AR visualizations;large-scale outdoor AR applications;geo-referenced images;mobile devices;text-based 3D geospatial data;city models;small-scale 3D meshes;visual-inertial tracking system;large-scale mobile outdoor augmented reality;user-aided global registration;geospatial 3D data;user-aided registration;global camera registration;digital 3D terrain;live AR view;mobile touch gestures;camera attitude;realistic occlusion behavior;Solid modeling;Three-dimensional displays;Urban areas;Cameras;Mobile handsets;Geospatial analysis;Augmented reality;Computing methodologies;Computer graphics;Graphics systems and interfaces;Mixed / augmented reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality},
	Month = {Nov},
	Pages = {104-109},
	Title = {User-Aided Global Registration Method using Geospatial 3D Data for Large-Scale Mobile Outdoor Augmented Reality},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00041}}

@inproceedings{9288462,
	Abstract = {Our research presented here tries to bridge the gap between technology-oriented lab work and the praxis of introducing VR technology into difficult to deploy-to contexts-in our case prisoners with high learning needs. We have developed a prototypical immersive VR application designed for delivering low-level literacy and numeracy content to illiterate adults. This development has been taken to the commercial sector and is currently under product development. The target population for the application are those currently held in a correctional facility, but who have the motivation and determination to educate themselves. In this paper we discuss the current lifecycle of this project including the development, initial tests, and an exploratory study we conducted. We conclude with a discussion of logistical issues, potential research opportunities, and current outcomes.},
	Author = {J. {Collins} and T. {Langlotz} and H. {Regenbrecht}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00042},
	Keywords = {computer aided instruction;virtual reality;virtual reality;immersive learning;technology-oriented lab work;VR technology;learning needs;immersive VR application;low-level literacy;illiterate adults;commercial sector;product development;correctional facility;Education;Sociology;Virtual environments;Product development;Statistics;Augmented reality;Guidelines;VR education;learning;in-the-field;Applied computing;Education;Interactive learning environments;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
	Month = {Nov},
	Pages = {110-115},
	Title = {Virtual Reality in Education: A Case Study on Exploring Immersive Learning for Prisoners},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00042}}

@inproceedings{9287824,
	Abstract = {We propose a set of stylized visual effects (VFX) meant to improve the sensation of contact with objects in Augmented Reality (AR). Various graphical effects have been conceived, such as virtual cracks, virtual wrinkles, or even virtual onomatopoeias inspired by comics. The VFX are meant to augment the perception of contact, with either real or virtual objects, in terms of material properties or contact location for instance. These VFX can be combined with a pseudohaptics approach to further increase the range of simulated physical properties of the touched materials. An illustrative setup based on a HoloLens headset was designed, in which our proposed VFX could be explored. The VFX appear each time a contact is detected between the user's finger and one object of the scene. Such VFX- based approach could be introduced in AR applications for which the perception and display of contact information are important.},
	Author = {V. {Mercado} and J. -M. {Normand} and A. {L{\'e}cuyer}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00043},
	Keywords = {augmented reality;haptic interfaces;VFX;augmented reality;graphical effects;virtual cracks;virtual wrinkles;virtual onomatopoeias;real objects;virtual objects;material properties;contact location;stylized visual effects;comics;pseudohaptics;HoloLens headset;Headphones;Fingers;Visual effects;Augmented reality;Physics;Material properties;Augmented-Reality;Virtual Environments;Visual Effects;Pseudo-Haptics;},
	Month = {Nov},
	Pages = {116-117},
	Title = {"Kapow!": Augmenting Contacts with Real and Virtual Objects Using Stylized Visual Effects},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00043}}

@inproceedings{9287825,
	Abstract = {The creation of realistic 3D human model is traditionally timeconsuming and cumbersome, and is typically done by professionals. In recent years computer vision technologies can assist in generating human models from controlled environments, we demonstrate a different but easy capturing scenario with less constraints on the subject or the environmental setup. The reconstruction process for 3D human model consists of various intermediate process such as semantic human segmentation, human skeletal keypoint detection, and texture generation. In order to achieve easy, scalable, and flexible deployment to different cloud environments, we have chosen the serverless architecture to offload some common service functionalities to the cloud infrastructure but focused on the core task,which is the reconstruction itself. The event-driven serverless architecture eases the building of such multimedia web services with minimal coding efforts, but simply defines the APIs and declares the APIs with correspondent lambda functions. The proposed approach in this paper allow anyone with a mobile phone to generate 3D models easily and quickly in the scale of few 2-3 minutes, rather than hours.},
	Author = {P. {Fasogbon} and Y. {You} and E. {Aksu}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00044},
	Keywords = {application program interfaces;cloud computing;computer vision;image reconstruction;image segmentation;realistic images;solid modelling;stereo image processing;event-driven serverless architecture;serverless environment;realistic 3D human model;computer vision;reconstruction process;semantic human segmentation;human skeletal keypoint detection;texture generation;cloud environments;API;lambda functions;Deep learning;Solid modeling;Three-dimensional displays;Computational modeling;Computer architecture;Image reconstruction;Augmented reality;3D-human-model;Visualization;SMPL;OpenPose;;Neural Network;Graph-cut;Serverless-Architecture;Smartphones-Texture Generation-OpenGL},
	Month = {Nov},
	Pages = {118-122},
	Title = {3D human model creation on a serverless environment},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00044}}

@inproceedings{9288368,
	Abstract = {Attention-Deficit/Hyperactivity Disorder or ADHD poses a severe concern for today's youth, especially when the costs, efficacy, side effects of medication and the lack of immediate risk discourage treatment. ADHD causes people to make impulsive decisions, making it harder to succeed in school, work, and other aspects of life. Neurofeedback Therapy has shown promising results as an alternative in treating mental disorders and improving cognition. It leverages the inherent mechanism of operant conditioning by presenting real-time feedback of the user's brainwave activity, which is usually acquired via an EEG. However, long sessions of monotonous feedback have proven tedious, and users lose motivation to continue. Engaging graphical interfaces and games have been developed to combat this issue and have been proven to improve the treatment's efficacy. In this work, we extend upon these methods to increase engagement by employing Augmented Reality in the context of a virtual telekinetic game. The system comprises three modules: an Emotiv headset for EEG acquisition, MATLAB for signal processing, and an AR mobile application to deliver the feedback. The hardware and software implementation, the signal processing methodology, and the Neurofeedback protocol are thoroughly outlined. Our next step is to conduct a pilot study with a subset of healthy children to evaluate the complexity of the process.},
	Author = {G. S. {Rajshekar Reddy} and L. {G.M.}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00045},
	Keywords = {augmented reality;brain-computer interfaces;cognition;electroencephalography;medical computing;medical disorders;medical signal processing;neurophysiology;patient treatment;impulsive decisions;Neurofeedback Therapy;mental disorders;cognition;operant conditioning;real-time feedback;graphical interfaces;virtual telekinetic game;signal processing methodology;neurofeedback protocol;brain-computer interface;Augmented Reality Neurofeedback;ADHD;virtual telekinesis approach;youth;attention-deficit/hyperactivity disorder;EEG acquisition;Emotiv headset;MATLAB;AR mobile application;Pediatrics;Medical treatment;Games;Signal processing;Electroencephalography;Neurofeedback;Augmented reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Applied computing;Law, social and behavioral sciences;Psychology},
	Month = {Nov},
	Pages = {123-128},
	Title = {A Brain-Computer Interface and Augmented Reality Neurofeedback to Treat ADHD: A Virtual Telekinesis Approach},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00045}}

@inproceedings{9288435,
	Abstract = {The ongoing automation of modern production processes requires novel human-computer interaction concepts that support employees in dealing with the unstoppable increase in time pressure, cognitive load, and the required fine-grained and process-specific knowledge. Augmented Reality (AR) systems support employees by guiding and teaching work processes. Such systems still lack a precise process quality analysis (monitoring), which is, however, crucial to close gaps in the quality assurance of industrial processes.We combine inertial sensors, mounted on work tools, with AR headsets to enrich modern assistance systems with a sense of process quality. For this purpose, we develop a Machine Learning (ML) classifier that predicts quality metrics from a 9-degrees of freedom inertial measurement unit, while we simultaneously guide and track the work processes with a HoloLens AR system. In our user study, 6 test subjects perform typical assembly tasks with our system. We evaluate the tracking accuracy of the system based on a precise optical reference system and evaluate the classification of each work step quality based on the collected ground truth data. Our evaluation shows a tracking accuracy of fast dynamic movements of 4.92mm and our classifier predicts the actions carried out with mean F1 value of 93.8% on average.},
	Author = {A. {Red{\v z}epagi{\'c}} and C. {L{\"o}ffler} and T. {Feigl} and C. {Mutschler}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00046},
	Keywords = {assembling;augmented reality;human computer interaction;learning (artificial intelligence);pattern classification;personnel;production engineering computing;quality assurance;assembly tasks;HoloLens AR system;9-degrees of freedom inertial measurement unit;machine learning classifier;AR headsets;process-specific knowledge;employees;augmented reality assisted process guidance;process quality analysis;work step quality;optical reference system;tracking accuracy;quality metrics;assistance systems;work tools;inertial sensors;industrial processes;quality assurance;augmented reality systems;cognitive load;time pressure;human-computer interaction;production processes;Quality assurance;Measurement units;Tracking;Tools;Task analysis;Augmented reality;Monitoring;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality},
	Month = {Nov},
	Pages = {129-134},
	Title = {A Sense of Quality for Augmented Reality Assisted Process Guidance},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00046}}

@inproceedings{9288408,
	Abstract = {The utilization of modern assistance system e.g. Augmented Reality (AR) has reached into industrial assembly scenarios. Beside the technical realization of AR assistance in the assembly scene the worker has to accept the new technology. Only both, user acceptance and technical user-interface design leads to an optimized overall system. Hence, this contribution gives a brief literature overview and analysis about AR acceptance and acceptance modeling. Then, a proprietary model for acceptance measurement is developed, which includes and synthesizes previous models (TAM and UTAUT) and simplifies them considerably for the purpose of industrial assembly. Following, a laboratory experiment is set-up in the FHWS c-Factory, which is a smart, IoT-based production environment. A survey and an assembly cycle time measurement is conducted to collect data to characterize AR assistance. The study participants assemble a toy truck once without and once with AR support. The evaluation shows that the mean assembly time decreases. The results show also, that AR is accepted by the participants supporting their work.},
	Author = {F. {Schuster} and U. {Sponholz} and B. {Engelmann} and J. {Schmitt}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00047},
	Keywords = {assembling;augmented reality;Internet of Things;production engineering computing;user interfaces;AR-assisted industrial assembly;AR assistance system;augmented reality;user acceptance;user-interface design;proprietary model;acceptance measurement;assembly cycle time measurement;mean assembly time;AR acceptance;FHWS c-Factory;IoT-based production environment;toy truck;Analytical models;Toy manufacturing industry;Laboratories;Production;Time measurement;Augmented reality;User acceptance;Augmented Reality;Assisted assembly},
	Month = {Nov},
	Pages = {135-140},
	Title = {A User Study on AR-assisted Industrial Assembly},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00047}}

@inproceedings{9288393,
	Abstract = {Navigation is a crucial cognitive skill that allows humans and animals to move from one place to another without getting lost. In neurological patients this skill can be impaired, when neural structures that form the brain networks important for spatial learning and navigation are impaired. Thus, spatial navigation represents an important measure of cognitive health that is impossible to test in a clinical examination, due to lack of space in examination rooms. Consequently, spatial navigation is largely neglected in the clinical assessment of neurological, neurosurgical and psychiatric patients. Virtual reality represents a unique opportunity to develop a systematic assessment of spatial navigation for diagnosis and therapeutic monitoring of millions of patients presenting with cognitive decline in the clinical routine. Therefore, we have adapted a classical spatial navigation paradigm that was developed for animal research, the "Morris Water Maze" as an openly available Virtual Reality (VR) application, that allows objective quantification of navigational skills in humans. This tool may be used in the future to aid the assessment of the human navigation system in health and neurological disease.},
	Author = {D. {Roth} and C. F. {Purps} and W. -J. {Neumann}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00048},
	Keywords = {brain;cognition;diseases;medical computing;navigation;neurophysiology;patient diagnosis;psychology;virtual reality;classical spatial navigation paradigm;virtual reality application;human navigation system;neurological disease;Virtual Morris Water Maze;crucial cognitive skill;neurological patients;spatial learning;cognitive health;clinical assessment;neurosurgical patients;psychiatric patients;cognitive decline;Systematics;Navigation;Animals;Design methodology;Tools;Synchronization;Open source software;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;Human-centered computing;Visualization;Visualization design and evaluation methods},
	Month = {Nov},
	Pages = {141-146},
	Title = {A Virtual Morris Water Maze to Study Neurodegenarative Disorders},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00048}}

@inproceedings{9288374,
	Abstract = {Social experience is important when audience are watching movies. Virtual reality (VR) movies engage audience through immersive environment and interactive narrative. However, VR headsets restrict audience to an individual experience, which disrupt the potential for shared social realities. In our study, we propose an approach to design an asynchronous social experience that allows the participant to receive other audiences' voice comments (such as their opinions, impressions or emotional reactions) in VR movies. We measured the participants' feedback on their engagement levels, recall abilities and social presence. The results showed that in VR-Voice Comment (VR-VC) movie, the audience's voice comments could affect participant's engagement and the recall of information in the scenes. The participants obtained social awareness and enjoyment at the same time. A few of them were worried mainly because of the potential auditory clutter that resulted from unpredictable voice comments. We discuss the design implications for this and directions for future research. Overall, we observe a positive tendency in watching VR-VC movie, which could be adapted for future VR movie experience.},
	Author = {S. {Yan} and W. {Jiang} and M. {Xiong} and X. {Shen}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00049},
	Keywords = {entertainment;virtual reality;engagement levels;social presence;social awareness;enjoyment;VR-VC movie;virtual reality movies;VR headsets;social realities;asynchronous social experience;impressions;emotional reactions;VR movie experience;audience voice comments;immersive environment;interactive narrative;recall abilities;Headphones;Atmospheric measurements;Prototypes;Motion pictures;Particle measurements;User experience;Information systems;VR movie;social presence;audience engagement;asynchronous interaction;voice comments},
	Month = {Nov},
	Pages = {147-152},
	Title = {An Exploratory Study for Designing Social Experience of Watching VR Movies Based on Audience's Voice Comments},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00049}}

@inproceedings{9288380,
	Abstract = {We describe a multi-user system enabling instant messaging in Augmented Reality. A user can get in contact with another one without requiring his/her identification number and can easily localize the person initiating the contact. It is also possible to exchange various types of personal information in a private manner. This innovative type of social interaction can significantly increase consumer interest for AR experiences.},
	Author = {P. {Jouet} and V. {Alleaume} and A. {Laurent} and M. {Fradet} and T. {Luo} and C. {Baillard}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00050},
	Keywords = {augmented reality;data privacy;electronic messaging;Internet;user experience;AR-Chat;instant messaging system;multiuser system;augmented reality;identification number;personal information;social interaction;AR experiences;Solid modeling;Three-dimensional displays;Protocols;Prototypes;Instant messaging;Servers;Augmented reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Computer systems organization;Architectures;Distributed architectures;Client-server architectures},
	Month = {Nov},
	Pages = {153-157},
	Title = {AR-Chat: an AR-based instant messaging system},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00050}}

@inproceedings{9288440,
	Abstract = {The regular studies on optical illusions through artistic practice usually focus on illusionist painting, placing its conceptualization and methodology within the framework of the pictorial tradition. However, few investigations have dealt with the phenomena of ambiguity, distortion, wrong motion perception, or color in artistic pieces created through the use of augmented reality. This offers a new field of study through which to explore optical illusions, hereby considered erroneous and unconscious perceptions of the actual physical characteristics of an image or object, through the hybridization of real and virtual elements. For this, an investigation of optical illusions is carried out through the work of various artists who use augmented reality technology to generate their artistic pieces. At the same time, the results of an experiment are shown, in which 20 participants evaluate three artistic prototypes created by the author, in which digital elements are superimposed on backgrounds whose characteristics have discordant relationships, of tension and contradiction between figure and background.},
	Author = {B. J. {P{\'e}rez}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00051},
	Keywords = {art;augmented reality;visual perception;augmented illusionism;optical illusions;artistic pieces;augmented reality;artistic prototypes;illusionist painting;virtual element hybridization;real element hybridization;Visualization;Optical distortion;Color;Optical imaging;Adaptive optics;Augmented reality;Optical devices;Augmented reality;optical illusions;art;perception},
	Month = {Nov},
	Pages = {158-164},
	Title = {Augmented illusionism. The influence of optical illusions through artworks with augmented reality},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00051}}

@inproceedings{9288400,
	Abstract = {Multimodal interaction is expected to offer better user experience in Augmented Reality (AR), and thus becomes a recent research focus. However, due to the lack of hardware-level support, most existing works only combine two modalities at a time, e.g., gesture and speech. Gaze-based interaction techniques have been explored for the screen-based application, but rarely been used in AR systemsy configurable augmented reality system. In this paper, we propose a multimodal interactive system that integrates gaze, gesture and speech in a flexibly configurable augmented reality system. Our lightweight head-mounted device supports accurate gaze tracking, hand gesture recognition and speech recognition simultaneously. More importantly, the system can be easily configured into different modality combinations to study the effects of different interaction techniques. We evaluated the system in the table lamps scenario, and compared the performance of different interaction techniques. The experimental results show that the Gaze+Gesture+Speech is superior in terms of performance.},
	Author = {Z. {Wang} and H. {Yu} and H. {Wang} and Z. {Wang} and F. {Lu}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00052},
	Keywords = {augmented reality;gesture recognition;helmet mounted displays;human computer interaction;interactive systems;object tracking;speech recognition;multimodal interaction;user experience;hardware-level support;gaze-based interaction techniques;screen-based application;AR systems;multimodal interactive system;flexibly configurable augmented reality system;lightweight head-mounted device;gaze tracking;hand gesture recognition;speech recognition;modality combination;single-modal interaction;Performance evaluation;Design methodology;Interactive systems;Speech recognition;Gesture recognition;User experience;Augmented reality;multimodal interaction;augmented reality;gaze;gesture;speech;AR system},
	Month = {Nov},
	Pages = {165-166},
	Title = {Comparing Single-modal and Multimodal Interaction in an Augmented Reality System},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00052}}

@inproceedings{9288396,
	Abstract = {Real-time tracking and visual feedback offer interactive AR-assisted capture systems as a convenient and low-cost alternative to specialized sensor rigs and robotic gantries. We present a simple strategy for decoupling localization and visual feedback in these applications from the primary sensor being used to capture the scene. Our strategy is to use an AR HMD and 6-DOF controller for tracking and feedback, synchronized with a separate primary sensor for capturing the scene. This approach allows for convenient real-time localization of sensors that cannot do their own localization (e.g., microphones). In this poster paper, we present a prototype implementation of this strategy and investigate the accuracy of decoupled tracking by mounting a high resolution camera as the primary sensor, and comparing decoupled runtime pose estimates to the pose estimates of a high-resolution offline structure from motion.},
	Author = {S. K. {Skovsen} and H. {Haraldsson} and A. {Davis} and H. {Karstoft} and S. {Belongie}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00053},
	Keywords = {augmented reality;cameras;helmet mounted displays;image motion analysis;image resolution;image sensors;interactive systems;object tracking;pose estimation;real-time localization;decoupled tracking;decoupled runtime;decoupled localization;HMD-based AR;interactive scene acquisition;real-time tracking;visual feedback;interactive AR-assisted capture systems;sensing;AR HMD;6-DOF controller;sensor;high resolution camera;pose estimates;high-resolution offline structure from motion;Visualization;Tracking;Prototypes;Robot sensing systems;Real-time systems;Sensors;Augmented reality;Human-centered computing;Human computer interaction (HCI);HCI theory;concepts and models;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality},
	Month = {Nov},
	Pages = {167-171},
	Title = {Decoupled Localization and Sensing with HMD-based AR for Interactive Scene Acquisition},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00053}}

@inproceedings{9288426,
	Abstract = {The research presented in this contribution aims to investigate user preferences about how to convey information in Industrial Augmented Reality (IAR) interfaces to the user. Our interest is focused on the opinion of potential technical writers of IAR documentation for assembly or maintenance operations. Authoring of IAR interfaces imply a choice among various visual assets, that is influenced by the information type and the AR display used. There are no specific standards in the literature to follow and it is challenging to extract guidelines from the literature. This study gathers preferences of 105 selected users that have knowledge about IAR issues, graphical user interfaces (GUI) designing, and assembly/maintenance procedures. The results of this survey show a great preference for 3D CAD models of components (product model) for almost all the information types. However, some alternative visual assets have also been proposed, such as video and auxiliary models. Contrary to common practices in industry, text was the least preferred visual asset. The insights from this research can help other IAR technical writers in the authoring of their interfaces.},
	Author = {M. {Gattullo} and L. {Dammacco} and F. {Ruospo} and A. {Evangelista} and M. {Fiorentino} and J. {Schmitt} and A. E. {Uva}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00054},
	Keywords = {assembling;augmented reality;authoring systems;CAD;data visualisation;graphical user interfaces;maintenance engineering;production engineering computing;design preferences;industrial augmented reality;user preferences;IAR documentation;authoring;IAR interfaces;graphical user interfaces;3D CAD models;IAR technical writers;visual asset;assembly;maintenance operations;AR display;GUI;product model;Visualization;Solid modeling;Three-dimensional displays;Multimedia systems;Augmented reality;Standards;Graphical user interfaces;Visual Asset;Industrial Augmented Reality;Graphic Interface;Manual Assembly},
	Month = {Nov},
	Pages = {172-177},
	Title = {Design preferences on Industrial Augmented Reality: a survey with potential technical writers},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00054}}

@inproceedings{9288423,
	Abstract = {We propose a distortion correction framework of the AR-HUD virtual image based on a multilayer feedforward neural network model (MFNN) and spatial continuous mapping(SCM). First, we put forward the concept and calculation method of the equivalent plane of the virtual image in the AR-HUD system. Then construct a network structure named MFNN-SCM, and train a network model that can predict the vertex coordinates and pre-distortion map of the equivalent plane of AR-HUD virtual image, and then obtain the eye position of the driver based on training. The network model calculates the virtual image projection mapping relationship under the current eye position of the driver. Finally, the virtual image projection mapping relationship is used to pre-distort the AR-HUD projected image, and the pre-distorted image is projected to improve the AR-HUD imaging effect observed by the driver. In addition, we have embedded the framework into the AR-HUD system of intelligent vehicles and tested it in the real vehicle. The results show that he projected virtual image in this paper has a small relative pixel drift at any eye position. On the premise of ensuring the real-time performance of the algorithm, our method has stronger flexibility, higher accuracy and lower cost than other existing methods.},
	Author = {K. {Li} and L. {Bai} and Y. {Li} and Z. {Zhou}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00055},
	Keywords = {augmented reality;cartography;computer vision;driver information systems;feedforward neural nets;head-up displays;intelligent transportation systems;multilayer perceptrons;AR-HUD virtual image;multilayer feedforward neural network model;eye position;virtual image projection mapping relationship;AR-HUD projected image;AR-HUD imaging effect;distortion correction;predistortion map;vertex coordinate prediction;predistorted image;spatial continuous mapping;SCM;MFNN;intelligent vehicles;Training;Machine learning algorithms;Predictive models;Distortion;Nonhomogeneous media;Real-time systems;Testing;distortion correction of the virtual image;AR-HUD system;multilayer forward network model;spatial continuous mapping relationship},
	Month = {Nov},
	Pages = {178-183},
	Title = {Distortion Correction Algorithm of AR-HUD Virtual Image based on Neural Network Model of Spatial Continuous Mapping},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00055}}

@inproceedings{9288445,
	Abstract = {Using an augmented reality head-mounted display(HMD) to extend the display of smartphone could open up new user interface solutions, benefiting users with an increased screen real-estate and visualizations that leverage each device's capabilities. Some previous works have explored the viability of extended displays, but knowledge regarding considerable design factors and constraints for such displays is still very limited. In our work, we conducted an exploratory study to investigate how different properties of augmented content in extended displays affect the user's task performance and subjective workload in an image comparison task. Through an experiment with 24 participants, we compared four augmented content placements (top, right, left, and bottom) and two augmented content sizes (small and large). The study results demonstrated that there are significant effects of both placement and size of augmented content on task performance and subjective workload. Based on the findings of our study, we propose some recommendations for the future design of extended displays.},
	Author = {S. {Bang} and H. {Lee} and W. {Woo}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00056},
	Keywords = {augmented reality;data visualisation;helmet mounted displays;mobile computing;smart phones;user experience;user interfaces;wearable computers;extended displays;user interface solutions;augmented content placements;user search experience;augmented reality head mounted display;smartphone display;visualizations;Performance evaluation;Visualization;Design methodology;User interfaces;Task analysis;Augmented reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed/augmented reality;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies},
	Month = {Nov},
	Pages = {184-188},
	Title = {Effects of Augmented Content's Placement and Size on User's Search Experience in Extended Displays},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00056}}

@inproceedings{9288417,
	Abstract = {Information in augmented reality (AR) consists of virtual and real contexts and is provided to the AR environment at different distances for each user. Therefore, it is important to understand how the integration of two different information influences the user's AR experience. However, little has been studied regarding the complexity and viewing distance of a real space. Our study investigates how the complexity of the physical environment and viewing distance influence workload and performance in a visual search task in an AR environment. We conducted an experiment in which participants performed conjunction search under three different levels of background complexity, at both near (1.5m) and far (3m) distances. The results indicated that as the complexity of the background increased, the users' performance time and workload were negatively impacted. In addition, when the distance between the user and the background was greater, search time increased. From the results of the study, we derived some recommendations for the design of AR interfaces. Our research contributes to the design of interfaces by demonstrating the necessity to consider the complexity of background and viewing distance.},
	Author = {H. {Lee} and S. {Bang} and W. {Woo}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00057},
	Keywords = {augmented reality;human computer interaction;information retrieval;AR interfaces;user performance time;AR experience;search time;conjunction search;physical environment;AR environment;augmented reality;AR visual search task;viewing distance;background complexity;Visualization;Design methodology;Complexity theory;Task analysis;Augmented reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed/augmented reality;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies},
	Month = {Nov},
	Pages = {189-194},
	Title = {Effects of Background Complexity and Viewing Distance on an AR Visual Search Task},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00057}}

@inproceedings{9288395,
	Abstract = {Camera pose estimation is commonly used for augmented reality, and it is currently expected to be integrated into sports assistant technologies. However, conventional methods face difficulties in simultaneously achieving fast estimation in milliseconds or less for sports, bright lighting environments of the outdoors, and capturing of large activity areas. In this paper, we propose EmnDash, M-sequence dashed markers on vector-based laser projection for an asynchronous high-speed dynamic camera, which provides both a graphical information display for humans and markers for the wearable high-speed camera with a high S/N ratio from a distance. One of the main notions is drawing a vector projection image with a single stroke using two dashed lines as markers. The other involves embedding the binary M-sequence as the length of each dashed line and its recognition method using locality. The recognition of the M-sequence dashed line requires only a one-shot image, which increases the robustness of tracking both in terms of camera orientation and occlusion. We experimentally confirm an increase in recognizable posture, sufficient tracking accuracy, and low-computational cost in the evaluation of a static camera. We also show good tracking ability and demonstrate immediate recovery from occlusion in the evaluation of a dynamic camera.},
	Author = {R. {Nishizono} and T. {Sueishi} and M. {Ishikawa}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00058},
	Keywords = {augmented reality;cameras;image motion analysis;image sensors;pose estimation;sport;conventional methods face difficulties;bright lighting environments;EmnDash;M-sequence dashed markers;vector-based laser projection;high-speed dynamic camera;graphical information display;wearable high-speed camera;vector projection image;binary M-sequence;recognition method;M-sequence dashed line;camera orientation;sufficient tracking accuracy;static camera;good tracking ability;robust high-speed spatial tracking;augmented reality;sports assistant technologies;Lasers;Pose estimation;Cameras;Robustness;Augmented reality;Sports;Signal to noise ratio;Human-centered computing;Human computer interaction (HCI);Interaction devices;Displays and imagers;Hardware;Communication hardware;interfaces and storage;Displays and imagers},
	Month = {Nov},
	Pages = {195-200},
	Title = {EmnDash: M-sequence Dashed Markers on Vector-based Laser Projection for Robust High-speed Spatial Tracking},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00058}}

@inproceedings{9288456,
	Abstract = {Redirected Walking (RDW) is commonly used to overcome the limitation of real walking locomotion while exploring virtual worlds. Although a few machine learning-based RDW algorithm is proposed, most of the system did not go through live user evaluation. In this work, we evaluated a novel RDW controller proposed by Chang et al., in which the formatted steering rule is replaced with reinforcement learning(RL), by simulation and live user experiment. We found the RL-based RDW controller reduced boundary collisions significantly in both simulation and user study comparing to the heuristic algorithm, Steer-to-Center(S2C); also, there are no noticeable differences in immersiveness. These results indicate that the novel controller is superior to the heuristic method. Furthermore, as we conducted experiments in a relatively simple space and still outperformed the heuristic method, we are optimistic that the RL-based controller can maintain the high-performance in complicated scenarios in the future.},
	Author = {T. {Ko} and L. {Su} and Y. {Chang} and K. {Matsumoto} and T. {Narumi} and M. {Hirose}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00059},
	Keywords = {control engineering computing;gait analysis;learning (artificial intelligence);metaheuristics;path planning;virtual reality;walking locomotion;virtual worlds;RDW algorithm;live user experiment;RL-based RDW controller;boundary collisions;heuristic algorithm;steer-to-center;optimal redirected walking planning;reinforcement learning;Legged locomotion;Machine learning algorithms;Heuristic algorithms;Computational modeling;Reinforcement learning;Planning;Task analysis;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
	Month = {Nov},
	Pages = {201-202},
	Title = {Evaluate Optimal Redirected Walking Planning Using Reinforcement Learning},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00059}}

@inproceedings{9288477,
	Abstract = {Supporting maintenance with 3D object enhanced instruction is one of the key applications of Augmented Reality (AR) in industry. For the breakthrough of AR in maintenance, it is important that the technicians themselves can create AR-instructions and perform the challenging task of placing 3D objects as they know best how to perform a task and what necessary information needs to be displayed. For this challenge, a 3D-content editor is being presented wherein a first step the 3D objects can roughly be placed using a 2D image of the machine, therefore, limiting the time required to access the machine. In a second step, the positions of the 3D objects can be fine-tuned at the machine site using live footage. The key challenges were to develop an easily accessible UI that requires no prior knowledge of AR content creation in a tool that works both with live footage and images and is usable with a touch screen and keyboard/mouse. The 3D-content editor was qualitatively assessed by technicians revealing its general applicability, but also the requirement for a lot of time to gain the necessary experience for positioning 3D objects.},
	Author = {M. {Lorenz} and S. {Knopp} and J. {Kim} and P. {Klimant}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00060},
	Keywords = {augmented reality;machining;maintenance engineering;personnel;production engineering computing;user interfaces;industrial augmented reality;3D-content editor;augmented reality maintenance worker support system;3D object enhanced instruction;2D image;3D object positioning;machine access;user interface;touch screen;keyboard;mouse;Three-dimensional displays;Two dimensional displays;Maintenance engineering;Touch sensitive screens;Tools;Task analysis;Augmented reality;Augmented Reality;3D content editing;User interface;Maintenance},
	Month = {Nov},
	Pages = {203-205},
	Title = {Industrial Augmented Reality: 3D-Content Editor for Augmented Reality Maintenance Worker Support System},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00060}}

@inproceedings{9288378,
	Abstract = {While directional guidance is essential for spatial navigation, little has been studied about providing nonvisual cues in 3D space for individuals who are blind or have limited visual acuity. To understand the effects of different nonvisual feedback for 3D directional guidance, we conducted a user study with 12 blind-folded participants. They were asked to search for a virtual target in a 3D space with a laser pointer as quickly as possible under 6 different feedback designs varying the feedback mode (beeping vs. haptic vs. beeping+haptic) and the presence of a stereo sound. Our findings show that beeping sound feedback with and without haptic feedback outperforms the mode where only haptic feedback is provided. We also found that stereo sound feedback generated from a target significantly improves both the task completion time and travel distance. Our work can help people who are blind or have limited visual acuity to understand the directional guidance in a 3D space.},
	Author = {S. {Chung} and K. {Lee} and U. {Oh}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00061},
	Keywords = {force feedback;handicapped aids;haptic interfaces;human computer interaction;light pens;navigation;vision defects;three-dimensional directional guidance;target pointing task;spatial navigation;nonvisual feedback;blind-folded participants;virtual target;haptic feedback;stereo sound feedback;blindness;3D directional guidance;laser pointer;beeping sound feedback;limited visual acuity;Visualization;Three-dimensional displays;Navigation;Laser modes;Haptic interfaces;Space exploration;Task analysis;Directional guidance;audio feedback;haptic feedback;3D environment},
	Month = {Nov},
	Pages = {206-210},
	Title = {Investigating Three-dimensional Directional Guidance with Nonvisual Feedback for Target Pointing Task},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00061}}

@inproceedings{9288458,
	Abstract = {This paper presents a novel method for simultaneous human detection and 3D shape reconstruction from a single RGB image. It offers a low-cost alternative to existing motion capture solutions, allowing to reconstruct realistic human 3D shapes and poses by leveraging the speed of an object-detection based architecture and the extended applicability of a parametric human mesh model. Evaluation results using a synthetic dataset show that our approach is on-par with conventional 3D reconstruction methods in terms of accuracy, and outperforms them in terms of inference speed, particularly in the case of multi-person images.},
	Author = {E. {Pe{\~n}a-Tapia} and R. {Hachiuma} and A. {Pasquali} and H. {Saito}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00062},
	Keywords = {image colour analysis;image motion analysis;image reconstruction;object detection;pose estimation;LCR-SMPL;real-time human detection;single RGB image;simultaneous human detection;3D shape reconstruction;existing motion capture solutions;realistic human 3D shapes;object-detection based architecture;parametric human mesh model;conventional 3D reconstruction methods;multiperson images;Solid modeling;Three-dimensional displays;Shape;Switches;Real-time systems;Image reconstruction;Augmented reality;Artificial Intelligence;Computer vision;Computer vision problems;Shape inference;Artificial Intelligence;Computer vision;Computer vision problems;Reconstruction},
	Month = {Nov},
	Pages = {211-212},
	Title = {LCR-SMPL: Toward Real-time Human Detection and 3D Reconstruction from a Single RGB Image},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00062}}

@inproceedings{9288372,
	Abstract = {The Social distancing rule has proven to be an effective measure against the spread of the infectious COronaVIrus Disease 2019 (COVID-19). Even with a lot of research focusing on static camera based solutions for monitoring the rule, the real issue with visualising and monitoring rules for public spaces still remain an open question. In this work we propose a Social Distancing Helmet (SDH) with basic prototyping for an outdoor augmentation system using body worn sensors for visualising and monitoring rules for shared spaces using AR. First results with some software components of the prototype are presented.},
	Author = {V. {kamalasanan} and M. {Sester}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00063},
	Keywords = {augmented reality;data visualisation;diseases;epidemics;image capture;microorganisms;mobile computing;public administration;smart phones;wearable computers;smartphone camera;image capture medium;augmented reality;prototype software components;rule visualisation;body worn sensors;outdoor augmentation system;social distancing helmet;rule monitoring;social distancing rulea;public spaces;static camera;coronavirus disease 2019;COVID-19;Visualization;Human factors;Social factors;Sensors;Monitoring;Viruses (medical);Rules for shared spaces;Visualization;Protective systems;Safety;Enabling rules with AR;Control;Visualization design and evaluation methods},
	Month = {Nov},
	Pages = {213-216},
	Title = {Living with Rules: An AR Approach},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00063}}

@inproceedings{9288446,
	Abstract = {Augmented/virtual reality applications can provide immersive and interactive virtual environment for motor rehabilitation using the collaborative stimulations of multiple sensory channels such as sight, hearing, and movement, enhance the rehabilitation effect through repetitions, feedbacks, and encouragement. In this paper, we propose an evaluating and training integrated application for the rehabilitation of patients with lower limb balance disorder. The AR-based evaluation module visualizes the limits of lower limbs patients' balance abilities and provides quantitative data to their therapists, then rehabilitation therapists can customize personalized VR training games accordingly.},
	Author = {S. {Chen} and B. {Hu} and Y. {Gao} and Z. {Liao} and J. {Li} and A. {Hao}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00064},
	Keywords = {augmented reality;medical computing;patient rehabilitation;patient treatment;serious games (computing);post-stroke patients;immersive environment;interactive virtual environment;motor rehabilitation;collaborative stimulations;sensory channels;rehabilitation effect;lower limb balance disorder;evaluation module;lower limbs patients;rehabilitation therapists;VR training games;lower limb balance rehabilitation;Training;Human computer interaction;Virtual environments;Medical treatment;Games;Usability;Augmented reality;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Social and professional topics;Computing / technology policy;Medical information policy;Medical technologies},
	Month = {Nov},
	Pages = {217-218},
	Title = {Lower Limb Balance Rehabilitation of Post-stroke Patients Using an Evaluating and Training Combined Augmented Reality System},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00064}}

@inproceedings{9288439,
	Abstract = {With the rapid development of mobile sensor, network infrastructure and cloud computing, the scale of AR application scenario is expanding from small or medium scale to large-scale environments. Localization in the large-scale environment is a critical demand for the AR applications. Most of the commonly used localization techniques require quite a number of data with groundtruth localization for algorithm benchmarking or model training. The existed groundtruth collection methods can only be used in the outdoors, or require quite expensive equipments or special deployments in the environment, thus are not scalable to large-scale environments or to massively produce a large amount of groundtruth data. In this work, we propose LSFB, a novel low-cost and scalable frame-work to build localization benchmark in large-scale environments with groundtruth poses. The key is to build an accurate HD map of the environment. For each visual-inertial sequence captured in it, the groundtruth poses are obtained by joint optimization taking both the HD map and visual-inertial constraints. The experiments demonstrate the obtained groundtruth poses are accurate enough for AR applications. We use the proposed method to collect a dataset of both mobile phones and AR glass exploring in large-scale environments, and will release the dataset as a new localization benchmark for AR.},
	Author = {H. {Liu} and M. {Jiang} and Z. {Zhang} and X. {Huang} and L. {Zhao} and M. {Hang} and Y. {Feng} and H. {Bao} and G. {Zhang}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00065},
	Keywords = {image sequences;least squares approximations;mobile computing;mobile handsets;mobile radio;optimisation;wireless sensor networks;building large-scale localization;medium scale;large-scale environment;groundtruth localization;scalable frame-work;localization benchmark;groundtruth poses;Training;Visualization;Three-dimensional displays;Glass;Benchmark testing;Mobile handsets;Optimization;Human-centered computing;Human Computer Interaction;Interaction Paradigms;Mixed / Augmented Reality;Computing Methodologies;Artificial Intelligence;Computer Vision;Tracking},
	Month = {Nov},
	Pages = {219-224},
	Title = {LSFB: A Low-cost and Scalable Framework for Building Large-Scale Localization Benchmark},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00065}}

@inproceedings{9288369,
	Abstract = {Human-robot collaboration could be valuable in some challenging tasks. Previous researches only consider the human-centered systems, but there will be many changes in the symmetrical reality (SR) systems because there are two perceptual centers in symmetrical reality. In this paper, we introduce the contents of the symmetrical reality-based human-robot collaboration and interpret the human- robot collaboration from the perspective of equivalent interaction. By analyzing task definition in symmetrical reality, we present the special features of human-robot collaboration. Furthermore, there are many fields in which the symmetrical reality can produce a remarkable effect, we only list some typical applications, such as service robots, remote training, interactive exhibition, digital assistants, companion robots, the immersive entertainment community and so forth. The current situation and future development of this framework are also analyzed to provide a kind of guidance for researchers.},
	Author = {Z. {Zhang} and X. {Wang}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00066},
	Keywords = {augmented reality;control engineering computing;human-robot interaction;intelligent robots;human-centered systems;symmetrical reality;human-robot collaboration;service robots;companion robots;machine intelligence;remote training;interactive exhibition;digital assistants;immersive entertainment community;augmented reality;Training;Service robots;Collaboration;Entertainment industry;Task analysis;Augmented reality;Machine intelligence;Human-centered computing---Human computer interaction (HCI)---HCI theory;concepts and models;Human-centered computing---Human computer interaction (HCI)--- Interaction paradigms---Mixed / augmented reality},
	Month = {Nov},
	Pages = {225-228},
	Title = {Machine Intelligence Matters: Rethink Human-Robot Collaboration Based on Symmetrical Reality},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00066}}

@inproceedings{9288449,
	Abstract = {3D object tracking with monocular RGB images faces many challenges in real environments. The popular color- and edge-based methods, although have been well studied, are still known to be limited in handling specific cases. We observed that the color and edge features are complementary for different cases, and thus propose to fuse them for improving tracking robustness. To optimize the combination and to cope with inconsistency between color and edge features, we propose to fuse different energy terms with respect to a set of local bundles. Each bundle represents a local region containing a set of pixel locations for computing color and edge energies, in which two energy terms are adaptively weighted to play advantages of them. Experiments show that the proposed method can improve the accuracy in challenging cases, especially in light changing and similar color condition.},
	Author = {J. {Li} and F. {Zhong} and X. {Qin}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00067},
	Keywords = {feature extraction;image colour analysis;object detection;object tracking;multifeature 3D object tracking;adaptively-weighted local bundles;monocular RGB images;edge-based methods;edge features;robustness tracking;energy terms;edge energies;color condition;Three-dimensional displays;Image color analysis;Fuses;Image edge detection;Robustness;Object tracking;Faces;Computing methodologies;Artificial intelligence;Computer vision---Tracking},
	Month = {Nov},
	Pages = {229-230},
	Title = {Multi-feature 3D Object Tracking with Adaptively-Weighted Local Bundles},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00067}}

@inproceedings{9288425,
	Abstract = {As networks become increasingly complex, professionals must familiarise themselves with the cabling rack to administer the network effectively. In line with this industry goal, we compared the usability, task load and perceptions of three similar network cabling tutoring systems: (1) a hand-held Augmented Reality (AR)-based cabling tutor (HAR); (2) a head-mounted AR-based cabling tutor (HMD) and (3) a 2-Dimensional (2D)-based cabling tutor (HH). While usability of different modalities have been compared previously, none of those comparisons used knowledge modelling approaches. So, in our comparison, each tutor uses knowledge space modelling (KSM) approaches to detect learner mistakes and show arrows on the rack to indicate the source of the mistake. While adding an AR sub-system to a network cabling tablet-based tutor may not necessarily improve usability, participants reported higher engagement over the AR hand held tablet when using the Head-Mounted Display (HMD) condition. Several potential reasons were identified to account for the side effect such as the potential for the AR sub-system to potentially influence learning perceptions; the additional physical effort of needing to point the tablet at the rack and perceived performance degradation.},
	Author = {B. M. {Herbert} and G. {Wigley} and B. {Ens} and M. {Billinghurst}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00068},
	Keywords = {augmented reality;cables (electric);computer aided instruction;computer networks;computer science education;helmet mounted displays;telecommunication computing;telecommunication engineering education;cabling rack;network cabling tablet-based tutor;head-mounted display condition;network cabling tutoring systems;2D-based cabling tutor;knowledge space modelling;KSM;augmented reality-based cabling tutor;head-mounted AR-based cabling tutor;Training;Visualization;Computational modeling;Resists;Usability;Task analysis;Augmented reality;Human-centered computing;Visualization;Visualization techniques;Augmented Reality;Human Computer Interaction (HCI);HCI design and evaluation methods},
	Month = {Nov},
	Pages = {231-236},
	Title = {Perceptions of Integrating Augmented Reality into Network Cabling Tutors},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00068}}

@inproceedings{9288450,
	Abstract = {This paper describes a three-part interactive museum exhibition targeted at the domain of Natural History, where various technological components are utilized to deliver a compelling Mixed Reality experience for the museum's visitors. The goal is to create new educational pathways for both adults and children wishing to learn about prehistoric life on the island of Crete, while simultaneously attracting a broader audience and maintaining its engagement with the museum digital content for longer periods of time. In these experiences, holographic technology, diminished reality and multi-view 3D reconstruction are combined with fact/fantasy storytelling, utilizing cost-efficient state-of-the-art solutions.},
	Author = {K. C. {Apostolakis} and G. {Margetis} and C. {Stephanidis}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00069},
	Keywords = {computer aided instruction;history;museums;solid modelling;virtual reality;3D modelling software;virtual space;fantasy storytelling;fact storytelling;multiview 3D reconstruction;island of Crete;interactive museum exhibition;Pleistocene Crete;diminished reality;holographic technology;museum digital content;educational pathways;natural history;prehistoric wildlife;interactive mixed reality exhibition;Deep learning;Three-dimensional displays;Design methodology;Wildlife;Tools;History;Usability;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
	Month = {Nov},
	Pages = {237-240},
	Title = {Pleistocene Crete: A narrative, interactive mixed reality exhibition that brings prehistoric wildlife back to life},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00069}}

@inproceedings{9288405,
	Abstract = {The PRISME model introduced in this article is part of ongoing research On VR and AR for ergonomics and the design of industrial operator platforms. In this context, PRISME is an innovative solution by proving an automatic link (removing the need for domain-specific adaptations) between operator tasks and interactions with their platform. This research will be presented in two stages: a generic topology of interactors in Mixed and Tangible reality followed by an interaction model based on MASCARET's activity description meta-model syntax. Finally, an aeronautical use case will validate the model by simulating the standard operations performed by an airplane controller.},
	Author = {J. -M. {FAZZARI} and S. {KUBICKI} and R. {QUERREC}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00070},
	Keywords = {augmented reality;ergonomics;human computer interaction;interaction model;domain activities;virtual environments;PRISME;MASCARET activity description meta model syntax;mixed interactor;tangible interactor;airplane controller;VR;AR;industrial operator platform design;ergonomics;Adaptation models;Atmospheric modeling;Computational modeling;Virtual environments;Syntactics;Topology;Task analysis;Human-centered computing;Augmented Reality;Virtual Reality;Interaction},
	Month = {Nov},
	Pages = {241-246},
	Title = {PRISME: An interaction model linking domain activities and mixed and tangible interactors in virtual environments},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00070}}

@inproceedings{9288444,
	Abstract = {Virtual Reality (VR) technology has been proliferating in the last decade, especially in the last few years. However, Simulator Sickness (SS) still represents a significant problem for its wider adoption. Currently, the most common way to detect SS is using the Simulator Sickness Questionnaire (SSQ). SSQ is a subjective measurement and is inadequate for real-time applications such as VR games. This research aims to investigate how to use machine learning techniques to detect SS based on in-game characters' and users' physiological data during gameplay in VR games. To achieve this, we designed an experiment to collect such data with three types of games. We trained a Long Short-Term Memory neural network with the dataset eye-tracking and character movement data to detect SS in real-time. Our results indicate that, in VR games, our model is an accurate and efficient way to detect SS in real-time.},
	Author = {J. {Wang} and H. -N. {Liang} and D. V. {Monteiro} and W. {Xu} and H. {Chen} and Q. {Chen}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00071},
	Keywords = {computer games;learning (artificial intelligence);physiology;psychology;recurrent neural nets;virtual reality;simulator sickness questionnaire;SSQ;real-time applications;VR games;in-game characters;gameplay;character movement data;real-time detection;virtual reality games;player psychophysiological data;subjective measurement;machine learning;long short-term memory neural network;eye-tracking;Solid modeling;Analytical models;Neural networks;Games;Brain modeling;Real-time systems;Physiology;Virtual Reality;Gaming;Simulator Sickness;Machine Learning;EEG;Eye-tracking},
	Month = {Nov},
	Pages = {247-248},
	Title = {Real-Time Detection of Simulator Sickness in Virtual Reality Games Based on Players' Psychophysiological Data during Gameplay},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00071}}

@inproceedings{9288466,
	Abstract = {We present Retargetable AR-a novel AR framework that yields an AR experience that is aware of scene contexts set in various real environments, achieving natural interaction between the virtual and real worlds. We characterize scene contexts with relationships among objects in 3D space. A context assumed by an AR content and a context formed by a real environment where users experience AR are represented as abstract graph representations, i.e. scene graphs. From RGB-D streams, our framework generates a volumetric map in which geometric and semantic information of a scene are integrated. Moreover, using the semantic map, we abstract scene objects as oriented bounding boxes and estimate their orientations. Then our framework constructs, in an online fashion, a 3D scene graph characterizing the context of a real environment for AR. The correspondence between the constructed graph and an AR scene graph denoting the context of AR content provides a semantically registered content arrangement, which facilitates natural interaction between the virtual and real worlds. We performed extensive evaluations on our prototype system through quantitative evaluation of the performance of the oriented bounding box estimation, subjective evaluation of the AR content arrangement based on constructed 3D scene graphs, and an online AR demonstration.},
	Author = {T. {Tahara} and T. {Seno} and G. {Narita} and T. {Ishikawa}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00072},
	Keywords = {augmented reality;graph theory;image colour analysis;natural scenes;stereo image processing;ubiquitous computing;user experience;RGB-D streams;3D scene graph;AR content arrangement;retargetable AR;oriented bounding box estimation;AR scene graph;oriented bounding boxes;abstract scene objects;semantic map;semantic information;geometric information;abstract graph representations;users experience AR;indoor scenes;context-aware augmented reality;Three-dimensional displays;Semantics;Prototypes;Estimation;User experience;Task analysis;Augmented reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Artificial intelligence;Computer vision;Computer vision tasks;Scene understanding},
	Month = {Nov},
	Pages = {249-255},
	Title = {Retargetable AR: Context-aware Augmented Reality in Indoor Scenes based on 3D Scene Graph},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00072}}

@inproceedings{9288474,
	Abstract = {We propose a transparent colored AR marker that allows 3D objects to be stacked in space. Conventional AR markers make it difficult to display multiple objects in the same position in space, or to manipulate the order or rotation of objects. The proposed transparent colored markers are designed to detect the order and rotation direction of each marker in the stack from the observed image, based on mathematical constraints. We describe these constraints to design markers, the implementation to detect its stacking order and rotation of each marker, and a proof-of-concept application Totem Poles. We also discuss the limitations of the current prototype and possible research directions.},
	Author = {X. {Zhang} and J. {Lundgren} and Y. {Mesaki} and Y. {Hiroi} and Y. {Itoh}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00073},
	Keywords = {augmented reality;data visualisation;human computer interaction;partially transparent markers;rotation direction;stacking order;stencil marker;augmented reality object stacking;transparent colored AR marker;3D object stacking;object manipulation;mathematical constraints;Totem Poles;Human computer interaction;Three-dimensional displays;Image color analysis;Stacking;Prototypes;Augmented reality;Human-centered computing;Visualization;Visualization techniques;Treemaps; Human-centered computing;Visualization;Visualization design and evaluation methods},
	Month = {Nov},
	Pages = {255-257},
	Title = {Stencil Marker: Designing Partially Transparent Markers for Stacking Augmented Reality Objects},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00073}}

@inproceedings{9288457,
	Abstract = {Assembly state detection, i.e., object state detection, has a critical meaning in computer vision tasks, especially in AR assisted assembly. Unlike other object detection problems, the visual difference between different object states can be subtle. For the better learning of such subtle appearance difference, we proposed a two-level group attention module (TGA), which consists of inter-group attention and intro-group attention. The relationship between feature groups as well as the representation within each feature group is simultaneously enhanced. We embedded the proposed TGA module in a popular object detector and evaluated it on two new datasets related to object state estimation. The result shows that our proposed attention module outperforms the baseline attention module.},
	Author = {H. {Liu} and Y. {Su} and J. {Rambach} and A. {Pagani} and D. {Stricker}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00074},
	Keywords = {augmented reality;computer vision;image representation;learning (artificial intelligence);object detection;assembly state detection;object state detection;computer vision;AR assisted assembly;object detection;visual difference;two-level group attention module;inter-group attention;intro-group attention;feature group;TGA module;object state estimation;Visualization;Computer vision;Object detection;Detectors;State estimation;Task analysis;Augmented reality;Computing methodologies;Machine learning;Machine learning approaches;Neural networks; Computing methodologies;Artificial intelligence;Computer vision;Computer vision tasks;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed/ augmented reality},
	Month = {Nov},
	Pages = {258-263},
	Title = {TGA: Two-level Group Attention for Assembly State Detection},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00074}}

@inproceedings{9288377,
	Abstract = {Designing a serious game for walking rehabilitation requires compliance with the theory of motor learning. Motivation, repetition, variability and feedback are key elements in improving and relearning a new walking pattern. As a preamble to the development of an AR rehabilitation game, and in order to choose the most effective feedback to provide to the patient, this article presents a preliminary study on the impact of presentation modalities on walking speed. We investigate which visual concurrent feedback modalities allows to reach and maintain a target speed (maximum or intermediate). Our first results on children with motor disabilities (n=10) show that some modalities improved walking performance and helped patients to better control their walking speed. In particular, a combination of targets anchored in the real world with a time indication seems to be effective in maintaining maximum walking speed, while simple moving objects could be used to control speed.},
	Author = {A. -L. {Guinet} and G. {Bouyer} and S. {Otmane} and E. {Desailly}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00075},
	Keywords = {augmented reality;control engineering computing;feedback;gait analysis;handicapped aids;medical computing;patient rehabilitation;serious games (computing);velocity control;visual concurrent feedback modalities;maximum walking speed;AR game;augmented feedback modalities;serious game;walking pattern;AR rehabilitation game;children with motor disabilities;speed control;motor learning theory;motivation;repetition;variability;feedback;Legged locomotion;Visualization;Pediatrics;Design methodology;Games;Augmented reality;Cerebral palsy;walking rehabilitation;augmented reality;serious game;feedback},
	Month = {Nov},
	Pages = {264-268},
	Title = {Towards an AR game for walking rehabilitation: Preliminary study of the impact of augmented feedback modalities on walking speed},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00075}}

@inproceedings{9288459,
	Abstract = {Sailing is a multidisciplinary activity that requires years to master. Recently this sustainable sport is becoming even harder due to the increasing number of onboard sensors, automation, artificial intelligence, and the high performances obtainable with modern vessels and sail designs. Augmented Reality technology (AR) has the potential to assist sailors of all ages and experience level and improve confidence, accessibility, situation awareness, and safety. This work presents our ongoing research and methodology for developing AR assisted sailing. We started with the problem definition followed by a state of the art using a systematic review. Secondly, we elicited the main task and variables using an online questionnaire with experts. Third, we extracted the main variables and conceptualized some visual interfaces using 3 different approaches. As final phase, we designed and implemented a user test platform using a VR headset to simulate AR in different marine scenarios. For a real deployment, we witness the lack of available AR devices, so we are developing one specific headset dedicated to this task. We also envision the possible redesign of the entire boat as a consequence of the introduction of AR technology.},
	Author = {F. {Laera} and M. M. {Foglia} and A. {Evangelista} and A. {Boccaccio} and M. {Gattullo} and V. M. {Manghisi} and J. L. {Gabbard} and A. E. {Uva} and M. {Fiorentino}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00076},
	Keywords = {augmented reality;data visualisation;decision support systems;ships;sport;virtual reality;multidisciplinary activity;sustainable sport;artificial intelligence;sail designs;augmented reality technology;sailors;experience level;situation awareness;AR assisted sailing;problem definition;systematic review;online questionnaire;visual interfaces;user test platform;AR technology;marine scenarios;Visualization;Three-dimensional displays;Navigation;Data visualization;Safety;Task analysis;Augmented reality;Augmented Reality;Sailing;Yacht;Nautical;Human Computer Interaction},
	Month = {Nov},
	Pages = {269-274},
	Title = {Towards Sailing supported by Augmented Reality: Motivation, Methodology and Perspectives},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00076}}

@inproceedings{9288442,
	Abstract = {Physical common sense is the intuitive knowledge that can be obtained from the physical world. But the common sense will be broken in symmetrical reality because of the integration of the physical world and the virtual world. In this paper, we will introduce the specific physics in symmetrical reality from two perspectives: existence and interaction. We emphasize the bi-directional mechanical control within the symmetrical reality framework and why free wills of machines can break the common sense. Afterward, we give the experiments of discovering the new physical common sense of symmetrical reality systems. Experiment I is about learning physical common sense from symmetrical reality, which is used to show what can be learned and how to learn in a symmetrical reality environment. Experiment II is about changing the physical common sense of symmetrical reality, which is used to show why physical common sense deserves much attention. Finally, we draw an initial conclusion about the physical common sense in symmetrical reality and give some suggestions for understanding symmetrical reality-based physical common sense.},
	Author = {Z. {Zhang}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00077},
	Keywords = {human computer interaction;learning (artificial intelligence);physics computing;virtual reality;physical world;symmetrical reality;physical common sense;existence perspective;interaction perspective;bidirectional mechanical control;machine learning;physics;Computational modeling;Bidirectional control;Augmented reality;Physics;Human-centered computing;Human computer interaction (HCI);HCI theory;concepts and models;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality},
	Month = {Nov},
	Pages = {275-276},
	Title = {Understanding Physical Common Sense in Symmetrical Reality},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00077}}

@inproceedings{9288385,
	Abstract = {Electrical repair tasks across domains use a common set of skills that combine problem solving, fine motor and spatial skills. Augmented Reality (AR) helps develop these skills by overlaying virtual objects on the real-world. So we designed a hand held AR-based wiring tutor which incorporates Constraint-Based Modelling (CBM) paradigms to detect learner errors in an electrical wiring task. We compared the performance and usability of our prototype with a state of the art hand held AR-based training system with a consistent user interface (UI) design, which lacks CBM approaches. Although, the CBM condition had significantly lower usability scores than the state of the art, participants using the CBM approach reported higher practical scores. We discuss reasons for the usability differences, including potential for positive perceptions of the system to be distorted by critical feedback needed to regulate learning in the electrical wiring domain. Next steps would include using the same system to evaluate the theoretical and practical learning outcomes.},
	Author = {B. M. {Herbert} and W. {Hoff} and M. {Billinghurst}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00078},
	Keywords = {augmented reality;computer based training;electrical engineering computing;electrical engineering education;maintenance engineering;user interfaces;wiring;hand held AR-based training system;electrical wiring;CBM;usability scores;user interface design;constraint-based modelling;hand held AR-based wiring tutor;virtual objects;spatial skills;electrical repair;augmented reality;Wiring;Training;Prototypes;User interfaces;Usability;Task analysis;Augmented reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies},
	Month = {Nov},
	Pages = {277-282},
	Title = {Usability Considerations of Hand Held Augmented Reality Wiring Tutors},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00078}}

@inproceedings{9288392,
	Abstract = {Virtual reality is a candidate to become the preferred interface for architectural design review, but the effectiveness and usability of such systems is still an issue. We put together a multidisciplinary team to implement a test methodology and system to compare VR with 2D interaction, with a coherent test platform using Rhinoceros as industry-standard CAD software. A direct and valid comparison of the two setups is made possible by using the same software for both conditions. We designed and modeled three similar CAD models of a 2 two-story villa (1 for the training and 2 for the test) and we implanted 13 artificial errors, simulating common CAD issues. Users were asked to find the errors in a 10 minutes fixed-time session for each setup respectively. We completed our test with 10 students from the design and architecture faculty, with proven experience of the 2D version of the CAD. We did not find any significant differences between the two modalities in cognitive workload, but the user preference was clearly towards VR. The presented work may provide interesting insights for future human-centered studies and to improve future VR architectural applications.},
	Author = {M. {Fiorentino} and E. M. {Klose} and M. {Lucia V. Alemanno} and I. {Giordano} and A. D. {Bellis} and I. {Cavaliere} and D. {Costantino} and G. {Fallacara} and O. {Straeter} and G. {Sorrento}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00079},
	Keywords = {architecture;CAD;civil engineering computing;cognition;human computer interaction;virtual reality;two-story villa;virtual reality;architectural design review;industry-standard CAD software;CAD models;VR architectural applications;architecture;Training;Solid modeling;Two dimensional displays;Software;Usability;Augmented reality;Virtual Reality;Architecture;design review;evaluation;user preference;workload},
	Month = {Nov},
	Pages = {283-288},
	Title = {User Study on Virtual Reality for Design Reviews in Architecture},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00079}}

@inproceedings{9288424,
	Abstract = {"Walkable" Augmented Reality (AR) experiences span floors of a building or involve exploring city neighbourhoods. In these cases setting greatly impacts object placement, interactive events, and narrative flow: in a zombie game for example, a standoff might best occur in an open foyer while a chase might be most effective in a narrow hallway. Spatial attributes are important when experiences are designed for a specific setting but also when settings are not known at design time. In this paper we explore how generic spatial attributes can facilitate design decisions in both cases. We conduct game design through the lens of space syntax, illustrating how attributes like openness, connectivity, and visual complexity can assist placement of walkable AR content in a site-specific narrative-driven scavenger hunt called ScavengAR and a "site-agnostic" game called Adventure AR. We contribute a Unity3D plugin that resolves design constraints expressed in terms of space syntax attributes to place AR content for a single setting or for multiple settings dynamically.},
	Author = {D. {Reilly} and S. {Mahajan} and A. {Singh} and J. {Moore} and I. {Fresia} and M. {Peachey} and J. {Malloch}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00080},
	Keywords = {augmented reality;computer games;data visualisation;software engineering;user experience;space syntax;augmented reality;object placement;interactive events;game design;walkable AR experiences;city neighbourhoods;visual complexity;narrative driven scavenger hunt;ScavengAR;Adventure AR site-agnostic game;Unity3D plugin;walkable AR design;Visualization;Urban areas;Buildings;Games;Syntactics;Tools;Augmented reality;Human-centered computing;Ubiquitous and mobile computing;Ubiquitous and mobile computing systems and tools},
	Month = {Nov},
	Pages = {289-294},
	Title = {Using Space Syntax to Enable Walkable AR Experiences},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00080}}

@inproceedings{9288382,
	Abstract = {Many technologies have been used recently to enhance the tourism experience, and Augmented Reality (AR) has proven to be one of the more promising. This research study aims to use AR to enhance the tourism experience at heritage sites, and in particular to show that AR can provide a richer understanding of how historical artifacts were used. This research adopts an exploratory approach comprising: an exploratory survey; an exploratory interview study; interview study approach in Saudi Arabia; the design of a User Interface (UI); and an evaluation of user experience. The expected outcomes of this research are: conclusions drawn from the interview data; and secondly, a User Interface application for the General Entertainment Authority of audi Arabia.},
	Author = {R. A. {Alakhtar}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00083},
	Keywords = {augmented reality;mobile computing;museums;user experience;user interfaces;augmented reality;explore museum;tourism experience;heritage sites;historical artifacts;exploratory approach;exploratory survey;exploratory interview study;interview study approach;user experience;user interface application;General Entertainment Authority;Saudi Arabia;Design methodology;Entertainment industry;User interfaces;User experience;Interviews;Augmented reality;Human-centered computing;Visualization;Visualization techniques;Treemaps;Human-centered computing;Visualization;Visualization design and evaluation methods},
	Month = {Nov},
	Pages = {295-299},
	Title = {Using Augmented Reality to Explore Museum Artifacts},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00083}}

@inproceedings{9288434,
	Abstract = {Digital Heritage and Digital Humanities focus on distinct typologies of heritage: tangible and intangible Cultural Heritage (CH) objects and their preservation, education, and research versus the application of digital technologies to support research in the humanities. Both allow scholars to go beyond textual sources to integrate digital tools into the humanistic study. This project aims at supporting a new way of experiencing CH in the Serralves Museum and Coa Archeologic Park through more involving and culturally-qualified user experience. The main goal is to understand the potential of eXtended Reality within CH while also proposing the idea of developing a digital experience platform: an authoring tool based on an engine with core experiences functions that can be applied for developing multiple experiences for CH. This platform will contribute to new approaches, technologies, and tools for creating, processing, and delivering immersive and interactive content for engaging and meaningful experiences in these specific CH environments.},
	Author = {M. {Silva} and L. {Teixeira}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00084},
	Keywords = {history;museums;user experience;virtual reality;digital experience platform;authoring tool;interactive experiences;Serralves Museum;Coa Archeologic Park;digital heritage;digital humanities;tangible cultural heritage;digital technologies;textual sources;digital tools;humanistic study;culturally-qualified user experience;extended reality platform;immersive experiences;intangible cultural heritage;augmented reality;Extended reality;Education;Tools;User experience;Cultural differences;Augmented reality;Engines;Cultural Heritage;Serralves Museum;Coa Archeological Park;Immersive and Interactive Experience;eXtended Reality;Platform},
	Month = {Nov},
	Pages = {300-302},
	Title = {Developing an eXtended Reality platform for Immersive and Interactive Experiences for Cultural Heritage: Serralves Museum and Coa Archeologic Park},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00084}}

@inproceedings{9288388,
	Abstract = {Intelligent adaptive agents (IAA) are rapidly evolving due to the sharp growth in computational power and recent advances in artificial intelligence research. From chatbots, over personal virtual assistants and decision-aiding systems, to self-driving systems, they can greatly facilitate common tasks and IAA are increasingly finding their way into many aspects of daily life. However, and often against better knowledge, users remain skeptical towards IAA, in particular when critical interests are involved. Virtual and augmented reality (VR/AR), with their experiential and immersive character, particularly allow for new ways of interaction and establishing trust between users and IAA. Taking a multidisciplinary approach, the research in this PhD project revolves around the fundamental factors and techniques to establish trust in order to facilitate and systemize the design of IAA in future VR/AR settings.},
	Author = {N. {Sun} and J. {Botev}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00085},
	Keywords = {artificial intelligence;augmented reality;chatbots;medical computing;multi-agent systems;trusted computing;augmented reality;IAA;intelligent adaptive agents;artificial intelligence;personal virtual assistants;decision-aiding systems;chatbots;self-driving systems;virtual reality;Economics;Sociology;Psychology;Object recognition;Task analysis;Augmented reality;Guidelines;Human-centered computing---Human computer interaction (HCI)---HCI design and evaluation methods;Human-centered computing---Interaction design;Computing methodologies---Computer graphics---Graphics systems and interfaces---Virtual reality;Computing methodologies---Computer graphics---Graphics systems and interfaces---Mixed/augmented reality},
	Month = {Nov},
	Pages = {303-305},
	Title = {Intelligent Adaptive Agents and Trust in Virtual and Augmented Reality},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00085}}

@inproceedings{9288433,
	Abstract = {Globally, it is estimated that up to 1 billion children aged 2-17 years, have experienced physical, sexual, or emotional violence in the past year [1], and 30% of the abused child is likely to develop Post-traumatic stress disorder (PTSD) [2]; 354 million adult war survivors are suffering from PTSD [3]; At where the natural disaster occurred, 70.7% of the survivor will suffer from acute PTSD [4]. PTSD has not only high prevalence but also high lethality, which is accompanied by multiple physical and mental comorbidities as well as strong suicidal tendencies [5]-[7]. This doctoral research aims to contribute to the development of PTSD treatment by investigating the possibility of adopting Augmented Reality (AR) narrative in treating PTSD. This four-year research project consists of three steps. In the first stage of research, we will conduct a comparative study between AR and VR narratives with healthy participants to verify whether AR narratives work better in eliciting the emotional engagement of the participants than VR narratives. In the second stage, we will create a system that integrates AR narratives with prolonged exposure (PE) treatment and experiment it with PTSD patients to verify its treatment efficacy. In the final stage, a semi-automatic and patient-authored AR system is expected to be achieved, through which the patients can design their own exposure environment via voice input. This project will provide valuable experimental samples and scientific analysis for the research of psychotherapy, narrative studies, and AR application.},
	Author = {L. {Chang} and A. {Cassinelli} and C. {Sandor}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00086},
	Keywords = {augmented reality;human computer interaction;injuries;medical computing;medical disorders;patient treatment;psychology;serious games (computing);VR narratives;PTSD patients;post-traumatic stress disorder treatment;physical violence;sexual violence;emotional violence;mental comorbidities;suicidal tendencies;PTSD treatment;adult war survivors;AR narratives;augmented reality narrative;time 2.0 year to 17.0 year;Three-dimensional displays;Tracking;Virtual environments;Medical services;Stress;Augmented reality;Biomedical imaging;Augmented Reality;Narrative Exposure;Post-Traumatic Stress Disorder;Prolong Exposure;Patient-authored AR narrative},
	Month = {Nov},
	Pages = {306-309},
	Title = {Augmented Reality Narratives for Post-Traumatic Stress Disorder Treatment},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00086}}

@inproceedings{9288416,
	Abstract = {In this paper we will present our application, which consists in a simulator for assessing stroke cases in Virtual Reality. Our project aims to increase the efficiency of the approach to training using serious games and virtual environments instead of traditional teaching methods. This paper describes the results obtained from a survey performed on our serious game application comparing two different simulation examples of a stroke case. The perception of the animation and the environment by the participant are discussed in conjunction with further development of the tool.},
	Author = {M. S. {Mancosu}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00087},
	Keywords = {biomedical education;computer animation;computer based training;computer simulation;educational computing;medical computing;serious games (computing);teaching;virtual reality;simulated virtual world;virtual reality;virtual environments;serious game application;stroke assessment;animation;teaching material;training material;medical students;e-learning applications;Training;Solid modeling;Virtual environments;Games;Medical services;Stroke (medical condition);Context modeling;Virtual Reality;Serious Games;Training;Healthcare;Stroke;Evaluation},
	Month = {Nov},
	Pages = {310-311},
	Title = {Evaluation of Stroke Assessment in Simulated Virtual World},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00087}}

@inproceedings{9287823,
	Abstract = {Maritime navigation, and specifically sailing, is an activity influenced by many external factors such as weather and sea conditions, the presence of near obstacles, the state of the vessel; these factors determine the complexity of navigation, making it an activity dedicated to expert enthusiasts and professionist. We believe that the use of Augmented Reality in the field of maritime navigation can greatly help experienced navigators and make this activity more accessible even to the less experienced, while still ensuring its safety. The concept of Easy Sailing thus evolves/innovates, thanks to the fundamental instrument of AR used as an amplifier of the human senses, designed to inform the navigator, compensating for his lack of experience.},
	Author = {F. {Laera}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00088},
	Keywords = {augmented reality;marine engineering;marine navigation;marine safety;augmented reality;maritime navigation;weather;sea conditions;navigator;safety;easy sailing;Visualization;Three-dimensional displays;Navigation;Layout;Data visualization;Wind forecasting;Augmented reality;Augmented Reality;Nautical;Sailing;Easy Sailing;Human Computer Interaction},
	Month = {Nov},
	Pages = {312-313},
	Title = {Augmented Reality for Easy Sailing},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00088}}

@inproceedings{9288452,
	Abstract = {The use of ICT mediums is prevalent in schools nowadays. Augmented Reality (AR) is an emerging technology that can provide fun and learner-centric ways of teaching in classrooms. The objective thus lie in the appropriate translation of the textbook content to support learning through AR using tablets or mobiles in the classroom. In the process, it's also required to define the smooth transition from instructor-mediated to learner-centric design for providing AR as a teaching-aid. The research thus focuses on identifying the design strategies and considerations in the form of a framework that are required to design such AR learning experiences for middle school students in a classroom scenario.},
	Author = {P. {Sarkar}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00089},
	Keywords = {augmented reality;computer aided instruction;educational institutions;mobile learning;teaching;learner centric design;middle school students;teaching aid;mobiles;tablets;ICT mediums;augmented reality learning experience;Education;Medical services;IEEE Fellows;Augmented reality;Information systems;Augmented reality;design strategies;design guidelines;classroom},
	Month = {Nov},
	Pages = {314-316},
	Title = {Exploring Design Strategies for Augmented Reality Learning Experience in Classrooms},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00089}}

@inproceedings{9288407,
	Abstract = {Vocational Education Training (VET) is becoming a more integral part of the curriculum to account for the requirement of skilled workforce in the global scenario. Augmented Reality (AR) can serve as a valuable medium to integrate this aspect into mainstream educational systems. In this work, we explore the feasibility of AR for vocational skill training of students at the K12 level based on the state of the art literature analysis. Furthermore, we propose opportunities concerned with the aspects of effective integration and usability of these training modules into the traditional pedagogical systems.},
	Author = {M. {Belani} and A. {Parnami}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00090},
	Keywords = {augmented reality;computer based training;computer science education;educational courses;educational institutions;vocational training;augmented reality;training modules;K12 Classrooms;vocational education training;skilled workforce;literature analysis;Training;Design methodology;Sociology;Usability;Statistics;Augmented reality;Human-centered computing;Visualization;Visualization techniques;Treemaps;Human-centered computing;Visualization;Visualization design and evaluation methods},
	Month = {Nov},
	Pages = {317-320},
	Title = {Augmented Reality for Vocational Education Training in K12 Classrooms},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00090}}

@inproceedings{9288455,
	Abstract = {The educational system suffers from early school dropout (i.e. graduation of the 8th grade at most), which is critical in Romania by its magnitude (16.4% in 2018). Unequal access to resources in education, the gap between rural and urban areas and the integration issue of the Roma population are challenges that result in deeper inequalities in society as a whole. Although many reforms have been applied and the budget for education kept increasing lately, Romania did not reach (or even get close to) any of the EU educational targets set for 2020 (i.e. 10% for early school dropout). The current situation of the education demands that more modern solutions should be discussed. In this paper, we propose an innovative technical system which facilitates remote learning experiences, especially for pupils with learning difficulties. We aim to explore the use of Augmented Reality (AR) to promote virtual co-location in education. That means, remote teachers can reach rather isolated pupils more easily and can smoothly engage in collaborative learning sessions. It is our belief that such an approach has the potential to reduce the risk of early school dropout.},
	Author = {M. A. {Cidota} and D. {Datcu}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00091},
	Keywords = {augmented reality;computer aided instruction;distance learning;educational administrative data processing;educational institutions;groupware;remote assistance system;augmented reality;early school dropout prevention;educational system;Romania;rural areas;urban areas;remote learning experiences;pupils with learning difficulties;virtual co-location;collaborative learning;Computers;Education;Urban areas;Tools;Writing;Pupils;Augmented reality;School dropout prevention;pupil-teacher remote collaboration;augmented reality;situational awareness},
	Month = {Nov},
	Pages = {321-325},
	Title = {Remote Assistance System in Augmented Reality for Early School Dropout Prevention},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00091}}

@inproceedings{9288428,
	Abstract = {The "ARsino{\"e}" project combines gamification, Augmented Reality (AR) and machine learning to discover Egyptian hieroglyphs in a single application for the smartphone or tablet. It targets K12 students in two ways. On the one hand, it offers new approaches to the study of hieroglyphs and thus Egyptian culture and history, on the other hand, the newly gained concepts can be transferred to the learning of languages with non-Latin writing systems.},
	Author = {D. A. {Plecher} and C. {Eichhorn} and K. M. {Seyam} and G. {Klinker}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00092},
	Keywords = {augmented reality;computer aided instruction;history;learning (artificial intelligence);smart phones;language learning;history;smartphone;machine learning;gamification;ARsino{\"e} project;Egyptian hieroglyphs;Neural networks;Machine learning;Writing;Real-time systems;Mobile handsets;Mobile applications;Augmented reality;AR;Deep Learning;Serious Application;Hieroglyphs;K12;Education;LeARning;Languages},
	Month = {Nov},
	Pages = {326-332},
	Title = {ARsino{\"e} - Learning Egyptian Hieroglyphs with Augmented Reality and Machine Learning},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00092}}

@inproceedings{9288421,
	Abstract = {It is widely acknowledged that geospatial information has immense applicability across a vast spectrum of human endeavors. Examples include oil and gas exploration, energy management, smart city engineering, weather forecasting, tracking, aviation, satellite ground systems, environmental planning, disaster management, public administration, civil planning and engineering, and science. All such activities entail gathering a significant amount of data and other critical information stored, accessed, managed, manipulated, analyzed, and visualized. This variety of applications requires novel methodologies and technologies capable of delivering both interactive visualization and intelligent complexity reduction.The main focus of this workshop report is the detailed dissection of these technologies, their relationship to one another, and their unique abilities to realize cross-reality capabilities and design principles in a multimodal immersive, and intelligent geographical environment. The goal is to enumerate (and prioritize) critical research and standards opportunities for merging geospatial technologies with smart manufacturing systems.},
	Author = {R. {de Amicis} and W. Z. {Bernstein} and J. {Scholz} and R. {Radkowski} and B. {Sim{\~o}es} and J. {Lieberman} and E. {Prather}},
	Booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
	Doi = {10.1109/ISMAR-Adjunct51615.2020.00093},
	Keywords = {data analysis;data visualisation;geographic information systems;intelligent manufacturing systems;virtual manufacturing;virtual reality;disaster management;public administration;civil planning;critical information;interactive visualization;intelligent complexity reduction;cross-reality capabilities;intelligent geographical environment;geospatial technologies;smart manufacturing systems;geospatial information;gas exploration;energy management;smart city engineering;weather forecasting;satellite ground systems;environmental planning;multimodal immersive environment;information storage;information access;information management;information manipulation;information analysis;information visualization;Merging;Data visualization;Geospatial analysis;Planning;X reality;Standards;Smart manufacturing;Digitization;Digital Twin;GIS;XR},
	Month = {Nov},
	Pages = {333-337},
	Title = {Merging Geospatial Technologies with Cross Reality in the context of smart manufacturing systems},
	Year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR-Adjunct51615.2020.00093}}
