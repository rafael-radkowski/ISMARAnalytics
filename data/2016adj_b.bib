@INPROCEEDINGS{7836522,
author={S. {Baldassi} and G. T. {Cheng} and J. {Chan} and M. {Tian} and T. {Christie} and M. T. {Short}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Exploring Immersive AR Instructions for Procedural Tasks: The Role of Depth, Motion, and Volumetric Representations},
year={2016},
volume={},
number={},
pages={300-305},
abstract={Wearable Augmented Reality (W-AR) is based on getting a computer as intimate as possible with the wearers' bodies and senses. We need to understand the cognitive and perceptual mechanisms leveraged by this technology and use them for designing AR applications. In this study we explored the potential benefit of W-AR to guide a procedural task of assembling a LEGO™ compared to traditional paper instructions. We measured the time used to complete each step and the subjective perception of helpfulness and effectiveness of the instructions along with the perceived time spent doing the task. The results show that adding motion cues to an AR stereo visualization of the instructions (Dynamic 3D) improved performance compared to both the paper instructions and an AR version with stereo only but no motion (Static 3D). Interestingly, performance for the Static 3D condition was the slowest of the three. Subjective reports did not show any difference across different instruction types, suggesting that advantage of Dynamic 3D instructions are not accessible by covert awareness of the participants. The results provide support to the idea that principles of neurosciences may have direct implications for the product development in Wearable Augmented Reality.},
keywords={augmented reality;data visualisation;image motion analysis;image representation;product development;stereo image processing;wearable computers;immersive AR instructions;procedural tasks;depth representation;motion representation;volumetric representation;wearable augmented reality;W-AR;LEGO;AR stereo visualization;static 3D condition;dynamic 3D instructions;product development;Three-dimensional displays;Two dimensional displays;Augmented reality;Time measurement;Solid modeling;Training;Companies;Mixed reality;augmented reality;depth perception;structure from motion;task guidance;procedural tasks},
doi={10.1109/ISMAR-Adjunct.2016.0101},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836523,
author={H. {Okumura} and K. {Shinohara}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Human Attention and fatigue for AR Head-Up Displays},
year={2016},
volume={},
number={},
pages={306-309},
abstract={We proposed and developed a novel monocular windshield augmented reality projector: WARP for AR and head-up display (HUD) applications. They use monocular vision that eliminates the depth cues caused by binocular parallax information. Our developed WARP system achieved not only a high hyper-reality performance with free depth perception and high visibility but also low eye-fatigue and wide field of attention.},
keywords={augmented reality;computer vision;head-up displays;low eye-fatigue;free depth perception;hyper-reality performance;WARP system;binocular parallax information;monocular vision;HUD applications;head-up display applications;monocular windshield augmented reality projector;AR head-up displays;human attention;Fatigue;Augmented reality;Mirrors;Automotive components;Prototypes;Visualization;Liquid crystal displays;HUD;AR;Fatigue;Attention;Display;Monocular},
doi={10.1109/ISMAR-Adjunct.2016.0102},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836524,
author={Z. {Yang} and D. {Weng} and Z. {Zhang} and Y. {Li} and Y. {Liu}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Perceptual Issues of a Passive Haptics Feedback Based MR System},
year={2016},
volume={},
number={},
pages={310-317},
abstract={This paper introduces a unique type of mixed reality system based on passive-haptic feedback, which enables its user to manipulate real objects with bare hands while seeing its counterpart in the HMD with more information attached. The system provides users in MR systems with a more realistic sense of touch and costs less money comparing to the active force feedback devices. In this paper, accuracy of tactile and visual perception in user interaction with real objects in close range MR environment is studied. A calibration method of the user's head and hands is proposed and the calibration accuracy of head and hands tracking has been evaluated respectively with different experiments. An application of chemical experiment is also implemented to prove the reliability of the proposed system.},
keywords={force feedback;haptic interfaces;virtual reality;perceptual issue;MR system;mixed reality system;passive haptic feedback;HMD;head-mounted displays;active force feedback devices;tactile perception;visual perception;user interaction;calibration method;Cameras;Haptic interfaces;Tracking;Virtual reality;Head;Resists;Visualization;Mixed reality;Augmented Reality;Human-computer interaction;Passive haptics;User Interfaces},
doi={10.1109/ISMAR-Adjunct.2016.0103},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836525,
author={Y. {Huang} and J. {Zhang} and T. {Liu} and Y. {Sung} and K. {Chang} and M. {Yang}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={AR-Based Learning and AR Guides as Strategy in Two-Phase Learning Enhancement: A Case Study},
year={2016},
volume={},
number={},
pages={318-321},
abstract={Traditional paper-based teaching activities are restricted by physical space. Therefore, one cannot integrate and collaborate with the design of the instructional materials effectively. As a result, it is unlikely that instructors are able to successfully moderate learners' focus on the content, as well as the order of their focus during their reading and learning processes, which forms a blind spot in teaching and learning. This study applied augmented reality (AR) and image positioning technologies to paper textbooks, and proposed using AR technology to develop two-phase learning processes to effectively control learners' progress in learning textbooks and their learning sequences, so that learning activities match the design of instructional materials. Preliminary empirical research demonstrated a good experience by students using the system and a positive learning attitude from the users.},
keywords={augmented reality;computer aided instruction;teaching;AR-based learning;AR guides;two-phase learning enhancement;paper-based teaching activities;instructional materials;reading processes;image positioning;learner progress;learning sequences;instructional materials;Education;Computers;Psychology;Employee welfare;Organizations;Augmented reality;Interference;Augmented Reality;Computer-assisted instruction;K-12 education},
doi={10.1109/ISMAR-Adjunct.2016.0104},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836526,
author={P. P. {Longoria} and J. A. L. {Cardoso}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Camouflage as an Adaptive Control System: Applications for Multimedia and Interactive Composition},
year={2016},
volume={},
number={},
pages={322-326},
abstract={We first make some considerations on the biological phenomenon of camouflage as a biological control adaptive process and discuss possible approaches for its mathematical modeling. We then focus on applications to interactive media using a simplified approach in which an L-system is imitated by means of a Markov chain. This model is implemented with computational vision and multichannel spatialization techniques as a tool for computer assisted composition.},
keywords={adaptive control;biology;computational vision;multichannel spatialization techniques;computer assisted composition;Markov chain;L-system;interactive media;mathematical modeling;biological control adaptive process;camouflage biological phenomenon;adaptive control system;Pattern recognition;Image color analysis;Context;Mathematical model;Sensor systems;Shape;Camouflage;Multimedia Composition;L-Systems;Pattern Recognition;Aural Recognition},
doi={10.1109/ISMAR-Adjunct.2016.0105},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836527,
author={ {The Human BIOS Project}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Organic Data Feeding: Synthesizing Digital World Phenomena Using Natural Data as a Source},
year={2016},
volume={},
number={},
pages={327-331},
abstract={After decades of using technology as a mean to synthesize new digital worlds, we've finally achieved such computational power that we are now able to provide major detail to most elements in any creation. Video game consoles, for instance, are capable of rendering multiple data layers that are used by developers in order to provide a credible universe for the player to get immersed into. On the other hand, the distribution of cheap electronic platforms allow us to manage any number of sensors and use the data over any number of devices. Organic data feeding is a paradigm based on taking data from nature and using it inside a digital solution so the results are less predictable, thus providing a more organic experience to the end user.},
keywords={data handling;organic data feeding;digital world phenomena;natural data;video game consoles;Protocols;Temperature sensors;Prediction algorithms;Computers;Electronic circuits;Hardware;Organic data;data processing;natural data},
doi={10.1109/ISMAR-Adjunct.2016.0106},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836528,
author={C. {Baillard} and V. {Alleaume} and M. {Fradet} and P. {Jouet} and A. {Laurent} and T. {Luo} and P. {Robert} and F. {Servant}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Mixed Reality Extended TV},
year={2016},
volume={},
number={},
pages={332-333},
abstract={The Extended TV application allows to enhance an audiovisual content displayed on a TV, using Mixed Reality technology. During preprocessing, the close environment of the TV is scanned using a consumer depth camera. The captured RGB-D data are analyzed, providing models for both the 3D geometry and the lighting of the real scene. During runtime, the TV is watched through a tablet, and virtual objects can apparently come out of the screen and start populating the user's environment. Virtual objects can be occluded by real objects, and virtual shadows are consistent with the real ones.},
keywords={audio-visual systems;augmented reality;computational geometry;data analysis;image capture;screens (display);television applications;mixed reality extended TV application;audiovisual content;consumer depth camera;captured RGB-D data analysis;3D geometry;lighting;tablet;virtual objects;virtual shadows;user environment;TV;Three-dimensional displays;Virtual reality;Light sources;Solid modeling;Rendering (computer graphics);Cameras;Mixed Reality;Augmented Reality;entertainment;3D scene analysis;spatial interactions;occlusions;light source analysis;immersive blending},
doi={10.1109/ISMAR-Adjunct.2016.0107},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836529,
author={D. C. {Rompapas} and A. {Rovira} and S. {Ikeda} and A. {Plopski} and T. {Taketomi} and C. {Sandor} and H. {Kato}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={EyeAR: Refocusable Augmented Reality Content through Eye Measurements},
year={2016},
volume={},
number={},
pages={334-335},
abstract={The human visual system always focuses at a distinct depth. Therefore, objects that lie at different depths appear blurred, a phenomenon known as Depth of Field (DoF); as the user's focus depth changes, different objects come in and out of focus. Augmented Reality (AR) is a technology that superimposes computer graphics (CG) images onto a user's view of the real world. A commonly used AR display device is an Optical See-Through Head-Mounted Display (OST-HMD), enabling users to observe the real-world directly, with CG added to it. A common problem in such systems is the mismatch between the DoF properties of the user's eyes and the virtual camera used to generate CG.In this demonstration, we present an improved version of the system presented in [11] as two implementations: The first as a high quality tabletop system, the second as a component which has been integrated into the Microsoft Hololens [18].},
keywords={augmented reality;computer graphics;helmet mounted displays;EyeAR;refocusable augmented reality content;eye measurements;human visual system;depth of field;computer graphics images;AR display device;optical see-through head-mounted display;OST-HMD;virtual camera;high quality tabletop system;Microsoft Hololens;Encyclopedias;Electronic publishing;Internet;Augmented reality;Cameras;Three-dimensional displays;Raytracing;Physically Based AR;Augmented Reality;Depth of Field;Optical Defocus},
doi={10.1109/ISMAR-Adjunct.2016.0108},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836530,
author={J. E. {Yu} and G. J. {Kim}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Blurry (Sticky) Finger: Proprioceptive Pointing and Selection of Distant Objects for Optical See-Through Based Augmented Reality},
year={2016},
volume={},
number={},
pages={336-337},
abstract={We demonstrate “Blurry (Sticky) Finger” in which one uses the unfocused blurred finger, sense of proprioception, to aim, point and directly select a distant object in the real world with both eyes open. We showcase two demo applications. The first illustrates the accuracy and usability of the proposed method with the target objects lying at a fixed depth on a monitor. The second is an AR based object inquiry system, a more practical application. The user aims and encircles a real 3D object whose image is captured with the eye-to-camera offset compensated. The image is searched through the data base with the result augmented on an OST display.},
keywords={augmented reality;mechanoception;OST display;blurry (sticky) finger;optical see-through based augmented reality;distant objects;proprioceptive selection;proprioceptive pointing;Cameras;Thumb;Glass;Augmented reality;Monitoring;Three-dimensional displays;AR;augmented reality;interface;pointing gesture;proprioception;hand-eye coordination},
doi={10.1109/ISMAR-Adjunct.2016.0109},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836531,
author={P. {Fleck} and C. {Arth} and D. {Schmalstieg}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Scalable Mobile Image Recognition for Real-Time Video Annotation},
year={2016},
volume={},
number={},
pages={338-339},
abstract={Traditional AR frameworks for gaming and advertising focus on tracking 2D static targets. This limits the plausible use of this solutions to certain application cases like brochures or posters, but deprives their use for dynamically changing 2D targets, such as video walls or electronic billboards used in advertising.In this demo, we show how to use a rapid, fully mobile image recognition system to introduce AR in videos playing on TV sets or other dynamic screens, without the need to alter or modify the content for trackability. Our approach uses a scalable and fully mobile concept, which requires a database with a very small memory footprint on mobiles for a video or even a collection of videos.The feasibility of the approach is demonstrated on over 16 hours of video from a popular TV series, indexing into the video and giving accurate time codes and full 6DOF tracking for AR augmentations.},
keywords={advertising;augmented reality;computer games;image recognition;mobile computing;real-time systems;target tracking;video signal processing;scalable mobile image recognition;real-time video annotation;traditional AR frameworks;gaming;advertising;2D static target tracking;Databases;Mobile communication;Streaming media;Advertising;Feature extraction;Image recognition;TV;Computer Vision;Augmented Reality;Mobile AR;Video Annotation;Image Recognition},
doi={10.1109/ISMAR-Adjunct.2016.0110},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836532,
author={H. {Liu} and G. {Zhang} and H. {Bao}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Robust Keyframe-Based Monocular SLAM for Augmented Reality},
year={2016},
volume={},
number={},
pages={340-341},
abstract={In this demo, we present RKSLAM, a robust keyframe-based monocular SLAM system that can reliably handle fast motion with strong rotation and ensure good AR experiences. We contribute two key technical contributions: a novel multi-homography based feature tracking method which is very robust and efficient, and a sliding-window based camera pose optimization scheme which imposes the motion prior constraints between consecutive frames through simulated or real IMU data. Based on RKSLAM, we develop an AR App on a mobile device, which allows the user to freely insert 3D furniture models into the scene to see the AR effect without imagination.},
keywords={augmented reality;feature extraction;mobile computing;object tracking;pose estimation;SLAM (robots);solid modelling;robust keyframe-based monocular SLAM;augmented reality;RKSLAM;AR experiences;multihomography based feature tracking method;sliding-window based camera pose optimization scheme;motion prior constraints;IMU data;AR app;mobile device;3D furniture models;Three-dimensional displays;Simultaneous localization and mapping;Cameras;Robustness;Solid modeling;Mobile handsets;Augmented reality;SLAM;augmented reality;tracking;multiple homography representation;mapping},
doi={10.1109/ISMAR-Adjunct.2016.0111},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836533,
author={Y. {Lee} and K. {Masai} and K. {Kunze} and M. {Sugimoto} and M. {Billinghurst}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={A Remote Collaboration System with Empathy Glasses},
year={2016},
volume={},
number={},
pages={342-343},
abstract={In this paper, we describe a demonstration of remote collaboration system using Empathy glasses. Using our system, a local worker can share a view of their environment with a remote helper, as well as their gaze, facial expressions, and physiological signals. The remote user can send back visual cues via a see-through head mounted display to help them perform better on a real world task. The system also provides some indication of the remote users face expression using face tracking technology.},
keywords={gaze tracking;helmet mounted displays;user interfaces;face tracking technology;remote users face expression;real world task;see-through head mounted display;visual cues;physiological signals;facial expressions;gaze;remote helper;empathy glasses;remote collaboration system;Collaboration;Glass;Heart rate;Biomedical monitoring;Cameras;Hardware;Computers;Remote Collaboration;Empathy;Gaze Tracking;Facial Expression},
doi={10.1109/ISMAR-Adjunct.2016.0112},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836534,
author={C. S. C. {Dalim} and T. {Piumsomboon} and A. {Dey} and M. {Billinghurst} and S. {Sunar}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={TeachAR: An Interactive Augmented Reality Tool for Teaching Basic English to Non-native Children},
year={2016},
volume={},
number={},
pages={344-345},
abstract={TeachAR is an Augmented Reality (AR) tool for teaching English colors, shapes, and spatial relationships to young children aged 4 to 6 years old who are non-native speakers of English. TeachAR utilizes the ARToolkit plugin for the Unity game engine for square marker tracking and game development. The Microsoft Kinect's microphone and speech API is used for isolated word speech recognition, a webcam for image capturing and a desktop monitor for viewing the AR scene. Previous language learning AR applications usually use audio output, however TeachAR uses speech as input for language learning. This paper describes the TeachAR demonstration and user experience with the application.},
keywords={application program interfaces;augmented reality;computer aided instruction;computer games;image capture;linguistics;speech recognition;teaching;interactive augmented reality tool;basic English teaching;nonnative children;TeachAR;AR tool;English colors;English shapes;spatial relationships;young children;nonnative English speakers;ARToolkit plugin;Unity game engine;square marker tracking;game development;Microsoft Kinect microphone;speech API;isolated word speech recognition;webcam;image capturing;desktop monitor;AR scene viewing;language learning;Shape;Speech recognition;Color;Speech;Augmented reality;Education;Games;Augmented Reality;Teaching and Learning;English Language;Children;Non-Native Speakers},
doi={10.1109/ISMAR-Adjunct.2016.0113},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836535,
author={H. {Wuest} and T. {Engekle} and F. {Wientapper} and F. {Schmitt} and J. {Keil}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={From CAD to 3D Tracking — Enhancing Scaling Model-Based Tracking for Industrial Appliances},
year={2016},
volume={},
number={},
pages={346-347},
abstract={For Augmented Reality to succeed in industrial appliances, industries demand not only robust and reliable tracking techniques, but also a scalable and performant pipeline, that is easy-to-integrate within the existing data- and content environment and that enables vendors to create tracking solutions on their own. In our demo, we present recent advances of our model tracking pipeline and tracking technology, which on the one hand is easy use and easy to integrate, while on the other hand robust enough during difficult environmental conditions and which delivers high accuracy for the industrial domain. We showcase our results inside an AR-manual scenario.},
keywords={augmented reality;CAD;mechanical engineering computing;object tracking;pipeline processing;augmented reality;3D tracking;CAD;AR-manual scenario;model tracking pipeline;industrial appliances;Solid modeling;Three-dimensional displays;Maintenance engineering;Home appliances;Target tracking;Robustness;Pipelines;Computer Vision;Mobile Augmented Reality;Maintenance Manuals},
doi={10.1109/ISMAR-Adjunct.2016.0114},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836536,
author={F. {Pankratz} and G. {Klinker}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={AR4AR Based on ARVIDA Reference Architecture: Application Demonstration},
year={2016},
volume={},
number={},
pages={348-349},
abstract={A recurring challenge when setting up any Augmented Reality system is integration as well as the correct calibration and registration of the involved devices. To gather, the input data the user has to interact with the system which requires certain knowledge on how to perform the task. It is possible the user performs the current task to the best of his knowledge and still produces suboptimal results. Augmented Reality for Augmented Reality (AR4AR) generates AR guides by using the already existing knowledge about the AR system which stems from the setup of the AR system itself. As these guides involve plenty of 3D information, it is again best presented through means of Augmented Reality, thus the name AR4AR.},
keywords={augmented reality;AR4AR;ARVIDA reference architecture;augmented reality for augmented reality;AR system;3D information;Augmented reality;Target tracking;Calibration;Performance evaluation;Cameras;Three-dimensional displays},
doi={10.1109/ISMAR-Adjunct.2016.0115},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836537,
author={M. {Salazar} and C. {Laorden}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Collaborative content creation with the OpenUIX Framework},
year={2016},
volume={},
number={},
pages={350-350},
abstract={The OpenUIX framework aims to provide end-users -and developers- with a level playing field where they can not only access 3D user interfaces adapted to their context (location, time of day, hardware platform, disabilities, etc.), but also create and share them with others. To achieve this goal, the framework has a built-in toolset that allows the collaborative edition of the different components of a interaction space; from the threedimensional representations of the different entities (either virtual or real in nature), to the lighting conditions or the logic behaviors that govern the user experience. With this demo, the ISMAR 2016 attendees will be able to test several of the aforementioned collaborative edition tools in different interaction spaces across the entire conference venue (and even outside of it, if the circumstances allow it). Initially, the attendees will have the opportunity to test the different tools at the table provided by the organization of the event (and with mobile platforms provided by the presenters, to ensure the proper execution of the demo). In this way, the presenters will be able to explain and guide them through a series of basic use cases, ranging from virtual painting on 2D surfaces or 3D modeling to how to employ the UI description language the framework employs to modify the lighting conditions of the interaction space (thus allowing a more seamless integration of virtual objects into the physical world). Once the attendees go through the basic use cases and prove their understanding of the interaction techniques, they will receive a code to download a development version of an AR browser created with this framework (available for Android and iOS platforms). With this browser installed in their own devices, the attendees will be able to access other interaction spaces across the venue (that the presenters will prepare beforehand).},
keywords={augmented reality;user interfaces;augmented reality;AR browser;UI description language;user experience;interaction space;3D user interfaces;OpenUIX framework;collaborative content creation;Collaboration;Mobile communication;Painting;Browsers;Human computer interaction;Hardware;Three-dimensional displays;3DUI;collaborative edition;interaction spaces},
doi={10.1109/ISMAR-Adjunct.2016.0116},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836538,
author={A. {García} and D. {Camargo} and X. {Gallegos} and R. {Maldonado} and E. {Luna} and D. {Godínez} and A. {Monroy} and J. {Lobato} and E. {Athié}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={The Object of Absence},
year={2016},
volume={},
number={},
pages={351-352},
abstract={Is an interactive piece in which a specific group of people; who work with their hands and space in a very particular way like sculptors, self-builders, poets, architects, real estate sellers, dancers, childrens and carpenters, shape with their hands in the air a related experiencie about the feeling of absence and the experiencie of inhabit. The movement of hands is captured by a sensor and translated into graphics that according to certain parameters create a model that prints in 3D in real time. This is how the evocation of absence materializes.},
keywords={computer graphics;gesture recognition;interactive systems;interactive piece;hand movement;graphics;evocation of absence;Three-dimensional displays;Solid modeling;Context;Real-time systems;Thumb;Printers;Inhabit;absence;model;free softwer;3.0;space;context},
doi={10.1109/ISMAR-Adjunct.2016.0117},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836539,
author={Y. T. {Nam} and J. {Oh}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Participatory Mixed Reality Space: Collective Memories},
year={2016},
volume={},
number={},
pages={353-354},
abstract={This project focuses on creating a participatory mixed reality space, situated in social contexts within urban smart cities. This study explores audiences' social interactions and experiences with tactile engagements from their smart devices and a form of interactive media façade. The association between NodeJS, Websocket.io-to-OSC and TouchDesigner designs integrated with the developed computer projection mapping system supports the participatory mixed reality environment. A framework is also developed from the concepts of the viewer as co-creators, the interactive system as facilitator and motivation as creating a sense of belonging: “being connected” - social presence. The framework is to seek new perspectives from the recent growing trend of participatory culture in public spaces. This project supports the collective interaction to redefine and reshape to whom, with how and what we are sharing together in the cocreation through social interaction and experience. It is vital that people learn how to engage when engaging with each other - together within the participatory mixed reality space.},
keywords={haptic interfaces;social sciences computing;virtual reality;social experience;social interaction;social presence;interactive system;computer projection mapping system;TouchDesigner;Websocket.io-to-OSC;NodeJS;interactive media facade;smart devices;tactile engagements;audience social experiences;audience social interactions;urban smart cities;collective memories;participatory mixed reality space;Media;Art;Virtual reality;Context;Social network services;Multimedia communication;Mobile communication;Mixed reality space;Public space;Participatory art;Participatory design;Social memory;Collective interaction;Cocreation;Co-creativity;Co-design;Social interaction;Social experience},
doi={10.1109/ISMAR-Adjunct.2016.0118},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836540,
author={R. {Perla} and G. {Gupta} and R. {Hebbalaguppe} and E. {Hassan}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={InspectAR: An Augmented Reality Inspection Framework for Industry},
year={2016},
volume={},
number={},
pages={355-356},
abstract={With the advancement in camera technologies and data streaming protocols, AR based applications are proving to be an important aid for inspection, training and supervision tasks in various operations including automotive industry, education etc. We demonstrate an AR based re-configurable inspection framework that can be utilized in cross-domain applications such as maintenance and repair assistance in industrial inspection and automotive/avionics domain inspection, amongst others. A deep learning component detects parts viewed in inspector's Field-of-View (FoV) accurately and the corresponding inspection check-list can be prioritized based on detection results. The back-end of the framework is easily configurable for different applications where instructions can be directly imported and visually integrated with inspection type. Accurate recording of status of inspection is provided through evidence capturing of images, notes and videos. Our current framework supports all the Android based devices and will be demonstrated on Google Glass, Google Cardboard with smartphone, and Tablet with the help of 3D printer inspection use-case.},
keywords={Android (operating system);augmented reality;automatic optical inspection;image capture;learning (artificial intelligence);printers;production engineering computing;smart phones;three-dimensional printing;augmented reality inspection;InspectAR;camera technologies;data streaming protocols;AR based applications;AR based reconfigurable inspection;cross-domain applications;deep learning component;inspector field-of-view;inspection check-list;part detection;inspection type;evidence image capturing;Android based devices;Google Glass;Google Cardboard;smartphone;Tablet;3D printer inspection use-case;Inspection;Google;Servers;Glass;Object detection;Augmented reality;Printers;H.5.1 [Information Interfaces and Presentation]: Artificial;Augmented;and Virtual Realities—; [Human-centered computing]: Ubiquitous and mobile computing—Ambient Intelligence I.4.8 [Computing Methodologies]: Image Processing and Computer Vi},
doi={10.1109/ISMAR-Adjunct.2016.0119},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836541,
author={J. {Oh} and S. {Kim} and Y. T. {Nam} and C. {Shi}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={deBallution — Interactive Artwork by Throwing Pseudo Balls Based on Cultural Heritages},
year={2016},
volume={},
number={},
pages={357-358},
abstract={The interactive artwork “deBallution” is to catch audience members' throwing movements on a virtual screen and drawing various generated kaleidoscope images to predict points from the audience throwing on the screen. Audience members threw the pseudo-balls for large-size screen and caused symbolic digital revolution, devolution.},
keywords={art;history;interactive systems;virtual reality;deBallution;interactive artwork;pseudoballs;cultural heritages;virtual screen;kaleidoscope images;large-size screen;symbolic digital revolution;symbolic digital devolution;Cultural differences;Visualization;Electronic mail;Shape;Image color analysis;Art;Multimedia communication;Public spaces;Participatory art;Interactive artwork;Pseudo balls;Cultural heritage},
doi={10.1109/ISMAR-Adjunct.2016.0120},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836542,
author={},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Author index},
year={2016},
volume={},
number={},
pages={359-363},
abstract={Presents an index of the authors whose articles are published in the conference proceedings record.},
keywords={},
doi={10.1109/ISMAR-Adjunct.2016.0121},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836543,
author={},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Publisher's Information},
year={2016},
volume={},
number={},
pages={364-364},
abstract={Provides a listing of current committee members and society officers.},
keywords={},
doi={10.1109/ISMAR-Adjunct.2016.0122},
ISSN={},
month={Sep.},}