@INPROCEEDINGS{7781703,
author={},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Title page i]},
year={2016},
volume={},
number={},
pages={i-i},
abstract={The following topics are dealt with: augmented reality; SLAM tracking; lighting; rendering; head mounted displays; virtual reality; computer aided instruction.},
keywords={augmented reality;computer aided instruction;helmet mounted displays;rendering (computer graphics);computer aided instruction;virtual reality;head mounted displays;rendering;lighting;SLAM tracking;augmented reality},
doi={10.1109/ISMAR.2016.1},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7781704,
author={},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Title page iii]},
year={2016},
volume={},
number={},
pages={iii-iii},
abstract={Presents the title page of the proceedings record.},
keywords={},
doi={10.1109/ISMAR.2016.2},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7781705,
author={},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Copyright notice]},
year={2016},
volume={},
number={},
pages={iv-iv},
abstract={Presents the copyright information for the conference. May include reprint permission information.},
keywords={},
doi={10.1109/ISMAR.2016.3},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7781706,
author={},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Table of contents},
year={2016},
volume={},
number={},
pages={v-vii},
abstract={Presents the table of contents/splash page of the proceedings record.},
keywords={},
doi={10.1109/ISMAR.2016.10},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7781707,
author={},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Message from the ISMAR 2016 General Chair and Deputy General Chairs},
year={2016},
volume={},
number={},
pages={viii-viii},
abstract={Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.},
keywords={},
doi={10.1109/ISMAR.2016.4},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7781708,
author={},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Message from the ISMAR 2016 Science and Technology Program Chairs and TVCG Guest Editors},
year={2016},
volume={},
number={},
pages={ix-ix},
abstract={Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.},
keywords={},
doi={10.1109/ISMAR.2016.5},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7781709,
author={},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Message from the ISMAR 2016 Science and Technology Program Chairs},
year={2016},
volume={},
number={},
pages={x-xi},
abstract={Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.},
keywords={},
doi={10.1109/ISMAR.2016.6},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7781710,
author={},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Message from the ISMAR 2016 Science and Technology Poster Chairs},
year={2016},
volume={},
number={},
pages={xii-xiii},
abstract={Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.},
keywords={},
doi={10.1109/ISMAR.2016.7},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7781711,
author={},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Message from the Workshop and Tutorial Chairs},
year={2016},
volume={},
number={},
pages={xiv-xiv},
abstract={Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.},
keywords={},
doi={10.1109/ISMAR.2016.8},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7781712,
author={},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Message from the Demonstration Chairs},
year={2016},
volume={},
number={},
pages={xv-xv},
abstract={Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.},
keywords={},
doi={10.1109/ISMAR.2016.9},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7781753,
author={},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Message from the Hackathon Chairs},
year={2016},
volume={},
number={},
pages={xvi-xvi},
abstract={Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.},
keywords={},
doi={10.1109/ISMAR.2016.29},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7781754,
author={},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={ISMAR 2016 Conference Committee Members},
year={2016},
volume={},
number={},
pages={xvii-xvii},
abstract={Provides a listing of current committee members and society officers.},
keywords={},
doi={10.1109/ISMAR.2016.30},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7781755,
author={},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={ISMAR 2016 Science and Technology Program Committee Members},
year={2016},
volume={},
number={},
pages={xviii-xviii},
abstract={Provides a listing of current committee members and society officers.},
keywords={},
doi={10.1109/ISMAR.2016.31},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7781756,
author={},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={ISMAR 2016 Steering Committee Members},
year={2016},
volume={},
number={},
pages={xix-xix},
abstract={Provides a listing of current committee members and society officers.},
keywords={},
doi={10.1109/ISMAR.2016.32},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7781757,
author={},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Reviewers},
year={2016},
volume={},
number={},
pages={xx-xx},
abstract={The conference offers a note of thanks and lists its reviewers.},
keywords={},
doi={10.1109/ISMAR.2016.33},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7781758,
author={},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Keynotes},
year={2016},
volume={},
number={},
pages={xxi-xxii},
abstract={These keynote speeches discuss the following: The history and future of visual SLAM; and Mixing it up, mixing it down.},
keywords={interactive systems;SLAM (robots);virtual reality;visual SLAM;simultaneous localisation and mapping;augmented reality;mixed reality;mediated reality;diminished reality;augmented virtuality},
doi={10.1109/ISMAR.2016.34},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7781759,
author={},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Sponsors and Supporters},
year={2016},
volume={},
number={},
pages={xxiii-xxv},
abstract={The conference organizers greatly appreciate the support of the various corporate sponsors listed.},
keywords={},
doi={10.1109/ISMAR.2016.35},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7781760,
author={H. {Liu} and G. {Zhang} and H. {Bao}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Robust Keyframe-based Monocular SLAM for Augmented Reality},
year={2016},
volume={},
number={},
pages={1-10},
abstract={Keyframe-based SLAM has achieved great success in terms of accuracy, efficiency and scalability. However, due to parallax requirement and delay of map expansion, traditional keyframe-based methods easily encounter the robustness problem in the challenging cases especially for fast motion with strong rotation. For AR applications in practice, these challenging cases are easily encountered, since a home user may not carefully move the camera to avoid potential problems. With the above motivation, in this paper, we present RKSLAM, a robust keyframe-based monocular SLAM system that can reliably handle fast motion and strong rotation, ensuring good AR experiences. First, we propose a novel multihomography based feature tracking method which is robust and efficient for fast motion and strong rotation. Based on it, we propose a real-time local map expansion scheme to triangulate the observed 3D points immediately without delay. A sliding-window based camera pose optimization framework is proposed, which imposes the motion prior constraints between consecutive frames through simulated or real IMU data. Qualitative and quantitative comparisons with the state-of-the-art methods, and an AR application on mobile devices demonstrate the effectiveness of the proposed approach.},
keywords={augmented reality;cameras;feature extraction;mobile handsets;object tracking;pose estimation;SLAM (robots);robust keyframe-based monocular SLAM;augmented reality;parallax requirement;map expansion delay;AR applications;RKSLAM;multihomography based feature tracking method;real-time local map expansion;sliding-window based camera pose optimization;real IMU data;mobile devices;Cameras;Three-dimensional displays;Simultaneous localization and mapping;Robustness;Tracking;Real-time systems;Optimization;I.4.8 [Image Processing and Computer Vision]: Scene Analysis—Tracking; H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial;augmented;and virtual realities},
doi={10.1109/ISMAR.2016.24},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7781761,
author={S. B. {Knorr} and D. {Kurz}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Leveraging the User's Face for Absolute Scale Estimation in Handheld Monocular SLAM},
year={2016},
volume={},
number={},
pages={11-17},
abstract={We present an approach to estimate absolute scale in handheld monocular SLAM by simultaneously tracking the user's face with a user-facing camera while a world-facing camera captures the scene for localization and mapping. Given face tracking at absolute scale, two images of a face taken from two different viewpoints enable estimating the translational distance between the two viewpoints in absolute units, such as millimeters. Under the assumption that the face itself stayed stationary in the scene while taking the two images, the motion of the user-facing camera relative to the face can be transferred to the motion of the rigidly connected world-facing camera relative to the scene. This allows determining also the latter motion in absolute units and enables reconstructing and tracking the scene at absolute scale.As faces of different adult humans differ only moderately in terms of size, it is possible to rely on statistics for guessing the absolute dimensions of a face. For improved accuracy the dimensions of the particular face of the user can be calibrated.Based on sequences of world-facing and user-facing images captured by a mobile phone, we show for different scenes how our approach enables reconstruction and tracking at absolute scale using a proof-of-concept implementation. Quantitative evaluations against ground truth data confirm that our approach provides absolute scale at an accuracy well suited for different applications. Particularly, we show how our method enables various use cases in handheld Augmented Reality applications that superimpose virtual objects at absolute scale or feature interactive distance measurements.},
keywords={augmented reality;cameras;face recognition;mobile computing;absolute scale estimation;handheld monocular SLAM;camera;face tracking;mobile phone;ground truth data;augmented reality;visual simultaneous localization and mapping;Cameras;Face;Simultaneous localization and mapping;Image reconstruction;Handheld computers;Three-dimensional displays;Calibration;SLAM;monocular;handheld;absolute scale;userfacing;face tracking;distance measurements;true size},
doi={10.1109/ISMAR.2016.20},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7781762,
author={B. W. {Babu} and S. {Kim} and Z. {Yan} and L. {Ren}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={σ-DVO: Sensor Noise Model Meets Dense Visual Odometry},
year={2016},
volume={},
number={},
pages={18-26},
abstract={In this paper we propose a novel method called s-DVO for dense visual odometry using a probabilistic sensor noise model. In contrast to sparse visual odometry, where camera poses are estimated based on matched visual features, we apply dense visual odometry which makes full use of all pixel information from an RGB-D camera. Previously, t-distribution was used to model photometric and geometric errors in order to reduce the impacts of outliers in the optimization. However, this approach has the limitation that it only uses the error value to determine outliers without considering the physical process. Therefore, we propose to apply a probabilistic sensor noise model to weigh each pixel by propagating linearized uncertainty. Furthermore, we find that the geometric errors are well represented with the sensor noise model, while the photometric errors are not. Finally we propose a hybrid approach which combines t-distribution for photometric errors and a probabilistic sensor noise model for geometric errors. We extend the dense visual odometry and develop a visual SLAM system that incorporates keyframe generation, loop constraint detection and graph optimization. Experimental results with standard benchmark datasets show that our algorithm outperforms previous methods by about a 25% reduction in the absolute trajectory error.},
keywords={distance measurement;image colour analysis;SLAM (robots);graph optimization;loop constraint detection;keyframe generation;visual SLAM system;photometric errors;geometric errors;linearized uncertainty;RGB-D camera;probabilistic sensor noise model;dense visual odometry;σ-DVO;Cameras;Visualization;Simultaneous localization and mapping;Optimization;Augmented reality;Three-dimensional displays;Robustness;Visual SLAM;Dense Visual Odometry;Camera Pose Tracking;3D Reconstruction;Augmented Reality},
doi={10.1109/ISMAR.2016.11},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7781763,
author={T. {Richter-Trummer} and D. {Kalkofen} and J. {Park} and D. {Schmalstieg}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Instant Mixed Reality Lighting from Casual Scanning},
year={2016},
volume={},
number={},
pages={27-36},
abstract={We present a method for recovering both incident lighting and surface materials from casually scanned geometry. By casual, we mean a rapid and potentially noisy scanning procedure of unmodified and uninstrumented scenes with a commodity RGB-D sensor. In other words, unlike reconstruction procedures which require careful preparations in a laboratory environment, our method works with input that can be obtained by consumer users. To ensure a robust procedure, we segment the reconstructed geometry into surfaces with homogeneous material properties and compute the radiance transfer on these segments. With this input, we solve the inverse rendering problem of factorization into lighting and material properties using an iterative optimization in spherical harmonics form. This allows us to account for self-shadowing and recover specular properties. The resulting data can be used to generate a wide range of mixed reality applications, including the rendering of synthetic objects with matching lighting into a given scene, but also re-rendering the scene (or a part of it) with new lighting. We show the robustness of our approach with real and synthetic examples under a variety of lighting conditions and compare them with ground truth data.},
keywords={image colour analysis;image reconstruction;rendering (computer graphics);virtual reality;mixed reality lighting;casual scanning;incident lighting;surface materials;scanned geometry;noisy scanning procedure;uninstrumented scenes;commodity RGB-D sensor;reconstruction procedures;laboratory environment;consumer users;robust procedure;homogeneous material properties;radiance transfer;inverse rendering problem;factorization;iterative optimization;spherical harmonics form;mixed reality applications;synthetic objects;scene rerendering;Lighting;Cameras;Image color analysis;Face;Geometry;Image reconstruction;Estimation;H.5.1 [Information Interfaces and Presentation]: Artificial;augmented;virtual realities; I.4.8 [Image Processing and Computer Vision]: Photometric registration—3D Reconstruction},
doi={10.1109/ISMAR.2016.18},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7781764,
author={T. {Schwandt} and W. {Broll}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={A Single Camera Image Based Approach for Glossy Reflections in Mixed Reality Applications},
year={2016},
volume={},
number={},
pages={37-43},
abstract={Proper scene inference provides the basis for a seamless integration of virtual objects into the real environment. While widely neglected in many AR/MR environments, previous approaches providing good results were based on rather complex setups, often involving mirrored balls, several HDR cameras, and fish eye lenses to achieve proper light probes. In this paper we present an approach requiring a single RGB-D camera image only for generating glossy reflections on virtual objects. Our approach is based on a partial 3D reconstruction of the real environment combined with a screen-space ray-tracing mechanism. We show that our approach allows for convincing reflections of the real environment as well as mutual reflections between virtual objects of an MR environment.},
keywords={augmented reality;cameras;image colour analysis;image reconstruction;photographic lenses;ray tracing;single camera image based approach;glossy reflections;mixed reality applications;scene inference;virtual object integration;AR environments;MR environments;mirrored balls;HDR cameras;fish eye lenses;light probes;RGB-D camera image;virtual objects;partial 3D reconstruction;screen-space ray-tracing mechanism;Cameras;Lighting;Image reconstruction;Geometry;Three-dimensional displays;Virtual reality;Probes;I.3.7 [Computing Methodologies]: COMPUTER GRAPHICS—Three-Dimensional Graphics and Realism I.4.8 [Computing Methodologies]: IMAGE PROCESSING AND COMPUTER VISION—Scene Analysis},
doi={10.1109/ISMAR.2016.12},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7781765,
author={A. {Morgand} and M. {Tamaazousti} and A. {Bartoli}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={An Empirical Model for Specularity Prediction with Application to Dynamic Retexturing},
year={2016},
volume={},
number={},
pages={44-53},
abstract={Specularities, which are often visible in images, may be problematic in computer vision since they depend on parameters which are difficult to estimate in practice. We present an empirical model called JOLIMAS: JOint LIght-MAterial Specularity, which allows specularity prediction. JOLIMAS is reconstructed from images of specular reflections observed on a planar surface and implicitly includes light and material properties which are intrinsic to specularities. This work was motivated by the observation that specularities have a conic shape on planar surfaces. A theoretical study on the well known illumination models of Phong and Blinn-Phong was conducted to support the accuracy of this hypothesis. A conic shape is obtained by projecting a quadric on a planar surface. We showed empirically the existence of a fixed quadric whose perspective projection fits the conic shaped specularity in the associated image. JOLIMAS predicts the complex phenomenon of specularity using a simple geometric approach with static parameters on the object material and on the light source shape. It is adapted to indoor light sources such as light bulbs or fluorescent lamps. The performance of the prediction was convincing on synthetic and real sequences. Additionally, we used the specularity prediction for dynamic retexturing and obtained convincing rendering results. Further results are presented as supplementary material.},
keywords={computer vision;image reconstruction;shape recognition;specularity prediction;dynamic retexturing;computer vision;JOLIMAS;joint light-material specularity;image reconstruction;planar surfaces;illumination models;conic shaped specularity;fluorescent lamps;light bulbs;indoor light sources;light source shape;Light sources;Computational modeling;Shape;Predictive models;Image reconstruction;Surface reconstruction;Cameras},
doi={10.1109/ISMAR.2016.13},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7781766,
author={C. {Du} and Y. {Chen} and M. {Ye} and L. {Ren}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Edge Snapping-Based Depth Enhancement for Dynamic Occlusion Handling in Augmented Reality},
year={2016},
volume={},
number={},
pages={54-62},
abstract={Dynamic occlusion handling is critical for correct depth perception in Augmented Reality (AR) applications. Consequently it is a key component to ensure realistic and immersive AR experiences. Existing solutions to tackle this challenge typically suffer from various limitations, e.g. assumption of a static scene or high computational complexity. In this work, we propose an algorithm for depth map enhancement for dynamic occlusion handling in AR applications. The key of our algorithm is an edge snapping approach, formulated as discrete optimization, that improves the consistency of object boundaries between RGB and depth data. The optimization problem is solved efficiently via dynamic programming and our system runs in near real-time on the tablet platform. Experimental evaluations demonstrate that our approach largely improves the raw sensor data and is particularly suitable compared to several related approaches in terms of both speed and quality. Furthermore, we demonstrate visually pleasing dynamic occlusion effects for multiple AR use cases based on our edge snapping results.},
keywords={augmented reality;computational complexity;dynamic programming;edge detection;image colour analysis;image enhancement;image sensors;edge snapping-based depth enhancement;dynamic occlusion handling;augmented reality;depth perception;immersive AR experiences;realistic AR experiences;computational complexity;depth map enhancement;discrete optimization;optimization problem;dynamic programming;raw sensor data;visually pleasing dynamic occlusion effects;Three-dimensional displays;Glass;Solid modeling;Image edge detection;Two dimensional displays;Visualization;Heuristic algorithms;Occlusion Handling;Depth Enhancement;Augmented Reality;AR Glasses},
doi={10.1109/ISMAR.2016.17},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7781767,
author={L. {Yang} and J. {Normand} and G. {Moreau}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Practical and Precise Projector-Camera Calibration},
year={2016},
volume={},
number={},
pages={63-70},
abstract={Projectors are important display devices for large scale augmented reality applications. However, precisely calibrating projectors with large focus distances implies a trade-off between practicality and accuracy. People either need a huge calibration board or a precise 3D model [12]. In this paper, we present a practical projector-camera calibration method to solve this problem. The user only needs a small calibration board to calibrate the system regardless of the focus distance of the projector. Results show that the root-mean-squared re-projection error (RMSE) for a 450cm projection distance is only about 4mm, even though it is calibrated using a small B4 (250×353mm) calibration board.},
keywords={augmented reality;calibration;cameras;display devices;optical projectors;display devices;large scale augmented reality;calibration board;root-mean-squared re-projection error;RMSE;projector-camera calibration method;Calibration;Cameras;Lead;Augmented reality;Three-dimensional displays;Distortion;Robustness;H.5.1 [INFORMATION INTERFACES AND PRESENTATION (e.g.;HCI)]: Multimedia Information Systems—Artificial;augmented;and virtual realities},
doi={10.1109/ISMAR.2016.22},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7781768,
author={J. R. {Rambach} and A. {Tewari} and A. {Pagani} and D. {Stricker}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Learning to Fuse: A Deep Learning Approach to Visual-Inertial Camera Pose Estimation},
year={2016},
volume={},
number={},
pages={71-76},
abstract={Camera pose estimation is the cornerstone of Augmented Reality applications. Pose tracking based on camera images exclusively has been shown to be sensitive to motion blur, occlusions, and illumination changes. Thus, a lot of work has been conducted over the last years on visual-inertial pose tracking using acceleration and angular velocity measurements from inertial sensors in order to improve the visual tracking. Most proposed systems use statistical filtering techniques to approach the sensor fusion problem, that require complex system modelling and calibrations in order to perform adequately. In this work we present a novel approach to sensor fusion using a deep learning method to learn the relation between camera poses and inertial sensor measurements. A long short-term memory model (LSTM) is trained to provide an estimate of the current pose based on previous poses and inertial measurements. This estimates then appropriately combined with the output of a visual tracking system using a linear Kalman Filter to provide a robust final pose estimate. Our experimental results confirm the applicability and tracking performance improvement gained from the proposed sensor fusion system.},
keywords={cameras;image fusion;Kalman filters;learning (artificial intelligence);object tracking;pose estimation;tracking performance improvement;linear Kalman Filter;LSTM;long short-term memory model;inertial sensor measurement;sensor fusion;visual-inertial pose tracking;illumination changes;occlusions;motion blur;camera images;augmented reality;visual-inertial camera pose estimation;deep-learning approach;Cameras;Visualization;Sensor fusion;Target tracking;Sensor systems;I.4.8 [Scene Analysis]: Sensor fusion—tracking; I.2.10 [Vision and Scene Understanding]: Motion—Modeling and recovery of physical attributes; I.2.6 [Artificial Intelligence]: Learning—Connectionism and neural nets},
doi={10.1109/ISMAR.2016.19},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7781769,
author={K. {Lien} and B. {Nuernberger} and T. {Höllerer} and M. {Turk}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={PPV: Pixel-Point-Volume Segmentation for Object Referencing in Collaborative Augmented Reality},
year={2016},
volume={},
number={},
pages={77-83},
abstract={We present a method for collaborative augmented reality (AR) that enables users from different viewpoints to interpret object references specified via 2D on-screen circling gestures. Based on a user's 2D drawing annotation, the method segments out the userselected object using an incomplete or imperfect scene model and the color image from the drawing viewpoint. Specifically, we propose a novel segmentation algorithm that utilizes both 2D and 3D scene cues, structured into a three-layer graph of pixels, 3D points, and volumes (supervoxels), solved via standard graph cut algorithms. This segmentation enables an appropriate rendering of the user's 2D annotation from other viewpoints in 3D augmented reality. Results demonstrate the superiority of the proposed method over existing methods.},
keywords={augmented reality;graph theory;image colour analysis;image segmentation;pixel-point-volume segmentation;object referencing;collaborative augmented reality;PPV;AR;on-screen circling gestures;2D drawing annotation;userselected object;incomplete scene model;imperfect scene model;color image;three-layer graph;3D points;supervoxels;volumes;standard graph cut algorithms;Three-dimensional displays;Two dimensional displays;Solid modeling;Image segmentation;Collaboration;Image reconstruction;Augmented reality;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial;augmented;and virtual realities},
doi={10.1109/ISMAR.2016.21},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7781770,
author={M. {Al-Kalbani} and I. {Williams} and M. {Frutos-Pascual}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Analysis of Medium Wrap Freehand Virtual Object Grasping in Exocentric Mixed Reality},
year={2016},
volume={},
number={},
pages={84-93},
abstract={This article presents an analysis into the accuracy and problems of freehand grasping in exocentric Mixed Reality (MR). We report on two experiments (1710 grasps) which quantify the influence different virtual object shape, size and position has on the most common physical grasp, a medium wrap. We propose two methods for grasp measurement, namely, the Grasp Aperture (GAp) and Grasp Displacement (GDisp). Controlled laboratory conditions are used where 30 right-handed participants attempt to recreate a medium wrap grasp. We present a comprehensive statistical analysis of the results giving pairwise comparisons of all conditions under test. The results illustrate that user Grasp Aperture varies less than expected in comparison to the variation of virtual object size, with common aperture sizes found. Regarding the position of the virtual object, depth estimation is often mismatched due to under judgement of the z position and x, y displacement has common patterns. Results from this work can be applied to aid in the development of freehand grasping and considered as the first study into accuracy of freehand grasping in MR, provide a starting point for future interaction design.},
keywords={augmented reality;human computer interaction;interactive devices;medium wrap freehand virtual object grasping;exocentric mixed reality;freehand grasping;grasp measurement;grasp aperture;GAp;grasp displacement;GDisp;statistical analysis;virtual object interaction;depth estimation;augmented reality;Grasping;Apertures;Virtual reality;Atmospheric measurements;Displacement measurement;Particle measurements;Grasping;Freehand Interaction;Natural Hand Interaction;Human Performance Measurement;Mixed Reality},
doi={10.1109/ISMAR.2016.14},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7781771,
author={A. {Plopski} and J. {Orlosky} and Y. {Itoh} and C. {Nitschke} and K. {Kiyokawa} and G. {Klinker}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Automated Spatial Calibration of HMD Systems with Unconstrained Eye-cameras},
year={2016},
volume={},
number={},
pages={94-99},
abstract={Properly calibrating an optical see-through head-mounted display (OST-HMD) and maintaining a consistent calibration over time can be a very challenging task. Automated methods need an accurate model of both the OST-HMD screen and the user's constantly changing eye-position to correctly project virtual information. While some automated methods exist, they often have restrictions, including fixed eye-cameras that cannot be adjusted for different users.To address this problem, we have developed a method that automatically determines the position of an adjustable eye-tracking camera and its unconstrained position relative to the display. Unlike methods that require a fixed pose between the HMD and eye camera, our framework allows for automatic calibration even after adjustments of the camera to a particular individual's eye and even after the HMD moves on the user's face. Using two sets of IR-LEDs rigidly attached to the camera and OST-HMD frame, we can calculate the correct projection for different eye positions in real time and changes in HMD position within several frames. To verify the accuracy of our method, we conducted two experiments with a commercial HMD by calibrating a number of different eye and camera positions. Ground truth was measured through markers on both the camera and HMD screens, and we achieve a viewing accuracy of 1.66 degrees for the eyes of 5 different experiment participants.},
keywords={calibration;cameras;gaze tracking;helmet mounted displays;pose estimation;eye-pose estimation;IR-LEDs;adjustable eye-tracking camera;eye position;OST-HMD;optical see-through head-mounted display;unconstrained eye-cameras;HMD systems;automated spatial calibration;Cameras;Resists;Cornea;Calibration;Light emitting diodes;Estimation;Optical distortion;OST-HMD calibration;eye pose estimation},
doi={10.1109/ISMAR.2016.16},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7781772,
author={L. {Shapira} and J. {Amores} and X. {Benavides}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={TactileVR: Integrating Physical Toys into Learn and Play Virtual Reality Experiences},
year={2016},
volume={},
number={},
pages={100-106},
abstract={We present TactileVR, a proof-of-concept virtual reality system in which a user is free to move around and interact with physical objects and toys, which are represented in the virtual world. By integrating tracking information from the head, hands and feet of the user, as well as the objects, we infer complex gestures and interactions such as shaking a toy, rotating a steering wheel, or clapping your hands. We create educational and recreational experiences for kids, which promote exploration and discovery, while feeling intuitive and safe. In each experience objects have a unique appearance and behavior e.g. in an electric circuits lab toy blocks serve as switches, batteries and light bulbs.We conducted a user study with children ages 5-11, who experienced TactileVR and interacted with virtual proxies of physical objects. Children took instantly to the TactileVR environment, intuitively discovered a variety of interactions, and completed tasks faster than with non-tactile virtual objects. Moreover, the presence of physical toys created the opportunity for collaborative play, even when only some of the kids were using a VR headset.},
keywords={haptic interfaces;virtual reality;TactileVR;physical toys;virtual reality system;user interaction;tracking information integration;complex gesture inference;toy shaking;steering wheel rotatation;hand clapping;electric circuit lab toy blocks;physical objects;VR headset;Haptic interfaces;Virtual reality;Headphones;Tracking;Games;Cameras;Shape;H.5.1 [Multimedia Information Systems]: Artificial;augmented and virtual realities—; H.5.2 [User Interfaces]: Interaction Style—; I.3.7 [3-D Graphics]: Virtual Reality—},
doi={10.1109/ISMAR.2016.25},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7781773,
author={J. {Zhang} and A. {Ogan} and T. {Liu} and Y. {Sung} and K. {Chang}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={The Influence of using Augmented Reality on Textbook Support for Learners of Different Learning Styles},
year={2016},
volume={},
number={},
pages={107-114},
abstract={It has been shown in numerous studies that the application of Augmented Reality (AR) to teaching and learning is beneficial, but determining the reasons behind its effectiveness, and in particular the characteristics of students for whom an AR is best suited, can bring forth new opportunities to integrate adaptive instruction and AR in the future. Through a quasi-experimental research design, our study recruited 66 participants in an 8-week long AR-assisted learning activity, and lag sequential analysis was used to analyze participants' behavior in an AR learning environment. We found that AR was more effective in enhancing the learning gains in elementary school science of learners who prefer a Kinesthetic approach to learning. We hypothesize that these effects are due to the increase in opportunity for hands-on activities, effectively increasing learners' concentration and passion for learning.},
keywords={augmented reality;computer aided instruction;teaching;augmented reality;textbook support;learning styles;teaching;learning;adaptive instruction;quasiexperimental research design;AR-assisted learning activity;lag sequential analysis;AR learning environment;elementary school science;kinesthetic approach;hands-on activities;Education;Augmented reality;Visualization;Systems operation;Computers;Instruments;Cameras;Augmented Reality;Computer-assisted instruction;K-12 education},
doi={10.1109/ISMAR.2016.26},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7781774,
author={L. {Shapira} and D. {Freedman}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Reality Skins: Creating Immersive and Tactile Virtual Environments},
year={2016},
volume={},
number={},
pages={115-124},
abstract={Reality Skins enables mobile and large-scale virtual reality experiences, dynamically generated based on the user's environment. A head-mounted display (HMD) coupled with a depth camera is used to scan the user's surroundings: reconstruct geometry, infer floor plans, and detect objects and obstacles. From these elements we generate a Reality Skin, a 3D environment which replaces office or apartment walls with the corridors of a spaceship or underground tunnels, replacing chairs and desks, sofas and beds with crates and computer consoles, fungi and crumbling ancient statues. The placement of walls, furniture and objects in the Reality Skin attempts to approximate reality, such that the user can move around, and touch virtual objects with tactile feedback from real objects. Each possible reality skins world consists of objects, materials and custom scripts. Taking cues from the user's surroundings, we create a unique environment combining these building blocks, attempting to preserve the geometry and semantics of the real world.We tackle 3D environment generation as a constraint satisfaction problem, and break it into two parts: First, we use a Markov Chain Monte-Carlo optimization, over a simple 2D polygonal model, to infer the layout of the environment (the structure of the virtual world). Then, we populate the world with various objects and characters, attempting to satisfy geometric (virtual objects should align with objects in the environment), semantic (a virtual chair aligns with a real one), physical (avoid collisions, maintain stability) and other constraints. We find a discrete set of transformations for each object satisfying unary constraints, incorporate pairwise and higher-order constraints, and optimize globally using a very recent technique based on semidefinite relaxation.},
keywords={computational geometry;haptic interfaces;inference mechanisms;Markov processes;mathematical programming;Monte Carlo methods;virtual reality;higher-order constraints;global optimization;semidefinite relaxation;pairwise constraints;unary constraints;physical constraints;semantic constraints;geometric constraints;2D polygonal model;Markov chain Monte-Carlo optimization;constraint satisfaction problem;real objects;tactile feedback;virtual objects;obstacle detection;object detection;floor plan inference;user surrounding scanning;geometry reconstruction;depth camera;HMD;head-mounted display;user environment;mobile large-scale virtual reality experiences;tactile virtual environment;immersive environment;reality skins;Skin;Three-dimensional displays;Layout;Virtual environments;Semantics;Image reconstruction;Index Terms: H.5.1 [Multimedia Information Systems]: Artificial;Augmented and Virtual Realities—; I.3.7 [Three Dimensional Graphics and Realism]: Virtual Reality— [G.1.6]: Optimization—Constrained Optimization},
doi={10.1109/ISMAR.2016.23},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7781775,
author={O. {Wasenmüller} and M. {Meyer} and D. {Stricker}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Augmented Reality 3D Discrepancy Check in Industrial Applications},
year={2016},
volume={},
number={},
pages={125-134},
abstract={Discrepancy check is a well-known task in industrial Augmented Reality (AR). In this paper we present a new approach consisting of three main contributions: First, we propose a new two-step depth mapping algorithm for RGB-D cameras, which fuses depth images with given camera pose in real-time into a consistent 3D model. In a rigorous evaluation with two public benchmarks we show that our mapping outperforms the state-of-the-art in accuracy. Second, we propose a semi-automatic alignment algorithm, which rapidly aligns a reference model to the reconstruction. Third, we propose an algorithm for 3D discrepancy check based on pre-computed distances. In a systematic evaluation we show the superior performance of our approach compared to state-of-the-art 3D discrepancy checks.},
keywords={augmented reality;image processing;image sensors;augmented reality 3D discrepancy check;industrial applications;industrial augmented reality;AR;discrepancy check;mapping algorithm;public benchmarks;semiautomatic alignment algorithm;Image reconstruction;Three-dimensional displays;Cameras;Real-time systems;Solid modeling;Geometry;Augmented reality;I.4.8 [Image Processing and Computer Vision]: Scene Analysis—Depth cues; I.2.10 [Artificial Intelligence]: Vision and Scene Understanding—3D/stereo scene analysis},
doi={10.1109/ISMAR.2016.15},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7781776,
author={},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Author index},
year={2016},
volume={},
number={},
pages={135-135},
abstract={Presents an index of the authors whose articles are published in the conference proceedings record.},
keywords={},
doi={10.1109/ISMAR.2016.27},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7781777,
author={},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Publishers' information]},
year={2016},
volume={},
number={},
pages={136-136},
abstract={Provides a listing of current committee members and society officers.},
keywords={},
doi={10.1109/ISMAR.2016.28},
ISSN={},
month={Sep.},}