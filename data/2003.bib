@INPROCEEDINGS{1240681,
author={},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Proceedings Second IEEE and ACM International Symposium on Mixed and Augmented Reality},
year={2003},
volume={},
number={},
pages={},
abstract={The following fields are dealt with: mixed reality; augmented reality; signal processing; computer vision; computer graphics; user interfaces; human factors; mobile computing; wearable computing; computer networks; distributed computing; information access; information visualization; and hardware designs for new displays and sensors.},
keywords={augmented reality;signal processing;computer vision;user interfaces;human factors;mobile computing;wearable computers;computer networks;distributed processing;hardware-software codesign;sensors;data visualisation;mixed reality;augmented reality;signal processing;computer vision;computer graphics;user interfaces;human factors;mobile computing;wearable computing;computer networks;distributed computing;information access;information visualization;hardware designs;displays;sensors},
doi={10.1109/ISMAR.2003.1240681},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240682,
author={N. {Navab}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Industrial augmented reality (IAR): challenges in design and commercialization of killer apps},
year={2003},
volume={},
number={},
pages={2-6},
abstract={},
keywords={Augmented reality;Commercialization;Computer industry;Mobile computing;Production facilities;Thyristors;Data engineering;Communication industry;Wireless communication;Image reconstruction},
doi={10.1109/ISMAR.2003.1240682},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240683,
author={K. {Ikeuchi} and A. {Nakazawa} and K. {Hasegawa} and T. {Ohishi}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={The great buddha project: modeling cultural heritage for VR systems through observation},
year={2003},
volume={},
number={},
pages={7-16},
abstract={},
keywords={Cultural differences;Virtual reality;Layout;Solid modeling;Graphics;Hardware;Merging;Iterative closest point algorithm;Computer vision;Geometry},
doi={10.1109/ISMAR.2003.1240683},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240684,
author={A. J. {Davison} and W. W. {Mayol} and D. W. {Murray}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Real-time localization and mapping with wearable active vision},
year={2003},
volume={},
number={},
pages={18-27},
abstract={We present a general method for real-time, vision-only single-camera simultaneous localisation and mapping (SLAM) - an algorithm which is applicable to the localisation of any camera moving through a scene - and study its application to the localisation of a wearable robot with active vision. Starting from very sparse initial scene knowledge, a map of natural point features spanning a section of a room is generated on-the-fly as the motion of the camera is simultaneously estimated in full 3D. Naturally this permits the annotation of the scene with rigidly-registered graphics, but further it permits automatic control of the robot's active camera: for instance, fixation on a particular object can be maintained during extended periods of arbitrary user motion, then shifted at will to another object which has potentially been out of the field of view. This kind of functionality is the key to the understanding or "management" of a workspace which the robot needs to have in order to assist its wearer usefully in tasks. We believe that the techniques and technology developed are of particular immediate value in scenarios of remote collaboration, where a remote expert is able to annotate, through the robot, the environment the wearer is working in.},
keywords={augmented reality;active vision;cameras;image sensors;position control;object detection;real-time localization;wearable active vision;vision-only localisation;single-camera localisation;simultaneous localisation;SLAM;algorithm;wearable robot;scene knowledge;natural point map;section spanning;scene annotation;registered graphics;automatic;active camera;user motion;remote collaboration;remote expert;visual sensor;wearable visual robot;augmentation;Robot vision systems;Layout;Wearable sensors;Cameras;Robot sensing systems;Simultaneous localization and mapping;Collaborative software;Magnetic sensors;Motion estimation;Graphics},
doi={10.1109/ISMAR.2003.1240684},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240685,
author={L. {Davis} and E. {Clarkson} and J. P. {Rolland}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Predicting accuracy in pose estimation for marker-based tracking},
year={2003},
volume={},
number={},
pages={28-35},
abstract={Tracking is a necessity for interactive virtual environments. Marker-based tracking solutions involve the placement of fiducials in a rigid configuration on the object(s) to be tracked, called a tracking probe. The realization that tracking performance is linked to probe performance necessitates investigation into the design of tracking probes for proponents of marker-based tracking. A challenge involved with probe design is predicting the accuracy of a tracking probe. We present a method for predicting the accuracy of a tracking probe based upon a first-order propagation of the errors associated with the markers on the probe. Results for two sample tracking probes show excellent agreement between measured and predicted errors.},
keywords={augmented reality;object detection;target tracking;accuracy prediction;pose estimation;marker-based tracking;interactive virtual environments;placement of fiducials;tracking probe;tracking performance;probe performance;probe design;first-order propagation;error prediction;Accuracy;Probes;Topology;Virtual environment;Error analysis;Augmented reality;Estimation error;Heuristic algorithms;Target tracking},
doi={10.1109/ISMAR.2003.1240685},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240686,
author={A. I. {Comport} and E. {Marchand} and F. {Chaumette}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={A real-time tracker for markerless augmented reality},
year={2003},
volume={},
number={},
pages={36-45},
abstract={Augmented reality has now progressed to the point where real-time applications are required and being considered. At the same time it is important that synthetic elements are rendered and aligned in the scene in an accurate and visually acceptable way. In order to address these issues a real-time, robust and efficient 3D model-based tracking algorithm is proposed for a 'video see through' monocular vision system. The tracking of objects in the scene amounts to calculating the pose between the camera and the objects. Virtual objects can then be projected into the scene using the pose. Here, non-linear pose computation is formulated by means of a virtual visual servoing approach. In this context, the derivation of point-to-curve interaction matrices is given for different features including lines, circles, cylinders and spheres. A local moving edge tracker is used in order to provide real-time tracking of points normal to the object contours. A method is proposed for combining local position uncertainty and global pose uncertainty in an efficient and accurate way by propagating uncertainty. Robustness is obtained by integrating an M-estimator into the visual control law via an iteratively re-weighted least squares implementation. The method presented in this paper has been validated on several complex image sequences including outdoor environments. Results show the method to be robust to occlusion, changes in illumination and mistracking.},
keywords={augmented reality;optical tracking;stereo image processing;image sequences;least squares approximations;computer vision;servomechanisms;rendering (computer graphics);edge detection;real-time tracker;markerless augmented reality;real-time applications;synthetic elements;3D model;tracking algorithm;monocular vision system;virtual objects;nonlinear pose computation;virtual visual serving approach;point-to-curve interaction;interaction matrices;moving edge tracker;object contours;local position uncertainty;global pose uncertainty;uncertainty propagation;visual control law;least squares implementation;complex image sequences;outdoor environments;occlusion;Augmented reality;Layout;Uncertainty;Robustness;Real time systems;Machine vision;Cameras;Visual servoing;Robust control;Least squares methods},
doi={10.1109/ISMAR.2003.1240686},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240687,
author={K. {Satoh} and S. {Uchiyama} and H. {Yamamoto} and H. {Tamura}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Robot vision-based registration utilizing bird's-eye view with user's view},
year={2003},
volume={},
number={},
pages={46-55},
abstract={This paper describes new vision-based registration methods utilizing not only cameras on a user's head-mounted display but also a bird's-eye view camera that observes the user from an objective viewpoint. Two new methods, the line constraint method (LCM) and global error minimization method (GEM), are proposed. The former method reduces the number of unknown parameters concerning the user's viewpoint by restricting it to be on the line of sight from the bird's-eye view. The other method minimizes the sum of errors, which is the sum of the distance between the fiducials on the view and the calculated positions of them based on the current viewing parameters, for both the user's view and the bird's-eye view. The methods proposed here reduce the number of points that should be observed from the user's viewpoint for registration, thus improving the stability. In addition to theoretical discussions, this paper demonstrates the effectiveness of our methods by experiments in comparison with methods that use only a user's view camera or a bird's-eye view camera.},
keywords={robot vision;image registration;augmented reality;cameras;robot vision;vision-based registration;cameras;head-mounted display;line constraint method;global error minimization method;unknown parameters;user viewpoint;fiducials;viewing parameters;user view camera;bird eye view camera;Robustness;Cameras;Space technology;Augmented reality;Magnetic sensors;Position measurement;Robust stability;Laboratories;Displays;Minimization methods},
doi={10.1109/ISMAR.2003.1240687},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240688,
author={M. A. {Livingston} and J. E. {Swan} and J. L. {Gabbard} and T. H. {Hollerer} and D. {Hix} and S. J. {Julier} and Y. {Baillot} and D. {Brown}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Resolving multiple occluded layers in augmented reality},
year={2003},
volume={},
number={},
pages={56-65},
abstract={A useful function of augmented reality (AR) systems is their ability to visualize occluded infrastructure directly in a user's view of the environment. This is especially important for our application context, which utilizes mobile AR for navigation and other operations in an urban environment. A key problem in the AR field is how to best depict occluded objects in such a way that the viewer can correctly infer the depth relationships between different physical and virtual objects. Showing a single occluded object with no depth context presents an ambiguous picture to the user. But showing all occluded objects in the environments leads to the "Superman's X-ray vision" problem, in which the user sees too much information to make sense of the depth relationships of objects. Our efforts differ qualitatively from previous work in AR occlusion, because our application domain involves far-field occluded objects, which are tens of meters distant from the user. Previous work has focused on near-field occluded objects, which are within or just beyond arm's reach, and which use different perceptual cues. We designed and evaluated a number of sets of display attributes. We then conducted a user study to determine which representations best express occlusion relationships among far-field objects. We identify a drawing style and opacity settings that enable the user to accurately interpret three layers of occluded objects, even in the absence of perspective constraints.},
keywords={hidden feature removal;augmented reality;data visualisation;computer displays;object detection;multiple occluded layers;augmented reality;AR systems;occluded infrastructure;mobile AR;navigation;occluded objects;virtual objects;AR occlusion;drawing style;opacity settings;perspective constraints;visualization;Augmented reality;Visualization;Laboratories;Displays;Navigation;Context awareness;Human factors;Switches;Virtual reality;Computer science},
doi={10.1109/ISMAR.2003.1240688},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240689,
author={R. {Azuma} and C. {Furmanski}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Evaluating label placement for augmented reality view management},
year={2003},
volume={},
number={},
pages={66-75},
abstract={View management, a relatively new area of research in Augmented Reality (AR) applications, is about the spatial layout of 2D virtual annotations in the view plane. This paper represents the first study in an actual AR application of a specific view management task: evaluating the placement of 2D virtual labels that identify information about real counterparts. Here, we objectively evaluated four different placement algorithms, including a novel algorithm for placement based on identifying existing clusters. The evaluation included both a statistical analysis of traditional metrics (e.g. counting overlaps) and an empirical user study guided by principles from human cognition. The numerical analysis of the three real-time algorithms revealed that our new cluster-based method recorded the best average placement accuracy while requiring only relatively moderate computation time. Measures of objective readability from the user study demonstrated that in practice, human subjects were able to read labels fastest with the algorithms that most quickly prevented overlap, even if placement wasn't ideal.},
keywords={augmented reality;statistical analysis;pattern clustering;algorithm theory;numerical analysis;computational complexity;cognition;label placement;augmented reality;view management;spatial layout;2D virtual annotations;view plane;2D virtual labels;clusters identification;statistical analysis;counting overlaps;empirical user study;human cognition;numerical analysis;three real-time algorithms;cluster-based method;placement accuracy;computation time;Augmented reality;Clustering algorithms;Humans;Laboratories;Statistical analysis;Cognition;Numerical analysis;Two dimensional displays;Virtual reality;Testing},
doi={10.1109/ISMAR.2003.1240689},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240690,
author={N. {Sugano} and H. {Kato} and K. {Tachibana}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={The effects of shadow representation of virtual objects in augmented reality},
year={2003},
volume={},
number={},
pages={76-83},
abstract={In this paper, we describe the effects of shadow representation of virtual objects in augmented reality. Optical consistency is important in order to create realistic augmented reality environments. We focus on providing accurate shadows and made two assumptions about the effects of shadow representation of virtual objects. First, that the shadow of virtual objects provides a stronger connection between the real world and virtual objects and so increases virtual object presence. Second, that the shadow of virtual objects provides depth cues and so makes three-dimensional perceptions easier for the users of the interface. We report on two experiments that show that these assumptions are correct. We also find that users report that a characteristic shadow shape provides more virtual object presence in spite of incorrect virtual light direction.},
keywords={augmented reality;image representation;lighting;rendering (computer graphics);user interfaces;shadow representation;virtual objects;augmented reality;optical consistency;real world;object presence;three-dimensional perceptions;user interface;virtual light direction;virtual reality;virtual environment;Augmented reality;Layout;Geometrical optics;Rendering (computer graphics);Shadow mapping;Light sources;Material properties;Lighting;Shape;Cameras},
doi={10.1109/ISMAR.2003.1240690},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240691,
author={D. {Belcher} and M. {Billinghurst} and S. E. {Hayes} and R. {Stiles}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Using augmented reality for visualizing complex graphs in three dimensions},
year={2003},
volume={},
number={},
pages={84-93},
abstract={In this paper we explore the effect of using augmented reality (AR) for three-dimensional graph link analysis. Two experiments were conducted. The first was designed to compare a tangible AR interface to a desktop-based interface. Different modes of viewing network graphs were presented using a variety of interfaces. The results of the first experiment show that a tangible AR interface is well suited to link analysis. The second experiment was designed to test the effect of stereographic viewing on graph comprehension. The results show that stereographic viewing has little effect on comprehension and performance. These experiments add support to the work of Ware and Frank, whose studies showed that depth and motion cues provide huge gains in spatial comprehension and accuracy in link analysis.},
keywords={augmented reality;data visualisation;three-dimensional displays;user interfaces;augmented reality;complex graph visualization;three dimension;graph link analysis;AR interface;desktop-based interface;network graphs;stereographic viewing;graph comprehension;depth cues;motion cues;spatial comprehension;Augmented reality;Visualization;Testing;Kinetic theory;Humans;Three dimensional displays;Motion analysis;Performance gain;Logic devices;Circuits},
doi={10.1109/ISMAR.2003.1240691},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240692,
author={V. {Lepetit} and L. {Vacchetti} and D. {Thalmann} and P. {Fua}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Fully automated and stable registration for augmented reality applications},
year={2003},
volume={},
number={},
pages={93-102},
abstract={We present a fully automated approach to camera registration for augmented reality systems. It relies on purely passive vision techniques to solve the initialization and real-time tracking problems, given a rough CAD model of parts of the real scene. It does not require a controlled environment, for example placing markers. It handles arbitrarily complex models, occlusions, large camera displacements and drastic aspect changes. This is made possible by two major contributions: the first one is a fast recognition method that detects the known part of the scene, registers the camera with respect to it, and initializes a real-time tracker, which is the second contribution. Our tracker eliminates drift and jitter by merging the information from preceding frames in a traditional recursive tracking fashion with that of a very limited number of key-frames created off-line. In the rare instances where it fails, for example because of large occlusion, it detects the failure and reinvokes the initialization procedure. We present experimental results on several different kinds of objects and scenes.},
keywords={image registration;cameras;augmented reality;computer vision;hidden feature removal;automated registration;stable registration;augmented reality applications;camera registration;passive vision techniques;real-time tracking;CAD model;controlled environment;complex models;occlusions;large camera displacements;drastic aspect changes;fast recognition method;merging the information;recursive tracking;failure detection;initialization procedure;Augmented reality;Cameras;Layout;Target tracking;Robustness;Jitter;Image databases;Spatial databases;Automatic control;Merging},
doi={10.1109/ISMAR.2003.1240692},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240693,
author={M. {Kourogi} and T. {Kurata}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Personal positioning based on walking locomotion analysis with self-contained sensors and a wearable camera},
year={2003},
volume={},
number={},
pages={103-112},
abstract={In this paper, we propose a method of personal positioning for a wearable augmented reality (AR) system that allows a user to freely move around indoors and outdoors. The user is equipped with self-contained sensors, a wearable camera, an inertial head tracker and display. The method is based on sensor fusion of estimates for relative displacement caused by human walking locomotion and estimates for absolute position and orientation within a Kalman filtering framework. The former is based on intensive analysis of human walking behavior using self-contained sensors. The latter is based on image matching of video frames from a wearable camera with an image database that was prepared beforehand.},
keywords={augmented reality;image matching;motion estimation;sensor fusion;cameras;position measurement;wearable computers;image registration;personal positioning;walking locomotion analysis;self-contained sensors;wearable camera;augmented reality;AR system;inertial head tracker;inertial head display;sensor fusion;relative displacement;absolute position;orientation;Kalman filtering framework;human walking behavior;image matching;video frames;image database;pedometer;human walking analysis;wearable computing;Legged locomotion;Wearable sensors;Cameras;Humans;Augmented reality;Head;Displays;Sensor fusion;Kalman filters;Filtering},
doi={10.1109/ISMAR.2003.1240693},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240694,
author={G. {Klein} and T. {Drummond}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Robust visual tracking for non-instrumental augmented reality},
year={2003},
volume={},
number={},
pages={113-122},
abstract={This paper presents a robust and flexible framework for augmented reality which does not require instrumenting either the environment or the workpiece. A model-based visual tracking system is combined with rate gyroscopes to produce a system which can track the rapid camera rotations generated by a head-mounted camera, even if images are substantially degraded by motion blur. This tracking yields estimates of head position at video field rate (50Hz) which are used to align computer-generated graphics on an optical see-through display. Nonlinear optimisation is used for the calibration of display parameters which include a model of optical distortion. Rendered visuals are pre-distorted to correct the optical distortion of the display.},
keywords={augmented reality;image registration;cameras;rendering (computer graphics);edge detection;helmet mounted displays;robot vision;noninstrumental augmented reality;flexible framework;model-based visual tracking;rate gyroscopes;camera rotations;head-mounted camera;motion blur;head position;video field rate;computer-generated graphics;optical see-through display;nonlinear optimisation;display parameters;optical distortion;visual rendering;Robustness;Augmented reality;Computer displays;Optical distortion;Tracking;Cameras;Instruments;Gyroscopes;Degradation;Yield estimation},
doi={10.1109/ISMAR.2003.1240694},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240695,
author={A. {MacWilliams} and C. {Sandor} and M. {Wagner} and M. {Bauer} and G. {Klinker} and B. {Bruegge}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Herding sheep: live system for distributed augmented reality},
year={2003},
volume={},
number={},
pages={123-132},
abstract={In the past, architectures of augmented reality systems have been widely different and tailored to specific tasks. In this paper, we use the example of the SHEEP game to show how the structural flexibility of DWARF, our component-based distributed wearable augmented reality framework, facilitates a rapid prototyping and online development process for building, debugging and altering a complex, distributed, highly interactive AR system. The SHEEP system was designed to test and demonstrate the potential of tangible user interfaces which dynamically visualize, manipulate and control complex operations of many inter-dependent processes. SHEEP allows the users more freedom of action and forms of interaction and collaboration, following the tool metaphor that bundles software with hardware in units that are easily understandable to the user. We describe how we developed SHEEP, showing the combined evolution of framework and application, as well as the progress from rapid prototype to final demonstration system. The dynamic aspects of DWARF facilitated testing and allowed us to rapidly evaluate new technologies. SHEEP has been shown successfully at various occasions. We describe our experiences with these demos.},
keywords={groupware;augmented reality;software prototyping;middleware;computer games;hardware-software codesign;graphical user interfaces;SHEEP game;structural flexibility;DWARF;component-based augmented reality;distributed wearable augmented reality;online development;AR building;AR debugging;AR altering;interactive AR system;user interfaces;operation visualization;operation manipulation;operation control;tool metaphor;combined evolution;demonstration system;prototyping;Augmented reality;Prototypes;Collaborative software;Buildings;Debugging;System testing;User interfaces;Visualization;Control systems;Collaborative tools},
doi={10.1109/ISMAR.2003.1240695},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240696,
author={K. {Kiyokawa} and M. {Billinghurst} and B. {Campbell} and E. {Woods}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={An occlusion capable optical see-through head mount display for supporting co-located collaboration},
year={2003},
volume={},
number={},
pages={133-141},
abstract={An ideal augmented reality (AR) display for multi-user co-located collaboration should have following three features: 1) any virtual object should be able to be shown at any arbitrary position, e.g. a user can see a virtual object in front of other users' faces. 2) Correct occlusion of virtual and real objects should be supported. 3) The real world should be naturally and clearly visible, which is important for face-to-face conversation. We have been developing an optical see-through display, ELMO (Enhanced see-through display using an LCD panel for Mutual Occlusion), that satisfies these three requirements. While previous prototype systems were not practical due to their size and weight, we have come up with an improved optics design which has reduced size and is lightweight enough to wear. In this paper, the characteristics of typical multi-user three-dimensional displays are summarized and the design details of the latest optics are then described. Finally, a collaborative AR application employing the new display and its user experience are explained.},
keywords={augmented reality;helmet mounted displays;groupware;user modelling;user interfaces;computer displays;hidden feature removal;computer games;occlusion;optical see-through display;head mount display;collaboration support;co-located collaboration;AR display;multiuser collaboration;virtual object;arbitrary position;face-to-face conversation;ELMO;Enhanced see-through display using an LCD panel for Mutual Occlusion;prototype systems;optics design;reduced size;lightweight;typical multiuser displays;three-dimensional displays;collaborative AR application;Collaboration;Three dimensional displays;Layout;Augmented reality;Humans;Laboratories;Face;Liquid crystal displays;Optical design;Collaborative work},
doi={10.1109/ISMAR.2003.1240696},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240697,
author={Y. {Baillot} and S. J. {Julier} and D. {Brown} and M. A. {Livingston}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={A tracker alignment framework for augmented reality},
year={2003},
volume={},
number={},
pages={142-150},
abstract={To achieve accurate registration, the transformations which locate the tracking system components with respect to the environment must be known. These transformations relate the base of the tracking system to the virtual world and the tracking system's sensor to the graphics display. In this paper we present a unified, general calibration method for calculating these transformations. A user is asked to align the display with objects in the real world. Using this method, the sensor to display and tracker base to world transformations can be determined with as few as three measurements.},
keywords={augmented reality;optical tracking;image registration;computer vision;tracker alignment framework;augmented reality;accurate registration;transformations;tracking system components;virtual world;tracking system sensor;graphics display;calibration method;motion capture;Augmented reality;Displays;Magnetic heads;Sensor systems;Graphics;Calibration;Tracking;Distortion measurement;Magnetic field measurement;Optical distortion},
doi={10.1109/ISMAR.2003.1240697},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240698,
author={E. {Foxlin} and L. {Naimark}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Miniaturization, calibration accuracy evaluation of a hybrid self-tracker},
year={2003},
volume={},
number={},
pages={151-160},
abstract={We have previously presented a prototype of a novel vision/inertial hybrid self-tracker intended for AR, wearable computing and mobile robotics applications. In this paper we describe a new prototype of the system which has been greatly reduced in size, weight, power consumption and cost, while simultaneously improved in performance through careful calibration. We describe the calibration approach in detail and present results to show the high accuracy levels achieved for the camera calibration and for the integrated tracking system.},
keywords={augmented reality;optical tracking;sensor fusion;computer vision;wearable computers;systems analysis;miniaturization;calibration;accuracy evaluation;vision hybrid self-tracker;inertial hybrid self-tracker;wearable computing;mobile robotics;system prototype;integrated tracking system;helmet mounted display;augmented reality;Calibration;Prototypes;Lenses;Charge coupled devices;Sensor fusion;Energy consumption;Sensor phenomena and characterization;Smart cameras;Virtual reality;System performance},
doi={10.1109/ISMAR.2003.1240698},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240699,
author={A. {Tang} and {Ji Zhou} and C. {Owen}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Evaluation of calibration procedures for optical see-through head-mounted displays},
year={2003},
volume={},
number={},
pages={161-168},
abstract={Optical see-through head-mounted displays (HMDs) are less commonly used because they are difficult to accurately calibrate. In this article, we report a user study to compare the accuracy of 4 variants of the SPAAM calibration method. Among the 4 variants, Stylus-marker calibration, where the user aligns a crosshair projected in the HMD with a tracked stylus tip, achieved the most accurate result. A decomposition and analysis of the calibration matrices from the trials is performed and the characteristics of the computed calibration matrices are examined. A physiological engineering point of view is also discussed to explain why calibrating optical see-through HMD is so difficult for users.},
keywords={helmet mounted displays;optical tracking;augmented reality;computer vision;human factors;optical see-through displays;head-mounted displays;user study;SPAAM calibration method;Stylus-marker calibration;crosshair alignment;tracked stylus tip;decomposition;calibration matrices;physiological engineering;augmented reality;human factors;Calibration;Computer displays;Computer graphics;Cameras;Laboratories;Matrix decomposition;Augmented reality;Mirrors;Head;Tracking},
doi={10.1109/ISMAR.2003.1240699},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240700,
author={H. {Kim} and {Seung-jun Yang} and {Kwanghoon Sohn}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={3D reconstruction of stereo images for interaction between real and virtual worlds},
year={2003},
volume={},
number={},
pages={169-176},
abstract={Mixed reality is different from the virtual reality in that users can feel immersed in a space which is composed of not only virtual but also real objects. Thus, it is essential to realize seamless integration and interaction of the virtual and real worlds. We need depth information of the real scene to synthesize the real and virtual objects. We propose a two-stage algorithm to find smooth and precise disparity vector fields with sharp object boundaries in a stereo image pair for depth estimation. Hierarchical region-dividing disparity estimation increases the efficiency and the reliability of the estimation process, and a shape-adaptive window provides high reliability of the fields around the object boundary region. At the second stage, the vector fields are regularized with a energy model which produces smooth fields while preserving their discontinuities resulting from the object boundaries. The vector fields are used to reconstruct 3D surface of the real scene. Simulation results show that the proposed algorithm provides accurate and spatially correlated disparity vector fields in various kinds of images, and synthesized 3D models produce natural space where the virtual objects interact with the real world as if they are in the same world.},
keywords={image reconstruction;virtual reality;stereo image processing;computational complexity;cameras;human computer interaction;3D reconstruction;stereo images;real-virtual interaction;mixed reality;virtual reality;virtual objects;real objects;seamless integration;two-stage algorithm;disparity vector fields;stereo image pair;depth estimation;region-dividing disparity estimation;estimation process;shape-adaptive window;object boundary region;energy model;fields while;3D surface;simulation results;synthesized 3D models;natural space;stereo camera;Image reconstruction;Stereo image processing;Virtual reality;Cameras;Layout;Stereo vision;Geometry;Surface reconstruction;Displays;Ultrasonic variables measurement},
doi={10.1109/ISMAR.2003.1240700},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240701,
author={T. {Koyama} and I. {Kitahara} and Y. {Ohta}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Live mixed-reality 3D video in soccer stadium},
year={2003},
volume={},
number={},
pages={178-186},
abstract={This paper proposes a method to realize a 3D video display system that can capture video from multiple cameras, reconstruct 3D models and transmit 3D video data in real time. We represent a target object with a simplified 3D model consisting of a single plane and a 2D texture extracted from multiple cameras. This 3D model is simple enough to be transmitted via a network. We have developed a prototype system that can capture multiple videos, reconstruct 3D models, transmit the models via a network, and display 3D video in real time. A 3D video of a typical soccer scene that includes a dozen players was processed at 26 frames per second.},
keywords={virtual reality;image reconstruction;three-dimensional displays;computer vision;image representation;sport;mixed reality;soccer stadium;3D video display system;video capture;multiple cameras;3D reconstruction;3D video data;real time system;target object;3D model;2D texture;prototype system;soccer scene;computer vision;image media;visual information;Virtual reality;Three dimensional displays;Image reconstruction;Real time systems;Layout;Space technology;Data engineering;Intelligent robots;Cameras;Prototypes},
doi={10.1109/ISMAR.2003.1240701},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240702,
author={N. {Inamoto} and H. {Saito}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Immersive evaluation of virtualized soccer match at real stadium model},
year={2003},
volume={},
number={},
pages={188-197},
abstract={This paper presents a novel observation system for immersive soccer match taken by multiple video cameras at a real stadium. The user sees the soccer field model in front of his/her eyes from the viewpoint through head-mounted display, while the images of players and a soccer ball are also rendered onto the display. For geometric registration between the soccer field model in the real world and the dynamic soccer scene in the rendered images, the viewpoint position of the user is computed by using only natural feature lines in the HMD camera image. Since it is difficult to strongly calibrate the HMD camera and the multiple cameras that capture the real soccer scene, we employ projective geometry for the registration and the rendering. For demonstrating the efficacy of the proposed system, video images of soccer matches taken at real stadium are rendered onto the HMD camera images of a tabletop stadium model. This is a completely new challenge to apply augmented reality to a dynamic event in a large-space.},
keywords={augmented reality;video cameras;helmet mounted displays;three-dimensional displays;rendering (computer graphics);image registration;feature extraction;sport;immersive evaluation;virtualized soccer match;real stadium model;observation system;multiple video cameras;soccer field model;head mounted display;images of players;soccer ball;geometric registration;real world;dynamic soccer scene;rendered images;natural feature lines;camera image;calibration;HMD camera;multiple cameras;projective geometry;rendering;video images;soccer matches;tabletop stadium model;augmented reality;computer vision;Cameras;Rendering (computer graphics);Layout;Virtual reality;Watches;Character generation;Image reconstruction;Displays;Augmented reality;TV broadcasting},
doi={10.1109/ISMAR.2003.1240702},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240703,
author={O. {Bimber} and A. {Grundhofer} and G. {Wetzstein} and S. {Knodel}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Consistent illumination within optical see-through augmented environments},
year={2003},
volume={},
number={},
pages={198-207},
abstract={We present techniques which create a consistent illumination between real and virtual objects inside an application specific optical see-through display: the virtual showcase. We use projectors and cameras to capture reflectance information from diffuse real objects and to illuminate them under new synthetic lighting conditions. Matching direct and indirect lighting effects, such as shading, shadows, reflections and color bleeding can be approximated at interactive rates in such a controlled mixed environment.},
keywords={augmented reality;rendering (computer graphics);three-dimensional displays;computer displays;lighting;hidden feature removal;object-oriented programming;image colour analysis;illumination;optical see-through environments;augmented environments;virtual objects;optical see-through display;virtual showcase;reflectance information;diffuse real objects;synthetic lighting conditions;lighting effects;shading;shadows;reflections;color bleeding;interactive rates;controlled mixed environment;Lighting;Reflectivity;Displays;Cameras;Augmented reality;Rendering (computer graphics);Optical reflection;Hemorrhaging;Hardware;Acceleration},
doi={10.1109/ISMAR.2003.1240703},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240704,
author={K. {Agusanto} and {Li Li} and {Zhu Chuangui} and {Ng Wan Sing}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Photorealistic rendering for augmented reality using environment illumination},
year={2003},
volume={},
number={},
pages={208-216},
abstract={Mixing 3D computer-generated images with real-scene images seamlessly in augmented reality has many desirable and wide areas of applications such as entertainment, cinematography, design visualization and medical trainings. The challenging task is to make virtual objects blend harmoniously into the real scene and appear as if they are like real. Apart from constructing detailed geometric 3D model representation and obtaining accurate surface properties for virtual objects, adopting real scene lighting information to render virtual objects is another important factor to achieve photorealistic rendering. Such a factor not only improves visual complexity of virtual objects, but also determines the consistency of illumination between the virtual objects and the surrounding real objects in the scene. Conventional rendering techniques such as ray tracing, and radiosity require intensive computation and data preparation to solve the lighting transport equation. Hence, they are less practical for rendering virtual objects in augmented reality, which demands a real-time performance. This work explores an image-based and hardware-based approach to improve photorealism for rendering synthetic objects in augmented reality. It uses a recent technique of image-based lighting, environment illumination maps, and a simple yet practical multi-pass rendering framework for augmented reality.},
keywords={augmented reality;rendering (computer graphics);lighting;computer displays;ray tracing;realistic images;photorealistic rendering;augmented reality;3D computer-generated images;real-scene images;entertainment;cinematography;design visualization;medical trainings;virtual objects;image construction;geometric 3D model;image representation;surface properties;lighting information;visual complexity;ray tracing;radiosity;intensive computation;data preparation;lighting transport equation;real-time performance;image-based approach;hardware-based approach;photorealism;synthetic objects;image-based lighting;environment illumination maps;multipass rendering;Augmented reality;Lighting;Rendering (computer graphics);Layout;Application software;Cinematography;Visualization;Biomedical imaging;Solid modeling;Ray tracing},
doi={10.1109/ISMAR.2003.1240704},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240705,
author={S. {Zokai} and J. {Esteve} and Y. {Genc} and N. {Navab}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Multiview paraperspective projection model for diminished reality},
year={2003},
volume={},
number={},
pages={217-226},
abstract={This paper introduces a "diminished reality" technique for removing an object or collection of objects and replacing it with an appropriate background image. Diminished reality can be considered an important part of many mixed and augmented reality applications. Our target application is the use of augmented reality (AR) to revamp procedures in industrial plants. An object or a region of interest is delineated on a single reference image. A paraperspective projection model is used to generate the correct background from multiple calibrated views of the scene. We propose methods to deal with approximately planar backgrounds with different orientations. We also propose a multi-resolution approach to deal with non-planar backgrounds. Different sets of experimental results demonstrate the success and limits of the algorithms. Results on real data from water treatment and power plants show the usefulness of this method for industrial applications.},
keywords={rendering (computer graphics);augmented reality;image processing;computational geometry;industrial plants;solid modelling;realistic images;multiview paraperspective;diminished reality;object removal;background image;augmented reality;target application;industrial plants;reference image;paraperspective projection model;multiple calibrated views;planar backgrounds;water treatment;power plants;industrial applications;rendering;object replacement;Rendering (computer graphics);Augmented reality;Layout;Power generation;Computer science;Pipelines;Image reconstruction;Industrial plants;Medical services;Biomedical equipment},
doi={10.1109/ISMAR.2003.1240705},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240706,
author={N. {Matsushita} and D. {Hihara} and T. {Ushiro} and S. {Yoshimura} and J. {Rekimoto} and Y. {Yamamoto}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={ID CAM: a smart camera for scene capturing and ID recognition},
year={2003},
volume={},
number={},
pages={227-236},
abstract={An ID recognition system is described that uses optical beacons and a high-speed image sensor. The ID sensor captures a scene like an ordinary camera and recognizes the ID of a beacon emitted over a long distance. The ID recognition system has three features. The system is robust to changes in the optical environment, e.g. complete darkness, spotlights, and sunlight. It can recognize up to 255 multiple optical beacons simultaneously. Furthermore, it can recognize beacons even over a long distance, e.g. 40 m indoors and 20 m outdoors. Implementation and evaluation of this ID recognition system showed that a mobile augmented reality system can be achieved by combining this ID recognition system with a PDA and a wireless network.},
keywords={image recognition;notebook computers;mobile computing;wireless LAN;augmented reality;video cameras;ID CAM;camera;scene capturing;ID recognition;optical beacons;image sensor;optical environment;complete darkness;spotlights;sunlight;mobile augmented reality;PDA;wireless network;Computer aided manufacturing;CADCAM;Smart cameras;Layout;High speed optical techniques;Stimulated emission;Optical sensors;Image recognition;Image sensors;Intelligent sensors},
doi={10.1109/ISMAR.2003.1240706},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240707,
author={J. {Zauner} and M. {Haller} and A. {Brandl} and W. {Hartman}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Authoring of a mixed reality assembly instructor for hierarchical structures},
year={2003},
volume={},
number={},
pages={237-246},
abstract={Mixed reality is a very useful and powerful instrument for the visualization of processes, including the assembly process. A Mixed Reality based step-by-step furniture assembly application is introduced. On the one hand context related actions are given to the user to install elements. On the other hand an intuitive way for authors to create new MR based assembly instructions is provided. Our goal is to provide a powerful, flexible and easy-to-use authoring wizard for assembly experts, allowing them to author their new assembly instructor for hierarchical structures. This minimizes the costs for the creation of new mixed reality assembly instructors.},
keywords={authoring systems;augmented reality;computer aided instruction;hypermedia markup languages;data structures;user interfaces;2D representation program;authoring tool;mixed reality;assembly instructor;hierarchical structures;process visualization;assembly process;furniture assembly application;MR based assembly instructions;authoring wizard;assembly experts;Virtual reality;Augmented reality;Assembly systems;Industrial training;Switches;Instruments;Visualization;Costs;Manuals;Computerized monitoring},
doi={10.1109/ISMAR.2003.1240707},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240708,
author={W. {Piekarski} and B. H. {Thomas}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={An object-oriented software architecture for 3D mixed reality applications},
year={2003},
volume={},
number={},
pages={247-256},
abstract={This paper presents a new software architecture for 3D mixed reality applications, named Tinmith-evo5. Currently there are a limited number of existing toolkits for the development of 3D mixed reality applications, each optimized for particular feature but at the detriment of others. Complex interactive user interfaces and applications require extensive supporting infrastructure, and can be hampered by inadequate support. The Tinmith-evo5 architecture is optimised to develop mobile augmented reality and other interactive 3D applications on portable platforms with limited resources. This architecture is implemented in C++ with an object-oriented data flow design, an object store based on the Unix file system model, and uses other ideas from existing previous work.},
keywords={software architecture;augmented reality;software tools;data flow computing;C++ language;user interfaces;Unix;object-oriented software;software architecture;3D mixed reality applications;Tinmith-evo5;software development tools;interactive user interfaces;infrastructure support;optimisation;mobile augmented reality;interactive 3D applications;portable platforms;C++ language;object-oriented data flow;data flow design;Unix file system;file system model;Software architecture;Virtual reality;Application software;User interfaces;Computer architecture;Hardware;Rendering (computer graphics);Wearable computers;Augmented reality;Programming},
doi={10.1109/ISMAR.2003.1240708},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240709,
author={D. {Beier} and R. {Billert} and B. {Bruderlin} and D. {Stichling} and B. {Kleinjohann}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Marker-less vision based tracking for mobile augmented reality},
year={2003},
volume={},
number={},
pages={258-259},
abstract={In this paper an object recognition and tracking approach for the mobile, marker-less and PDA-based augmented reality system AR-PDA is described. For object recognition and localization 2D features are extracted from images and compared with a priori known 3D models. The system consists of a 2D graph matching, 3D hypothesis generation and validation and an additional texture based validation step.},
keywords={augmented reality;object recognition;rendering (computer graphics);image matching;mobile computing;computer vision;notebook computers;object detection;markerless vision based tracking;mobile augmented reality;object recognition;tracking approach;marker-less augmented reality;PDA-based augmented reality;AR-PDA;localization 2D features;image extraction;3D models;2D graph matching;3D hypothesis generation;texture based validation step;Augmented reality;Cameras;Object recognition;Feature extraction;Mobile computing;Layout;Computer graphics;Intelligent systems;Production;Shape},
doi={10.1109/ISMAR.2003.1240709},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240710,
author={T. {Okuma} and T. {Kurata} and K. {Sakaue}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Fiducial-less 3-D object tracking in AR systems based on the integration of top-down and bottom-up approaches and automatic database addition},
year={2003},
volume={},
number={},
pages={260-261},
abstract={We propose a novel fiducial-less 3-D object tracking method. Our method consists of three components: 1) bottom-up approach (BUA), 2) top-down approach (TDA), and 3) automatic database addition (ADA). An experimental result shows an accuracy and robustness of our method.},
keywords={parameter estimation;augmented reality;image matching;object detection;fiducialless 3D object tracking;AR systems;top-down approaches;bottom-up approaches;automatic database addition;Robustness;Computer vision;Augmented reality;Measurement errors;Parameter estimation;Costs;Image databases;Spatial databases;Detectors;History},
doi={10.1109/ISMAR.2003.1240710},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240711,
author={T. {Kawano} and Y. {Ban} and K. {Uehara}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={A coded visual marker for video tracking system based on structured image analysis},
year={2003},
volume={},
number={},
pages={262-263},
abstract={This paper proposes a new kind of visual marker for a wearable computer based augmented reality (AR) system. Various marker systems have already been developed for using with an AR system. However, the improvement is needed to use these marker systems on a wearable AR system in terms of the amount of information or the width of the recognizable area. The main feature of our visual marker is a large amount of information that it can possess. Our marker consists of 32 bits data area with error detection code. This feature enables a wearable AR system to be used in various ways.},
keywords={wearable computers;augmented reality;video coding;data compression;optical tracking;edge detection;image colour analysis;coded visual marker;video tracking system;structured image analysis;wearable computer;augmented reality;AR system;data area;error detection code;recognition algorithm;marker system;Image analysis;Wearable computers;Augmented reality;Space technology;Parity check codes;Labeling;Pixel;Information science;Computer errors;Thyristors},
doi={10.1109/ISMAR.2003.1240711},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240712,
author={ {Yong Liu} and M. {Storring} and T. B. {Moeslund} and C. B. {Madsen} and E. {Granum}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Computer vision based head tracking from re-configurable 2D markers for AR},
year={2003},
volume={},
number={},
pages={264-267},
abstract={This paper presents a computer vision based head tracking system for augmented reality. A camera is attached to a head mounted display and used to track markers in the user's field of view. These markers are movable on a table and used as interface with virtual objects. Furthermore, they are used as landmarks to track the user's viewpoint and viewing direction by a homography based camera pose estimation algorithm. By integrating this computer vision tracker with a commercially available InterSense tracker, we take the advantages of the small jitter of the former one without losing the tracking speed of the later one. For static and slow head motion the system has less than 0.3mm RMS of position jitter and 0.165 degrees RMS of orientation jitter.},
keywords={computer vision;optical tracking;helmet mounted displays;augmented reality;gesture recognition;computer vision;head tracking;reconfigurable 2D markers;augmented reality;camera;head mounted display;user viewfield;user interface;virtual object;viewing direction;homography;pose estimation algorithm;InterSense tracker;tracking speed;head motion;position jitter;orientation jitter;Computer vision;Magnetic heads;Jitter;Cameras;Augmented reality;Optical sensors;Layout;Delay;Laboratories;Computer displays},
doi={10.1109/ISMAR.2003.1240712},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240713,
author={W. {Piekarski} and B. {Avery} and B. H. {Thomas} and P. {Malbezin}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Hybrid indoor and outdoor tracking for mobile 3D mixed reality},
year={2003},
volume={},
number={},
pages={266-267},
abstract={},
keywords={Virtual reality;Cameras;Global Positioning System;Augmented reality;Costs;Application software;Navigation;Wearable computers;Mobile computing;Firewire},
doi={10.1109/ISMAR.2003.1240713},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240714,
author={D. {Roetenberg} and H. {Luinge} and P. {Veltink}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Inertial and magnetic sensing of human movement near ferromagnetic materials},
year={2003},
volume={},
number={},
pages={268-269},
abstract={This paper describes a Kalman filter design to estimate orientation of human body segments by fusing gyroscope, accelerometer and magnetometer signals. Ferromagnetic materials near the sensor disturb the local magnetic field and therefore the orientation estimation. The magnetic disturbance can be detected by looking at the total magnetic density and a magnetic disturbance vector can be calculated. Results show the capability of this filter to correct for magnetic disturbances.},
keywords={Kalman filters;magnetic sensors;gyroscopes;magnetometers;gesture recognition;image motion analysis;inertial sensors;magnetic sensing;human movement;ferromagnetic materials;Kalman filter design;orientation estimation;human body segments;gyroscope fusing;accelerometer;magnetometer signals;magnetic field;magnetic density;magnetic disturbance vector;magnetic disturbances;Magnetic materials;Gyroscopes;Magnetic separation;Accelerometers;Magnetic field measurement;Magnetometers;Magnetic sensors;Magnetic flux;Filters;Signal design},
doi={10.1109/ISMAR.2003.1240714},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240715,
author={S. {Vogt} and A. {Khamene} and F. {Sauer} and A. {Keil} and H. {Niemann}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={A high performance AR system for medical applications},
year={2003},
volume={},
number={},
pages={270-271},
abstract={We report on a new single PC based stereoscopic video-see-through AR system which we developed for medical applications. Recent advances in graphics hardware, memory bandwidth, and computing power of standard PCs made it possible that this system outperforms an earlier version which included 3 networked SGI workstations. We designed and implemented a new AR software platform. It is component based and - in conjunction with XML configuration files - provides efficiency, modularity, and extensibility for fast and robust prototyping of AR applications. The system has a compelling real-time performance with 30 frames/second, displaying stereoscopic augmented video views with XGA resolution.},
keywords={augmented reality;medical computing;video cameras;software architecture;XML;object-oriented programming;AR system;medical applications;stereoscopic video-see-through AR;graphics hardware;memory bandwidth;computing power;networked SGI workstations;AR software platform;XML configuration files;modularity;extensibility;prototyping;real-time performance;stereoscopic augmented video views;XGA resolution;AR visualization;head mounted display;Medical services;Biomedical equipment;Graphics;Hardware;Bandwidth;Computer networks;Personal communication networks;Workstations;XML;Robustness},
doi={10.1109/ISMAR.2003.1240715},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240716,
author={R. J. {Lapeer} and A. C. {Tan} and A. {Linney} and G. {Alusi}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Stereo depth assessment experiment for microscope-based surgery},
year={2003},
volume={},
number={},
pages={272-273},
abstract={We present experimental data on the use of autostereoscopic displays as complementary visualization aids to the surgical stereo microscope for augmented reality surgery. Five experts in the use of the microscope, and five non-experts, performed a depth experiment to assess stereo cues as provided by two autostereoscopic displays (DTI 2015XLS Virtual Window and SHARP micro-optic twin), the surgical microscope and the "naked" eye.},
keywords={medical computing;augmented reality;stereo image processing;optical microscopes;surgery;computer displays;stereo depth assessment;experimental data;autostereoscopic displays;visualization aids;surgical stereo microscope;augmented reality surgery;stereo cues;DTI 2015XLS Virtual Window;SHARP microoptic twin;surgical microscope;naked eye;microscope-based surgery;Surgery;Diffusion tensor imaging;Augmented reality;Computer displays;Visualization;Control systems;Biomedical optical imaging;Optical crosstalk;Optical microscopy;Liquid crystal displays},
doi={10.1109/ISMAR.2003.1240716},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240717,
author={T. {Reicher} and A. {Mac Williams} and B. {Brugge} and G. {Klinker}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Results of a study on software architectures for augmented reality systems},
year={2003},
volume={},
number={},
pages={274-275},
abstract={Research prototypes in AR usually do not emphasize software architecture. Nevertheless, their architecture is not arbitrary, but results from specific needs. Architectural approaches embodying research contributions are of particular value for reuse at component and architectural levels. We have conducted a study of existing AR software architectures for the ARVIKA project by W. Friedrich et al (2001). This has resulted in a catalog of important desired quality attributes for AR systems, a reference architecture for comparison of AR architectures, and a catalog of architectural approaches used in current AR systems. We believe this lays the foundation for further research in AR software architectures.},
keywords={software architecture;augmented reality;software prototyping;object-oriented programming;software architectures;augmented reality systems;research prototypes;software architecture;architectural approaches;research contributions;component levels;architectural levels;ARVIKA;quality attributes;AR systems;Computer architecture;Augmented reality;Software architecture;Context modeling;Software prototyping;Information systems;Image analysis;Bars;Wire;Assembly},
doi={10.1109/ISMAR.2003.1240717},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240718,
author={W. {Pasman} and C. {Woodward}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Implementation of an augmented reality system on a PDA},
year={2003},
volume={},
number={},
pages={276-277},
abstract={We present a client/server implementation for running demanding mobile AR application on a PDA device. The system incorporates various data compression methods to make it run as fast as possible on a wide range of communication networks, from GSM to WLAN.},
keywords={mobile computing;client-server systems;augmented reality;notebook computers;wireless LAN;data compression;augmented reality system;PDA;client-server implementation;mobile AR application;data compression;communication networks;GSM;WLAN;handheld displays;camera;personal digital assistant;Augmented reality;Image coding;GSM;Wireless LAN;Displays;Cameras;Decoding;Hardware;Software architecture;Rendering (computer graphics)},
doi={10.1109/ISMAR.2003.1240718},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240719,
author={S. {Hirooka} and H. {Saito}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Displaying digital documents on real paper surface with arbitrary shape},
year={2003},
volume={},
number={},
pages={278-279},
abstract={In this paper, we propose a system that displays digital components on real paper surface with arbitrary shape, so that the viewer can feel as if the digital document images are printed on the real paper surface. Such displaying of the digital documents is realized by rendering the document images on the arbitrary shaped surface via a projector. We apply a holography between a source image plane and a projector image plane to render the images on the surface. For adapting the arbitrary shape of the surface, we divide the shaped surface into many small rectangular regions, and generate warp images of each region by calculating this holography of the plane of each divided region. By protecting the warp image on the real surface, the image can be observed as if the image is printed on the surface. Since the system always compute the holography of each divided region, the image can be aligned onto the surface even the surface moves.},
keywords={rendering (computer graphics);document image processing;image segmentation;digital documents;paper surface;digital components;arbitrary shape;digital images;image rendering;projector;holography;image plane;rectangular regions;warp images;real surface;Shape;Books;Rendering (computer graphics);Cameras;Image generation;Liquid crystal displays;Computer science;Computer displays;Cultural differences;Cathode ray tubes},
doi={10.1109/ISMAR.2003.1240719},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240720,
author={J. {Sands} and S. W. {Lawson}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Towards a usable stereoscopic augmented reality interface for the manipulation of virtual cursors},
year={2003},
volume={},
number={},
pages={280-281},
abstract={The combination of augmented reality (AR) systems and stereoscopic display devices has created a powerful tool with which to "supplement" our view. One application for such systems is for the augmented teleoperation of unmanned vehicles, deployed in remote or hazardous environments. Research in this area has highlighted the need for accurate 3D measurement techniques - such as through the use of virtual cursors. This paper describes our work in the development of an AR interface designed to assist the accurate selection of position in 3D space. We describe some preliminary experimental work using virtual cursors before discussing how we believe depth cues can be utilized to allow a user to make a more informed judgment of depth in unprepared environments. It is expected that the guidelines outlined in this report will be used as a benchmark for the development of further 3D ARA cursors.},
keywords={stereo image processing;augmented reality;graphical user interfaces;stereoscopic augmented reality;augmented reality interface;virtual cursors;AR systems;stereoscopic display devices;augmented teleoperation;unmanned vehicles;remote environments;3D measurement techniques;AR interface;3D space;depth cues;benchmark;3D ARA cursors;Augmented reality;Computer displays;Computer interfaces;Surface texture;Vehicles;Measurement techniques;Guidelines;Layout;Three dimensional displays;Mice},
doi={10.1109/ISMAR.2003.1240720},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240721,
author={C. {Loscos} and H. R. {Widenfeld} and M. {Roussou} and A. {Meyer} and F. {Tecchia} and G. {Drettakis} and E. {Gallo} and A. R. {Martinez} and N. {Tsingos} and Y. {Chrysanthou} and L. {Robert} and M. {Bergamasco} and A. {Dettori} and S. {Soubra}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={The CREATE project: mixed reality for design, education, and cultural heritage with a constructivist approach},
year={2003},
volume={},
number={},
pages={282-283},
abstract={The global scope of the CREATE project is to develop a mixed-reality framework that enables highly interactive real-time construction and manipulation of photo-realistic, virtual worlds based on real data sources. This framework will be tested and applied to cultural heritage content in an educational context, as well as to the design and review of architectural/urban planning settings. The evaluation of the project is based on a human-centered, constructivist approach to working and learning, with special attention paid to the evaluation of the resulting mixed reality experience. Through this approach, participants in an activity "construct" their own knowledge by testing ideas and concepts based on their prior knowledge and experience, applying these to a new situation, and integrating the new knowledge gained with pre-existing intellectual constructs. CREATE project uses a high degree of interactivity, and includes provision for other senses (haptics and sound). The application developed in CREATE are designed to run on different platforms, and the targeted running systems are SGI and PC driven, with immersive stereo-displays such as a workbench, a ReaCTor (CAVE-like environment), and a wide projection screen.},
keywords={humanities;town and country planning;education;virtual reality;haptic interfaces;realistic images;CREATE project;constructivist approach;mixed-reality framework;interactive real-time construction;photo-realistic virtual worlds;real data sources;cultural heritage content;educational context;architectural planning;urban planning;human-centered approach;mixed reality experience;intellectual constructs;running systems;stereo displays;workbench;ReaCTor;CAVE-like environment;projection screen;haptic interface;Virtual reality;Cultural differences;Layout;Displays;Testing;Haptic interfaces;Vegetation mapping;Geometry;Cities and towns;Educational institutions},
doi={10.1109/ISMAR.2003.1240721},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240722,
author={T. {Ohshima} and T. {Kuroki} and H. {Yamamoto} and H. {Tamura}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={A mixed reality system with visual and tangible interaction capability: application to evaluating automobile interior design},
year={2003},
volume={},
number={},
pages={284-285},
abstract={This paper presents a mixed reality (MR) system with tangible interface as well as visual fusion in an MR space. The sense of touch is given by physical objects on which computer generated imagery is accurately registered and superimposed. The proposed approach is especially useful in industrial design where digital mockups and physical mockups are thoroughly utilized.},
keywords={augmented reality;haptic interfaces;image registration;automobile industry;computer graphics;mixed reality;visual interaction;tangible interaction;automobile interior design;interior design evaluation;MR system;tangible interface;visual fusion;MR space;computer generated imagery;industrial design;digital mockups;physical mockups;user interface;haptic interface;object registration;Virtual reality;Automobiles;Image generation;Augmented reality;Haptic interfaces;Character generation;Shape;Feedback;Laboratories;Physics computing},
doi={10.1109/ISMAR.2003.1240722},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240723,
author={T. {Nakatsura} and Y. {Yokokohji} and D. {Eto} and T. {Yoshikawa}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Image overlay on optical see-through displays for vehicle navigation},
year={2003},
volume={},
number={},
pages={286-287},
abstract={In this paper, we propose a method for image overlay on the front glass of a vehicle to navigate a driver to a desired destination. By overlaying the navigation information on the front glass, the driver need not gaze at the console panel. Therefore, accidents caused by gazing at console panel could be reduced. To overlay the image accurately on the target object through the front glass, both the vehicle's position/orientation and the driver's position/information are estimated by vision-based tracking and measuring angular velocities of the vehicle's wheels. Experimental results show the validity of the proposed method.},
keywords={navigation;computer vision;image registration;augmented reality;road vehicles;image overlay;optical see-through displays;vehicle navigation;front glass;desired destination;navigation information;console panel;target object;vehicle position;vehicle orientation;driver position;driver information;vision-based tracking;angular velocities measurement;vehicle wheels;car navigation systems;helmet mounted display;virtual image;Displays;Navigation;Glass;Vehicle driving;Accidents;Target tracking;Position measurement;Velocity measurement;Angular velocity;Wheels},
doi={10.1109/ISMAR.2003.1240723},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240724,
author={H. {Maeda} and K. {Hirose} and J. {Yamashita} and K. {Hirota} and M. {Hirose}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={All-around display for video avatar in real world},
year={2003},
volume={},
number={},
pages={288-289},
abstract={This paper describes the methodology and prototype for an all-around display system for video avatar presentation in the real world. This system enables us to reconstruct a video avatar which users can look at from all around, by spinning a flat panel display which has a small viewing angle and changing the image on the display panel depending on the display's orientation.},
keywords={virtual reality;stereo image processing;video signal processing;computer displays;all-around display;real world;methodology;prototype;video avatar presentation;flat panel display;viewing angle;display panel;display orientation;telepresence;Avatars;Flat panel displays;Image reconstruction;Spinning;Motion pictures;Two dimensional displays;Prototypes;Information science;Humans;Virtual environment},
doi={10.1109/ISMAR.2003.1240724},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240725,
author={H. {Regenbrecht} and C. {Ott} and M. {Wagner} and T. {Lum} and P. {Kohler} and W. {Wilke} and E. {Mueller}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={An augmented virtuality approach to 3D videoconferencing},
year={2003},
volume={},
number={},
pages={290-291},
abstract={This paper describes the concept, prototypical implementation, and usability evaluation of an augmented virtuality (AV) based videoconferencing (VC) system: "cAR/PE!". We present a first solution which allows three participants at different locations to communicate over a network in an environment simulating a traditional face-to-face meeting. Integrated into the AV environment are live video streams of the participants spatially arranged around a virtual table, a large virtual presentation screen for 2D display and application sharing, and 3D geometry (models) within the room and on top of the table.},
keywords={teleconferencing;groupware;virtual reality;augmented reality;multimedia systems;augmented virtuality;3D videoconferencing;prototypical implementation;usability evaluation;AV based videoconferencing;VC system;cAR/PE!;communication network;face-to-face meeting;AV environment;video streams;virtual table;virtual presentation screen;2D display;application sharing;3D geometry models;distant computer supported collaborative work;CSCW;Augmented virtuality;Teleconferencing;Virtual prototyping;Usability;Virtual colonoscopy;Video sharing;Streaming media;Two dimensional displays;Geometry;Solid modeling},
doi={10.1109/ISMAR.2003.1240725},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240726,
author={S. {Ikeda} and T. {Sato} and M. {Kanbara} and N. {Yokoya}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Telepresence system using high-resolution omnidirectional movies and a reactive display},
year={2003},
volume={},
number={},
pages={292-293},
abstract={This paper describes a novel telepresence system that uses high-resolution movies and a reactive display system with a treadmill. In this system, users can walk through a virtualized environment by actually walking on a treadmill. According to walking motion which is detected by using 3-D position sensors put on both legs, the virtualized environment captured by an omnidirectional multi-camera system is projected on a multi-screen display.},
keywords={cameras;computer displays;augmented reality;position measurement;sensor fusion;telepresence system;high-resolution movies;omnidirectional movies;reactive display;treadmill;virtualized environment;walking motion;3D position sensors;omnidirectional multi-camera;multiscreen display;immersive screen;motion detection;Motion pictures;Displays;Legged locomotion;Graphics;Motion detection;Automatic control;Personal communication networks;Sensor systems;Leg;Belts},
doi={10.1109/ISMAR.2003.1240726},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240727,
author={A. {Woolard} and V. {Laliodati} and N. {Hedley} and N. {Carrigan} and M. {Hammond} and J. {Julien}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Case studies in application of augmented reality in future media production},
year={2003},
volume={},
number={},
pages={294-295},
abstract={In this application-based poster, we describe three case studies about potential application of augmented reality (AR) in the broadcasting and entertainment industry. The poster covers the potential impact on BBC's principal objectives to "entertain, educate and inform" in a variety of environments such as broadcast studios, classrooms and in the home.},
keywords={augmented reality;multimedia computing;entertainment;broadcasting;educational computing;case studies;augmented reality;media production;application-based poster;broadcasting industry;entertainment industry;BBC;principal objectives;news information;science education;broadcast studios;classrooms;home;Computer aided software engineering;Augmented reality;Production;Broadcasting;Multimedia communication;Animation;Displays;Broadcast technology;Research and development;Textile industry},
doi={10.1109/ISMAR.2003.1240727},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240728,
author={V. {Sundareswaran} and K. {Wang} and S. {Chen} and R. {Behringer} and J. {McGee} and C. {Tam} and P. {Zahorik}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={3D audio augmented reality: implementation and experiments},
year={2003},
volume={},
number={},
pages={296-297},
abstract={Augmented reality (AR) presentations may be visual or auditory. Auditory presentation has the potential to provide hands-free and visually non-obstructing cues. Recently, we have developed a 3D audio wearable system that can be used to provide alerts and informational cues to a mobile user in such a manner as to appear to emanate from specific locations in the user's environment. In order to study registration errors in 3D audio AR representations, we conducted a perceptual training experiment in which visual and auditory cues were presented to observers. The results of this experiment suggest that perceived registration errors may be reduced through head movement and through training presentations that include both visual and auditory cues.},
keywords={augmented reality;audio-visual systems;user modelling;three-dimensional displays;3D audio augmented reality;auditory presentation;nonobstructing cues;3D audio wearable system;informational cues;mobile user;user environment;registration errors;audio AR representations;perceptual training experiment;visual cues;auditory cues;head movement;training presentations;Augmented reality;Magnetic heads;Navigation;Position measurement;Global Positioning System;Engines;Auditory displays;Feedback;Three dimensional displays;Application software},
doi={10.1109/ISMAR.2003.1240728},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240729,
author={S. {Di Verdi} and D. {Nurmi} and T. {Hollerer}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={ARWin - a desktop augmented reality Window Manager},
year={2003},
volume={},
number={},
pages={298-299},
abstract={We present ARWin, a single user 3D augmented reality desktop. We explain our design considerations and system architecture and discuss a variety of applications and interaction techniques designed to take advantage of this new platform.},
keywords={augmented reality;graphical user interfaces;ARWin;desktop augmented reality;Window Manager;user 3D augmented reality;system architecture;interaction techniques;desktop computer;graphical user interface;Augmented reality;Application software;Computer displays;Cameras;Clocks;Computer architecture;Keyboards;Mice;Computer science;Computer graphics},
doi={10.1109/ISMAR.2003.1240729},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240730,
author={A. {Olwal} and H. {Benko} and S. {Feiner}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={SenseShapes: using statistical geometry for object selection in a multimodal augmented reality},
year={2003},
volume={},
number={},
pages={300-301},
abstract={We introduce a set of statistical geometric tools designed to identify the objects being manipulated through speech and gesture in a multimodal augmented reality system. SenseShapes are volumetric regions of interest that can be attached to parts of the user's body to provide valuable information about the user's interaction with objects. To assist in object selection, we generate a rich set of statistical data and dynamically choose which data to consider based on the current situation.},
keywords={augmented reality;speech recognition;audio user interfaces;computational geometry;haptic interfaces;speech-based user interfaces;SenseShapes;statistical geometry;object selection;multimodal augmented reality;geometric tools;speech identification;gesture identification;volumetric regions;user body;user interaction;statistical data;Geometry;Augmented reality;Head;Computer science;Buildings;Fusion power generation;Microphones;Monitoring;Speech recognition;Optical sensors},
doi={10.1109/ISMAR.2003.1240730},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240731,
author={R. {Grasset} and J. -. {Gascuel} and {Schmalstieg}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Interactive mediated reality},
year={2003},
volume={},
number={},
pages={302-303},
abstract={Mediated reality describes the concept of filtering or vision of reality, typically using a head-worn video mixing display. In this paper, we propose a generalized concept and new tools for interactively mediated reality. We present also our first prototype system for painting, grabbing and gluing together real and virtual elements.},
keywords={augmented reality;computer graphics;helmet mounted displays;human computer interaction;graphical user interfaces;interactive mediated reality;reality filtering;reality vision;head-worn display;video mixing display;prototype system;computer graphics;painting tool;grab tool;glue tool;Painting;Augmented reality;Computer graphics;Geometry;Brushes;Video sharing;Displays;Virtual prototyping;Information filtering;Information filters},
doi={10.1109/ISMAR.2003.1240731},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240732,
author={M. {Cavazza} and O. {Martin} and F. {Charles} and X. {Marichal} and S. J. {Mead}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={User interaction in mixed reality interactive storytelling},
year={2003},
volume={},
number={},
pages={304-305},
abstract={In this paper, we describe a mixed reality system based on a "magic mirror" model, in which the user's image is captured in real time by a video camera, extracted from his/her background and mixed with a 3D graphic model of a virtual image including the synthetic characters taking part in the story. The resulting image is projected on a large screen facing the user, who sees his/her own image embedded in the virtual stage with the synthetic actors. The graphic component of the mixed reality world is based on a game engine, Unreal Tournament 2003. This engine not only performs graphic rendering and character animation but incorporates a new version of our previously described storytelling engine. A single 2D camera facing the user analyses the image in real-time by segmenting the user's contours.},
keywords={user interfaces;augmented reality;cameras;entertainment;computer animation;rendering (computer graphics);edge detection;user interaction;mixed reality;interactive storytelling;magic mirror model;user image;real time;video camera;3D graphic model;virtual image;synthetic characters;virtual stage;synthetic actors;graphic component;game engine;Unreal Tournament 2003;graphic rendering;character animation;storytelling engine;user contour;Virtual reality;Graphics;Engines;Cameras;Mirrors;Real time systems;Rendering (computer graphics);Animation;Image analysis;Image segmentation},
doi={10.1109/ISMAR.2003.1240732},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240733,
author={H. {Kato} and T. {Naemura} and H. {Harashima}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Graphic shadow: augmenting your shadow on the floor},
year={2003},
volume={},
number={},
pages={306-307},
abstract={This paper proposes real-time interactive systems illustrating your shadows cast on the floor, which we name "graphic shadow." They will make you experience an exciting space of interaction performing an illusion on your own shadows.},
keywords={augmented reality;floors;lighting;cameras;computer graphics;human computer interaction;graphic shadow;augmented reality;real-time interactive systems;graphical illusion;textured shadows;pictorial shadows;Graphics;Cameras;Humans;Space technology;Light sources;Image converters;Layout;Information science;Real time systems;Interactive systems},
doi={10.1109/ISMAR.2003.1240733},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240734,
author={D. {Aiteanu} and B. {Hillers} and A. {Graser}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={A step forward in manual welding: demonstration of augmented reality helmet},
year={2003},
volume={},
number={},
pages={309-310},
abstract={A new welding helmet for the manual welding process has been developed. The welders working conditions are improved by augmenting the visual information before and during welding. The image is improved by providing a better view of the working area. An online quality assistant is available during welding, suggesting the correction of the guns position or pointing out welding errors, by analyzing the electrical welding parameters. An assembly advisor will suggest the assembly sequence, by displaying the type and the position of the following piece into the actual ensemble. In addition, an available online documentation of the welding process gives an opportunity to reduce the effort of post process quality assurance which often uses expensive X-ray investigations.},
keywords={welding;augmented reality;helmet mounted displays;computer based training;user interfaces;computer vision;manual welding;augmented reality helmet;welding process;visual information;online quality assistant;guns position;welding errors;electrical welding parameters;assembly advisor;assembly sequence;actual ensemble;online documentation;x-ray investigations;Welding;Augmented reality;Documentation;Protection;Employee welfare;Computer aided manufacturing;Assembly systems;Robotic assembly;Cameras;Glass},
doi={10.1109/ISMAR.2003.1240734},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240735,
author={H. {Kruger} and L. {Klingbeil} and E. {Kraft} and R. {Hamburger}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={BlueTrak - a wireless six degrees of freedom motion tracking system},
year={2003},
volume={},
number={},
pages={311-312},
abstract={We resent six degrees of freedom tracking system, which is wireless and scalable concerning the tracking volume and the number of devices being tracked. This is achieved by the modular design of the system consisting of two different types of modules: an arbitrary number of tracked user modules and a number of fixed reference modules. It provides a flexible setup for head tracking in virtual and augmented reality environments and for various other applications such as motion capture and analysis. The system combines inertial sensor data with ultrasonic ranging measurements to determine orientation and absolute position.},
keywords={augmented reality;optical tracking;motion estimation;image sensors;computer vision;position measurement;BlueTrak;six degrees freedom;motion tracking system;wireless tracking system;tracking volume;modular design;tracked user modules;fixed reference modules;flexible setup;head tracking;virtual reality;augmented reality;motion capture;motion analysis;inertial sensor data;ultrasonic ranging measurements;orientation determination;absolute position;Tracking;Ultrasonic variables measurement;Pulse measurements;Augmented reality;Sensor systems;Universal Serial Bus;Bluetooth;Motion analysis;Position measurement;Collaboration},
doi={10.1109/ISMAR.2003.1240735},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240736,
author={H. {Najafi} and G. {Klinker}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Model-based tracking with stereovision for AR},
year={2003},
volume={},
number={},
pages={313-314},
abstract={This demo shows a robust model-based tracker using stereovision. The combined use of a 3D model with stereoscopic analysis allows accurate pose estimation in the presence of partial occlusions by non rigid objects like the hands of the user. Furthermore, using a second camera improves the stability of tracking and also simplifies the algorithm.},
keywords={optical tracking;stereo image processing;hidden feature removal;cameras;augmented reality;position measurement;model-based tracking;stereovision;3D model;stereoscopic analysis;pose estimation;partial occlusions;camera;tracking stability;algorithm;visual tracking;marker-less tracking;Cameras;Target tracking;Robustness;Stability;Karhunen-Loeve transforms;Computer science;Image segmentation;Optical feedback;Image motion analysis;Feedback loop},
doi={10.1109/ISMAR.2003.1240736},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240737,
author={A. J. {Davison} and W. W. {Mayol} and D. W. {Murray}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Real-time workspace localisation and mapping for wearable robot},
year={2003},
volume={},
number={},
pages={315-316},
abstract={This demo showcases breakthrough results in the general field real-time simultaneous localization and mapping (SLAM) using vision and in particular its vital role in enabling a wearable robot to assists its user. In our approach, a wearable active vision system ("wearable robot") is mounted at the shoulder. As the wearer moves around his environment, typically browsing a workspace in which a task must be completed, the robot acquires images continuously and generates a map of natural visual features on-the-fly while estimating its ego-motion.},
keywords={active vision;real-time systems;cameras;user interfaces;robot vision;real-time localisation;workspace localisation;wearable robot;real-time localization;wearable active vision;image acquisition;visual features;ego-motion estimating;SLAM;real-time mapping;Robot vision systems;Cameras;Robot kinematics;Simultaneous localization and mapping;Layout;Collaborative work;Biomedical monitoring;Robotics and automation;Graphics;Collaborative software},
doi={10.1109/ISMAR.2003.1240737},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240738,
author={W. {Piekarski} and B. H. {Thomas}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Tinmith - mobile outdoor augmented reality modelling demonstration},
year={2003},
volume={},
number={},
pages={317-318},
abstract={This paper outlines some of the capabilities of the Tinmith-Metro modeling system, based on mobile outdoor augmented reality technology. This system implements a new user interface based on tracked pinch gloves and a series of techniques named construction at a distance for the capture and creation of 3D geometry. The user controls the modeling process using hand and head motions, with modelling accuracy guided by the requirements of the user. Using Tinmith-Metro, users are able to model outdoor geometry representing buildings and natural features in an intuitive fashion.},
keywords={augmented reality;data gloves;computational geometry;gesture recognition;mobile augmented reality;augmented reality modelling demonstration;Tinmith-Metro;modeling system;outdoor augmented reality;user interface;tracked pinch gloves;3D geometry capture;3D geometry creation;modeling process;outdoor geometry;natural features;intuitive fashion;Augmented reality;User interfaces;Virtual reality;Avatars;Wearable computers;Information geometry;Visualization;Fingers;Laboratories;Information science},
doi={10.1109/ISMAR.2003.1240738},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240739,
author={T. {Pettersen} and J. {Pretlove} and C. {Skourup} and T. {Engedal} and T. {Lokstad}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Augmented reality for programming industrial robots},
year={2003},
volume={},
number={},
pages={319-320},
abstract={Existing practice for programming robots involves teaching it a sequence of waypoints in addition to process-related events, which defines the complete robot path. The programming process is time consuming, error prone and, in most cases, requires several iterations before the program quality is acceptable. By introducing augmented reality technologies in this programming process, the operator gets instant real-time, visual feedback of a simulated process in relation to the real object, resulting in reduced programming time and increased quality of the resulting robot program. This paper presents a demonstrator of a standalone augmented reality pilot system allowing an operator to program robot waypoints and process specific events related to paint applications. During the programming sequence, the system presents visual feedback of the paint result for the operator, allowing him to inspect the process result before the robot has performed the actual task.},
keywords={augmented reality;industrial robots;robot programming;path planning;augmented reality;industrial robots;programming robots;process-related events;robot path;programming process;time consuming;error prone;program quality;real-time feedback;visual feedback;simulated process;real object;programming time;robot program;AR pilot system;robot waypoints;process specific events;paint applications;programming sequence;process inspection;computer-animated design;Augmented reality;Robot programming;Service robots;Educational robots;Paints;Robot kinematics;Feedback;Application software;Design automation;Cameras},
doi={10.1109/ISMAR.2003.1240739},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240740,
author={T. {Karitsuka} and K. {Sato}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={A wearable mixed reality with an on-board projector},
year={2003},
volume={},
number={},
pages={321-322},
abstract={One of the methods achieving mixed reality (MR) displays is the texture projection method using projectors. Another kind of emerging information environment is a wearable information device, which realizes ubiquitous computing. It is very promising to integrate these technologies. Using this kind of fusion system, two or more users can get the same MR environments without using HMD at the same moment. In this demonstration, we propose a wearable MR system with an on-board projector and introduce some applications with this system.},
keywords={wearable computers;augmented reality;helmet mounted displays;ubiquitous computing;wearable mixed reality;on-board projector;MR displays;texture projection;projectors;information environment;wearable information device;ubiquitous computing;fusion system;MR environments;wearable MR system;head mounted displays;Virtual reality;Wearable computers;Prototypes;Application software;Computer displays;Optical sensors;Physics computing;Cameras;Fingers;Identity management systems},
doi={10.1109/ISMAR.2003.1240740},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240741,
author={K. {Nakayama} and K. {Sato}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={A LCD cube transporting high dynamic range light environments},
year={2003},
volume={},
number={},
pages={323-324},
abstract={In the MR world, it needs to overlay virtual objects onto real scene with harmonizing the photometric consistency. We developed a new illumination device "LCD cube" for keeping photometric consistency. In the demonstration, we plan to present the method to display high dynamic range light environments by "LCD cube" for image-based lighting. In addition, we performed light reproduction in real-time.},
keywords={lighting;augmented reality;liquid crystal displays;computer graphics;LCD cube;light environments;mixed reality;virtual object overlay;photometric consistency;illumination device;image-based lighting;light reproduction;Dynamic range;Lighting;Charge-coupled image sensors;Photometry;Liquid crystal displays;Charge coupled devices;Light sources;Mirrors;Degradation;Layout},
doi={10.1109/ISMAR.2003.1240741},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240742,
author={M. {Diaz} and E. {Hernandez} and L. {Escalona} and I. {Rudomin} and D. {Rivera}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Capturing water and sound waves to interact with virtual nature},
year={2003},
volume={},
number={},
pages={325-326},
abstract={To improve the interaction of people with virtually generated environments we need first to break the barriers that prevent the user from getting an experience as close to reality as possible. The main problem is to give the user the sensation that his/her presence affects the virtual world and then to let the user perceive that the actions he/she takes on the real world can change the virtual one in a smooth natural way in order to achieve virtual biofeedback. Interacting with virtual nature can transport us out from reality. We developed an interactive application with two interfaces on which the user can pretend to be the wind and interact with a virtual pond. Our application makes the user believe and feel that he/she is modifying a 3D virtual pond by the interaction with a small water receptacle in the real world using innovative wave-sensing device. The user, by speaking to a microphone, can also interact with a virtual tree by making it move according to his/her wishes. The physics for the tree are calculated to present the user's action as a wind force making this an entertaining experience.},
keywords={virtual reality;human computer interaction;realistic images;water wave;wave capture;sound wave;virtual nature;user sensation;virtual biofeedback;interactive application;3D virtual pond;water receptacle;wave-sensing device;microphone;virtual tree;user action;human-computer interaction;virtual reality hardware;immersive systems;ambient media;tangible user interface;Biological control systems;Virtual reality;Hardware;Cities and towns;Computer science;Microphones;Physics;User interfaces;Virtual environment;Keyboards},
doi={10.1109/ISMAR.2003.1240742},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240743,
author={K. {Kobayashi} and M. {Hirano} and A. {Narita} and H. {Ishii}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={IP network designer: interface for IP network simulation},
year={2003},
volume={},
number={},
pages={327-328},
abstract={In this demonstration, we present IP network designer: interfaces for IP network simulation. The IP network designer consists of two subsystems: the IP network design workbench and a 3D simulator. IP network design workbench is intended to support a collaborative design and simulation of an IP network by a group of network designers and their customers. This system is based on a tangible user interface platform called "sensetable" and allows users to directly manipulate network topologies. Users can control parameters of nodes and links using physical pucks on the sensing table and simultaneously see the simulation results projected onto the table. 3D simulator provides a 3D view of simulation results. Users can see traffic packets flow as if they are inside the network. This system allows users to understand network behavior intuitively.},
keywords={network topology;IP networks;user interfaces;digital simulation;groupware;network parameters;IP network;network designer;network simulation;network design workbench;3D simulator;collaborative design;network designers;tangible user interface;sensetable;network topologies;parameter control;node parameters;physical pucks;sensing table;traffic packet flow;network behavior;tangible user interfaces;IP networks;Computational modeling;Network topology;Traffic control;Liquid crystal displays;Computer architecture;Collaborative work;User interfaces;Communication system traffic control;Network servers},
doi={10.1109/ISMAR.2003.1240743},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240744,
author={B. {MacIntyre} and M. {Gandy} and J. {Bolter} and S. {Dow} and B. {Hannigan}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={DART: the Designer's Augmented Reality Toolkit},
year={2003},
volume={},
number={},
pages={329-330},
abstract={This demonstration highlights the Designer's Augmented Reality Toolkit (DART), a system that allows users to easily create augmented reality (AR) experiences. Over the past year our research has focused on the creation of this toolkit that can be used by technologists, designers, and students alike to rapidly prototype AR applications. Current approaches to AR development involve extensive programming and content creation as well as knowledge of technical topics involving cameras, trackers, and 3D geometry. The result is that it is very difficult even for technologists to create AR experiences. Our goal was to eliminate these obstacles that prevent such users from being able to experiment with AR. The DART system is based on the Macromedia Director multimedia-programming environment, the de facto standard for multimedia content creation. DART uses the familiar Director paradigms of a score, sprites and behaviors to allow a user to visually create complex AR applications. DART also provides low-level support for the management of trackers, sensors, and camera via a Director plug-in Xtra. This demonstration will show the wide range of AR and other types of multimedia applications that can be created with DART, and visitors will have the opportunity to use DART to create their own experiences.},
keywords={augmented reality;software tools;multimedia computing;visual programming;programming environments;Designer Augmented Reality Toolkit;application prototyping;AR application;AR development;extensive programming;content creation;cameras;trackers;3D geometry;DART system;Macromedia Director;multimedia-programming environment;multimedia content;Director paradigms;low-level support;Director plug-in Xtra;multimedia applications;Augmented reality;Prototypes;Testing;Cameras;Programming profession;Geometry;Multimedia systems;Sprites (computer);Human computer interaction},
doi={10.1109/ISMAR.2003.1240744},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240745,
author={G. {Reitmayr} and M. {Billinghurst} and D. {Schmalstieg}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={WireAR - legacy applications in augmented reality},
year={2003},
volume={},
number={},
pages={331-332},
abstract={Current augmented reality (AR) applications require that the application software be written to support a specific AR interface set up. WireAR was developed to enable output from any OpenGL application to be viewed in an AR fashion. This enables the output from any legacy graphical or scientific visualization applications to be viewed in a collaborative AR setting. This demonstration shows how the output of standard desktop visualization programs can be embedded into an augmented reality experience.},
keywords={augmented reality;user interfaces;groupware;WireAR;legacy applications;augmented reality;AR applications;application software;AR interface;OpenGL;graphical applications;scientific applications;visualization applications;collaborative AR;desktop programs;visualization programs;Computer Supported Collaborative Work;Augmented reality;Application software;Rendering (computer graphics);Visualization;User interfaces;Geometry;Chromium;Layout;Collaborative work;Computer interfaces},
doi={10.1109/ISMAR.2003.1240745},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240746,
author={I. {Barakonyi} and T. {Fahmy} and D. {Schmalstieg} and K. {Kosina}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Collaborative work with volumetric data using augmented reality},
year={2003},
volume={},
number={},
pages={333-334},
abstract={The augmented reality videoconferencing system is a novel remote collaboration tool combining a desktop-based AR system and a videoconferencing module. The novelty of our system is the combination of these tools i.e. superimposing AR applications on live video background displaying the conference parties' real environment, thus merging the advantages of videoconferencing (natural face-to-face communication) and AR (interaction with distributed virtual objects using tangible physical artifacts). We demonstrate the system's collaborative features with a volume rendering application that allows users to display and examine volumetric data simultaneously and to highlight or explore slices of the volume by manipulating an optical marker as a cutting plane interaction device.},
keywords={groupware;teleconferencing;augmented reality;software tools;data visualisation;rendering (computer graphics);collaborative work;volumetric data;augmented reality;videoconferencing system;remote collaboration tool;desktop-based AR system;videoconferencing module;AR applications;live video background;face-to-face communication;distributed virtual objects;tangible physical artifacts;collaborative features;volume rendering application;optical marker;interaction device;Computer Supported Collaborative Work;Collaborative work;Augmented reality;Teleconferencing;Videoconference;Application software;Computer displays;Collaboration;Data mining;Collaborative tools;Biomedical optical imaging},
doi={10.1109/ISMAR.2003.1240746},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240747,
author={D. {Wagner} and I. {Barakonyi}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Augmented reality kanji learning},
year={2003},
volume={},
number={},
pages={335-336},
abstract={ARToolKit programmers are familiar with the kanji symbols supplied with the distribution. Most of them do not know what these kanji symbols mean. We propose a piece of educational software that uses collaborative augmented reality (AR) to teach users the meaning of kanji symbols. The application is laid out as a two player augmented reality computer game. The novelty of our approach is that we do not use regular workstations or laptops to host the AR (augmented reality) application. Instead we use fully autonomous PDAs, running the application together with an optical marker-based tracking module that makes this application not only available for a broad audience but also optimally mobile.},
keywords={augmented reality;courseware;notebook computers;entertainment;computer games;augmented reality;kanji learning;ARToolKit programmers;kanji symbols;educational software;collaborative AR;AR computer game;workstations;laptops;AR application;PDA;optical marker-based tracking;tracking module;broad audience;Augmented reality;Application software;Personal digital assistants;Collaborative software;Workstations;Libraries;Cameras;Programming profession;Collaborative work;Portable computers},
doi={10.1109/ISMAR.2003.1240747},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240748,
author={T. {Pintaric}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Augmented reality live-action compositing},
year={2003},
volume={},
number={},
pages={337-},
abstract={This report describes a system that performs live-action compositing of physical and virtual objects to a panoramic background image in real-time at interactive rates. A static camera is directed towards a 40 cm/sup 3/ miniature stage, whose backdrop has been colored in chromatic green. Users can add virtual objects and manipulate their parameters within the scene by using a proxy device that consists of a small rod attached to a fiducial marker. Our system runs on commodity hardware such as a notebook equipped with a firewire video camera. The necessary chroma-keying and adaptive difference-matting algorithms have been implemented on a GPU using fragment shading.},
keywords={augmented reality;video cameras;optical tracking;rendering (computer graphics);notebook computers;augmented reality;live-action compositing;physical objects;virtual objects;background image;interactive rates;chromatic green;proxy device;fiducial marker;commodity hardware;notebook computer;firewire video camera;chroma keying;adaptive difference matting;difference-matting algorithms;GPU;fragment shading;programmable shading;Augmented reality;Contracts;Real time systems;Cameras;Layout;Hardware;Firewire;Streaming media;Virtual reality;Multimedia systems},
doi={10.1109/ISMAR.2003.1240748},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240749,
author={R. {Berry} and M. {Makino} and N. {Hikawa} and M. {Suzuki}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={The augmented composer project: the music table},
year={2003},
volume={},
number={},
pages={338-339},
abstract={The music table enables the composition of musical patterns by arranging cards on a tabletop. An overhead camera allows the computer to track the movements and positions of the cards and to provide immediate feedback in the form of music and on-screen computer generated images. Musical structure is experienced as a tangible space enriched with physical and visual cues about the music produced.},
keywords={music;augmented reality;cameras;software tools;computer graphics;augmented composer project;music table;music composition;musical patterns;tabletop;computer generated images;musical structure;physical cues;visual cues;tracking;Multiple signal classification;Augmented reality;Instruments;Cameras;Tracking;Feedback;Image generation;Timing;Information science;Laboratories},
doi={10.1109/ISMAR.2003.1240749},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240750,
author={H. {Kato} and K. {Tachibana} and M. {Tanabe} and T. {Nakajima} and Y. {Fukuda}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={A city-planning system based on augmented reality with a tangible interface},
year={2003},
volume={},
number={},
pages={340-341},
abstract={This demonstration shows a city-planning system based on augmented reality with tangible user interface. Miniature models, illustrations and graphical computer displays have been used for the comparison and consideration in city-planning process. Augmented reality technology enables users to consider city plans more effectively and easily. One important issue of the augmented reality environment is how user can manipulate 3D structures that are displayed as virtual objects. It has to be intuitive and easy so that it may not disturb user's thought. We propose a new direct manipulation method based on the concept called tangible user interface. User holds a transparent cup upside down and can pick up, move or delete a virtual object by using it.},
keywords={town and country planning;augmented reality;computer displays;haptic interfaces;city-planning system;augmented reality;tangible interface;user interface;miniature models;computer graphics;computer displays;city-planning process;3D manipulation;3D structures;virtual objects;manipulation method;Augmented reality;Computer displays;Urban planning;Computational modeling;Cameras;User interfaces;Production systems;Cities and towns;Computer simulation;Shape},
doi={10.1109/ISMAR.2003.1240750},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240751,
author={M. {Kourogi} and T. {Kurata}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={A wearable augmented reality system with personal positioning based on walking locomotion analysis},
year={2003},
volume={},
number={},
pages={342-343},
abstract={In this paper, we present a wearable augmented reality (AR) system with personal positioning based on walking locomotion analysis that allows a user to freely mover around indoors and outdoors. The user is equipped with self-contained sensors, a wearable camera, an inertial head tracker and display. The system is based on the sensor fusion of estimates for relative displacement caused by human walking locomotion and estimates for absolute position and orientation within a Kalman filtering framework. The former is based on intensive analysis of human walking behavior using self-contained sensors. The latter is based on image matching of video frames from a wearable camera with an image database that was prepared beforehand.},
keywords={cameras;augmented reality;Kalman filters;position measurement;sensor fusion;image matching;gesture recognition;wearable computers;wearable augmented reality;personal positioning;walking locomotion analysis;self-contained sensors;wearable camera;inertial head tracker;inertial head display;sensor fusion;relative displacement;human walking locomotion;absolute position;Kalman filtering framework;intensive analysis;human walking behavior;image matching;video frames;image database;Augmented reality;Legged locomotion;Wearable sensors;Cameras;Humans;Head;Displays;Sensor fusion;Kalman filters;Filtering},
doi={10.1109/ISMAR.2003.1240751},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240752,
author={R. {Tenmoku} and M. {Kanbara} and N. {Yokoya}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={A wearable augmented reality system for navigation using positioning infrastructures and a pedometer},
year={2003},
volume={},
number={},
pages={344-345},
abstract={This paper describes a wearable augmented reality system using positioning infrastructures and a pedometer. To realize augmented reality systems, the position and orientation of user's viewpoint should be obtained in real time. The proposed system measures the orientation of user's viewpoint by an inertial sensor and the user's position using positioning infrastructures in environments and a pedometer. The system specifies the user's position using the position ID received from RFID tags or IrDA markers which are the components of positioning infrastructures. When the user goes away from them, the user's position is alternatively estimated by using a pedometer. We have developed a navigation system using the proposed techniques and have proven the feasibility of the system with experiments.},
keywords={augmented reality;wearable computers;navigation;image sensors;position measurement;wearable augmented reality;augmented reality system;navigation;positioning infrastructures;pedometer;user viewpoint;real time;inertial sensor;user position;position ID;RFID tags;IrDA markers;Augmented reality;Navigation;Wearable sensors;Sensor systems;Wearable computers;Layout;Computer displays;Position measurement;Hardware;Image databases},
doi={10.1109/ISMAR.2003.1240752},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240753,
author={V. {Lepetit} and L. {Vacchetti} and D. {Thalmann} and P. {Fua}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Real-time augmented face},
year={2003},
volume={},
number={},
pages={346-347},
abstract={This real-time augmented reality demonstration relies on our tracking algorithm described in V. Lepetit et al (2003). This algorithm considers natural feature points, and then does not require engineering of the environment. It merges the information from preceding frames in traditional recursive tracking fashion with that provided by a very limited number of reference frames. This combination results in a system that does not suffer from jitter and drift, and can deal with drastic changes. The tracker recovers the full 3D pose of the tracked object, allowing insertion of 3D virtual objects for augmented reality applications.},
keywords={augmented reality;face recognition;real-time systems;tracking;solid modelling;real-time augmented face;real-time systems;augmented reality demonstration;tracking algorithm;natural feature points;information merging;recursive tracking;reference frames;3D pose;3D virtual objects;Augmented reality;Firewire;Cameras;Robust stability;Jitter;Hardware;Portable computers;Humans;Face;Glass},
doi={10.1109/ISMAR.2003.1240753},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240754,
author={M. {Inami} and N. {Kawakami} and S. {Tachi}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Optical camouflage using retro-reflective projection technology},
year={2003},
volume={},
number={},
pages={348-349},
abstract={This paper describes a kind of active camouflage system named optical camouflage. Optical camouflage uses the retro-reflective projection technology, a projection-based augmented-reality system composed of a projector with a small iris and a retro-reflective screen. The object that needs to be made transparent is painted or covered with retro-reflective material. Then a projector projects the background image on it making the masking object virtually transparent.},
keywords={optical projectors;active vision;augmented reality;image matching;optical camouflage;retroreflective projection technology;active camouflage system;projection-based augmented-reality system;retroreflective screen;transparent object;retroreflective material;background image;masking object;virtual transparency;Optical refraction;Optical variables control;Iris;Optical materials;Displays;Haptic interfaces;Cameras;Virtual reality;Mercury (metals);Humans},
doi={10.1109/ISMAR.2003.1240754},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240755,
author={E. {Hosoya} and M. {Kitabata} and H. {Sato} and I. {Harada} and H. {Nojima} and F. {Morisawa} and S. {Mutoh} and A. {Onozawa}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={A mirror metaphor interaction system: touching remote real objects in an augmented reality environment},
year={2003},
volume={},
number={},
pages={350-351},
abstract={We propose a real-world-oriented interface called the "mirror metaphor interaction system". The display shows a mirror image from a camera facing a user, and the user can "touch" objects without making direct contact with the display. The "touched" object displays a menu or works directly. Objects can be placed in a remote room as well as in the user's room, and can also be moved around in a room. The user can therefore control equipment or interact with objects anywhere through the display of the system by combining images from local and remote places translucently. Our demonstration shows how the interface makes it easy to establish contact with movable objects in a remote room by "touching" them in the display.},
keywords={augmented reality;gesture recognition;computer graphics;mirror metaphor interaction system;remote real objects;augmented reality environment;real-world-oriented interface;mirror image;nondirect contact;remote room;user room;equipment control;object interaction;image combinations;translucent combination;movable objects;translucent self-image;Mirrors;Augmented reality;Displays;Cameras;Character generation;Sensor systems;Infrared sensors;Control equipment;Ubiquitous computing;Home computing},
doi={10.1109/ISMAR.2003.1240755},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240756,
author={H. {Kato} and T. {Naemura} and H. {Harashima}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Textured shadow},
year={2003},
volume={},
number={},
pages={352-353},
abstract={We demonstrate a real-time interactive system illuminating your shadows cast on the floor. A user occluding the projector light may create undesirable shadow in front-projector-based systems. We overcome and turn this problem to our advantage with an elaborately designed optics including multi-projection techniques. The systems will make you experience an exciting space of interaction performing an illusion on your shadow.},
keywords={augmented reality;lighting;gesture recognition;computer graphics;image texture;textured shadow;real-time interactive system;shadow illuminating;projector light;front-projector-based systems;optics design;multiprojection techniques;interaction space;illusion;spatially augmented reality;light effects;Cameras;Augmented reality;Image converters;Information science;Real time systems;Interactive systems;Optical design;Optical design techniques;Art;Casting},
doi={10.1109/ISMAR.2003.1240756},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240757,
author={C. B. {Stapleton} and C. E. {Hughes} and J. M. {Moshell}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={MIXED FANTASY: exhibition of entertainment research for mixed reality},
year={2003},
volume={},
number={},
pages={354-355},
abstract={},
keywords={Virtual reality;Convergence;Industrial training;Collaboration;Laboratories;Logic arrays;Computer science;Computer industry;Computer science education;Distributed algorithms},
doi={10.1109/ISMAR.2003.1240757},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240758,
author={M. {Takemura} and S. {Haraguchi}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={BLADESHIPS: an interactive attraction in mixed reality},
year={2003},
volume={},
number={},
pages={356-357},
abstract={Our purpose is to construct a new type of interactive attraction which detects the user's hand actions as triggers in a multi-participated mixed environment. "BLADESHIPS" is a game in which players compete with each other controlling virtually expressed belt-shaped flying objects, which are called "ships", in the real environment, trying to drive each other hit real/virtual obstacles. For the purpose of reproducing subtle and spatially varying material properties of the real object, we use a 3D scanner and a high-resolution camera.},
keywords={augmented reality;computer games;user interfaces;games of skill;image scanners;cameras;BLADESHIPS;interactive attraction;mixed reality;user hand actions;multiparticipated mixed environment;computer game;player competition;belt-shaped flying objects;real environment;hit real/virtual obstacles;3D scanner;high-resolution camera;game control;Virtual reality;Marine vehicles;Rendering (computer graphics);Material properties;Displays;Magnetic heads;Electromagnetic measurements;Character generation;Digital cameras;Design engineering},
doi={10.1109/ISMAR.2003.1240758},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240759,
author={Y. {Okuno} and H. {Kakuta} and T. {Takayama} and K. {Asai}},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Jellyfish party: blowing soap bubbles in mixed reality space},
year={2003},
volume={},
number={},
pages={358-359},
abstract={This paper describes a mixed reality installation named Jellyfish Party, for enjoying playing with soap bubbles. A special feature of this installation is the use of a spirometer sensor to measure the amount and speed of expelled air used to blow virtual soap bubbles.},
keywords={virtual reality;sensors;computer games;jellyfish party;virtual soap bubble;mixed reality space;spirometer sensor;Simple object access protocol;Virtual reality;Velocity measurement;Character generation;Games;Tracking;Switches;Educational institutions;Fluid flow measurement;Head},
doi={10.1109/ISMAR.2003.1240759},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1240760,
author={},
booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, title={Author index},
year={2003},
volume={},
number={},
pages={360-362},
abstract={The author index contains an entry for each author and coauthor included in the proceedings record.},
keywords={},
doi={10.1109/ISMAR.2003.1240760},
ISSN={},
month={Oct},}