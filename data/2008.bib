@INPROCEEDINGS{4637298,
author={},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={[Title page]},
year={2008},
volume={},
number={},
pages={i-i},
abstract={The following topics are dealt with: mixed reality; augmented reality; sensor fusion; head mounted display; rendering graphics; scene acquisition and object tracking.},
keywords={augmented reality;mixed reality;augmented reality;sensor fusion;head mounted display;rendering graphics;scene acquisition;object tracking},
doi={10.1109/ISMAR.2008.4637298},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637299,
author={},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={[Copyright notice]},
year={2008},
volume={},
number={},
pages={ii-ii},
abstract={Presents the copyright information for the conference. May include reprint permission information.},
keywords={},
doi={10.1109/ISMAR.2008.4637299},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637300,
author={},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Contents},
year={2008},
volume={},
number={},
pages={iii-vi},
abstract={Presents the table of contents of the proceedings.},
keywords={},
doi={10.1109/ISMAR.2008.4637300},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637301,
author={},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Corporate supporter},
year={2008},
volume={},
number={},
pages={vi-vi},
abstract={The conference organizers greatly appreciate the support of the various corporate sponsors listed.},
keywords={},
doi={10.1109/ISMAR.2008.4637301},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637302,
author={},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Messages},
year={2008},
volume={},
number={},
pages={vii-vii},
abstract={Presents the introductory welcome message from the conference proceedings.},
keywords={},
doi={10.1109/ISMAR.2008.4637302},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637303,
author={},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={From the program cochairs},
year={2008},
volume={},
number={},
pages={viii-viii},
abstract={Presents the introductory welcome message from the conference proceedings.},
keywords={},
doi={10.1109/ISMAR.2008.4637303},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637304,
author={},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={IEEE Visualization and Graphics Technical Committee (VGTC)},
year={2008},
volume={},
number={},
pages={ix-ix},
abstract={Provides a listing of current committee members.},
keywords={},
doi={10.1109/ISMAR.2008.4637304},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637305,
author={},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Technical Committee on Wearable Information Systems (TCWIS)},
year={2008},
volume={},
number={},
pages={x-x},
abstract={Provides a listing of current committee members.},
keywords={},
doi={10.1109/ISMAR.2008.4637305},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637306,
author={},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Task Force on Human Centered Computing (TFHCC)},
year={2008},
volume={},
number={},
pages={xi-xi},
abstract={},
keywords={},
doi={10.1109/ISMAR.2008.4637306},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637307,
author={},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Conference committee},
year={2008},
volume={},
number={},
pages={xii-xii},
abstract={Provides a listing of current committee members.},
keywords={},
doi={10.1109/ISMAR.2008.4637307},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637308,
author={},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Reviewers},
year={2008},
volume={},
number={},
pages={xiii-xiii},
abstract={The publication offers a note of thanks and lists its reviewers.},
keywords={},
doi={10.1109/ISMAR.2008.4637308},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637309,
author={P. {McIlroy}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Hawk-Eye: Augmented reality in sports broadcasting and officiating},
year={2008},
volume={},
number={},
pages={xiv-xiv},
abstract={Hawk-Eye is the camera based ball-tracking system that enables players to challenge line-calling decisions on the courts of major tennis events such as Wimbledon. The author uncovers the machine vision technology behind the verdict on the big screen. He describes how the International Tennis Federation tested and accredited the system as an officiating aid and the technical challenges overcome to meet their stringent criteria for accuracy and reliability. He also reveals the pitfalls encountered in taking a vision system out of the lab and attempting to integrate it into the unforgiving world of broadcast television in remote corners of the earth.},
keywords={augmented reality;broadcasting;computer vision;sport;target tracking;Hawk-Eye;augmented reality;sports broadcasting;sports officiating;camera based ball-tracking system;line-calling decisions;machine vision},
doi={10.1109/ISMAR.2008.4637309},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637310,
author={R. R. {Hainich}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Near-eye displays - A look into the Christmas ball},
year={2008},
volume={},
number={},
pages={xv-xv},
abstract={We first have a look at the development of AR in the recent 15 years and its current state. Given recent advances in computing and micro system technologies, it is hardly conceivable why AR technology should not finally be entering into mass market applications, the only way to amortize the development of such a complex technology. Nevertheless, achieving a dasiacritical masspsila of working detail solutions for a complete product will still be a paramount effort, especially concerning hardware. Addressing this central issue, the current status of hardware technologies is reviewed, including micro systems, micro mechanics and special optics, the requirements and components needed for a complete system, and possible solutions providing successful applications that could catalyze the evolution towards full fledged, imperceptible, private near eye display and sensorial interface systems, allowing for the everyday use of virtual objects and devices greatly exceeding the capabilities of any physical archetypes.},
keywords={augmented reality;computer displays;microdisplays;near-eye display;Christmas ball;augmented reality technology;computing technology;microsystem technology;hardware technology;micromechanic technology;optical technology;sensorial interface system;virtual object},
doi={10.1109/ISMAR.2008.4637310},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637311,
author={D. {Campbell}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Physical gaming: Out of the lap and into the living room},
year={2008},
volume={},
number={},
pages={xvi-xvi},
abstract={Diarmid Campbell talks about camera based games such as those using the EyeToy and, more recently, the PlayStation Eye camera. He looks at the many different techniques that can applied to analysing video images and how to use them as a game input mechanism. For each technique, he explains how it works, ways in which it can be used and, more importantly, where it can fail. The presentation is illustrated with plenty of videos and live demos to keep you awake. It looks at techniques from past games but also those from games currently in development and finishes with what lies on the horizon.},
keywords={computer vision;games of skill;physical gaming;camera based games;game input mechanism},
doi={10.1109/ISMAR.2008.4637311},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637312,
author={},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={2007 Paper awards},
year={2008},
volume={},
number={},
pages={xvii-xvii},
abstract={The award winners and the titles of their award winning papers are listed.},
keywords={},
doi={10.1109/ISMAR.2008.4637312},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637313,
author={},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={[Blank page]},
year={2008},
volume={},
number={},
pages={xviii-xviii},
abstract={This page or pages intentionally left blank.},
keywords={},
doi={10.1109/ISMAR.2008.4637313},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637314,
author={},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Papers},
year={2008},
volume={},
number={},
pages={xix-xix},
abstract={Start of the above-titled section of the conference proceedings record.},
keywords={},
doi={10.1109/ISMAR.2008.4637314},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637315,
author={},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={[Blank page]},
year={2008},
volume={},
number={},
pages={xx-xx},
abstract={This page or pages intentionally left blank.},
keywords={},
doi={10.1109/ISMAR.2008.4637315},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637316,
author={G. {Bleser} and D. {Strickery}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Using the marginalised particle filter for real-time visual-inertial sensor fusion},
year={2008},
volume={},
number={},
pages={3-12},
abstract={The use of a particle filter (PF) for camera pose estimation is an ongoing topic in the robotics and computer vision community, especially since the FastSLAM algorithm has been utilised for simultaneous localisation and mapping (SLAM) applications with a single camera. The major problem in this context consists in the poor proposal distribution of the camera pose particles obtained from the weak motion model of a camera moved freely in 3D space. While the FastSLAM 2.0 extension is one possibility to improve the proposal distribution, this paper addresses the question of how to use measurements from low-cost inertial sensors (gyroscopes and accelerometers) to compensate for the missing control information. However, the integration of inertial data requires the additional estimation of sensor biases, velocities and potentially accelerations, resulting in a state dimension, which is not manageable by a standard PF. Therefore, the contribution of this paper consists in developing a real-time capable sensor fusion strategy based upon the marginalised particle filter (MPF) framework. The performance of the proposed strategy is evaluated in combination with a marker-based tracking system and results from a comparison with previous visual-inertial fusion strategies based upon the extended Kalman filter (EKF), the standard PF and the MPF are presented.},
keywords={Kalman filters;particle filtering (numerical methods);pose estimation;robot vision;sensor fusion;marginalised particle filter;real-time visual-inertial sensor fusion;camera pose estimation;inertial sensors;gyroscopes;accelerometers;inertial data;marker-based tracking system;extended Kalman filter;Acceleration;Cameras;Noise;Mathematical model;Atmospheric measurements;Particle measurements;Proposals;real-time;sensor fusion;inertial sensors;nonlinear filtering;(marginalised) particle filter;(extended) Kalman filter;Fast-SLAM;G.3 [Probability and statistics]: Experimental design, Markov processes, Multivariate statistics, Nonparametric statistics, Probabilistic algorithms (including Monte Carlo), Statistical computing;I.2.9 [Artificial intelligence]: Robotics—Kinematics and dynamics, Sensors;I.2.10 [Artificial intelligence]: Vision and Scene Understanding—Motion, Video analysis;I.4.8 [Image processing and computer vision]: Scene analysis—Motion, Sensor fusion, Tracking;I.3.m [Computer graphics]: Miscellaneous—Augmented Reality;Algorithms;Design;Experimentation;Performance;Theory;Verification},
doi={10.1109/ISMAR.2008.4637316},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637317,
author={D. {Pustka} and G. {Klinker}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Dynamic gyroscope fusion in Ubiquitous Tracking environments},
year={2008},
volume={},
number={},
pages={13-20},
abstract={Ubiquitous tracking (Ubitrack) setups, consisting of many previously unknown sensors, offer many possibilities to perform sensor fusion in order to increase robustness and accuracy. In particular, the dynamic combination of mobile and stationary trackers enables the creation of new wide-area tracking concepts. In this work, we present a setup in which a gyroscope is dynamically fused with three different mobile and stationary sensors, based on the concepts of spatial relationship graphs (SRGs) and patterns. For this, we contribute new patterns that, based on well-known algorithms, enable the transformation of rotation velocity and the fusion with different absolute trackers. The usefulness of the approach is shown in a system that automatically reconfigures the SRG based on course tracking data, and, depending on the structure of this SRG, automatically selects a suitable fusion algorithm.},
keywords={augmented reality;gyroscopes;sensor fusion;tracking;dynamic gyroscope fusion;ubiquitous tracking environments;sensor fusion;spatial relationship graphs;Gyroscopes;Rotation measurement;Sensors;Quaternions;Cameras;Calibration;Tracking;Augmented Reality;Tracking;Calibration;Sensor Fusion;Gyroscopes;Inertial Sensors;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities;I.3.1 [Computer Graphics]: Hardware Architecture—Input devices},
doi={10.1109/ISMAR.2008.4637317},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637318,
author={J. D. {Hol} and T. B. {Schon} and F. {Gustafsson}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Relative pose calibration of a spherical camera and an IMU},
year={2008},
volume={},
number={},
pages={21-24},
abstract={This paper is concerned with the problem of estimating the relative translation and orientation of an inertial measurement unit and a spherical camera, which are rigidly connected. The key is to realize that this problem is in fact an instance of a standard problem within the area of system identification, referred to as a gray-box problem. We propose a new algorithm for estimating the relative translation and orientation, which does not require any additional hardware, except a piece of paper with a checkerboard pattern on it. The experimental results show that the method works well in practice.},
keywords={calibration;cameras;image sensors;pose estimation;pose calibration;spherical camera;inertial measurement unit;system identification;gray-box problem;inertial sensors;Cameras;Calibration;Predictive models;Quaternions;Position measurement;Measurement uncertainty;Optical sensors},
doi={10.1109/ISMAR.2008.4637318},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637319,
author={W. T. {Fong} and S. K. {Ong} and A. Y. C. {Nee}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={A differential GPS carrier phase technique for precision outdoor AR tracking},
year={2008},
volume={},
number={},
pages={25-28},
abstract={This paper presents a differential GPS carrier phase technique for 3D outdoor position tracking in mobile augmented reality (AR) applications. It has good positioning accuracy, low drift and jitter, and low computation requirement. It eliminates the resolution of integer ambiguities. The position from an initial point is tracked by accumulating the displacement in each time step, which is determined using Differential Single Difference. Preliminary results using low cost GPS receivers show that the position error is 10 cm, and the drift is 0.001 ms-1, which can be compensated using linear models. Stable and accurate augmentations in outdoor scenes are demonstrated.},
keywords={augmented reality;Global Positioning System;mobile computing;tracking;differential GPS carrier phase;precision outdoor AR tracking;3D outdoor position tracking;mobile augmented reality;integer ambiguities;differential single difference;Global positioning system;Receivers;Satellites;Accuracy;Phase measurement;Mobile communication;Jitter;Position tracking;Global Positioning System;differential carrier phase;outdoor augmented reality;H.5.1 Multimedia Information Systems: Artificial, augmented, and virtual realities;H.5.2 User Interfaces: Input devices},
doi={10.1109/ISMAR.2008.4637319},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637320,
author={O. {Cakmakci} and S. {Vo} and S. {Vogl} and R. {Spindelbalker} and A. {Ferscha} and J. P. {Rolland}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Optical free-form surfaces in off-axis head-worn display design},
year={2008},
volume={},
number={},
pages={29-32},
abstract={Head-worn displays are becoming a viable visual display option for mobile augmented reality. In this paper, we highlight the challenges and some recent progress towards compact and lightweight head-worn displays tending towards the eyeglass form factor. Single and dual-element eyeglass display designs that leverage the advances in free-form optics fabrication technology will be presented. There are three new contributions in this paper: first, we present a single-element eyeglass display design and the fabricated mirror prototype; second, we present our second generation dual-element eyeglass display design with the revised opto-mechanical packaging; third, we present our initial studies on the field of view limit with an 8 mm pupil for the dual-element design.},
keywords={augmented reality;helmet mounted displays;mirrors;optical design techniques;optical fabrication;optical glass;optical free-form surface;lightweight off-axis head-worn display design;mobile augmented reality;single-element eyeglass display design;dual-element eyeglass display design;free-form optics fabrication technology;fabricated mirror prototype;opto-mechanical packaging;Optics;optical system design;free-form;radial basis functions;B.4.2 [Hardware]: Input/output devices;G.1.10 [Numerical Analysis]: Applications},
doi={10.1109/ISMAR.2008.4637320},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637321,
author={ {Sheng Liu} and {Dewen Cheng} and {Hong Hua}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={An optical see-through head mounted display with addressable focal planes},
year={2008},
volume={},
number={},
pages={33-42},
abstract={Most existing stereoscopic head mounted displays (HMDs), presenting a pair of stereoscopic images at a fixed focal distance, lack the ability to correctly render the naturally coupled accommodation and convergence cues. Psychophysical studies have shown that such displays may cause many adverse consequences such as visual fatigue, diplopic vision, degraded oculomotor response, and depth perception errors. In this paper, we present a see-through HMD with addressable focal planes utilizing a novel active optical element - a liquid lens. The element, with a varying optical power from -5 to 20 diopters, is able to address the focal distance of the HMD from infinity to the near point of the eye. A monocular prototype was built from off-the-shelf elements and experimental results are presented to validate the proposed designs. We also describe both subjective and objective measurements of the accommodation responses of the viewer to the focal distances presented by the prototype.},
keywords={focal planes;helmet mounted displays;stereo image processing;optical see-through head mounted display;addressable focal plane;stereoscopic image;active optical element;liquid lens;Display hardware;head mounted display;accommodation;usability studies and experiments;I.3.1 [Computer Graphics]: Hardware Architecture—Three-Dimensional Displays;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, Augmented, and Virtual Realities;C.4 [Performance of Systems]: Performance Attributes},
doi={10.1109/ISMAR.2008.4637321},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637322,
author={E. {Veas} and E. {Kruijff}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Vesp’R: design and evaluation of a handheld AR device},
year={2008},
volume={},
number={},
pages={43-52},
abstract={This paper focuses on the design of devices for handheld spatial interaction. In particular, it addresses the requirements and construction of a new platform for interactive AR, described from an ergonomics stance, prioritizing human factors of spatial interaction. The result is a multi-configurable platform for spatial interaction, evaluated in two AR application scenarios. The user tests validate the design with regards to grip, weight balance and control allocation, and provide new insights on the human factors involved in handheld spatial interaction.},
keywords={augmented reality;human computer interaction;human factors;interactive devices;VesppsilaR design;interactive handheld augmented reality device;ergonomics stance;human factor;handheld spatial interaction;multiconfigurable platform;Construction industry;Sensors;Ergonomics;Fatigue;Performance evaluation;Force;Fingers;I.3.6 [Computer Graphics]: Methodology and Techniques - Interaction Techniques;Augmented Reality;handheld devices;mobile computing;3D user interface;garage interface design},
doi={10.1109/ISMAR.2008.4637322},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637323,
author={ {Yusaku Nishina} and {Bunyo Okumura} and {Masayuki Kanbara} and {Naokazu Yokoya}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Photometric registration by adaptive high dynamic range image generation for augmented reality},
year={2008},
volume={},
number={},
pages={53-56},
abstract={This paper describes photometric registration for augmented reality (AR) using a high-dynamic-range (HDR) image. In photorealistic AR, estimating the lighting environment of virtual objects is difficult because of low dynamic range cameras. In order to overcome this problem, we propose a method that estimates the lighting environment from an HDR image and renders virtual objects using an HDR environment map. Virtual objects are overlaid in real-time by adjusting the dynamic range of the rendered image with tone mapping according to the exposure time of the camera. The HDR image is generated from multiple images captured with various exposure times. We have found through experimentation that the updating rate is improved by effectively limiting the dynamic range, depending on the exposure time. We have verified the effect of limiting the dynamic range on the reality of virtual objects.},
keywords={augmented reality;image registration;photometry;photometric registration;adaptive high dynamic range image generation;augmented reality;virtual object rendering;tone mapping;Image generation;Dynamic range;Lighting;Cameras;Light sources;Estimation;Reflectivity;H.5.1 [Information interfaces and presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities},
doi={10.1109/ISMAR.2008.4637323},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637324,
author={G. {Klein} and D. {Murray}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Compositing for small cameras},
year={2008},
volume={},
number={},
pages={57-60},
abstract={To achieve a realistic integration of virtual and real imagery in video see-through augmented reality, the rendered images should have a similar appearance and quality to those captured by the video camera. This paper describes a compositing method which models the artefacts produced by a small low-cost camera, and adds these effects to an ideal pinhole image produced by conventional rendering methods. We attempt to model and simulate each step of the imaging process, including distortions, chromatic aberrations, blur, Bayer masking, noise and colour-space compression, all while requiring only an RGBA image and an estimate of camera velocity as inputs.},
keywords={aberrations;augmented reality;rendering (computer graphics);video signal processing;compositing;small cameras;virtual imagery;real imagery;augmented reality;image rendering;image distortions;chromatic aberrations;image blur;Bayer masking;noise compression;colour-space compression;RGBA image;Graphics;Pixel;Image generation;Cameras;Image color analysis;Lenses;Noise},
doi={10.1109/ISMAR.2008.4637324},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637325,
author={P. {Bunnun} and W. W. {Mayol-Cuevas}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={OutlinAR: an assisted interactive model building system with reduced computational effort},
year={2008},
volume={},
number={},
pages={61-64},
abstract={This paper presents a system that allows online building of 3D wireframe models through a combination of user interaction and automated methods from a handheld camera-mouse. Crucially, the model being built is used to concurrently compute camera pose, permitting extendable tracking while enabling the user to edit the model interactively. In contrast to other model building methods that are either off-line and/or automated but computationally intensive, the aim here is to have a system that has low computational requirements and that enables the user to define what is relevant (and what is not) at the time the model is being built. OutlinAR hardware is also developed which simply consists of the combination of a camera with a wide field of view lens and a wheeled computer mouse.},
keywords={augmented reality;human computer interaction;interactive systems;optical tracking;solid modelling;OutlinAR;assisted interactive model building system;online building;3D wireframe model;user interaction;handheld camera-mouse;camera pose;tracking;augmented reality;Three dimensional displays;Computational modeling;Cameras;Solid modeling;Visualization;Buildings;Augmented reality},
doi={10.1109/ISMAR.2008.4637325},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637326,
author={J. {Wither} and C. {Coffin} and J. {Ventura} and T. {Hollerer}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Fast annotation and modeling with a single-point laser range finder},
year={2008},
volume={},
number={},
pages={65-68},
abstract={This paper presents methodology for integrating a small, single-point laser range finder into a wearable augmented reality system. We first present a way of creating object-aligned annotations with very little user effort. Second, we describe techniques to segment and pop-up foreground objects. Finally, we introduce a method using the laser range finder to incrementally build 3D panoramas from a fixed observerpsilas location. To build a 3D panorama semi-automatically, we track the systempsilas orientation and use the sparse range data acquired as the user looks around in conjunction with real-time image processing to construct geometry around the userpsilas position. Using full 3D panoramic geometry, it is possible for new virtual objects to be placed in the scene with proper lighting and occlusion by real world objects, which increases the expressivity of the AR experience.},
keywords={augmented reality;image segmentation;laser ranging;single-point laser range finder;wearable augmented reality system;object-aligned annotations;object segmentation;pop-up foreground objects;3D panoramas;real-time image processing;Three dimensional displays;Laser applications;Pixel;Image color analysis;Solid modeling;Lasers;Cameras;I.4.8 [Image Processing and Computer Vision]: Scene Analysis—Tracking;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism—Virtual Reality},
doi={10.1109/ISMAR.2008.4637326},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637327,
author={B. {Avery} and B. H. {Thomas} and W. {Piekarski}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={User evaluation of see-through vision for mobile outdoor augmented reality},
year={2008},
volume={},
number={},
pages={69-72},
abstract={We have developed a system built on our mobile AR platform that provides users with see-through vision, allowing visualization of occluded objects textured with real-time video information. We present a user study that evaluates the userpsilas ability to view this information and understand the appearance of an outdoor area occluded by a building while using a mobile AR computer. This understanding was compared against a second group of users who watched video footage of the same outdoor area on a regular computer monitor. The comparison found an increased accuracy in locating specific points from the scene for the outdoor AR participants. The outdoor participants also displayed more accurate results, and showed better speed improvement than the indoor group when viewing more than one video simultaneously.},
keywords={augmented reality;computer vision;data visualisation;mobile computing;video signal processing;user evaluation;see-through vision;mobile outdoor augmented reality;object visualization;occluded object;real-time video information;Augmented reality;Visualization;Accuracy;Streaming media;Cameras;Three dimensional displays;Buildings;Outdoor Augmented Reality;Wearable Computers;Telepresence;Image-based Rendering;Occlusion;I.3.6 [Computer Graphics]: Methodology and Techniques — Interaction Techniques;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism — Virtual Reality},
doi={10.1109/ISMAR.2008.4637327},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637328,
author={C. M. {Robertson} and B. {MacIntyre} and B. N. {Walker}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={An evaluation of graphical context when the graphics are outside of the task area},
year={2008},
volume={},
number={},
pages={73-76},
abstract={An ongoing research problem in Augmented Reality (AR) is to improve tracking and display technology in order to minimize registration errors. However, perfect registration is not always necessary for users to understand the intent of an augmentation. This paper describes the results of an experiment to evaluate the effects of graphical context in a Lego block placement task when the graphics are located outside of the task area. Four conditions were compared: fully registered AR; non-registered AR; a heads-up display (HUD) with the graphics always visible in the field of view; and a HUD with the graphics not always visible in the field of view. The results of this experiment indicated that registered AR outperforms both non-registered AR and graphics displayed on a HUD. The results also indicated that non-registered AR does not offer any significant performance advantages over a HUD, but is rated as less intrusive and can keep non-registered graphics from cluttering the task space.},
keywords={augmented reality;graphical context;augmented reality;AR;registration error minimization;Lego block placement task;heads-up display;human-computer interaction;Graphics;Augmented reality;Sensors;Maintenance engineering;Psychology;Sensor systems;Cameras;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Evaluation/methodology;augmented reality;communicative intent;augmented environments;human-computer interaction},
doi={10.1109/ISMAR.2008.4637328},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637329,
author={M. A. {Livingston} and {Zhuming Ai}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={The effect of registration error on tracking distant augmented objects},
year={2008},
volume={},
number={},
pages={77-86},
abstract={We conducted a user study of the effect of registration error on performance of tracking distant objects in augmented reality. Categorizing error by types that are often used as specifications, we hoped to derive some insight into the ability of users to tolerate noise, latency, and orientation error. We used measurements from actual systems to derive the parameter settings. We expected all three errors to influence userspsila ability to perform the task correctly and the precision with which they performed the task. We found that high latency had a negative impact on both performance and response time. While noise consistently interacted with the other variables, and orientation error increased user error, the differences between ldquohighrdquo and ldquolowrdquo amounts were smaller than we expected. Results of userspsila subjective rankings of these three categories of error were surprisingly mixed. Users believed noise was the most detrimental, though statistical analysis of performance refuted this belief. We interpret the results and draw insights for system design.},
keywords={augmented reality;image registration;object detection;tracking;augmented reality;distant object tracking;registration error effect;statistical analysis;Vehicles;Noise;Buildings;Measurement uncertainty;Calibration;Distance measurement;Graphics;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities;H.5.2 [Information Interfaces and Presentation]: User Interfaces—Evaluation/Methodology;H.1.2 [Models and Principles]: User/Machine Systems—Human factors},
doi={10.1109/ISMAR.2008.4637329},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637330,
author={J. {Tumler} and F. {Doil} and R. {Mecke} and G. {Paul} and M. {Schenk} and E. A. {Pfister} and A. {Huckauf} and I. {Bockelmann} and A. {Roggentin}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Mobile Augmented Reality in industrial applications: Approaches for solution of user-related issues},
year={2008},
volume={},
number={},
pages={87-90},
abstract={Augmented Reality (AR) uses computer-generated virtual information to enhance the userpsilas information access. While numerous previous studies have demonstrated the large potential of AR to improve industrial processes by enhancing product quality and reducing production times it is still unclear if and how long term usage of such AR technology produces stress and strain. This paper presents an approach to use the analysis of Heart Rate Variability to objectively measure current user strain during different work tasks. Results of a user study comparing strain during an AR supported and a non-AR supported work task in a laboratory setting are presented and discussed.},
keywords={augmented reality;human factors;mobile computing;mobile augmented reality;industrial manufacturing;computer-generated virtual information;product quality;heart rate variability analysis;Strain;Heart rate variability;Augmented reality;Electronic mail;Mobile communication;Psychology;Electrocardiography;H.1.2 [MODELS AND PRINCIPLES]: User/Machine Systems—Human factors;H.5 [INFORMATION INTERFACES AND PRESENTATION]: General},
doi={10.1109/ISMAR.2008.4637330},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637331,
author={B. {Schwerdtfeger} and G. {Klinker}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Supporting order picking with Augmented Reality},
year={2008},
volume={},
number={},
pages={91-94},
abstract={We report on recent progress in the iterative process of exploring, evaluating and refining Augmented Reality-based methods to support the order picking process. We present our findings from three user studies and from demonstrations at several exhibitions. The resulting setup is a combined visualization to precisely and efficiently guide the user, even if the augmentation is not always in the field of view of the HMD.},
keywords={augmented reality;data visualisation;goods dispatch data processing;logistics data processing;order picking;user interfaces;order picking process;augmented reality;iterative process;data visualization;HMD;user interface;logistics;Visualization;Three dimensional displays;Navigation;Augmented reality;Fading;Distance measurement;Mobile communication;H.5.1 [ INFORMATION INTERFACES AND PRESENTATION]: Multimedia Information Systems—Artificial, augmented, and virtual realities; Evaluation/methodology;H.5.2 [ INFORMATION INTERFACES AND PRESENTATION]: User Interfaces — User-centered design},
doi={10.1109/ISMAR.2008.4637331},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637332,
author={G. {Schall} and E. {Mendez} and D. {Schmalstieg}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Virtual redlining for civil engineering in real environments},
year={2008},
volume={},
number={},
pages={95-98},
abstract={Field workers of utility companies are regularly engaged in outdoor tasks such as network planning and inspection of underground infrastructure. Redlining is the term used for manually annotating either printed paper maps or a 2D geographic information system on a notebook computer taken to the field. Either of these approaches requires finding the physical location to be annotated on the physical or digital map. In this paper, we describe a mobile Augmented Reality (AR) system capable of supporting virtual redlining. The AR visualization delivered by the system is constructed from data directly extracted from a GIS used in day-to-day production by utility companies. We also report on encouraging trials and interviews performed with professional field workers from the utility sector.},
keywords={augmented reality;civil engineering computing;data visualisation;engineering graphics;geographic information systems;microcomputer applications;public utilities;virtual redlining;civil engineering;network planning;underground infrastructure inspection;2D geographic information system;notebook computer;digital map;Augmented Reality system;AR visualization;Geographic information systems;Augmented reality;Three dimensional displays;Solid modeling;Data visualization;Companies;Planning;underground infrastructure visualization;redlining;mobile augmented reality;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities},
doi={10.1109/ISMAR.2008.4637332},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637333,
author={R. {Grasset} and A. {Dunser} and M. {Billinghurst}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={The design of a mixed-reality book: Is it still a real book?},
year={2008},
volume={},
number={},
pages={99-102},
abstract={In this paper we present the results of our long term development of a mixed reality book. Most previous work in the area has focused on the technology of augmented reality books, such as providing registration using fiducial markers. In this work, however, we focused on exploring the design and development process of this type of application in a broader sense. We studied the semantics of a mixed reality book, the design space and the user experience with this type of interface.},
keywords={augmented reality;mixed-reality book design;augmented reality books;user experience;Books;Three dimensional displays;Virtual reality;Augmented reality;Visualization;USA Councils;Layout;H.5.1 [Information interfaces and presentation]: Multimedia Information systems—Artificial, augmented, and virtual realities},
doi={10.1109/ISMAR.2008.4637333},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637334,
author={T. {Miyashita} and P. {Meier} and T. {Tachikawa} and S. {Orlic} and T. {Eble} and V. {Scholz} and A. {Gapel} and O. {Gerl} and S. {Arnaudov} and S. {Lieberknecht}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={An Augmented Reality museum guide},
year={2008},
volume={},
number={},
pages={103-106},
abstract={Recent years have seen advances in many enabling augmented reality technologies. Furthermore, much research has been carried out on how augmented reality can be used to enhance existing applications. This paper describes our experiences with an AR-museum guide that combines some of the latest technologies. Amongst other technologies, markerless tracking, hybrid tracking, and an ultra-mobile-PC were used. Like existing audio guides, the AR-guide can be used by any museum visitor, during a six-month exhibition on Islamic art. We provide a detailed description of the museumpsilas motivation for using AR, of our experiences in developing the system, and the initial results of user surveys. Taking this information into account, we can derive possible system improvements.},
keywords={art;augmented reality;exhibitions;augmented reality;museum guide;markerless tracking;hybrid tracking;ultra-mobile-PC;exhibition;Islamic art;Animation;Cameras;Augmented reality;Computer graphics;Robustness;Mobile communication;Interviews;augmented reality;mobile computing;multimedia in the museum;1.3.6 [Computer Graphics]: Methodology and Techniques — Interaction Techniques;H5.1. [Information Systems]: Artificial, augmented and virtual realities},
doi={10.1109/ISMAR.2008.4637334},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637335,
author={J. {Quarles} and S. {Lampotang} and I. {Fischler} and P. {Fishwick} and B. {Lok}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Collocated AAR: Augmenting After Action Review with Mixed Reality},
year={2008},
volume={},
number={},
pages={107-116},
abstract={This paper proposes collocated after action review (AAR) of training experiences. Through mixed reality (MR), collocated AAR allows users to review past training experiences in situ with the userpsilas current, real-world experience. MR enables a user-controlled egocentric viewpoint, a visual overlay of virtual information, and playback of recorded training experiences collocated with the userpsilas current experience. Collocated AAR presents novel challenges for MR, such as collocating time, interactions, and visualizations of previous and current experiences. We created a collocated AAR system for anesthesia education, the augmented anesthesia machine visualization and interactive debriefing system (AAMVID). The system was evaluated in two studies by students (n=19) and educators (n=3). The results demonstrate how collocated AAR systems such as AAMVID can: (1) effectively direct student attention and interaction during AAR and (2) provide novel visualizations of aggregate student performance and insight into student understanding for educators.},
keywords={augmented reality;biomedical education;computer based training;data visualisation;medical computing;collocated after action review;mixed reality;user-controlled egocentric;virtual information;augmented anesthesia machine visualization and interactive debriefing system;Anesthesia;Book reviews;Visualization;Training;Lenses;Virtual reality;Data visualization;Mixed Reality;After Action Review;Anesthesia Machine;User Studies;J.3 [Computer Applications]: Life and Medical Sciences — Health},
doi={10.1109/ISMAR.2008.4637335},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637336,
author={Y. {Park} and V. {Lepetit} and {Woontack Woo}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Multiple 3D Object tracking for augmented reality},
year={2008},
volume={},
number={},
pages={117-120},
abstract={We present a method that is able to track several 3D objects simultaneously, robustly, and accurately in real-time. While many applications need to consider more than one object in practice, the existing methods for single object tracking do not scale well with the number of objects, and a proper way to deal with several objects is required. Our method combines object detection and tracking: Frame-to-frame tracking is less computationally demanding but is prone to fail, while detection is more robust but slower. We show how to combine them to take the advantages of the two approaches, and demonstrate our method on several real sequences.},
keywords={augmented reality;computer vision;object detection;tracking;multiple 3D object tracking;augmented reality;object detection;frame-to-frame tracking;computer vision;Three dimensional displays;Robustness;Feature extraction;Tracking;Target tracking;Solid modeling;Object detection;Augmented Reality;Computer Vision;Tracking},
doi={10.1109/ISMAR.2008.4637336},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637337,
author={D. {Wagner} and T. {Langlotz} and D. {Schmalstieg}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Robust and unobtrusive marker tracking on mobile phones},
year={2008},
volume={},
number={},
pages={121-124},
abstract={Marker tracking has revolutionized augmented reality about a decade ago. However, this revolution came at the expense of visual clutter. In this paper, we propose several new marker techniques, which are less obtrusive than the usual black and white squares. Furthermore, we report methods that allow tracking beyond the visibility of these markers further improving robustness. All presented techniques are implemented in a single tracking library, are highly efficient in their memory and CPU usage and run at interactive frame rates on mobile phones.},
keywords={augmented reality;mobile computing;mobile handsets;mobile phone marker tracking;augmented reality;visual clutter;Pixel;Tracking;Cameras;Robustness;Mobile handsets;Augmented reality;Feature extraction;marker tracking;pixel flow;mobile phones;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities;I.4.8 [Image Processing and Computer Vision]: Scene Analysis — Tracking},
doi={10.1109/ISMAR.2008.4637337},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637338,
author={D. {Wagner} and G. {Reitmayr} and A. {Mulloni} and T. {Drummond} and D. {Schmalstieg}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Pose tracking from natural features on mobile phones},
year={2008},
volume={},
number={},
pages={125-134},
abstract={In this paper we present two techniques for natural feature tracking in real-time on mobile phones. We achieve interactive frame rates of up to 20 Hz for natural feature tracking from textured planar targets on current-generation phones. We use an approach based on heavily modified state-of-the-art feature descriptors, namely SIFT and Ferns. While SIFT is known to be a strong, but computationally expensive feature descriptor, Ferns classification is fast, but requires large amounts of memory. This renders both original designs unsuitable for mobile phones. We give detailed descriptions on how we modified both approaches to make them suitable for mobile phones. We present evaluations on robustness and performance on various devices and finally discuss their appropriateness for augmented reality applications.},
keywords={augmented reality;feature extraction;image texture;mobile computing;mobile handsets;pose estimation;target tracking;pose tracking;mobile phone;natural feature tracking;textured planar target;current-generation phone;SIFT feature descriptor;Ferns feature descriptor;augmented reality;Target tracking;Feature extraction;Mobile handsets;Computational modeling;Detectors;Cameras;Real time systems;pose tracking;natural features;mobile phones;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities;I.4.8 [Image Processing and Computer Vision]: Scene Analysis — Tracking},
doi={10.1109/ISMAR.2008.4637338},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637339,
author={N. {Hagbi} and O. {Bergig} and J. {El-Sana} and K. {Kedem} and M. {Billinghurst}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={In-place Augmented Reality},
year={2008},
volume={},
number={},
pages={135-138},
abstract={In this paper we present a new vision-based approach for transmitting virtual models for augmented reality (AR). A two dimensional representation of the virtual models is embedded in a printed image. We apply image-processing techniques to interpret the printed image and extract the virtual models, which are then overlaid back on the printed image. The main advantages of our approach are: (1) the image of the embedded virtual models and their behaviors are understandable to a human without using an AR system, and (2) no database or network communication is required to retrieve the models. The latter is useful in scenarios with large numbers of users. We implemented an AR system that demonstrates the feasibility of our approach. Applications in education, advertisement, gaming, and other domains can benefit from our approach, since content providers need only to publish the printed content and all virtual information arrives with it.},
keywords={augmented reality;image processing;augmented reality;image-processing techniques;printed content;Augmented reality;Image color analysis;Libraries;Solid modeling;Pixel;Visualization;Databases;Augmented Reality content;content transmission;model embedding;dual perception encoding;H.5.1 [Multimedia Information Systems]: Artificial, augmented, and virtual realities;I.4.0 [Image Processing and Computer Vision]: Image processing software},
doi={10.1109/ISMAR.2008.4637339},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637340,
author={ {Kohei Tanaka} and {Yasue Kishino} and {Masakazu Miyamae} and {Tsutomu Terada} and {Shojiro Nishio}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={An information layout method for an optical see-through head mounted display focusing on the viewability},
year={2008},
volume={},
number={},
pages={139-142},
abstract={Accessing information when we are on the move is a key feature if mobile computing environments, and using an optical see-through head mounted display (HMD) is one of the most suitable ways to do this. Although the HMD can display information without interfering with the user's view, when the sight behind the display is too complex or too bright, the information displayed can bee very difficult to see. To solve this problem, we have created a way of laying out information for the optical see-through HMD. The ideal area for displaying information is determined by evaluating the sight image behind the HMD captured by a pantoscopic camera mounted on it. Moreover, if there is no suitable area for displaying information, our method select involves using the sight image around users use to the ideal direction and instructing them to face the direction. Our method displays information to ideal areas.},
keywords={helmet mounted displays;mobile computing;information layout method;optical see-through head mounted display;mobile computing;pantoscopic camera;Layout;Cameras;Optical imaging;Mathematical model;Augmented reality;Equations;Electronic mail;H.5.2 [Information Interfaces and Presentation]: User Interfaces—Prototyping},
doi={10.1109/ISMAR.2008.4637340},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637341,
author={S. D. {Peterson} and M. {Axholt} and S. R. {Ellis}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Label segregation by remapping stereoscopic depth in far-field augmented reality},
year={2008},
volume={},
number={},
pages={143-152},
abstract={This paper describes a novel technique for segregating overlapping labels in stereoscopic see-through displays. The present study investigates the labeling of far-field objects, with distances ranging 100-120 m. At these distances the stereoscopic disparity difference between objects is below 1 arcmin, so labels rendered at the same distance as their corresponding objects appear as if on a flat layer in the display. This flattening is due to limitations of both display and human visual resolution. By remapping labels to pre-determined depth layers on the optical path between the observer and the labeled object, an interlayer disparity ranging from 5 to 20 arcmin can be achieved for 5 overlapping labels. The present study evaluates the impact of such depth separation of superimposed layers, and found that a 5 arcmin interlayer disparity yields a significantly lower response time, over 20% on average, in a visual search task compared to correctly registering labels and objects in depth. Notably the performance does not improve when doubling the interlayer disparity to 10 arcmin and, surprisingly, the performance degrades significantly when again doubling the interlayer disparity to 20 arcmin, approximating the performance in situations with no interlayer disparity. These results confirm that our technique can be used to segregate overlapping labels in the far visual field, without the cost associated with traditional label placement algorithms.},
keywords={augmented reality;stereo image processing;three-dimensional displays;label segregation;stereoscopic depth remapping;far-field augmented reality;overlapping labels;stereoscopic see-through displays;stereoscopic disparity difference;visual resolution;interlayer disparity;depth separation;superimposed layers;Visualization;Distance measurement;Time factors;Clutter;Optical imaging;Observers;Layout;Label placement;user interfaces;stereoscopic displays;augmented reality;visual clutter;information layering;H.5.2 [Information Systems]: User Interfaces;I.3 [Computing Methodologies]: Computer Graphics},
doi={10.1109/ISMAR.2008.4637341},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637342,
author={},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Posters},
year={2008},
volume={},
number={},
pages={153-154},
abstract={Start of the above-titled section of the conference proceedings record.},
keywords={},
doi={10.1109/ISMAR.2008.4637342},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637343,
author={ {Xinli Chen} and {Xubo Yang} and {Shuangjiu Xiao} and {Meng Li}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={A practical radiometric compensation method for projector-based augmentation},
year={2008},
volume={},
number={},
pages={155-156},
abstract={Radiometric compensation has made it possible for a projector to display on ordinary surface with colors and textures. For previous methods, itpsilas necessary to calibrate both the projector and camera at first. The calibration can be time-consuming and needs to be redone once the system settings change. We present a method that simplifies the calibration process. As a result, the system is more practicable for ad-hoc setups.},
keywords={augmented reality;image colour analysis;image texture;radiometry;practical radiometric compensation;projector-based augmentation;surface colors;surface textures;Cameras;Pixel;Calibration;Transfer functions;Radiometry;Real time systems;Image resolution;Image Correction;Radiometric Compensation;Virtual and Augmented Realities;Projector-Camera Systems;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, Augmented, and Virtual Realities;I.3.3 [Computer Graphics]: Picture/ImageGeneration—Display Algorithms},
doi={10.1109/ISMAR.2008.4637343},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637344,
author={ {Yifan Chen} and {Xubo Yang} and {Shuangjiu Xiao} and {Xiaodong Ding}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Relighting with real incident light source},
year={2008},
volume={},
number={},
pages={157-158},
abstract={This paper presents an approach that can relight captured image with real incident light source. We first captured basis images of objects illuminated with a projector as the basis light source. We then established a pixel level mapping relationship between a real incident light source and the basis light source. Finally we developed a relighting algorithm that can simulate the relit effects with the real light source. The techniques can be applied to mixed reality.},
keywords={image resolution;light sources;incident light source;projector;pixel level mapping relationship;basis light source;Light sources;Cameras;Pixel;Arrays;Augmented reality;Reflectivity;Glass;Image-Based Relighting;Near-Field Reflectance;Real Incident Light Acquisition;AR/MR;K.6.1 [Management of Computing and Information Systems]: Project and People Management—Life Cycle;I.4.1 [Image Processing And Computer Vision]: Digitization and Image Capture—Sampling;I.3.3 [Computer Graphics]: Picture Image Generation—Digitizing and scanning},
doi={10.1109/ISMAR.2008.4637344},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637345,
author={S. {Hay} and J. {Newman} and R. {Harle}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Optical tracking using commodity hardware},
year={2008},
volume={},
number={},
pages={159-160},
abstract={We describe a method for using Nintendo Wii controllers as a stereo vision system to perform 3D tracking or motion capture in real time. Commodity consumer hardware allows a wireless, portable tracker to be created that obtains accurate results for a fraction of the cost of conventional setups. Consequently, tracking becomes viable in situations where cost or space were previously prohibitive. Initial results show an accuracy of plusmn2 mm over a large tracking volume.},
keywords={motion compensation;optical tracking;stereo image processing;optical tracking;commodity hardware;Nintendo Wii controllers;stereo vision system;3D tracking;motion capture;Cameras;Calibration;Optical sensors;Light emitting diodes;Accuracy;Tracking;Hardware;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, augmented and virtual realities;I.4.1 [Image Processing and Computer Vision]: Digitization and Image Capture},
doi={10.1109/ISMAR.2008.4637345},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637346,
author={J. {Hwang} and {Sangyup Lee} and {Sang Chul Ahn} and {Hyoung-gon Kim}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Augmented robot agent: Enhancing co-presence of the remote participant},
year={2008},
volume={},
number={},
pages={161-162},
abstract={In this paper, we present a tele-meeting system which uses an augmented robot agent as the representation of the remote participant. In this system, we augment a 3D volume video of the remote participant over the on-site robot. The robot agent in this system represents a remote user with camera, microphone, and mobility. Using this robot agent, the remote user and the local user can show their appearances and interact with tangible interfaces. This robot agent system can be applied to various tele-meeting applications such as tele-marketing and tele-tutoring. For example, we implemented a tele-marketing system to show the feasibility of the suggested system.},
keywords={augmented reality;telerobotics;user interfaces;augmented robot agent;tele-meeting system;3D volume video augmentation;tangible interface;tele-marketing;tele-tutoring;Robots;Three dimensional displays;Visualization;Cameras;Robot vision systems;Real time systems;Electronic mail;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities},
doi={10.1109/ISMAR.2008.4637346},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637347,
author={C. {Scherrer} and J. {Pilet} and P. {Fua} and V. {Lepetit}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={The haunted book},
year={2008},
volume={},
number={},
pages={163-164},
abstract={This paper describes an artwork that relies on recent computer vision and augmented reality techniques to animate the illustrations of a poetry book. Because we donpsilat need markers, we can achieve seamless integration of real and virtual elements to create the desired atmosphere. The visualization is done on a computer screen to avoid cumbersome head-mounted displays. The camera is hidden into a desk lamp for easing even more the spectator immersion.},
keywords={art;augmented reality;computer displays;computer vision;artwork;computer vision;augmented reality techniques;poetry book;visualization;computer screen;haunted book;Books;Augmented reality;Computers;Visualization;Electronic mail;Animation;Cameras},
doi={10.1109/ISMAR.2008.4637347},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637348,
author={C. {Bichlmeier} and B. {Ockert} and S. M. {Heining} and A. {Ahmadi} and N. {Navab}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Stepping into the operating theater: ARAV — Augmented Reality Aided Vertebroplasty},
year={2008},
volume={},
number={},
pages={165-166},
abstract={Augmented reality (AR) for preoperative diagnostics and planning, intra operative navigation and postoperative follow-up examination has been a topic of intensive research over the last two decades. However, clinical studies showing AR technology integrated into the real clinical environment and workflow are still rare. The incorporation of an AR system as a standard tool into the real clinical workflow has not been presented so far. This paper reports on the strategies and intermediate results of the ARAV - augmented reality aided vertebroplasty project that has been initiated to make an AR system based on a stereo video see-through head mounted display that is permanently available in the operating room (OR).},
keywords={augmented reality;medical computing;ARAV;augmented reality aided vertebroplasty;preoperative diagnostics;preoperative planning;intraoperative navigation;postoperative follow-up examination;operating room;Surgery;Fault currents;Computed tomography;Current transformers;Circuit faults;Biomedical imaging;Augmented reality;H.5.1 [Information Interfaces and Presentation]: Artificial, augmented, and virtual realities;J.3 [Life and Medical Sciences]},
doi={10.1109/ISMAR.2008.4637348},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637349,
author={M. {Hakkarainen} and C. {Woodward} and M. {Billinghurst}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Augmented assembly using a mobile phone},
year={2008},
volume={},
number={},
pages={167-168},
abstract={We present a mobile phone based augmented reality (AR) assembly system that enable users to view complex models on their mobile phones. It is based on a client-server architecture, where complex model information is located on a PC, and a mobile phone with the camera is used as a thin client access device to this information. With this system users are able to see an AR view that provides step by step guidance for a real world assembly task. We also present results from a pilot user study evaluating the system, showing that people felt the interface was intuitive and very helpful in supporting the assembly task.},
keywords={assembling;augmented reality;mobile handsets;augmented assembly system;mobile phone based augmented reality;client-server architecture;complex model information;thin client access device;augmented reality view;assembly task;Assembly;Mobile handsets;Servers;Augmented reality;Maintenance engineering;Solid modeling;Software},
doi={10.1109/ISMAR.2008.4637349},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637350,
author={M. {Dima} and D. K. {Arvind} and J. {Lee} and M. {Wright}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Haptically extended augmented prototyping},
year={2008},
volume={},
number={},
pages={169-170},
abstract={This project presents a new display concept, which brings together haptics, augmented and mixed reality and tangible computing within the context of an intuitive conceptual design environment. The project extends the paradigm of augmented prototyping by allowing modelling of virtual geometry on the physical prototype, which can be touched by means of a haptic device. Wireless tracking of the physical prototype is achieved in three different ways by attaching to it a 'Speck', a tracker and Nintendo Wii Remote and it provides continuous tangible interaction. The physical prototype becomes a tangible interface augmented with mixed reality and with a novel 3D haptic design system.},
keywords={augmented reality;haptic interfaces;software prototyping;extended haptic augmented prototyping;augmented reality;mixed reality;tangible computing;intuitive conceptual design environment;virtual geometry modelling;haptic device;wireless tracking;3D haptic design system;Prototypes;Haptic interfaces;Mirrors;Augmented reality;Three dimensional displays;Geometry;Glass;Tangible Computing;Mixed Reality;Augmented Reality;Product Design;Tactile & Haptic UIs;H.5.2 User Interfaces},
doi={10.1109/ISMAR.2008.4637350},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637351,
author={P. {Georgel} and P. {Schroeder} and S. {Benhimane} and M. {Appel} and N. {Navab}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={How to augment the second image? Recovery of the translation scale in image to image registration},
year={2008},
volume={},
number={},
pages={171-172},
abstract={In this paper, we present an automatic pose estimation (6 DoF) technique to augment images using keyframes pre-registered to a CAD model. State of the art techniques recover the essential matrix (5 DoF) in an automatic manner, but include a manual step to align the image with the CAD reference system because the essential matrix does not provide the scale of the translation. We propose using planar structures to recover this scale automatically and to offer immediate augmentation. These techniques have been implemented in our augmented reality software. Qualitative tests are performed in an industrial environment.},
keywords={augmented reality;CAD;image registration;pose estimation;image translation scale;image registration;automatic pose estimation technique;CAD reference system;planar structures;augmented reality software;Three dimensional displays;Solid modeling;Design automation;Computational modeling;Streaming media;Distance measurement;Augmented reality},
doi={10.1109/ISMAR.2008.4637351},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637352,
author={ {Gaku Nakano} and {Itaru Kitahara} and {Yuichi Ohta}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Generating perceptually-correct shadows for mixed reality},
year={2008},
volume={},
number={},
pages={173-174},
abstract={When human cannot perceive the inconsistency of artificial shadows which are not physically correct, they are acceptable as ldquoperceptually-correctrdquo shadows. This paper focuses on the simplification of light-source models for generating the perceptually-correct artificial shadows. First, we conducted subjective evaluations to obtain knowledge about the human perception of the shadows. Then the knowledge was applied to control the resolution of the light-source map to generate perceptually-correct artificial shadows. Comparative studies among artificial and real shadows justified perceptually correctness. All experiments were done using still images, not videos. Our research becomes a reference to determine the resolution of light-source map in an MR scene.},
keywords={virtual reality;mixed reality;light-source models;perceptually-correct artificial shadows;still images;Fluorescence;Image generation;Virtual reality;Image resolution;Augmented reality;Lighting;Distance measurement;[I.3.7] Three-Dimensional Graphics and Realism;Photometric Consistency;Soft Shadow;Image Based Lighting;Subjective Evaluation Experiment},
doi={10.1109/ISMAR.2008.4637352},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637353,
author={H. M. {Park} and {Seok Han Lee} and {Jong Soo Choi}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Wearable augmented reality system using gaze interaction},
year={2008},
volume={},
number={},
pages={175-176},
abstract={Undisturbed interaction is essential to provide immersive AR environments. There have been a lot of approaches to interact with VEs (virtual environments) so far, especially in hand metaphor. When the userpsilas hands are being used for hand-based work such as maintenance and repair, necessity of alternative interaction technique has arisen. In recent research, hands-free gaze information is adopted to AR to perform original actions in concurrence with interaction. [3, 4]. There has been little progress on that research, still at a pilot study in a laboratory setting. In this paper, we introduce such a simple WARS (wearable augmented reality system) equipped with an HMD, scene camera, eye tracker. We propose dasiaAgingpsila technique improving traditional dwell-time selection, demonstrate AR gallery - dynamic exhibition space with wearable system.},
keywords={augmented reality;helmet mounted displays;human computer interaction;motion estimation;wearable computers;wearable augmented reality system;gaze interaction;AR;virtual environments;VE;hand-based work;alternative interaction technique;WARS;HMD;head mounted display;scene camera;eye tracker;Aging;Augmented reality;Maintenance engineering;Virtual environment;Integrated optics;Cameras;Art;wearable;augmented reality;gaze interaction;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities;H.5.2 [Information Interfaces and Presentation]: User Interfaces},
doi={10.1109/ISMAR.2008.4637353},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637354,
author={H. {Seichter} and J. {Looser} and M. {Billinghurst}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={ComposAR: An intuitive tool for authoring AR applications},
year={2008},
volume={},
number={},
pages={177-178},
abstract={This paper introduces ComposAR, a tool to allow a wide audience to author AR and MR applications. It is unique in that it supports both visual programming and interpretive scripting, and an immediate mode for runtime testing. ComposAR is written in Python which means the user interface and runtime behavior can be easily customized and third-party modules can be incorporated into the authoring environment. We describe the design philosophy and the resulting user interface, lessons learned and directions for future research.},
keywords={augmented reality;authoring systems;graphical user interfaces;program testing;visual programming;augmented reality;mixed reality;authoring tool;visual programming;interpretive scripting;runtime testing;Python;ComposAR;user interface;Augmented reality;Three dimensional displays;Virtual reality;Graphical user interfaces;Programming;Visualization;User interfaces;H.5.1 [Multimedia Information Systems]: Artificial, augmented, and virtual realities;H.5.2 [User Interfaces]: Graphical user interfaces (GUI)},
doi={10.1109/ISMAR.2008.4637354},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637355,
author={A. {Sherstyuk} and A. {Treskunov} and B. {Berg}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Fast geometry acquisition for mixed reality applications using motion tracking},
year={2008},
volume={},
number={},
pages={179-180},
abstract={Mixing real and virtual elements into one environment often involves creating geometry models of physical objects. Traditional approaches include manual modeling by 3D artists or use of dedicated devices. Both approaches require special skills or special hardware and may be costly. We propose a new method for fast semi-automatic 3D geometry acquisition, based upon unconventional use of motion tracking equipment. The proposed method is intended for quick surface prototyping for virtual, augmented and mixed reality applications where quality of visualization of objects is not required or is of low priority.},
keywords={augmented reality;path planning;motion tracking;fast semiautomatic 3D geometry acquisition;surface prototyping;virtual reality;augmented reality;mixed reality;Three dimensional displays;Radar tracking;Tracking;Solid modeling;Shape;Measurement by laser beam;Laser radar},
doi={10.1109/ISMAR.2008.4637355},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637356,
author={A. {Stafford} and B. H. {Thomas} and W. {Piekarski}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Efficiency of techniques for mixed-space collaborative navigation},
year={2008},
volume={},
number={},
pages={181-182},
abstract={This paper describes the results of a study conducted to determine the efficiency of visual cues for a collaborative navigation task in a mixed-space environment. The task required a user with an exocentric view of a virtual room to navigate a fully immersed user with an egocentric view to an exit. The study compares natural hand-based gestures, a mouse-based interface and an audio only technique to determine their relative efficiency on task completion times. The results show that visual cue-based collaborative navigation techniques are significantly more efficient than an audio-only technique.},
keywords={groupware;mouse controllers (computers);navigation;user interfaces;mixed-space collaborative navigation;natural hand-based gestures;mouse-based interface;audio only technique;visual cue;Navigation;Collaboration;Visualization;Augmented reality;Three dimensional displays;Computers;Electronic mail;H.5.2 [Information Interfaces and Presentation]: User Interfaces—Evaluation/methodology;I.3.6 [Computer Graphics]: Methodology and Techniques—Interaction techniques;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism—Virtual reality},
doi={10.1109/ISMAR.2008.4637356},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637357,
author={J. {Steinbis} and W. {Hoff} and T. L. {Vincent}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={3D fiducials for scalable AR visual tracking},
year={2008},
volume={},
number={},
pages={183-184},
abstract={A new vision and inertial pose estimation system was implemented for real-time handheld augmented reality (AR). A sparse set of 3D cone fiducials are utilized for scalable indoor/outdoor tracking, as opposed to traditional planar patterns. The cones are easy to segment and have a large working volume which makes them more suitable for many applications. The pose estimation system receives measurements from the camera and IMU at 30 Hz and 100 Hz respectively. With a dual-core workstation, all measurements can be processed in real-time to update the pose of virtual graphics within the AR display.},
keywords={augmented reality;pose estimation;real-time systems;3D fiducials;scalable AR visual tracking;vision pose estimation system;inertial pose estimation system;real-time handheld augmented reality;virtual graphics;frequency 30 Hz;frequency 100 Hz;Augmented reality;Image segmentation;Cameras;Three dimensional displays;Distance measurement;Real time systems;Image color analysis;Augmented Reality;Pose Estimation;H.5.2 [Information Interfaces and Presentation]: User Interfaces—Graphical User Interfaces},
doi={10.1109/ISMAR.2008.4637357},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637358,
author={T. N. {Hoang} and B. H. {Thomas}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Augmented reality in-situ 3D model menu for outdoors},
year={2008},
volume={},
number={},
pages={185-186},
abstract={We present a design and implementation of an in-situation menu system for loading and visualising 3D models in a physical world context. The menu system uses 3D objects as menu items, and the whole menu is placed within the context of the augmented environment. The use of 3D objects supports the visualisation and placement of 3D models into the augmented world. The menu system employs techniques for the placement of 3D models in two relative coordinate systems: head relative and world relative.},
keywords={augmented reality;data visualisation;graphical user interfaces;augmented reality;in-situ 3D model menu;outdoors;head relative coordinate system;world relative coordinate system;H.5.2 [User Interfaces]: Graphical user interfaces (GUI) — Interaction styles;I.3.7 [Three-Dimensional Graphics and Realism]: Virtual reality — Animation},
doi={10.1109/ISMAR.2008.4637358},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637359,
author={M. {Heinrich} and B. H. {Thomas} and S. {Mueller}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={ARWeather — An Augmented Reality Weather ystem},
year={2008},
volume={},
number={},
pages={187-188},
abstract={This paper presents the design and development of an ARWeather simulation application, which can simulate various types of precipitation: rain, snow, and hail. We analysed various real occurrences weather types and how they could be simulated in a mobile Augmented Reality system. The Tinmith system is wearable computer system for the development and deployment of the final ARWeather system that allows for autonomous and free movement for the user. The users can move freely inside the simulated weather without limitation.},
keywords={augmented reality;mobile computing;wearable computers;augmented reality weather system;ARWeather simulation application;weather types;mobile augmented reality system;Tinmith system;wearable computer system;ARWeather system;simulated weather;Meteorology;Rain;Snow;Atmospheric modeling;Augmented reality;Solid modeling;Computational modeling;I.3.7 [Three-Dimensional Graphics and Realism]: Virtual reality—Animation;I.6.8 [Types of Simulation]: Animation—Visual},
doi={10.1109/ISMAR.2008.4637359},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637360,
author={M. {Tonnis} and L. {Klein} and G. {Klinker}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Perception thresholds for augmented reality navigation schemes in large distances},
year={2008},
volume={},
number={},
pages={189-190},
abstract={Because the resolution of see-through displays is lower than the resolution of the human eye, perception of AR schemes is complicated in large distances. To discover, how design issues of perception correlate with presentation in large distances, we developed three different variants of arrow-based route guidance systems. With a large-scale head-up display having a large focal depth, we tested the variants under different conditions on perception and interpretability.},
keywords={augmented reality;head-up displays;human factors;user interfaces;perception thresholds;augmented reality navigation;see-through display;arrow-based route guidance system;head-up display;user interface;Distance measurement;Shape;Navigation;Augmented reality;Visualization;Pixel;Humans;H.1.2 [Models and Principles]: User/Machine Systems—Human information processing;H.5.2 [Information Interfaces and Presentation]: User Interfaces—Prototyping},
doi={10.1109/ISMAR.2008.4637360},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637361,
author={},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={State of the art report (STAR)},
year={2008},
volume={},
number={},
pages={191-192},
abstract={Start of the above-titled section of the conference proceedings record.},
keywords={},
doi={10.1109/ISMAR.2008.4637361},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637362,
author={ {Feng Zhou} and H. B. {Duh} and M. {Billinghurst}},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Trends in augmented reality tracking, interaction and display: A review of ten years of ISMAR},
year={2008},
volume={},
number={},
pages={193-202},
abstract={Although Augmented Reality technology was first developed over forty years ago, there has been little survey work giving an overview of recent research in the field. This paper reviews the ten-year development of the work presented at the ISMAR conference and its predecessors with a particular focus on tracking, interaction and display research. It provides a roadmap for future augmented reality research which will be of great value to this relatively young field, and also for helping researchers decide which topics should be explored when they are beginning their own studies in the area.},
keywords={augmented reality;augmented reality;virtual reality;ISMAR;Solid modeling;Tracking;Computational modeling;Visualization;Collaboration;Three dimensional displays;Sensors;H5.1. [Information System]: Multimedia information systems—Artificial, augmented, and virtual realities;Augmented reality;tracking;interaction;calibration and registration;AR application;AR display},
doi={10.1109/ISMAR.2008.4637362},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637363,
author={},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Panel},
year={2008},
volume={},
number={},
pages={203-208},
abstract={Provides an abstract of the presentation and a brief professional biography of the presenter. The complete presentation was not made available for publication as part of the conference proceedings.},
keywords={},
doi={10.1109/ISMAR.2008.4637363},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637364,
author={},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Tutorials and workshop},
year={2008},
volume={},
number={},
pages={209-212},
abstract={Provides an abstract for each of the presentations and a brief professional biography of each presenter. The complete presentations were not made available for publication as part of the conference proceedings.},
keywords={},
doi={10.1109/ISMAR.2008.4637364},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637365,
author={},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={[Front cover]},
year={2008},
volume={},
number={},
pages={c1-c1},
abstract={Presents the front cover or splash screen of the proceedings.},
keywords={},
doi={10.1109/ISMAR.2008.4637365},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4637366,
author={},
booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Author index},
year={2008},
volume={},
number={},
pages={i-iv},
abstract={The author index contains an entry for each author and coauthor included in the proceedings record.},
keywords={},
doi={10.1109/ISMAR.2008.4637366},
ISSN={},
month={Sep.},}