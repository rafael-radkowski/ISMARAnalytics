@INPROCEEDINGS{5643531,
author={},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={[Title page]},
year={2010},
volume={},
number={},
pages={i-i},
abstract={Conference proceedings title page.},
keywords={},
doi={10.1109/ISMAR.2010.5643531},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643532,
author={},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={[Copyright notice]},
year={2010},
volume={},
number={},
pages={ii-ii},
abstract={Presents the copyright information for the conference. May include reprint permission information.},
keywords={},
doi={10.1109/ISMAR.2010.5643532},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643533,
author={},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Contents},
year={2010},
volume={},
number={},
pages={iii-vii},
abstract={Presents the table of contents of the proceedings.},
keywords={},
doi={10.1109/ISMAR.2010.5643533},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643534,
author={},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Supporting organizations},
year={2010},
volume={},
number={},
pages={viii-viii},
abstract={The conference organizers greatly appreciate the support of the various corporate sponsors listed.},
keywords={},
doi={10.1109/ISMAR.2010.5643534},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643535,
author={H. {Ko} and G. J. {Kim}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={From the symposium general chairs},
year={2010},
volume={},
number={},
pages={ix-ix},
abstract={Welcome to ISMAR 2010 in Seoul, Korea. This year's ISMAR already marks its 9th occasion and it seems that mixed and augmented reality technology has finally come of age, attracting the due attention from the general public. This is especially due to the popularity of the smart mobile devices, and thus, it is quite fitting that ISMAR is being held in Seoul, one of the most networked and dynamic place in adopting new information technology and their deployments. Especially with recent explosion of smart phones, Korea may provide a fertile ground for the wide deployment of mobile augmented reality technology on smart phones.},
keywords={},
doi={10.1109/ISMAR.2010.5643535},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643536,
author={T. {Höllerer} and V. {Lepetit} and J. {Park}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={From the Science Technology program chairs},
year={2010},
volume={},
number={},
pages={x-xi},
abstract={Welcome to the ISMAR 2010 Science and Technology (S&T) track, which follows in the proud tradition of several IWAR, ISMR, and ISAR meetings, as well as eight previous ISMAR symposia to bring you research excellence in mixed and augmented reality.},
keywords={},
doi={10.1109/ISMAR.2010.5643536},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643537,
author={},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={IEEE Visualization and Graphics Technical Committee (VGTC)},
year={2010},
volume={},
number={},
pages={xii-xii},
abstract={The IEEE Visualization and Graphics Technical Committee (VGTC) is a formal subcommittee of the Technical Activities Board (TAB) of the IEEE Computer Society. The VGTC provides technical leadership and organizes technical activities in the areas of visualization, computer graphics, virtual and augmented reality, and interaction.},
keywords={},
doi={10.1109/ISMAR.2010.5643537},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643538,
author={},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Task force on Human centered Computing (TFHCC)},
year={2010},
volume={},
number={},
pages={xiii-xiii},
abstract={The field of Human-Centered Computing (HCC) has emerged from the convergence of multiple disciplines and research areas that are concerned both with understanding human beings and with the design of computational devices and interfaces. Researchers and designers of human-centered computing include individuals from computer science, sociology, psychology, cognitive science, engineering, graphic design, and industrial design.},
keywords={},
doi={10.1109/ISMAR.2010.5643538},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643539,
author={},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Conference committee},
year={2010},
volume={},
number={},
pages={xiv-xiv},
abstract={Provides a listing of current committee members.},
keywords={},
doi={10.1109/ISMAR.2010.5643539},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643540,
author={},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={International Program Committee and Reviewers},
year={2010},
volume={},
number={},
pages={xi-xiii},
abstract={Provides a listing of current committee members.},
keywords={},
doi={10.1109/ISMAR.2010.5643540},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643541,
author={H. {Fuchs}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Augmenting reality for medicine, training, presence and telepresence},
year={2010},
volume={},
number={},
pages={xiv-xiv},
abstract={At least since Sutherland's 1968 head-mounted display, augmented reality systems have inspired (and frustrated) generations of developers, users, and enthusiasts. These inspirations have led to decades of effort that yielded major innovations in technologies for 3D capture, 3D displays, tracking, and real-time image generation. Some of these component technologies, such as head-mounted displays, have proven much more difficult to bring to widespread adoption than many of us expected. Others, such as real-time image generation, have become phenomenally successful, with major societal impacts extending far beyond the initial applications. In this talk, I will describe several developments in these areas, and illustrate the resulting systems that exploited them: 1) enhancing 3D scene acquisition by laser scanning, or with structured light, or with multiple acquisition cameras; 2) augmenting a physician's view of her patient with registered internal imagery; 3) augmenting a user's surroundings with projection onto multiple nearby surfaces; 4) augmenting a user's remote presence using a human-sized avatar that mimics appearance, pose, and gestures; 5) augmenting tabletop displays with multi-user autostereoscopic capabilities. The common goal of all these systems is to enrich users' immediate surroundings with computer-generated or -controlled enhancements, which can run the gamut from mere virtual imagery to full-fledged robotic androids. I will also speculate about the possible paths to progress in the coming years. The future is encouraging, as the decreasing cost of the necessary components lowers the barriers to entry and encourages ever-increasing participation in innovation, development and use. Coupled with the rapidly advancing technologies in sensors, cameras, displays, robotics, and networks, this should enable us to accelerate bringing the visions of augmenting reality to daily life.},
keywords={augmented reality;avatars;helmet mounted displays;medical computing;realistic images;solid modelling;telecontrol;head mounted display;augmented reality system;real time image generation;3D scene acquisition;multiple acquisition camera;internal patient imagery;human sized avatar;virtual imagery;telepresence application},
doi={10.1109/ISMAR.2010.5643541},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643542,
author={B. {Debackere}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Augmented dreams},
year={2010},
volume={},
number={},
pages={xv-xv},
abstract={Many forms of artistic expression might be considered as an augmentation of reality. What are the challenges for Augmented Reality - technologies that enhance physical reality by layering interactive computer-generated content to it - when used as an artistic medium? While the underlying technologies (tracking, displaying and rendering) that drive augmented reality focus on a transparent creation, distribution and access of information the artist should critically examine their implementation. V2_Lab considers the artistic Research and Development (aRt&D) as an important element that can introduce specific qualities in the field of technological innovation and realization. Especially in the interdisciplinary collaboration with engineers and computer scientists, the artistic approach is unconventional in connecting different research fields, leaving behind discipline-specific paradigms.},
keywords={art;augmented reality;interactive systems;augmented dream;augmented reality;interactive computer generated content;V2_Lab;artistic research and development},
doi={10.1109/ISMAR.2010.5643542},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643543,
author={},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={[Blank page]},
year={2010},
volume={},
number={},
pages={1-1},
abstract={This page or pages intentionally left blank.},
keywords={},
doi={10.1109/ISMAR.2010.5643543},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643528,
author={},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Science technology papers [breaker page]},
year={2010},
volume={},
number={},
pages={1-1},
abstract={Start of the "Science & technology papers" section of the conference proceedings.},
keywords={},
doi={10.1109/ISMAR.2010.5643528},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643529,
author={},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={[Blank page]},
year={2010},
volume={},
number={},
pages={2-2},
abstract={This page or pages intentionally left blank.},
keywords={},
doi={10.1109/ISMAR.2010.5643529},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643530,
author={E. {Kruijff} and J. E. {Swan} and S. {Feiner}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Perceptual issues in augmented reality revisited},
year={2010},
volume={},
number={},
pages={3-12},
abstract={This paper provides a classification of perceptual issues in augmented reality, created with a visual processing and interpretation pipeline in mind. We organize issues into ones related to the environment, capturing, augmentation, display, and individual user differences. We also illuminate issues associated with more recent platforms such as handhelds or projector-camera systems. Throughout, we describe current approaches to addressing these problems, and suggest directions for future research.},
keywords={virtual reality;visual perception;augmented reality;visual processing;interpretation pipeline;perceptual issues;Image color analysis;Lenses;Cameras;Visualization;Lighting;Adaptive optics;Object segmentation;Human perception;augmented reality;handheld devices;mobile computing},
doi={10.1109/ISMAR.2010.5643530},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643544,
author={T. {Blum} and M. {Wieczorek} and A. {Aichert} and R. {Tibrewal} and N. {Navab}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={The effect of out-of-focus blur on visual discomfort when using stereo displays},
year={2010},
volume={},
number={},
pages={13-17},
abstract={Visual discomfort is a major problem for head-mounted displays and other stereo displays. One effect that is known to reduce visual comfort is double vision, which can occur due to high disparities. Previous studies suggest that adding artificial out-of-focus blur increases the fusional limits, where the left and right image can be fused without double vision. We investigate the effect of adding artificial out-of-focus blur on visual discomfort using two different setups. One uses a stereo monitor and an eye tracker to change the depth of focus based on the gaze of the user. The other one uses a video-see through head mounted display. A study involving 18 subjects showed that the viewing comfort when using blur is significantly higher in both setups for virtual scenes. However we can not confirm without doubt that the higher viewing comfort is only related to an increase of the fusional limits, as many subjects reported that double vision did not occur during the experiment. Results for additional photographed images that have been shown to the subjects were less significant. A first prototype of an AR system extracting a depth map from stereo images and adding artificial out-of-focus blur is presented.},
keywords={helmet mounted displays;stereo image processing;virtual reality;out-of-focus blur;visual discomfort;stereo displays;head mounted displays;double vision;stereo monitor;eye tracker;virtual scenes;depth map;Cameras;Visualization;Monitoring;Stereo vision;Pixel;Augmented reality;Three dimensional displays;H.5.1 [Information Interfaces and Presentation]: Artificial, augmented, and virtual realities},
doi={10.1109/ISMAR.2010.5643544},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643545,
author={},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={[Blank page]},
year={2010},
volume={},
number={},
pages={18-18},
abstract={This page or pages intentionally left blank.},
keywords={},
doi={10.1109/ISMAR.2010.5643545},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643546,
author={S. {Zollmann} and D. {Kalkofen} and E. {Mendez} and G. {Reitmayr}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Image-based ghostings for single layer occlusions in augmented reality},
year={2010},
volume={},
number={},
pages={19-26},
abstract={In augmented reality displays, X-Ray visualization techniques make hidden objects visible through combining the physical view with an artificial rendering of the hidden information. An important step in X-Ray visualization is to decide which parts of the physical scene should be kept and which should be replaced by overlays. The combination should provide users with essential perceptual cues to understand the relationship of depth between hidden information and the physical scene. In this paper we present an approach that addresses this decision in unknown environments by analyzing camera images of the physical scene and using the extracted information for occlusion management. Pixels are grouped into perceptually coherent image regions and a set of parameters is determined for each region. The parameters change the X-Ray visualization for either preserving existing structures or generating synthetic structures. Finally, users can customize the overall opacity of foreground regions to adapt the visualization.},
keywords={augmented reality;cameras;data visualisation;feature extraction;hidden feature removal;rendering (computer graphics);solid modelling;X-ray imaging;image based ghosting;single layer occlusion management;augmented reality display;X-Ray visualization technique;artificial rendering;camera images analysis;information extraction;Pixel;Image edge detection;Transfer functions;Cameras;Image color analysis;Solid modeling;Visualization;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems— Artificial, augmented, and virtual realities;I.4.8 [Image Processing and Computer Vision]: Scene Analysis—Depth cues},
doi={10.1109/ISMAR.2010.5643546},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643547,
author={C. {Sandor} and A. {Cunningham} and A. {Dey} and V. {Mattila}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={An Augmented Reality X-Ray system based on visual saliency},
year={2010},
volume={},
number={},
pages={27-36},
abstract={In the past, several systems have been presented that enable users to view occluded points of interest using Augmented Reality X-ray visualizations. It is challenging to design a visualization that provides correct occlusions between occluder and occluded objects while maximizing legibility. We have previously published an Augmented Reality X-ray visualization that renders edges of the occluder region over the occluded region to facilitate correct occlusions while providing foreground context. While this approach is simple and works in a wide range of situations, it provides only minimal context of the occluder object. In this paper, we present the background, design, and implementation of our novel visualization technique that aims at providing users with richer context of the occluder object. While our previous visualization only employed one salient feature (edges) to determine which parts of the occluder to display, our novel visualization technique is an initial attempt to explore the design space of employing multiple salient features for this task. The prototype presented in this paper employs three additional salient features: hue, luminosity, and motion. We have conducted two evaluations with human participants to investigate the benefits and limitations of our prototype compared to our previous system. The first evaluation showed that although our novel visualization provides a richer context of the occluder object, it does not impede users to select objects in the occluded area; but, it also indicated problems in our prototype. In the second evaluation, we have investigated these problems through an online survey with systematically varied occluder and occluded scenes, focussing on the qualitative aspects of our visualizations. The results were encouraging, but pointed out that our novel visualization needs a higher level of adaptiveness.},
keywords={augmented reality;data visualisation;X-ray imaging;visual saliency;augmented reality x-ray visualizations;occluder object;Visualization;Context;Rendering (computer graphics);Image color analysis;X-ray imaging;Image edge detection;Prototypes;augmented reality;visualization;evaluation;augmented reality X-ray;saliency},
doi={10.1109/ISMAR.2010.5643547},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643548,
author={D. {Pustka} and J. {Willneff} and O. {Wenisch} and P. {Lükewille} and K. {Achatz} and P. {Keitler} and G. {Klinker}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Determining the point of minimum error for 6DOF pose uncertainty representation},
year={2010},
volume={},
number={},
pages={37-45},
abstract={In many augmented reality applications, in particular in the medical and industrial domains, knowledge about tracking errors is important. Most current approaches characterize tracking errors by 6×6 covariance matrices that describe the uncertainty of a 6DOF pose, where the center of rotational error lies in the origin of a target coordinate system. This origin is assumed to coincide with the geometric centroid of a tracking target. In this paper, we show that, in case of a multi-camera fiducial tracking system, the geometric centroid of a body does not necessarily coincide with the point of minimum error. The latter is not fixed to a particular location, but moves, depending on the individual observations. We describe how to compute this point of minimum error given a covariance matrix and verify the validity of the approach using Monte Carlo simulations on a number of scenarios. Looking at the movement of the point of minimum error, we find that it can be located surprisingly far away from its expected position. This is further validated by an experiment using a real camera system.},
keywords={augmented reality;cameras;covariance matrices;measurement uncertainty;Monte Carlo methods;optical tracking;pose estimation;position measurement;6D0F pose uncertainty representation;augmented reality;tracking error;covariance matrices;geometric centroid;target tracking;multicamera fiducial tracking system;Monte Carlo simulation;point of minimum error;Cameras;Covariance matrix;Target tracking;Monte Carlo methods;Uncertainty;Erbium;Jacobian matrices;I.4.8 [Image Processing and Computer Vision]: Scene Analysis—Tracking;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities},
doi={10.1109/ISMAR.2010.5643548},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643549,
author={},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={[Blank page]},
year={2010},
volume={},
number={},
pages={46-46},
abstract={This page or pages intentionally left blank.},
keywords={},
doi={10.1109/ISMAR.2010.5643549},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643550,
author={A. {Dame} and E. {Marchand}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Accurate real-time tracking using mutual information},
year={2010},
volume={},
number={},
pages={47-56},
abstract={In this paper we present a direct tracking approach that uses Mutual Information (MI) as a metric for alignment. The proposed approach is robust, real-time and gives an accurate estimation of the displacement that makes it adapted to augmented reality applications. MI is a measure of the quantity of information shared by signals that has been widely used in medical applications. Since then, and although MI has the ability to perform robust alignment with illumination changes, multi-modality and partial occlusions, few works propose MI-based applications related to object tracking in image sequences due to some optimization problems. In this work, we propose an optimization method that is adapted to the MI cost function and gives a practical solution for augmented reality application. We show that by refining the computation of the Hessian matrix and using a specific optimization approach, the tracking results are far more robust and accurate than the existing solutions. A new approach is also proposed to speed up the computation of the derivatives and keep the same optimization efficiency. To validate the advantages of the proposed approach, several experiments are performed. The ESM and the proposed MI tracking approaches are compared on a standard dataset. We also show the robustness of the proposed approach on registration applications with different sensor modalities: map versus satellite images and satellite images versus airborne infrared images within different AR applications.},
keywords={augmented reality;geophysical image processing;Hessian matrices;image registration;image sequences;optical tracking;optimisation;target tracking;real-time tracking;mutual information;displacement estimation;augmented reality;illumination;multimodality;partial occlusion;MI-based application;image sequence;object tracking;optimization;Hessian matrix;image registration;sensor modality;satellite image;airborne infrared image;Mutual information;Histograms;Entropy;Joints;Robustness;Equations;Estimation},
doi={10.1109/ISMAR.2010.5643550},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643551,
author={W. {Lee} and Y. {Park} and V. {Lepetit} and W. {Woo}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Point-and-shoot for ubiquitous tagging on mobile phones},
year={2010},
volume={},
number={},
pages={57-64},
abstract={We propose a novel way to augment a real scene with minimalist user intervention on a mobile phone: The user only has to point the phone camera to the desired location of the augmentation. Our method is valid for vertical or horizontal surfaces only, but this is not a restriction in practice in man-made environments, and avoids to go through any reconstruction of the 3D scene, which is still a delicate process. Our approach is inspired by recent work on perspective patch recognition and we show how to modify it for better performances on mobile phones and how to exploit the phone accelerometers to relax the need for fronto-parallel views. In addition, our implementation allows to share the augmentations and the required data over peer-to-peer communication to build a shared AR space on mobile phones.},
keywords={accelerometers;augmented reality;cameras;image recognition;image reconstruction;mobile computing;mobile handsets;peer-to-peer computing;ubiquitous tagging;mobile phone;user intervention;phone camera;real scene augmentation;3D scene reconstruction;patch recognition;phone accelerometer;peer-to-peer communication;AR space;augmented reality;point-and-shoot;Cameras;Mobile handsets;Three dimensional displays;Graphics processing unit;Accelerometers;Kernel;Pixel},
doi={10.1109/ISMAR.2010.5643551},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643552,
author={S. {Martedi} and H. {Uchiyama} and G. {Enriquez} and H. {Saito} and T. {Miyashita} and T. {Hara}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Foldable augmented maps},
year={2010},
volume={},
number={},
pages={65-72},
abstract={This paper presents folded surface detection and tracking for augmented maps. For the detection, plane detection is iteratively applied to 2D correspondences between an input image and a reference plane because the folded surface is composed of multiple planes. In order to compute the exact folding line from the detected planes, the intersection line of the planes is computed from their positional relationship. After the detection is done, each plane is individually tracked by frame-by-frame descriptor update. For a natural augmentation on the folded surface, we overlay virtual geographic data on each detected plane. The user can interact with the geographic data by finger pointing because the finger tip of the user is also detected during the tracking. As scenario of use, some interactions on the folded surface are introduced. Experimental results show the accuracy and performance of folded surface detection for evaluating the effectiveness of our approach.},
keywords={augmented reality;feature extraction;object detection;foldable augmented map;folded surface detection;folding line;intersection line;frame-by-frame descriptor update;natural augmentation;virtual geographic data;finger pointing;finger tip detection;Cameras;Databases;Solid modeling;Estimation;Image edge detection;Electronic mail;Three dimensional displays;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities},
doi={10.1109/ISMAR.2010.5643552},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643553,
author={P. {Keitler} and B. {Becker} and G. {Klinker}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Management of tracking for industrial AR setups},
year={2010},
volume={},
number={},
pages={73-82},
abstract={The accuracy of a real time tracking system for industrial AR (IAR) applications often needs to comply with production tolerances. Such a system typically incorporates different off-/online devices so that the overall precision and accuracy cannot be trivially stated. Additionally, tracking needs to be flexible to not interfere with existing working processes and it needs to be operated and maintained free of error by on-site personnel who typically have a quality management (QM) background. For the final validation of such a complex tracking setup, empiric testing alone is either too expensive or lacks generality. This paper demonstrates a new approach to define and verify, deploy and validate, as well as to operate and maintain an IAR tracking infrastructure. We develop our concepts on the basis of an IAR application in the field of QM in the aircraft production process. It integrates a qualitative visual comparison with accurate quantitative measurements of 3D coordinates using a metrological probe. The focus is on the verification, validation, and error free operation. Monte Carlo simulation predicts the error for arbitrary system states. Using a limited set of empiric measurements in the target environment allows us to validate the simulation and thereby validate the application. This combination assures compliance of the IAR application with the required production tolerances. We show that our simulation model yields realistic results, using an in-depth analysis of an optical IR tracking system and a high-precision coordinate measurement machine capable of densely sampling the entire tracking volume. Additionally, it allows for a straightforward derivation of run-time consistency checks for the automatic identification of possible system failures. Also, estimation of the system performance during the planning and definition phases becomes possible, using the elementary accuracy specifications of the involved sensor systems.},
keywords={aircraft;augmented reality;Monte Carlo methods;optical tracking;personnel;production engineering computing;quality management;spatial variables measurement;system recovery;industrial AR setup;tracking management;real time tracking system;offline device;online device;onsite personnel;quality management;IAR tracking;aircraft production process;3D coordinate measurement;metrological probe;Monte Carlo simulation;target tracking;production tolerance;optical IR tracking system;run-time consistency check;system failure;augmented reality;Variable speed drives;Computational modeling;Calibration;Electronic mail;Analytical models;Sensors;Maintenance engineering},
doi={10.1109/ISMAR.2010.5643553},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643554,
author={M. {Goto} and Y. {Uematsu} and H. {Saito} and S. {Senda} and A. {Iketani}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Task support system by displaying instructional video onto AR workspace},
year={2010},
volume={},
number={},
pages={83-90},
abstract={This paper presents an instructional support system based on augmented reality (AR). This system helps a user to work intuitively by overlaying visual information in the same way of a navigation system. In usual AR systems, the contents to be overlaid onto real space are created with 3D Computer Graphics. In most cases, such contents are newly created according to applications. However, there are many 2D videos that show how to take apart or build electric appliances and PCs, how to cook, etc. Therefore, our system employs such existing 2D videos as instructional videos. By transforming an instructional video to display, according to the user's view, and by overlaying the video onto the user's view space, the proposed system intuitively provides the user with visual guidance. In order to avoid the problem that the display of the instructional video and the user's view may be visually confused, we add various visual effects to the instructional video, such as transparency and enhancement of contours. By dividing the instructional video into sections according to the operations to be carried out in order to complete a certain task, we ensure that the user can interactively move to the next step in the instructional video after a certain operation is completed. Therefore, the user can carry on with the task at his/her own pace. In the usability test, users evaluated the use of the instructional video in our system through two tasks: a task involving building blocks and an origami task. As a result, we found that a user's visibility improves when the instructional video is transformed to display according to his/her view. Further, for the evaluation of visual effects, we can classify these effects according to the task and obtain the guideline for the use of our system as an instructional support system for performing various other tasks.},
keywords={augmented reality;computer aided instruction;computer graphics;task support system;instructional video display;AR workspace;augmented reality;3D Computer Graphics;origami task;building blocks;instructional support system;Cameras;Streaming media;Visualization;Image color analysis;Visual effects;Real time systems;Calibration;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities;H.5.2 [Information Interfaces and Presentation]: User Interfaces—Training, help, and documentation},
doi={10.1109/ISMAR.2010.5643554},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643555,
author={C. {Bichlmeier} and E. {Euler} and T. {Blum} and N. {Navab}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Evaluation of the virtual mirror as a navigational aid for augmented reality driven minimally invasive procedures},
year={2010},
volume={},
number={},
pages={91-97},
abstract={The paper presents a user study investigating perceptual effects of a virtual mirror being used as a tangible interactive tool in a medical Augmented Reality scenario. In particular, we evaluated whether an additional perspective provided by the virtual mirror supports instrument guidance in terms of precision and straightness. 31 participants were asked to navigate an endoscopic instrument in a simulated minimally invasive port setting. Results show a significant higher precision of instrument guidance when a virtual mirror provides additional views of the target region.},
keywords={augmented reality;virtual mirror;navigational aid;augmented reality;tangible interactive tool;instrument guidance;endoscopic instrument;Mirrors;Target tracking;Instruments;Navigation;Surgery;Cameras;Biomedical imaging;H.5.1 [Multimedia Information Systems]: Artificial, augmented, and virtual realities;H.5.2 [Information Interfaces and Presentation]: Interaction styles—Evaluation/Methodology},
doi={10.1109/ISMAR.2010.5643555},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643701,
author={},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={[Blank page]},
year={2010},
volume={},
number={},
pages={98-98},
abstract={This page or pages intentionally left blank.},
keywords={},
doi={10.1109/ISMAR.2010.5643701},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643556,
author={M. {Knecht} and C. {Traxler} and O. {Mattausch} and W. {Purgathofer} and M. {Wimmer}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Differential Instant Radiosity for mixed reality},
year={2010},
volume={},
number={},
pages={99-107},
abstract={In this paper we present a novel plausible realistic rendering method for mixed reality systems, which is useful for many real life application scenarios, like architecture, product visualization or edutainment. To allow virtual objects to seamlessly blend into the real environment, the real lighting conditions and the mutual illumination effects between real and virtual objects must be considered, while maintaining interactive frame rates (20-30fps). The most important such effects are indirect illumination and shadows cast between real and virtual objects. Our approach combines Instant Radiosity and Differential Rendering. In contrast to some previous solutions, we only need to render the scene once in order to find the mutual effects of virtual and real scenes. The dynamic real illumination is derived from the image stream of a fish-eye lens camera. We describe a new method to assign virtual point lights to multiple primary light sources, which can be real or virtual. We use imperfect shadow maps for calculating illumination from virtual point lights and have significantly improved their accuracy by taking the surface normal of a shadow caster into account. Temporal coherence is exploited to reduce flickering artifacts. Our results show that the presented method highly improves the illusion in mixed reality applications and significantly diminishes the artificial look of virtual objects superimposed onto real scenes.},
keywords={brightness;image processing;light sources;rendering (computer graphics);virtual reality;differential instant radiosity;plausible realistic rendering method;mixed reality system;product visualization;virtual object;lighting condition;interactive frame rates;differential rendering;image stream;fish eye lens camera;light sources;virtual point light;temporal coherence;flickering artifacts;Lighting;Light sources;Rendering (computer graphics);Virtual reality;Real time systems;Cameras;Shadow mapping;Mixed Reality;Real-time Global Illumination;Differential Rendering;Instant Radiosity},
doi={10.1109/ISMAR.2010.5643556},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643557,
author={},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={[Blank page]},
year={2010},
volume={},
number={},
pages={108-108},
abstract={This page or pages intentionally left blank.},
keywords={},
doi={10.1109/ISMAR.2010.5643557},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643558,
author={B. V. {Lu} and T. {Kakuta} and R. {Kawakami} and T. {Oishi} and K. {Ikeuchi}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Foreground and shadow occlusion handling for outdoor augmented reality},
year={2010},
volume={},
number={},
pages={109-118},
abstract={Occlusion handling in augmented reality (AR) applications is challenging in synthesizing virtual objects correctly into the real scene with respect to existing foregrounds and shadows. Furthermore, outdoor environment makes the task more difficult due to the unpredictable illumination changes. This paper proposes novel outdoor illumination constraints for resolving the foreground occlusion problem in outdoor environment. The constraints can be also integrated into a probabilistic model of multiple cues for a better segmentation of the foreground. In addition, we introduce an effective method to resolve the shadow occlusion problem by using shadow detection and recasting with a spherical vision camera. We have applied the system in our digital cultural heritage project named Virtual Asuka (VA) and verified the effectiveness of the system.},
keywords={augmented reality;image segmentation;object detection;shadow occlusion handling;foreground occlusion handling;outdoor augmented reality;foreground segmentation;shadow detection;shadow recasting;spherical vision camera;virtual Asuka;Lighting;Sun;Motion segmentation;Cameras;Image color analysis;Pixel;Image segmentation;I.3.7 [Computer graphics]: Three-dimensional Graphics and Realism—virtual Reality;I.4.6 [Image Processing and Computer Vision]: Segmentation—Pixel Classificatio},
doi={10.1109/ISMAR.2010.5643558},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643559,
author={E. {Prytz} and S. {Nilsson} and A. {Jönsson}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={The importance of eye-contact for collaboration in AR systems},
year={2010},
volume={},
number={},
pages={119-126},
abstract={Eye contact is believed to be an important factor in normal human communication and as a result of this a head mounted display (HMD) is often seen as something intrusive and limiting. This can be especially problematic when AR is used in a collaborative setting. The study presented in this paper aims to investigate the effects an HMD-based AR system can have on eye contact behaviour between participants in a collaborative task and thus, in extension, the effects of the HMD on collaboration itself. The focus of the study is on task-oriented collaboration between professionals. The participants worked through three different scenarios alternating between HMDs and regular paper maps with the purpose of managing the crisis response to a simulated major forest fire. Correlations between eye contact between participants and questionnaire items concerning team- and taskwork were analysed, indicating that, for the paper map condition, a high amount of eye contact is associated with low confidence and trust in the artefacts used (i.e. paper map and symbols). The amount of eye-contact in both conditions was very low. It was significantly higher for conditions without HMDs. However, the confidence and trust in the artefacts was generally rated significantly higher with HMDs than without. In conclusion, the decrease in eye contact with HMDs does not seem to have a direct effect on the collaboration in a professional, task-oriented context. This is contrary to popular assumptions and the results are relevant for future design choices for AR systems using HMDs.},
keywords={augmented reality;groupware;helmet mounted displays;eye contact;AR systems collaboration;human communication;head mounted display;augmented reality systems;collaborative task;Correlation;Cameras;Command and control systems;Biological system modeling;Teamwork;Collaborative work;H.5.1 [INFORMATION INTERFACES AND PRESENTATION]: Multimedia Information Systems—Artificial, augmented, and virtual realities},
doi={10.1109/ISMAR.2010.5643559},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643560,
author={M. {Gandy} and R. {Catrambone} and B. {MacIntyre} and C. {Alvarez} and E. {Eiriksdottir} and M. {Hilimire} and B. {Davidson} and A. C. {McLaughlin}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Experiences with an AR evaluation test bed: Presence, performance, and physiological measurement},
year={2010},
volume={},
number={},
pages={127-136},
abstract={This paper discusses an experiment carried out in an AR test bed called “the pit”. Inspired by the well-known VR acrophobia study of Meehan et al. [18], the experimental goals were to explore whether VR presence instruments were useful in AR (and to modify them where appropriate), to compare additional measures to these well-researched techniques, and to determine if findings from VR evaluations can be transferred to AR. An experimental protocol appropriate for AR was developed. The initial experimental findings concern varying immersion factors (frame rate) and their effect on feelings of presence, user performance and behavior. Unlike the VR study, which found differing frame rates to affect presence measures, there were few differences in the five frame rate modes in our study as measured by the qualitative and quantitative instruments, which included physiological responses, a custom presence questionnaire, task performance, and user behavior. The AR presence questionnaire indicated users experienced a high feeling of presence in all frame rate modes. Behavior, performance, and interview results indicated the participants felt anxiety in the pit environment. However, the physiological data did not reflect this anxiety due to factors of user experience and experiment design. Efforts to develop a useful AR test bed and to identify results from a large data set has produced a body of knowledge related to AR evaluation that can inform others seeking to create AR experiments.},
keywords={augmented reality;physiological measurement;augmented reality evaluation test bed;virtual reality acrophobia study;immersion factors;frame rate;physiological responses;custom presence questionnaire;task performance;user behavior;Atmospheric measurements;Particle measurements;Visualization;Instruments;Laboratories;Software;Augmented Reality;Presence;Evaluation;Physiological Measures},
doi={10.1109/ISMAR.2010.5643560},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643561,
author={R. {Zhang} and H. {Hua}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Effects of a retroreflective screen on depth perception in a head-mounted projection display},
year={2010},
volume={},
number={},
pages={137-145},
abstract={The perceived depth accuracy is a fundamental performance metrics for an augmented reality display system. Many factors may affect the perceived depth of a head-mounted display (HMD), such as display resolution, interpupillary distance (IPD), conflicting depth cues, stereoacuity, and head tracking error. Besides these typical limiting factors to an HMD-type system, the perceived depth through a head-mounted projection display (HMPD) may be further affected by the usage of a retroreflective screen. In this paper, we will evaluate the perceived depth accuracy of an HMPD system using a perceptual depth matching method. The main factor to be investigated in our study is the position of a retroreflective screen relative to the projection image plane, with the projection image plane placed at different distances to the user. Overall, it is found that the ratio of the judged distance to real distance and the standard deviation of the judged distance increase linearly with the reference object distance. A transition effect from depth underestimation to overestimation has been observed at the reference object distance of around 1.4m. The position of a retroreflective screen only has a significant effect on the depth judgment error around this switching point. The paper also analyzes various effects brought by a retroreflective screen on the depth judgment. The depth cue and the image luminance reduction brought by a retroreflective screen could be the main factors that affect the depth judgment accuracy.},
keywords={augmented reality;computer vision;helmet mounted displays;retroreflective screen;depth perception;head mounted projection display;perceived depth accuracy;augmented reality display system;display resolution;interpupillary distance;image luminance reduction;Accuracy;Calibration;Focusing;Legged locomotion;Optical imaging;Cameras;Adaptive optics;HEAD-MOUNTED DISPLAYS;AUGMENTED REALITY},
doi={10.1109/ISMAR.2010.5643561},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643562,
author={},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={[Blank page]},
year={2010},
volume={},
number={},
pages={146-146},
abstract={This page or pages intentionally left blank.},
keywords={},
doi={10.1109/ISMAR.2010.5643562},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643563,
author={G. {Ye} and A. {State} and H. {Fuchs}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={A practical multi-viewer tabletop autostereoscopic display},
year={2010},
volume={},
number={},
pages={147-156},
abstract={This paper introduces a multi-user autostereoscopic tabletop display and its associated real-time rendering methods. Tabletop displays that support both multiple viewers and autostereoscopy have been extremely difficult to construct. Our new system is inspired by the “Random Hole Display” design that modified the pattern of openings in a barrier mounted in front of a flat panel display from thin slits to a dense pattern of tiny, pseudo-randomly placed holes. This allows viewers anywhere in front of the display to see a different subset of the display's native pixels through the random-hole screen. However, a fraction of the visible pixels will be observable by more than a single viewer. Thus the main challenge is handling these “conflicting” pixels, which ideally must show different colors to each viewer. We introduce several solutions to this problem and describe in detail the current method of choice, a combination of color blending and approximate error diffusion, performing in real time in our GPU-based implementation. The easily reproducible design uses a pattern film barrier affixed to the display by means of a transparent polycarbonate layer spacer. We use a commercial optical tracker for viewers' locations and synthesize the appropriate image (or a stereoscopic image pair) for each viewer. The system supports graceful degradation with increasing number of simultaneous views, and graceful improvement as the number of views decreases.},
keywords={flat panel displays;rendering (computer graphics);multiviewer tabletop;autostereoscopic display;real time rendering method;random hole display design;flat panel display;approximate error diffusion;GPU based implementation;Pixel;Image color analysis;Rendering (computer graphics);Lenses;Image resolution;Calibration;Three dimensional displays;autostereoscopic display;3D display;tabletop display;mixed reality;multi-user display;collaborative display;random hole barrier;parallax barrier},
doi={10.1109/ISMAR.2010.5643563},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643564,
author={L. {Gruber} and S. {Gauglitz} and J. {Ventura} and S. {Zollmann} and M. {Huber} and M. {Schlegel} and G. {Klinker} and D. {Schmalstieg} and T. {Höllerer}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={The City of Sights: Design, construction, and measurement of an Augmented Reality stage set},
year={2010},
volume={},
number={},
pages={157-163},
abstract={We describe the design and implementation of a physical and virtual model of an imaginary urban scene-the “City of Sights”- that can serve as a backdrop or “stage” for a variety of Augmented Reality (AR) research. We argue that the AR research community would benefit from such a standard model dataset which can be used for evaluation of such AR topics as tracking systems, modeling, spatial AR, rendering tests, collaborative AR and user interface design. By openly sharing the digital blueprints and assembly instructions for our models, we allow the proposed set to be physically replicable by anyone and permit customization and experimental changes to the stage design which enable comprehensive exploration of algorithms and methods. Furthermore we provide an accompanying rich dataset consisting of video sequences under varying conditions with ground truth camera pose. We employed three different ground truth acquisition methods to support a broad range of use cases. The goal of our design is to enable and improve the replicability and evaluation of future augmented reality research.},
keywords={augmented reality;image sequences;rendering (computer graphics);solid modelling;user interfaces;City of Sights;augmented reality stage set;imaginary urban scene;rendering test;collaborative AR design;user interface design;digital blueprints sharing;video sequence;ground truth camera pose;ground truth acquisition methods;Solid modeling;Cameras;Three dimensional displays;Calibration;Subspace constraints;Accuracy;Computational modeling},
doi={10.1109/ISMAR.2010.5643564},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643565,
author={},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={[Blank page]},
year={2010},
volume={},
number={},
pages={164-164},
abstract={This page or pages intentionally left blank.},
keywords={},
doi={10.1109/ISMAR.2010.5643565},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643566,
author={B. R. {Jones} and R. {Sodhi} and R. H. {Campbell} and G. {Garnett} and B. P. {Bailey}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Build your world and play in it: Interacting with surface particles on complex objects},
year={2010},
volume={},
number={},
pages={165-174},
abstract={We explore interacting with everyday objects by representing content as interactive surface particles. Users can build their own physical world, map virtual content onto their physical construction and play directly with the surface using a stylus. A surface particle representation allows programmed content to be created independent of the display object and to be reused on many surfaces. We demonstrated this idea through a projector-camera system that acquires the object geometry and enables direct interaction through an IR tracked stylus. We present three motivating example applications, each displayed on three example surfaces. We discuss a set of interaction techniques that show possible avenues for structuring interaction on complicated everyday objects, such as Surface Adaptive GUIs for menu selection. Through a preliminary informal evaluation and interviews with end users, we demonstrate the potential of interacting with surface particles and identify improvements necessary to make this interaction practical on everyday surfaces.},
keywords={computational geometry;graphical user interfaces;solid modelling;virtual reality;complex object;interactive surface particle;physical world;virtual content;projector camera system;object geometry;IR tracked stylus;GUI;Sprites (computer);Three dimensional displays;Surface treatment;Games;Surface texture;Face;Visualization;H5.2 [Information interfaces and presentation]: User Interfaces—Input devices and strategies, Interaction styles;H5.1 [Information interfaces and presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities},
doi={10.1109/ISMAR.2010.5643566},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643567,
author={J. {Karlekar} and S. Z. {Zhou} and W. {Lu} and Z. C. {Loh} and Y. {Nakayama} and D. {Hii}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Positioning, tracking and mapping for outdoor augmentation},
year={2010},
volume={},
number={},
pages={175-184},
abstract={This paper presents a novel approach for user positioning, robust tracking and online 3D mapping for outdoor augmented reality applications. As coarse user pose obtained from GPS and orientation sensors is not sufficient for augmented reality applications, sub-meter accurate user pose is then estimated by a one-step silhouette matching approach. Silhouette matching of the rendered 3D model and camera data is carried out with shape context descriptors as they are invariant to translation, scale and rotational errors, giving rise to a non-iterative registration approach. Once the user is correctly positioned, further tracking is carried out with camera data alone. Drifts associated with vision based approaches are minimized by combining different feature modalities. Robust visual tracking is maintained by fusing frame-to-frame and model-to-frame feature matches. Frame-to-frame tracking is accomplished with corner matching while edges are used for model-to-frame registration. Results from individual feature tracker are fused using a pose estimate obtained from an extended Kalman filter (EKF) and a weighted M-estimator. In scenarios where dense 3D models of the environment are not available, online 3D incremental mapping and tracking is proposed to track the user in unprepared environments. Incremental mapping prepares the 3D point cloud of the outdoor environment for tracking.},
keywords={augmented reality;cameras;Global Positioning System;image matching;image sequences;Kalman filters;pose estimation;rendering (computer graphics);sensor fusion;shape recognition;solid modelling;tracking;user positioning;robust tracking;online 3D mapping;outdoor augmented reality application;coarse user pose estimation;GPS;orientation sensor;one step silhouette matching approach;rendered 3D model;camera data;shape context descriptor;rotational error;noniterative registration approach;vision based approach;robust visual tracking;model to frame feature match;frame to frame tracking;corner matching;feature tracker;extended Kalman filter;weighted M-estimator;online 3D incremental mapping;3D point cloud;Cameras;Shape;Context;Robustness;Three dimensional displays;Target tracking;Augmented reality;user positioning;robust tracking;shape matching;3D mapping;sensor fusion},
doi={10.1109/ISMAR.2010.5643567},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643568,
author={J. R. {Sánchez} and H. {Álvarez} and D. {Borro}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Towards real time 3D tracking and reconstruction on a GPU using Monte Carlo simulations},
year={2010},
volume={},
number={},
pages={185-192},
abstract={This paper addresses the problem of camera tracking and 3D reconstruction from image sequences, i.e., the monocular SLAM problem. Traditionally, this problem is solved using non-linear minimization techniques that are very accurate but hardly used in real time. This work presents a highly parallelizable random sampling approach based on Monte Carlo simulations that fits very well on the graphics hardware. The proposed algorithm achieves the same precision as non linear optimization, getting real time performance running on commodity graphics hardware. Both accuracy and performance are evaluated using synthetic data and real video sequences captured with a hand-held camera. Moreover, results are compared with an implementation of Bundle Adjustment showing that the presented method gets similar results in much less time.},
keywords={computer graphic equipment;coprocessors;image sensors;image sequences;minimisation;object detection;real time 3D tracking;GPU;Monte Carlo simulations;real time 3D reconstruction;camera tracking;image sequences;monocular SLAM problem;nonlinear minimization techniques;nonlinear optimization;commodity graphics hardware;hand held camera;bundle adjustment;Three dimensional displays;Cameras;Graphics processing unit;Kernel;Real time systems;Simultaneous localization and mapping;Estimation},
doi={10.1109/ISMAR.2010.5643568},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643569,
author={K. {Kim} and V. {Lepetit} and W. {Woo}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Keyframe-based modeling and tracking of multiple 3D objects},
year={2010},
volume={},
number={},
pages={193-198},
abstract={We propose a real-time solution for modeling and tracking multiple 3D objects in unknown environments. Our contribution is two-fold: First, we show how to scale with the number of objects. This is done by combining recent techniques for image retrieval and online Structure from Motion, which can be run in parallel. As a result, tracking 40 objects in 3D can be done within 6 to 25 milliseconds per frame, even under difficult conditions for tracking. Second, we propose a method to let the user add new objects very quickly. The user simply has to select in an image a 2D region lying on the object. A 3D primitive is then fitted to the features within this region, and adjusted to create the object 3D model. In practice, this procedure takes less than a minute.},
keywords={image motion analysis;image retrieval;target tracking;3d object tracking;image retrieval;image motion analysis;Three dimensional displays;Solid modeling;Cameras;Target tracking;Image reconstruction;Feature extraction;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, Augmented, and Virtual Realities;I.4.8 [Image Processing and Computer Vision]: Scene Analysis—Shape},
doi={10.1109/ISMAR.2010.5643569},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643570,
author={J. {Bastian} and B. {Ward} and R. {Hill} and A. {van den Hengel} and A. {Dick}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Interactive modelling for AR applications},
year={2010},
volume={},
number={},
pages={199-205},
abstract={We present a method for estimating the 3D shape of an object from a sequence of images captured by a hand-held device. The method is well suited to augmented reality applications in that minimal user interaction is required, and the models generated are of an appropriate form. The method proceeds by segmenting the object in every image as it is captured and using the calculated silhouette to update the current shape estimate. In contrast to previous silhouette-based modelling approaches, however, the segmentation process is informed by a 3D prior based on the previous shape estimate. A voting scheme is also introduced in order to compensate for the inevitable noise in the camera position estimates. The combination of the voting scheme with the closed-loop segmentation process provides a robust and flexible shape estimation method. We demonstrate the approach on a number of scenes where segmentation without a 3D prior would be challenging.},
keywords={augmented reality;image segmentation;image sequences;shape recognition;solid modelling;interactive modelling;AR applications;3D shape estimation;image sequence;closed loop segmentation process;voting scheme;Image segmentation;Three dimensional displays;Cameras;Solid modeling;Shape;Pixel;Image color analysis;I.4.6 [Image Processing and Computer Vision]: Segmentation—Pixel classification;I.4.8 [Image Processing and Computer Vision]: Scene Analysis—Shape;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities},
doi={10.1109/ISMAR.2010.5643570},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643571,
author={},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={[Blank page]},
year={2010},
volume={},
number={},
pages={206-206},
abstract={This page or pages intentionally left blank.},
keywords={},
doi={10.1109/ISMAR.2010.5643571},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643572,
author={J. {Herling} and W. {Broll}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Advanced self-contained object removal for realizing real-time Diminished Reality in unconstrained environments},
year={2010},
volume={},
number={},
pages={207-212},
abstract={While Augmented Reality has always been restricted to adding artificial content to the real environment, Diminished Reality allows for removing real world content. Existing approaches however, either require complex setups or are not applicable in real-time. In this paper we present our approach for removing real-world objects from a live video stream of the user's real environment. Our approach is based on a simple setup and neither requires any pre-processing nor any information on the structure and location of the objects to be removed or on their background. Our approach is based on the identification of the objects to be removed combined with an image completion and synthesis algorithm. The performance of our approach is one to two magnitudes better than that of previous work in the area of image completion, providing real-time object cancellation on standard laptop or tablet computers.},
keywords={augmented reality;computer vision;augmented reality;video stream;diminished reality;image completion;image synthesis;object cancellation;Cameras;Pixel;Real time systems;Streaming media;Image color analysis;Pipelines;Coherence;Diminished Reality;Augmented Reality;Mediated Reality;image completion;image synthesis},
doi={10.1109/ISMAR.2010.5643572},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643573,
author={},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Science technology posters [breaker page]},
year={2010},
volume={},
number={},
pages={213-214},
abstract={Start of the "Science & technology posters" section of the conference proceedings record.},
keywords={},
doi={10.1109/ISMAR.2010.5643573},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643574,
author={W. {Chen} and F. {Hsiao} and C. {Lin}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={An automatic parallax adjustment method for stereoscopic augmented reality systems},
year={2010},
volume={},
number={},
pages={215-216},
abstract={This paper presents an automatic parallax adjustment method that considers the border effect to produce more realistic stereo images on a stereoscopic augmented reality system. Three-dimensional (3D) imaging is an emerging method of displaying three-dimensional information and providing an immersive and intuitive experience with augmented reality. However, the protruding parts of displayed stereoscopic images may be blurry and cause viewing discomfort. Furthermore, the border effect may make it difficult for an imaging system to display regions next to screen borders, even with considerable negative parallax. This paper proposes a method of automatically adjusting the parallax of displayed stereo images by analyzing the feature points in regions near screen borders to produce better stereo effects. Experimental results and a subjective assessment of human factor issues indicate that the proposed method makes stereoscopic augmented reality systems significantly more attractive and comfortable to view.},
keywords={augmented reality;stereo image processing;automatic parallax adjustment method;stereo images;stereoscopic augmented reality system;three dimensional imaging;negative parallax;Three dimensional displays;Visualization;Augmented reality;Feature extraction;Cameras;Stereo vision;acceptance of MR/AR technology;MR/AR for entertainment;usability studies and experiments},
doi={10.1109/ISMAR.2010.5643574},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643575,
author={J. {Chen} and G. {Turk} and B. {MacIntyre}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Painterly rendering with coherence for augmented reality},
year={2010},
volume={},
number={},
pages={217-218},
abstract={A seamless blending of the real and virtual worlds is key to increased immersion and improved user experiences for augmented reality (AR). Photorealistic and non-photorealistic rendering (NPR) are two ways to achieve this goal. Non-photorealistic rendering creates an abstract version of both the real and virtual world by stylization to make them indistinguishable. We presented a painterly rendering algorithm for AR applications. This algorithm paints composed AR video frames with bump-mapping curly brushstrokes. Tensor fields are created for each frame to define direction for brushstrokes. The anchor point of a brushstroke is tracked or warped from frame to frame. Brushstrokes are also reshaped to provide better temporal coherence. The major difference between our algorithm and existing NPR work in general graphics and AR/VR areas is we use feature points across composed AR video frames to maintain coherence in the rendering.},
keywords={augmented reality;rendering (computer graphics);video signal processing;painterly rendering;augmented reality;nonphotorealistic rendering;abstract version;virtual world;AR video frame;bump mapping;brushstroke;temporal coherence;feature point;Rendering (computer graphics);Coherence;Tensile stress;Augmented reality;Pixel;Streaming media;Paints;Non-photorealistic rendering;augmented reality;painterly rendering;temporal coherence},
doi={10.1109/ISMAR.2010.5643575},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643576,
author={J. {Choi} and Y. {Kim} and M. {Lee} and G. J. {Kim} and Y. {Nam} and Y. {Kwon}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={k-MART: Authoring tool for mixed reality contents},
year={2010},
volume={},
number={},
pages={219-220},
abstract={In this poster, we present an authoring tool for mixed reality (MR) contents, called k-MART. The tool is comprehensive to accommodate many different types of mixed reality applications and platforms, being able to mix and match different types of contexts and behaviors for content creation. The content is intuitively modeled as a collection of “context-behavior” pairs and saved in a declarative manner to be “played” by a content browser. The tool is designed to minimize user programming and offers many abstractions of functionalities with only important details to be filled in by the user using graphical user interfaces. We believe that our approach has a good potential for the wide dissemination and sharing of MR contents.},
keywords={augmented reality;authoring systems;graphical user interfaces;k-MART;mixed reality contents;authoring tool;context-behavior pairs;content browser;graphical user interfaces;mixed and augmented reality authoring tool;Virtual reality;Context;Three dimensional displays;User interfaces;Standards;Software;Context modeling;Authoring;Mixed Reality;Context;Behavior;Contents},
doi={10.1109/ISMAR.2010.5643576},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643577,
author={J. {Ehnes}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={A precise controllable projection system for projected virtual characters and its calibration},
year={2010},
volume={},
number={},
pages={221-222},
abstract={In this paper we describe a system to project virtual characters that shall live with us in the same environment. In order to project the characters' visual representations onto room surfaces we use a controllable projector.},
keywords={calibration;optical projectors;virtual reality;calibration;project virtual characters;visual representations;controllable projector;Cameras;Calibration;Conferences;Control systems;Humans;Visualization;Lenses},
doi={10.1109/ISMAR.2010.5643577},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643578,
author={T. {Engelke} and S. {Webel} and N. {Gavish}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Generating vision based Lego augmented reality training and evaluation systems},
year={2010},
volume={},
number={},
pages={223-224},
abstract={The creation of training applications using Augmented Reality (AR) is still a new field of research. In order to get good training results therefore evaluation should be performed. For the creation of such systems the questions arising are related to the general process of generation, visualization, evaluation and its psychological background. An important aspect of vision based AR is also the robust tracking and initialization of objects for correct augmentation. In this work we present a concept of an entire processing chain, which allows for efficient and automatic generation of such training systems that can also be used for evaluation. We do this in the context of a Lego training system. While explaining the whole process of application generation and usage, we also present a novel approach for robust marker free initialization of colored partly occluded plates and their tracking using one off the shelf monocular camera.},
keywords={augmented reality;cameras;computer vision;object detection;training;vision based Lego augmented reality training;psychological background;robust tracking;Lego training system;robust marker free initialization;colored partly occluded plate;shelf monocular camera;Training;Solid modeling;Augmented reality;Image color analysis;Cognitive science;Visualization;Robustness;H.5.1 [INFORMATION INTERFACES AND PRESENTATION]: Multimedia Information Systems—Artificial, augmented, and virtual realities;I.5.4 [PATTERN RECOGNITION]: Applications—Computer vision},
doi={10.1109/ISMAR.2010.5643578},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643579,
author={V. {Gay-Bellile} and P. {Lothe} and S. {Bourgeois} and E. {Royer} and S. {Naudet Collette}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Augmented reality in large environments: Application to aided navigation in urban context},
year={2010},
volume={},
number={},
pages={225-226},
abstract={This paper addresses the challenging issue of vision-based localization in urban context. It briefly describes our contributions in large environments modeling and accurate camera localization. The efficiency of the resulting system is illustrated through Augmented Reality results on large trajectory of several hundred meters.},
keywords={augmented reality;computer vision;image sensors;augmented reality;urban context;vision based localization;camera localization;Three dimensional displays;Cities and towns;Databases;Image reconstruction;Solid modeling;Augmented reality;Cameras;H.5.1 [Multimedia Information Systems]: Artificial, augmented, and virtual realities—Life Cycle;I.4.8 [Scene Analysis]: Object Recognition—Tracking},
doi={10.1109/ISMAR.2010.5643579},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643580,
author={L. {Gruber} and D. {Kalkofen} and D. {Schmalstieg}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Color harmonization for Augmented Reality},
year={2010},
volume={},
number={},
pages={227-228},
abstract={In this paper we discuss color harmonization for Augmented Reality. Color harmonization is a technique used to adjust the combination of colors in order to follow aesthetic guidelines. We implemented a system which is able to harmonize the combination of the colors in video based AR systems. The presented approach is able to re-color virtual and real-world items, achieving overall more visually pleasant results. In order to allow preservation of certain colors in an AR composition, we furthermore introduce the concept of constraint color harmonization.},
keywords={augmented reality;image colour analysis;video signal processing;aesthetic guidelines;video based augmented color reality;constraint color harmonization;Image color analysis;Visualization;Pixel;Augmented reality;Histograms;Real time systems;Cameras},
doi={10.1109/ISMAR.2010.5643580},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643581,
author={J. {Grubert} and D. {Hamacher} and R. {Mecke} and I. {Böckelmann} and L. {Schega} and A. {Huckauf} and M. {Urbina} and M. {Schenk} and F. {Doil} and J. {Tümler}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Extended investigations of user-related issues in mobile industrial AR},
year={2010},
volume={},
number={},
pages={229-230},
abstract={The potential of Augmented Reality (AR) to support industrial processes has been demonstrated in several studies. While there have been first investigations on user related issues in the long-duration use of mobile AR systems, to date the impact of theses systems on physiological and psychological aspects is not explored extensively. We conducted an extended study in which 19 participants worked 4 hours continuously in an order picking process with and without AR support. Results of the study comparing strain and work efficiency are presented and open issues are discussed.},
keywords={augmented reality;industrial engineering;order picking;user related issues;mobile industrial AR;augmented reality;physiological aspects;psychological aspects;order picking process;Visualization;Strain;Error analysis;Navigation;Heart rate variability;Target tracking;Mobile communication;H.1.2 [Models and Principles]: User/Machine Systems—Human factors;H.5.1 [Information Interfaces and Presentation]: User Interfaces—User-centered Design},
doi={10.1109/ISMAR.2010.5643581},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643582,
author={O. {Hayashi} and K. {Kasada} and T. {Narumi} and T. {Tanikawa} and M. {Hirose}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Digital Diorama system for museum exhibition},
year={2010},
volume={},
number={},
pages={231-232},
abstract={In this paper, we proposed the Digital Diorama system to convey background information vividly. The system superimposes computer generated diorama scene reconstructed from related image/video materials on real exhibits. In order to switch and superimpose real exhibits and past photos seamlessly, we implement a matching system for estimating the camera position where photos are taken. By applying this subsystem to 26 past photos about the steam locomotive exhibit, we succeeded in estimating their camera position. Thus, we implement and install a prototype system at estimated position to superimposing virtual scene and real exhibit in the Railway Museum.},
keywords={locomotives;museums;digital diorama system;museum exhibition;camera position;matching system;steam locomotive;position estimation;Cameras;Videos;Prototypes;Educational institutions;Rail transportation;Materials;Electronic mail;I.3.3 [Computer Graphics]: Picture/Image Generation—Viewing algorithm;I.3.6 [Computer Graphics]: Methodology and Techniques—Interaction Techniques},
doi={10.1109/ISMAR.2010.5643582},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643583,
author={A. {Hill} and B. {MacIntyre} and M. {Gandy} and B. {Davidson} and H. {Rouzati}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={KHARMA: An open KML/HTML architecture for mobile augmented reality applications},
year={2010},
volume={},
number={},
pages={233-234},
abstract={Widespread future adoption of augmented reality technology will rely on a broadly accessible standard for authoring and distributing content with, at a minimum, the flexibility and interactivity provided by current web authoring technologies. We introduce KHARMA, an open architecture based on KML for geospatial and relative referencing combined with HTML, JavaScript and CSS technologies for content development and delivery. This architecture uses lightweight representations that decouple infrastructure and tracking sources from authoring and content delivery. Our main contribution is a re-conceptualization of KML that turns HTML content formerly confined to balloons into first-class elements in the scene. We introduce the KARML extension that gives authors increase control over the presentation of HTML content and its spatial relationship to other content.},
keywords={augmented reality;authoring systems;hypermedia markup languages;Internet;mobile computing;software architecture;KHARMA;open KML-HTML architecture;mobile augmented reality technology;Web authoring technologies;JavaScript technology;CSS technology;content development;Servers;HTML;Augmented reality;Computer architecture;Browsers;Global Positioning System;Protocols;Augmented Reality;World Wide Web;Authoring},
doi={10.1109/ISMAR.2010.5643583},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643584,
author={S. {Hwang} and H. {Jo} and J. {Ryu}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={EXMAR: EXpanded view of mobile augmented reality},
year={2010},
volume={},
number={},
pages={235-236},
abstract={There have been many studies to minimize the psychological and physical load increase caused by mobile augmented reality systems. In this paper, we propose a new technique called “EXMAR”, which enables the user to explore his/her surroundings with an expanded field of view, resulting in a decrease of physical movement. Through this novel interaction technique, the user can explore off-screen point of interests with environmental contextual information by simple dragging gestures. To evaluate this initial approach, we conducted a proof of concept usability test under a set of scenarios such as “Exploring objects behind the user”, “Avoiding the invasion of personal space” and “Walk and type with front-view.” Through this initial examination, we found that users can explore off-screen point of interests and grasp the spatial relations without the increase of mental effort. We believe that this preliminary study gives a meaningful indication that employing the interactive field of view can be a useful method to decrease the physical load without any additional mental efforts in a mixed and augmented reality environment.},
keywords={augmented reality;mobile computing;mobile augmented reality system;EXMAR technique;interaction technique;dragging gestures;expanded field of view;off-screen point of interests;environmental contextual information;proof of concept usability test;Visualization;Three dimensional displays;Indexes;Augmented Reality;Mixed Reality;Distortion Correction;Expanded Field Of View (EFOV);Fish-eye lens;interaction},
doi={10.1109/ISMAR.2010.5643584},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643585,
author={S. {Jeon} and B. {Knoerlein} and M. {Harders} and S. {Choi}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Haptic simulation of breast cancer palpation: A case study of haptic augmented reality},
year={2010},
volume={},
number={},
pages={237-238},
abstract={Haptic augmented reality (AR) allows to modulate the haptic properties of a real object by providing virtual haptic feedback. We previously developed a haptic AR system wherein the stiffness of a real object can be augmented with the aid of a haptic interface. To demonstrate its potential, this paper presents a case study for medical training of breast cancer palpation. A real breast model made of soft silicone is augmented with a virtual tumor rendered inside. Haptic stimuli for the virtual tumor are generated based on a contact dynamics model identified via real measurements, without the need of geometric information on the breast. A subjective evaluation confirmed the realism and fidelity of our palpation system.},
keywords={augmented reality;cancer;haptic interfaces;silicones;breast cancer palpation;haptic augmented reality;virtual haptic feedback;haptic interface;soft silicone;virtual tumor;Tumors;Haptic interfaces;Breast;Biological system modeling;Solid modeling;Data models;Force;H.5.2 [Information Interfaces and Presentation]: User Interfaces—Haptic I/O;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities},
doi={10.1109/ISMAR.2010.5643585},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643586,
author={S. {Kagami}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Range-finding projectors: Visualizing range information without sensors},
year={2010},
volume={},
number={},
pages={239-240},
abstract={We describe sensorless visualizing techniques of 3D range information of object surfaces, in which the range information is projected directly onto the real-world surface by projectors. The proposed system consists of two projectors that merely project still images. Two techniques, namely, contour map projection based on the moire method and pseudo-color map projection based on complementary colors, are shown to be used simultaneously.},
keywords={augmented reality;distance measurement;range-finding projectors;range information visualization;object surfaces;contour map projection;moire method;pseudocolor map projection;complementary colors;augmented reality;sensorless visualizing techniques;Image color analysis;Cameras;Color;Sensors;Pixel;Gratings;Data visualization;I.4.9 [Computing Methodologies]: Image Processing and Computer Vision—Applications;H.5.1 [Information Systems]: Information Interfaces and Presentation—Multimedia Information Systems - Artificial, augmented, and virtual realities},
doi={10.1109/ISMAR.2010.5643586},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643587,
author={S. {Kahn} and H. {Wuest} and D. {Stricker} and D. W. {Fellner}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={3D discrepancy check via Augmented Reality},
year={2010},
volume={},
number={},
pages={241-242},
abstract={For many tasks like markerless model-based camera tracking it is essential that the 3D model of a scene accurately represents the real geometry of the scene. It is therefore very important to detect deviations between a 3D model and a scene. We present an innovative approach which is based on the insight that camera tracking can not only be used for Augmented Reality visualization but also to solve the correspondence problem between 3D measurements of a real scene and their corresponding positions in the 3D model. We combine a time-of-flight camera (which acquires depth images in real time) with a custom 2D camera (used for the camera tracking) and developed an analysis-by-synthesis approach to detect deviations between a scene and a 3D model of the scene.},
keywords={augmented reality;computational geometry;data visualisation;image sensors;object detection;3D discrepancy check;markerless model based camera tracking;scene geometry;augmented reality visualization;time-of-flight camera;custom 2D camera;Three dimensional displays;Cameras;Solid modeling;Pixel;Mathematical model;Data models;Computational modeling;I.2.10 [Artificial Intelligence]: Vision and Scene Understanding—3D/stereo scene analysis, Video analysis;I.3.m [Computer graphics]: Miscellaneous—Augmented Reality, Difference Visualization;I.4.8 [Image processing and computer vision]: Scene analysis—Range Data, Sensor fusion, Shape, Tracking},
doi={10.1109/ISMAR.2010.5643587},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643588,
author={G. {Kamei} and T. {Matsuyama} and K. {Okada}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Augmentation of check in/out model for remote collaboration with Mixed Reality},
year={2010},
volume={},
number={},
pages={243-244},
abstract={This paper proposes augmentation of check in/out model for remote collaboration with Mixed Reality (MR). We add a 3D shared and private space into the real workspace by MR technology, and augment the check in/out model to remote collaboration. By our proposal, user can intuitively receive remote partner's work via virtual objects in the shared space and stop sharing information about an object by just moving the object into the private space if the user doesn't want to share it. We implement a system which achieves our proposal and evaluate it.},
keywords={augmented reality;groupware;remote collaboration;mixed reality;check in-out model;Collaboration;Three dimensional displays;Space technology;Solid modeling;Virtual reality;Aerospace electronics;Brushes;Remote Collaboration;Mixed Reality;Check In/Out Model},
doi={10.1109/ISMAR.2010.5643588},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643589,
author={T. {Kantonen}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Sensor Synchronization for AR Applications},
year={2010},
volume={},
number={},
pages={245-246},
abstract={In this paper, we give a brief introduction to the sensor synchronization problem, highlight how the choice of hardware affects what synchronization methods can be applied, and present our ongoing research on sensor synchronization. Our work is based on estimating timestamps on the host processor, using a method suitable for mobile phones and other low-cost consumer grade devices since it does not require special hardware support. We also describe an experiment to measure sensor synchronization performance using a simple calibration rig. Our initial results show that the estimated timestamps provide a stable synchronization and a clear improvement in synchronization accuracy compared to the direct method of using sample arrival times as timestamps.},
keywords={augmented reality;cameras;computer vision;mobile handsets;sensors;synchronisation;sensor synchronization;AR application;hardware affect;host processor;mobile phone;low cost consumer grade device;stable synchronization;Synchronization;Clocks;Cameras;Universal Serial Bus;Accuracy;Calibration;Estimation;Sensor synchronization;Temporal matching},
doi={10.1109/ISMAR.2010.5643589},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643590,
author={O. {Korkalo} and M. {Aittala} and S. {Siltanen}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Light-weight marker hiding for augmented reality},
year={2010},
volume={},
number={},
pages={247-248},
abstract={In augmented reality, marker-based tracking is the most common method for camera pose estimation. Most of the markers are black and white patterns that are visually obtrusive, but they can be hidden from the video using image inpainting methods. In this paper, we present a computationally efficient approach to achieve this. We use a high-resolution hiding texture, which is captured and generated only once. To capture continuous changes in illumination, reflections and exposure, we also compute a very low-resolution texture at each frame. The coarse and fine textures are combined to obtain a detailed hiding texture which reacts to changing conditions and runs efficiently in mobile phone environments.},
keywords={augmented reality;computer graphics;data encapsulation;feature extraction;image resolution;image sensors;image texture;motion estimation;pose estimation;video signal processing;light weight marker hiding;augmented reality;marker based tracking;camera pose estimation;black white pattern;image inpainting method;high resolution hiding texture;mobile environment;Lighting;Image resolution;Pixel;Image color analysis;Real time systems;Augmented reality;Mobile handsets;H.5 [Information Interfaces and Presentation]—Artificial, augmented, and virtual realities;I.4.8 [Image processing and computer vision]: Scene Analysis—Color-Time-varying imagery},
doi={10.1109/ISMAR.2010.5643590},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643591,
author={S. {Lee} and J. {Ko} and S. {Kang} and J. {Lee}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={An immersive e-learning system providing virtual experience},
year={2010},
volume={},
number={},
pages={249-250},
abstract={This paper introduces immersive e-learning system which provides vivid learning experience using augmented reality(AR) technology. This system gives illusion that participants feel as if they are in foreign environment by synthesizing images of participants, virtual environment, foreign-language speakers in real-time. Furthermore, surrounding virtual environment reacts to the behavior of each participant including student, local teacher, remote teacher. The system has been installed along with 10 scenarios at 14 public elementary schools and conducted during regular class time. This paper presents our motivations for the system development, a detailed design, and its contents.},
keywords={augmented reality;computer aided instruction;educational institutions;real-time systems;software engineering;immersive e-learning system;virtual experience;augmented reality;illusion system;public elementary school;software development;foreign-language speakers;virtual environment;Image segmentation;Cameras;Virtual reality;Software;Gesture recognition;Hardware;Rendering (computer graphics);MR/AR for art;cultural heritage;or education and training (primary keyword);distributed and collaborative MR/AR},
doi={10.1109/ISMAR.2010.5643591},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643592,
author={P. {Maier} and M. {Tönnis} and G. {Klinker}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Designing and comparing two-handed gestures to confirm links between user controlled objects},
year={2010},
volume={},
number={},
pages={251-252},
abstract={Systems using two-handed spatial manipulation techniques also require strategies to enable system control tasks. These strategies make it possible to interact with the system comfortably while controlling two hand-held objects simultaneously. The Augmented Chemical Reactions project makes intense use of such two-handed interaction tasks. Users control virtual molecules and subsets that are registered to physical markers and try to combine those by selecting and then confirming a specific bond. When a desired bond has been selected, the user needs a way to confirm that bond without letting an atom go out of position. We developed and investigated two separate methods of confirming a selected bond when both hands are already doing a two-handed symmetric interaction task. The first method is a waiting method and the second method is a back&forth motion gesture. We evaluated the two methods in a user study, showing that the first technique, holding still, outperforms the other technique.},
keywords={augmented reality;chemical reactions;gesture recognition;two handed gesture;user controlled object;spatial manipulation technique;augmented chemical reaction;symmetric interaction task;Visualization;Chemicals;Jitter;Time measurement;Augmented reality;Three dimensional displays;Interviews},
doi={10.1109/ISMAR.2010.5643592},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643593,
author={E. {Molla} and V. {Lepetit}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Augmented reality for board games},
year={2010},
volume={},
number={},
pages={253-254},
abstract={We introduce a new type of Augmented Reality games: By using a simple webcam and Computer Vision techniques, we turn a standard real game board pawns into an AR game. We use these objects as a tangible interface, and augment them with visual effects. The game logic can be performed automatically by the computer. This results in a better immersion compared to the original board game alone and provides a different experience than a video game. We demonstrate our approach on Monopoly™, but it is very generic and could easily be adapted to any other board game.},
keywords={augmented reality;computer games;computer vision;graphical user interfaces;augmented reality;webcam;computer vision;board game;tangible interface;Games;Augmented reality;Three dimensional displays;Computers;Detectors;Computer vision;Cameras},
doi={10.1109/ISMAR.2010.5643593},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643594,
author={F. {Nagl} and P. {Grimm} and B. {Birnbach} and D. F. {Abawi}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={PoP-EYE environment: Mixed Reality using 3D Photo Collections},
year={2010},
volume={},
number={},
pages={255-256},
abstract={The paper is talking about PoP-EYE, a mixed reality environment, using 3D photo collections. This augmentation enhance the quality of the virtual (artificial) object hence providing realistic image-based rendering.},
keywords={augmented reality;image processing;realistic images;rendering (computer graphics);PoP-EYE environment;mixed reality environment;3D photo collection;virtual object;artificial augmentation;realistic image based rendering;Three dimensional displays;Virtual reality;Lighting;Light sources;Cameras;Image segmentation;Rendering (computer graphics);Mixed Reality;3D Photo Collection;Occlusion;Image Quality;Illumination;Image-based lighting;Image-based rendering},
doi={10.1109/ISMAR.2010.5643594},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643595,
author={O. {Nestares} and Y. {Gat} and H. {Haussecker} and I. {Kozintsev}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Video stabilization to a global 3D frame of reference by fusing orientation sensor and image alignment data},
year={2010},
volume={},
number={},
pages={257-258},
abstract={Estimating the 3D orientation of the camera in a video sequence within a global frame of reference is useful for video stabilization when displaying the video in a virtual 3D environment, as well as for accurate navigation and other applications. This task requires the input of orientation sensors attached to the camera to provide absolute 3D orientation in a geographical frame of reference. However, high-frequency noise in the sensor readings makes it impossible to achieve accurate orientation estimates required for visually stable presentation of video sequences that were acquired with a camera subject to jitter, such as a handheld camera or a vehicle mounted camera. On the other hand, image alignment has proven successful for image stabilization, providing accurate frame-to-frame orientation estimates but drifting over time due to error and bias accumulation and lacking absolute orientation. In this paper we propose a practical method for generating high accuracy estimates of the 3D orientation of the camera within a global frame of reference by fusing orientation estimates from an efficient image-based alignment method, and the estimates from an orientation sensor, overcoming the limitations of the component methods.},
keywords={image fusion;image sequences;jitter;solid modelling;video cameras;video signal processing;video stabilization;global 3D frame;orientation sensor fusion;3D orientation estimation;video sequence;virtual 3D environment;geographical frame;high frequency noise;handheld camera;vehicle mounted camera;image stabilization;frame-to-frame orientation;image based alignment method;Cameras;Three dimensional displays;Robot sensing systems;Jitter;Augmented reality;Virtual environment;Video sequences},
doi={10.1109/ISMAR.2010.5643595},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643596,
author={F. {Okura} and M. {Kanbara} and N. {Yokoya}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Augmented telepresence using autopilot airship and omni-directional camera},
year={2010},
volume={},
number={},
pages={259-260},
abstract={This study is concerned with a large-scale telepresence system based on remote control of mobile robot or aerial vehicle. The proposed system provides a user with not only view of remote site but also related information by AR technique. Such systems are referred to as augmented telepresence in this paper. Aerial imagery can capture a wider area at once than image capturing from the ground. However, it is difficult for a user to change position and direction of viewpoint freely because of the difficulty in remote control and limitation of hardware. To overcome these problems, the proposed system uses an autopilot airship to support changing user's viewpoint and employs an omni-directional camera for changing viewing direction easily. This paper describes hardware configuration for aerial imagery, an approach for overlaying virtual objects, and automatic control of the airship, as well as experimental results using a prototype system.},
keywords={aircraft control;airships;augmented reality;cameras;control engineering computing;mobile robots;telerobotics;augmented telepresence;autopilot airship;omnidirectional camera;large-scale telepresence system;mobile robot;remote control;aerial vehicle;AR technique;aerial imagery;airship automatic control;Cameras;Global Positioning System;Gyroscopes;Real time systems;Prototypes;Vehicles;Parameter estimation;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities;I.2.9 [Artificial Intelligence]: Robotics—Autonomous vehicles},
doi={10.1109/ISMAR.2010.5643596},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643597,
author={T. {Orikasa} and S. {Kagami} and K. {Hashimoto}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Time-domain augmented reality based on locally adaptive video sampling},
year={2010},
volume={},
number={},
pages={261-262},
abstract={We propose a new approach for augmented reality, in which real-world scene images are augmented with video fragments manipulated in the time domain. The proposed system aims to display slow-motion video sequences of moving objects instantly without accumulated time lag so that a user can recognize and observe high-speed motion on the spot. Images from a high-speed camera are analyzed to detect regions with important visual features, which are overlaid on a normal-speed video sequence.},
keywords={augmented reality;image sampling;image sensors;image sequences;time domain augmented reality;locally adaptive video sampling;real world scene images;video fragments;slow motion video sequences;high speed camera;high speed motion;Pixel;Streaming media;Video sequences;Time domain analysis;Cameras;Instruction sets;Real time systems;I.4.9 [Computing Methodologies]: Image Processing and Computer Vision—Applications;H.5.1 [Information Systems]: Information Interfaces and Presentation—Multimedia Information Systems - Artificial, augmented, and virtual realities},
doi={10.1109/ISMAR.2010.5643597},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643598,
author={J. {Park} and J. {Park}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={3DOF tracking accuracy improvement for outdoor Augmented Reality},
year={2010},
volume={},
number={},
pages={263-264},
abstract={Outdoor Augmented Reality (AR) gained popularity recently due to its potential for location based mobile services. However, most commercially available Global Positioning Systems (GPS), except for the expensive high-end models, do not provide accurate location information that is enough to be used for displaying practically meaningful location based information. In this paper, we present a computer vision based method for improving user's two dimensional location and one-dimensional orientation, the initial values of which are obtained from a GPS and a digital compass. Our method utilizes corner positions of buildings in the map and the vertical edges of the buildings in the captured images. We applied anisotropic diffusion in order to filter noise and preserve edges, and dual vertical edge filters on gray and saturation images. Our method is suitable for mobile services in urban environments where tall buildings degrade GPS signals. In average, our method improved 15.0 meters in position and 2.2 degrees in orientation.},
keywords={augmented reality;computer vision;filtering theory;Global Positioning System;image denoising;3DOF tracking accuracy improvement;outdoor augmented reality;global positioning systems;location based mobile services;computer vision based method;two dimensional location;one-dimensional orientation;digital compass;anisotropic diffusion;dual vertical edge filters;saturation images;gray images;GPS signals;Global Positioning System;Image edge detection;Buildings;Accuracy;Compass;Anisotropic magnetoresistance;Augmented reality;outdoor tracking;LBS;anisotropic diffusion},
doi={10.1109/ISMAR.2010.5643598},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643599,
author={S. R. {Porter} and M. R. {Marner} and R. T. {Smith} and J. E. {Zucco} and B. H. {Thomas}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Validating Spatial Augmented Reality for interactive rapid prototyping},
year={2010},
volume={},
number={},
pages={265-266},
abstract={This paper investigates the use of Spatial Augmented Reality in the prototyping of new human-machine interfaces, such as control panels or car dashboards. The prototyping system uses projectors to present the visual appearance of controls onto a mock-up of a product. Finger tracking is employed to allow real-time interactions with the controls. This technology can be used to quickly and inexpensively create and evaluate interface prototypes for devices. In the past, evaluating a prototype involved constructing a physical model of the device with working components such as buttons. We have conducted a user study to compare these two methods of prototyping and to validate the use of spatial augmented reality for rapid iterative interface prototyping. Participants of the study were required to press pairs of buttons in sequence and interaction times were measured. The results indicate that while slower, users can interact naturally with projected control panels.},
keywords={augmented reality;man-machine systems;rapid prototyping (industrial);spatial augmented reality;interactive rapid prototyping;human-machine interfaces;finger tracking;real-time interactions;Prototypes;Presses;Augmented reality;Visualization;Three dimensional displays;Automotive engineering;Analysis of variance;Spatial Augmented Reality;Rapid Prototyping;Industrial Design},
doi={10.1109/ISMAR.2010.5643599},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643600,
author={G. {Schall} and A. {Mulloni} and G. {Reitmayr}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={North-centred orientation tracking on mobile phones},
year={2010},
volume={},
number={},
pages={267-268},
abstract={Magnetic compasses and accelerometers provide absolute orientation measurements within the earth's reference frame. However, sensor output typically suffers from jitter and external disturbances. Conversely, visual tracking provides more stable orientation estimation relative to an unknown initial orientation rather than to true north. We propose a 3-degree-of-freedom orientation tracking approach combining the accuracy and stability of vision tracking with the absolute orientation from inertial and magnetic sensors by estimating the offset between the initial orientation of the vision tracker and true north. We demonstrate that the approach improves absolute orientation estimation on a mobile phone device.},
keywords={accelerometers;augmented reality;compasses;computer vision;jitter;magnetic sensors;magnetic variables measurement;mobile computing;mobile handsets;mobility management (mobile radio);tracking;north centred orientation tracking;mobile phone;magnetic compass;accelerometer;visual tracking;orientation estimation;magnetic sensor;vision tracker;inertial sensors;Tracking;Visualization;Magnetometers;Accuracy;Estimation;Augmented reality;Compass;H.5.1 [Information Systems]: Multimedia Information Systems—Augmented Reality;I.4.8 [Image Processing and Computer Vision]: Scene Analysis—Sensor fusion},
doi={10.1109/ISMAR.2010.5643600},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643601,
author={F. {Scheer} and S. {Müller}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Large area indoor tracking for industrial augmented reality},
year={2010},
volume={},
number={},
pages={269-270},
abstract={A precise tracking with minimal setup times, minimal changes to the environment and acceptable costs, satisfying industrial demands in large factory buildings is still a challenging task for augmented reality (AR) applications. We present a system to determine the pose for monitor based AR systems in large indoor environments, e.g. 200 × 200 meters and more. An infrared laser detects retroreflective targets and computes a 2D position and orientation based on the information of a preprocessed map of the targets. Based on this information the 6D pose of a video camera attached to a servo motor, that is further mounted on a mobile cart is obtained by identifying the transformation between the laser scanner and the several adjustable views of the camera through a calibration method. The adjustable steps of the servo motor are limited to a discrete number of steps to limit the calibration effort. The positional accuracy of the system is estimated by error propagation and presented.},
keywords={augmented reality;calibration;infrared detectors;video cameras;industrial augmented reality;indoor tracking;2D position;video camera;laser scanner;infrared laser;retroreflective target;calibration method;error propagation;Cameras;Three dimensional displays;Target tracking;Servomotors;Production facilities;Laser modes;I.4.8 [Image Processing and Computer Vision]: Scene Analysis—Tracking;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities},
doi={10.1109/ISMAR.2010.5643601},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643602,
author={J. {Shingu} and E. {Rieffel} and D. {Kimber} and J. {Vaughan} and P. {Qvarfordt} and K. {Tuite}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Camera pose navigation using Augmented Reality},
year={2010},
volume={},
number={},
pages={271-272},
abstract={We propose an Augmented Reality (AR) system that helps users take a picture from a designated pose, such as the position and camera angle of an earlier photo. Repeat photography is frequently used to observe and document changes in an object. Our system uses AR technology to estimate camera poses in real time. When a user takes a photo, the camera pose is saved as a “view bookmark”. To support a user in taking a repeat photo, two simple graphics are rendered in an AR viewer on the camera's screen to guide the user to this bookmarked view. The system then uses image adjustment techniques to create an image based on the user's repeat photo that is even closer to the original.},
keywords={augmented reality;image sensors;pose estimation;camera pose navigation;augmented reality;photography;camera pose estimation;image adjustment technique;Cameras;Navigation;Augmented reality;Photography;Three dimensional displays;Solid modeling;Augmented Reality;Repeat Photography;Rephotography;Camera Pose Navigation},
doi={10.1109/ISMAR.2010.5643602},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643603,
author={M. {Sukan} and S. {Feiner}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={SnapAR: Storing snapshots for quick viewpoint switching in hand-held augmented reality},
year={2010},
volume={},
number={},
pages={273-274},
abstract={Many tasks require a user to move between various locations within an environment to get different perspectives. This can take significant time and effort, especially when the user must switch among those viewpoints repeatedly. We explore augmented reality interaction techniques that involve taking still pictures of a physical scene using a tracked hand-held magic lens and seamlessly switching between augmenting either the live view or one of the still views, without needing to physically revisit the snapshot locations. We describe our optical-marker-tracking-based implementation and how we represent and switch among snapshots. To determine the effectiveness of our techniques, we developed a test application that lets its user view physical and virtual objects from different viewpoints.},
keywords={augmented reality;computer vision;optical tracking;photographic lenses;SnapAR;quick viewpoint switching;handheld augmented reality;augmented reality interaction technique;still picture;physical scene;tracked handheld magic lens;snapshot location;optical marker tracking based implementation;virtual object;Cameras;Optical switches;Augmented reality;Visualization;Arrays;Three dimensional displays;Augmented reality;viewpoint switching},
doi={10.1109/ISMAR.2010.5643603},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643604,
author={C. {Waechter} and M. {Huber} and P. {Keitler} and M. {Schlegel} and G. {Klinker} and D. {Pustka}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={A multi-sensor platform for wide-area tracking},
year={2010},
volume={},
number={},
pages={275-276},
abstract={Indoor tracking scenarios still face challenges in providing continuous tracking support in wide-area workplaces. This is especially the case in Augmented Reality since such augmentations generally require exact full 6DOF pose measurements in order to continuously display 3D graphics from user-related view points. Many single sensor systems have been explored but only few of them have the capability to track reliably in wide-area environments. We introduce a mobile multi-sensor platform to overcome the shortcomings of single sensor systems. The platform is equipped with a detachable optical camera and a rigidly mounted odometric measurement system providing relative positions and orientations with respect to the ground plane. The camera is used for marker-based as well as for marker-less (feature-based) inside-out tracking as part of a hybrid approach. We explain the principle tracking technologies in our competitive/cooperative fusion approach and show possible enhancements to further developments. This inside-out approach scales well with increasing tracking range, as opposed to stationary outside-in tracking.},
keywords={augmented reality;cameras;distance measurement;mobile computing;sensor fusion;solid modelling;tracking;wireless sensor networks;wide area tracking;indoor tracking;augmented reality;3D graphics;mobile multisensor platform;optical camera;principle tracking technology;odometric measurement system;Augmented reality;Robot sensing systems;Cameras;Optical sensors;Mobile communication;Accuracy;Three dimensional displays;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities;I.3.1 [Computer Graphics]: Hardware Architecture—Input devices},
doi={10.1109/ISMAR.2010.5643604},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643605,
author={L. {Wang} and M. {Springer} and H. {Heibel} and N. {Navab}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Floyd-Warshall all-pair shortest path for accurate multi-marker calibration},
year={2010},
volume={},
number={},
pages={277-278},
abstract={We propose a novel method to compute the poses of randomly positioned square markers in one world coordinate frame from multiple camera views, by taking the predicted accuracy of the camera pose estimation for each marker into account. The problem of computing the best closed-form solution of the world pose of each marker is modeled as all-pair shortest path problem in graph theory. The computed world poses are further optimized by minimizing the geometric distances in images. Experimental results show that incorporating the predicted accuracy of the pose estimation for each marker yields constant high quality calibration results independent of the order of image sequences compared to cases when this knowledge is not used.},
keywords={calibration;graph theory;image sequences;pose estimation;Floyd-Warshall;calibration;square markers;pose estimation;all-pair shortest path;graph theory;image sequences;Cameras;Calibration;Accuracy;Estimation;Closed-form solution;Tracking;Image sequences;multi-maker calibration;visual marker based tracking},
doi={10.1109/ISMAR.2010.5643605},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643606,
author={D. {Wei} and S. Z. {Zhou} and D. {Xie}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={MTMR: A conceptual interior design framework integrating Mixed Reality with the Multi-Touch tabletop interface},
year={2010},
volume={},
number={},
pages={279-280},
abstract={This paper introduces a conceptual interior design framework - Multi-Touch Mixed Reality (MTMR), which integrates mixed reality with the multi-touch tabletop interface, to provide an intuitive and efficient interface for collaborative design and an augmented 3D view to users at the same time. Under this framework, multiple designers can carry out design work simultaneously on the top view displayed on the tabletop, while live video of the ongoing design work is captured and augmented by overlaying virtual 3D furniture models to their 2D virtual counterparts, and shown on a vertical screen in front of the tabletop. Meanwhile, the remote client's camera view of the physical room is augmented with the interior design layout in real time, that is, as the designers place, move, and modify the virtual furniture models on the tabletop, the client sees the corresponding life-size 3D virtual furniture models residing, moving, and changing in the physical room through the camera view on his/her screen. By adopting MTMR, which we argue may also apply to other kinds of collaborative work, the designers can expect a good working experience in terms of naturalness and intuitiveness, while the client can be involved in the design process and view the design result without moving around heavy furniture. By presenting MTMR, we hope to provide reliable and precise freehand interactions to mixed reality systems, with multi-touch inputs on tabletop interfaces.},
keywords={augmented reality;cameras;groupware;user interfaces;multitouch tabletop interface;conceptual interior design framework;multitouch mixed reality;collaborative design;augmented 3D view;virtual 3D furniture models;2D virtual counterparts;virtual furniture models;3D virtual furniture models;freehand interactions;Three dimensional displays;Virtual reality;Cameras;Solid modeling;Streaming media;Collaborative work;Layout;Mixed reality;multi-touch tabletop interfaces},
doi={10.1109/ISMAR.2010.5643606},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643607,
author={N. {Yabuki} and K. {Miyashita} and T. {Fukuda}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={AR-based visibility evaluation for preserving landscapes of historical buildings},
year={2010},
volume={},
number={},
pages={281-282},
abstract={Building tall structures behind an aesthetic and historical building tends to destroy the good landscape. To avoid such situations, public agencies must regulate height of buildings and other structures near the landscape target. In order to check the visibility of portions of high, future structures, in this research, a new method using Augmented Reality (AR) was proposed. In this method, a number of virtual rectangular objects with a scale are located on the grid of 3D geographical model. And then, the virtual rulers are shown in an overlapping manner with the actual landscape from multiple viewpoints using the AR technology. The user measures the maximum skyline-preserving height for each rectangular object at a grid point. Using the measured data, the government or public agencies can establish appropriate height regulations for all surrounding areas of the target structures. To verify the proposed method, a system was developed deploying AR Toolkit and was applied to a scenic building. The performance of the system was checked and then, the errors of the obtained data were evaluated. In conclusion, the proposed method was evaluated feasible and effective.},
keywords={augmented reality;building;computational geometry;data visualisation;solid modelling;structural engineering computing;AR based visibility evaluation;preserving landscape;historical building;building height regulation;Augmented Reality;skyline preserving height;AR toolkit;3D geographical model;virtual rulers;scenic building;Buildings;Augmented reality;Cameras;Accuracy;Three dimensional displays;Solid modeling;Prototypes;Augmented reality;landscape;invisible depth},
doi={10.1109/ISMAR.2010.5643607},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643608,
author={T. {Yoshida} and M. {Tsukadaira} and A. {Kimura} and F. {Shibata} and H. {Tamura}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Various tangible devices suitable for mixed reality interactions},
year={2010},
volume={},
number={},
pages={283-284},
abstract={In this paper, we present various novel tangible devices suitable for interactions in a mixed reality (MR) environment. They are aimed at making the best use of the features of MR, which allows users to touch or handle both virtual and physical objects. Furthermore, we consider usability and intuitiveness as important characteristics of the interface, and thus designed our devices to imitate traditional tools and help users understand their use.},
keywords={user interfaces;virtual reality;tangible devices;mixed reality interactions;virtual objects;physical objects;Virtual reality;Image color analysis;User interfaces;Painting;Shape;Pixel;Mixed reality;tangible device;interaction;tool;user interface;tabletop},
doi={10.1109/ISMAR.2010.5643608},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643609,
author={J. {Yu} and J. {Kim}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Camera motion tracking in a dynamic scene},
year={2010},
volume={},
number={},
pages={285-286},
abstract={To insert a virtual object into a real image, the position of the object must appear seamlessly as the camera moves. This requires camera tracking with estimations of all internal and external parameters in each frame with an adequate degree of stability to ensure negligible visible drift between the real and virtual elements. In the post production of film, matchmoving software based on SfM is typically used in the camera tracking process. However, most of this type of software fails when attempting to track the camera in a dynamic scene in which a moving foreground object such as a real actor occupies a large part of the background. Therefore, this study proposes a camera tracking system that uses an auxiliary camera to estimate the motion of the main shooting camera and 3D position of background features in a dynamic scene. A novel reconstruction and connection method was developed for feature tracks that are occluded by a foreground object. Experimentation with a 2K sequence demonstrated the feasibility of the proposed method.},
keywords={cameras;image reconstruction;image sequences;motion estimation;target tracking;camera tracking;position estimation;motion estimation;image sequence;image reconstruction;dynamic scene;Cameras;Tracking;Three dimensional displays;Image reconstruction;Computer vision;Calibration;Dynamics;Camera calibration;Motion;Tracking},
doi={10.1109/ISMAR.2010.5643609},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643610,
author={F. {Zheng} and H. {Li}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={ARCrowd-a tangible interface for interactive crowd simulation},
year={2010},
volume={},
number={},
pages={287-288},
abstract={Manipulating a large virtual crowd in an interactive virtual reality environment is a challenging task due to the limitations of the traditional user interface. To address this problem, a tangible interface based on augmented reality (AR) is introduced. Through the tangible AR interface, the user could manipulate the virtual characters directly, or control the crowd behaviors with markers. These markers are used to adjust the environment factors, the decision-making processes of virtual crowds, and their reactions. The AR interface provides more intuitive means of control for the users, promoting the efficiency of human-machine interface.},
keywords={augmented reality;decision making;graphical user interfaces;man-machine systems;virtual crowd;virtual reality;augmented reality;virtual characters;human-machine interface;tangible interface;crowd behaviors control;decision making;tangible user interface;Fires;Switches;Solid modeling;Augmented reality;Three dimensional displays;User interfaces;Computational modeling},
doi={10.1109/ISMAR.2010.5643610},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643611,
author={K. {Zhu} and O. N. N. {Fernando} and A. D. {Cheok} and M. {Fiala} and T. W. {Yang}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Origami recognition system using natural feature tracking},
year={2010},
volume={},
number={},
pages={289-290},
abstract={This paper introduces a system that can recognize different type of paper-folding by users. The system allows users to register and use their desired paper in the interaction, and detect the folding by using Speed Up Robust Feature (SURF) algorithm. The paper also describes a paper-based tower defense game which has been developed as a proof of concept of our method. This method can be considered as the initial step for seamlessly migrating meaningful traditional art of origami into the digital world as a part of the interactive media.},
keywords={interactive systems;origami recognition system;natural feature tracking;paper folding;speed up robust feature algorithm;paper-based tower defense game;interactive media;Robustness;Games;Feature extraction;Electronic mail;Poles and towers;Detectors;Registers},
doi={10.1109/ISMAR.2010.5643611},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643612,
author={},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Tutorials and workshops},
year={2010},
volume={},
number={},
pages={291-297},
abstract={Provides an abstract for each of the presentations and a brief professional biography of each presenter. The complete presentations were not made available for publication as part of the conference proceedings.},
keywords={},
doi={10.1109/ISMAR.2010.5643612},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643613,
author={F. {Shibata} and S. {Ikeda} and T. {Kurata} and H. {Uchiyama}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={An intermediate report of TrakMark WG ∼international voluntary activities on establishing benchmark test schemes for AR/MR geometric registration and tracking methods},
year={2010},
volume={},
number={},
pages={298-302},
abstract={In the study of AR/MR field, tracking and geometric registration methods are very important topics that are actively discussed. Especially, the study on tracking is flourishing and many algorithms are being proposed every year. With this trend in mind, we, the TrakMark WG, had proposed benchmark test schemes for geometric registration and tracking in AR/MR at ISMAR 2009 [1]. This paper is an intermediate report of the TrakMark WG, which describes its activities and the first proposal on benchmarking image sequences.},
keywords={Geometric Registration;Tracking;Benchmark},
doi={10.1109/ISMAR.2010.5643613},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643614,
author={},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Demonstrations [breaker page]},
year={2010},
volume={},
number={},
pages={303-305},
abstract={Start of the "Demonstrations" section of the proceedings record.},
keywords={},
doi={10.1109/ISMAR.2010.5643614},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643615,
author={T. {Lee} and S. {Soatto}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Feature tracking and object recognition on a hand-held},
year={2010},
volume={},
number={},
pages={306-306},
abstract={We demonstrate a visual recognition system operating on a hand-held device, with the help of an efficient and robust feature tracking and an object recognition mechanism that can be used for interactive mobile applications. In our recognition system, corner features are detected from captured video frames in a multi-scale image pyramid, and are tracked between consecutive frames efficiently. In order to perform object recognition, local descriptors are calculated on the tracked features, and quantized using a vocabulary tree. For each object, a bag-of-words model is learned from multiple views. The learned objects are recognized by computing the ranking score for the set of features in a single video frame. Our feature tracking algorithm and local descriptors are different than the Lucas-Kanade algorithm in image pyramid or the SIFT descriptor, however improving the efficiency and accuracy. For our implementation on a mobile phone, we used an iPhone 3GS with a 600MHz ARM chip CPU. The video frame is captured from a camera preview screen at a rate of 15 frames per second using the public API. The task of object recognition on a mobile phone runs at around 7 frames per second, including the feature tracking and descriptor calculation.},
keywords={computer vision;feature extraction;image recognition;mobile handsets;object recognition;tracking;video cameras;video signal processing;vocabulary;feature tracking;object recognition;visual recognition system;handheld device;interactive mobile application;corner feature;captured video frame;multiscale image pyramid;vocabulary tree;bag of words model;learned object;Lucas-Kanade algorithm;SIFT descriptor;mobile phone;iPhone 3GS;600MHz ARM chip CPU;public API;descriptor calculation},
doi={10.1109/ISMAR.2010.5643615},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643616,
author={K. {Zhu} and O. N. N. {Fernando} and T. W. {Yang} and A. D. {Cheok} and M. {Fiala}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Origami recognition system using natural feature tracking},
year={2010},
volume={},
number={},
pages={307-307},
abstract={In this demonstration we introduce a system that allows users to register and use their desired paper for the interaction, and recognize the folding during the interaction in real time. This method can be considered as an initial step for seamlessly migrating meaningful traditional art of origami into the digital world and as part of the interactive media.},
keywords={art;computer vision;feature extraction;image recognition;interactive systems;origami recognition system;natural feature tracking;computer vision;feature recognition;real time system;digital world;interactive media;folding process},
doi={10.1109/ISMAR.2010.5643616},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643617,
author={S. {Jeon} and B. {Knoerlein} and M. {Harders} and G. {Han} and S. {Choi}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Breast cancer palpation system using haptic augmented reality},
year={2010},
volume={},
number={},
pages={308-308},
abstract={In haptic augmented reality (AR), a stimulus from a real object is augmented by a synthetic haptic signal. We previously developed a haptic AR system wherein the stiffness of a real object can be augmented with the aid of a haptic interface. This demonstration presents a case study of this technology for medical training of breast cancer palpation. A real breast model made of soft silicone is augmented with a virtual tumor rendered inside. Haptic stimuli for the virtual tumor are generated based on a contact dynamics model identified via real measurements. With our palpation system, a user can experience excellent realism comparable to that of a real breast mock-up containing a real tumor.},
keywords={augmented reality;biomedical education;cancer;haptic interfaces;rendering (computer graphics);tumours;breast cancer palpation system;haptic augmented reality;synthetic haptic signal;haptic interface;medical training;virtual tumor rendered;virtual tumor},
doi={10.1109/ISMAR.2010.5643617},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643618,
author={C. {Traxler} and M. {Knecht}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Demo for differential Instant Radiosity for Mixed Reality},
year={2010},
volume={},
number={},
pages={309-309},
abstract={This laboratory demo is a showcase for the research results published in our ISMAR 2010 paper [3], where we describe a method to simulate the mutual shading effects between virtual and real objects in Mixed Reality applications. The aim is to provide a plausible illusion so that virtual objects seem to be really there. It combines Instant Radiosity [2] with Differential Rendering [1] to a method suitable for MR applications. The demo consists of two scenarios, a simple one to focus on mutual shading effects and an MR game based on LEGO®.},
keywords={rendering (computer graphics);virtual reality;differential instant radiosity;mixed reality;mutual shading effect;virtual object;differential rendering},
doi={10.1109/ISMAR.2010.5643618},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643619,
author={P. {Belimpasakis} and P. {Selonen} and Y. {You}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={A Web Service Platform dedicated to building mixed reality solutions},
year={2010},
volume={},
number={},
pages={310-310},
abstract={While many attempts have been done towards creating mixed reality platforms for mobile client devices, there have not been any significant efforts at the server/infrastructure side. We demonstrate our Mixed Reality Web Service Platform (MRS-WS) dedicated to enabling rapid creation of mixed reality solutions, those being either desktop or mobile. Focusing on common interfaces and functions across user generated and commercial geo-content, we provide an appealing developer offering, which we are currently evaluating via a closed set of university partners. Our plan is to gradually expand the developer API access to more partners, before deciding if it is ready for fully public developer access.},
keywords={application program interfaces;structural engineering computing;virtual reality;Web services;building mixed reality solution;mobile client device;mixed reality Web service platform;MRS-WS;commercial geocontent;user generated geocontent;API},
doi={10.1109/ISMAR.2010.5643619},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643620,
author={D. {Weng} and D. {Li} and W. {Xu} and Y. {Liu} and Y. {Wang}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={AR Shooter: An augmented reality shooting game system},
year={2010},
volume={},
number={},
pages={311-311},
abstract={This paper presents the features and functionalities of AR Shooter, an augmented reality shooting game system which is based on infrared marker tracking. The proposed system consists of two parts: a gun with video cameras and infrared markers composed of LED. When the gun aims on the infrared markers, some monsters will appear on the LCD equipped on the gun. Then, the user can open fire and shoot at the monsters. Figure 1 shows the diagram of the proposed system.},
keywords={augmented reality;computer games;light emitting diodes;liquid crystal displays;optical tracking;video cameras;AR shooter;augmented reality shooting game system;infrared marker tracking;video camera;gun;LED;LCD;Augmented Reality;Infrared Marker;Shooting Game},
doi={10.1109/ISMAR.2010.5643620},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643621,
author={S. {Martedi} and H. {Uchiyama} and G. {Enriquez} and H. {Saito} and T. {Miyashita} and T. {Hara}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Foldable augmented maps},
year={2010},
volume={},
number={},
pages={312-312},
abstract={This demonstration presents folded surface detection and tracking for augmented maps. We model the folded surface as multiple planes. To detect a folded surface, plane detection is iteratively applied to 2D correspondences between an input image and a reference plane. In order to compute the exact folding line from the detected planes, the intersection line of the planes is computed from their positional relationship. After the detection is done, each plane is individually tracked by frame-by-frame descriptor update. For a natural augmentation on the folded surface, we overlay virtual geographic data on each detected plane.},
keywords={},
doi={10.1109/ISMAR.2010.5643621},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643622,
author={M. {Kurze} and A. {Roselius}},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Smart Glasses: An open environment for AR apps},
year={2010},
volume={},
number={},
pages={313-313},
abstract={We present an architecture [fig. 1] and runtime environment for mobile Augmented Reality applications. The architecture is based on a plugin-concept on the device, a set of basic functionalities available for all apps and a cloud-oriented processing approach. As a first running sample app, we show a face recognition service running on a mobile phone, conventional wearable displays and upcoming see-through-goggles. We invite interested 3rd parties to try out the environment, face recognition app and platform.},
keywords={augmented reality;eye protection;face recognition;Internet;mobile handsets;mobile augmented reality;cloud oriented processing;face recognition;mobile phone;goggles},
doi={10.1109/ISMAR.2010.5643622},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643623,
author={},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={[Blank page]},
year={2010},
volume={},
number={},
pages={314-314},
abstract={This page or pages intentionally left blank.},
keywords={},
doi={10.1109/ISMAR.2010.5643623},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643624,
author={},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Color plates},
year={2010},
volume={},
number={},
pages={315-317},
abstract={Presents color images for various figures that appeared throughout this issue of the publication.},
keywords={},
doi={10.1109/ISMAR.2010.5643624},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643625,
author={},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={[Blank page]},
year={2010},
volume={},
number={},
pages={318-318},
abstract={This page or pages intentionally left blank.},
keywords={},
doi={10.1109/ISMAR.2010.5643625},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5643626,
author={},
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, title={Author index},
year={2010},
volume={},
number={},
pages={319-319},
abstract={The author index contains an entry for each author and coauthor included in the proceedings record.},
keywords={},
doi={10.1109/ISMAR.2010.5643626},
ISSN={},
month={Oct},}