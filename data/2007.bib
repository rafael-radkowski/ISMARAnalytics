@INPROCEEDINGS{4538819,
author={D. {Schmalstieg} and D. {Wagner}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Experiences with Handheld Augmented Reality},
year={2007},
volume={},
number={},
pages={3-18},
abstract={In this paper, we present Studierstube ES, a framework for the development of handheld Augmented Reality. The applications run self-contained on handheld computers and smartphones with Windows CE. A detailed description of the performance critical tracking and rendering components are given. We also report on the implementation of a client-server architecture for multi-user applications, and a game engine for location based museum games that has been built on top of this infrastructure. Details on two games that were created, permanently deployed and evaluated in two Austrian museums illustrate the practical value of the framework and lessons learned from using it.},
keywords={Augmented reality;Personal digital assistants;Games;Handheld computers;Application software;Smart phones;Wearable computers;Computer interfaces;Personal communication networks;Engines;Mobile augmented reality;wearable computing;cultural heritage;augmented reality games},
doi={10.1109/ISMAR.2007.4538819},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538820,
author={D. {Kurz} and F. {Hantsch} and M. {Grosse} and A. {Schiewe} and O. {Bimber}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Laser Pointer Tracking in Projector-Augmented Architectural Environments},
year={2007},
volume={},
number={},
pages={19-26},
abstract={We present a system that employs a custom-built pan-tilt-zoom camera for laser pointer tracking in arbitrary real environments. Once placed in a room, it carries out a fully automatic self-registration, registrations of projectors, and sampling of surface parameters, such as geometry and reflectivity. After these steps, it can be used for tracking a laser spot on the surface as well as an LED marker in 3D space, using inter-playing fish-eye context and controllable detail cameras. The captured surface information can be used for masking out areas that are problematic for laser pointer tracking, and for guiding geometric and radiometric image correction techniques that enable a projector-based augmentation on arbitrary surfaces. We describe a distributed software framework that couples laser pointer tracking for interaction, projector-based AR as well as video see-through AR for visualizations, with the domain specific functionality of existing desktop tools for architectural planning, simulation and building surveying.},
keywords={augmented reality;computer vision;data visualisation;image sensors;optical projectors;laser pointer tracking;projector-augmented architectural environments;pan-tilt-zoom camera;fully automatic self-registration;radiometric image correction techniques;distributed software framework;architectural planning;building surveying;Surface emitting lasers;Cameras;Sampling methods;Geometrical optics;Reflectivity;Light emitting diodes;Automatic control;Optical control;Radiometry;Software tools;I.3.3 [Computer Graphics]: Picture/Image Generation¿Digitizing and scanning;I.4.8 [Image Processing and Computer Vision]: Scene Analysis¿Tracking;J.5 [Computer Applications]: Arts and Humanities¿Architecture},
doi={10.1109/ISMAR.2007.4538820},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538821,
author={M. {Sareika} and D. {Schmalstieg}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Urban Sketcher: Mixed Reality on Site for Urban Planning and Architecture},
year={2007},
volume={},
number={},
pages={27-30},
abstract={Urban sketcher, a mixed reality application, is designed to encourage and improve communication on urban design among stakeholders. A mix of multimodal input devices enhances collaborative interaction in real-time, while visual feedback is given to all participants on a projected live video augmentation from urban sketcher. Sketching, modifying the scene on site, in the space of the video augmentation supports the exchange of information with interactive visual support. Urban Sketcher is instrumental for developing visions of future urban spaces by augmenting the real environment with sketches, facades, buildings, green spaces or skylines.},
keywords={augmented reality;computer vision;video signal processing;urban sketcher;urban planning;architecture;collaborative interaction;visual feedback;live video augmentation;Virtual reality;Urban planning;Mixed reality;augmented reality;urban planning;architecture;natural multimodal interaction;collaboration},
doi={10.1109/ISMAR.2007.4538821},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538822,
author={K. {Pentenrieder} and C. {Bade} and F. {Doil} and P. {Meier}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Augmented Reality-based factory planning - an application tailored to industrial needs},
year={2007},
volume={},
number={},
pages={31-42},
abstract={Over the past years, a variety of augmented reality (AR)-based applications were created, to support industrial processes. Although these first demonstrator applications or prototypes cover all parts of the industrial product process (design, planning and production, service and maintenance), only a few of them actually turned into established and applied solutions. In the automotive industry, one successful field for AR-based applications is factory design and planning, metaio and Volkswagen Group Research have concerted their research work to continuously enhance a first prototype in order to create a serviceable and accepted software tool, which is tailored to the needs of AR-based factory planning processes. This paper presents the developing process of this application, the requirements analysis and the realization as a complete system. A concrete example in industry describes the usage of the system for factory planning and an outlook on future work in this area closes the document.},
keywords={augmented reality;automobile industry;production engineering computing;production planning;augmented reality-based factory planning;industrial needs;software tool;factory planning process;Production facilities;Process planning;Application software;Augmented reality;Prototypes;Process design;Production planning;Automotive engineering;Computer industry;Software prototyping;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems¿Artificial, augmented and virtual realities;J.2 [Physical Science and Engineering]: Engineering;J.6 [Computer-Aided Engineering]: Computer-Aided Manufacturing},
doi={10.1109/ISMAR.2007.4538822},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538823,
author={S. {Minatani} and I. {Kitahara} and Y. {Kameda} and Y. {Ohta}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Face-to-Face Tabletop Remote Collaboration in Mixed Reality},
year={2007},
volume={},
number={},
pages={43-46},
abstract={This paper proposes a novel remote face-to-face mixed reality (MR) system that enables two people in distant places to share MR space. Challenging issues to realize such an MR system include capturing, sending, and rendering each user's appearance in real time. We developed a method to represent user's upper body and hands on the table as a single deformed-billboard. An MR Othello game is implemented as a test bed of the remote face-to-face MR system. Users can play the tabletop game as if their opponent were sitting across from the table, despite being physically separated. By detecting and sending the status of each real game board to the other site, both users feel that they are sharing tabletop objects.},
keywords={computer graphics;computer vision;user interfaces;face-to-face tabletop remote collaboration;mixed reality system;rendering;Othello game;Collaboration;Virtual reality;Rendering (computer graphics);Shape;Cameras;Real time systems;Information systems;Application software;Teleconferencing;Computer graphics;Shared Mixed-Reality;Billboard;Tele-Immersion;Real-Time Modeling and Rendering},
doi={10.1109/ISMAR.2007.4538823},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538824,
author={S. {White} and L. {Lister} and S. {Feiner}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Visual Hints for Tangible Gestures in Augmented Reality},
year={2007},
volume={},
number={},
pages={47-50},
abstract={Tangible augmented reality (AR) systems imbue physical objects with the ability to act and respond in new ways. In particular, physical objects and gestures made with them gain meaning that does not exist outside the tangible AR environment. The existence of this new set of possible actions and outcomes is not always apparent, making it necessary to learn new movements or gestures. Addressing this opportunity, we present visual hints, which are graphical representations in AR of potential actions and their consequences in the augmented physical world. Visual hints enable discovery, learning, and completion of gestures and manipulation in tangible AR. Here, we discuss our investigation of a variety of representations of visual hints and methods for activating them. We then describe a specific implementation that supports gestures developed for a tangible AR user interface to an electronic field guide for botanists, and present results from a pilot study.},
keywords={augmented reality;computer vision;user interfaces;augmented reality;tangible gestures;visual hints;graphical representations;user interface;Augmented reality;Displays;User interfaces;Chromium;Multimedia systems;Virtual reality;Documentation;Morphology;Radio access networks;History;Tangible augmented reality;visual hints;gestures;electronic field guide},
doi={10.1109/ISMAR.2007.4538824},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538825,
author={J. {Looser} and R. {Grasset} and M. {Billinghurst}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={A 3D Flexible and Tangible Magic Lens in Augmented Reality},
year={2007},
volume={},
number={},
pages={51-54},
abstract={The Magic Lens concept is a focus and context technique which facilitates the visualization of complex and dense data. In this paper, we propose a new type of 3D tangible Magic Lens in the form of a flexible sheet. We describe new interaction techniques associated with this tool, and demonstrate how it can be applied in different AR applications.},
keywords={augmented reality;data visualisation;user interfaces;3D tangible magic lens;augmented reality;dense data visualization;flexible sheet;interaction techniques;Lenses;Augmented reality;Graphics;Hardware;Data visualization;User interfaces;Filtering;Glass;Information systems;Context awareness;H.5.2 [Information Systems]: Information Interfaces and Presentation¿User Interfaces},
doi={10.1109/ISMAR.2007.4538825},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538826,
author={M. {Anabuki} and H. {Ishii}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={AR-Jig: A Handheld Tangible User Interface for Modification of 3D Digital Form via 2D Physical Curve},
year={2007},
volume={},
number={},
pages={55-66},
abstract={We introduce AR-Jig, a new handheld tangible user interface for 3D digital modeling in augmented reality (AR) space. AR-Jig has a pin array that displays a 2D physical curve coincident with a contour of a digitally displayed 3D form. It supports physical interaction with a portion of a 3D digital representation, allowing 3D forms to be directly touched and modified. Traditional tangible user interfaces physically embody all the data; in contrast, this project leaves the majority of the data in the digital domain but gives physicality to any portion of the larger digital dataset via a handheld tool. This tangible intersection enables the flexible manipulation of digital artifacts, both physically and virtually. Through an informal test by end-users and interviews with professionals, we confirmed the potential of the AR-Jig concept while identifying the improvements necessary to make AR-Jig a practical tool for 3D digital design.},
keywords={augmented reality;user interfaces;AR-Jig;handheld tangible user interface;3D digital form;2D physical curve;augmented reality;3D digital representation;digital artifacts;User interfaces;Three dimensional displays;Two dimensional displays;Prototypes;Laboratories;Augmented reality;Imaging phantoms;Space technology;Pins;Humans;actuated interface;augmented reality;digital modeling;handheld tool;pin array display},
doi={10.1109/ISMAR.2007.4538826},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538827,
author={G. {Reitmayr} and E. {Eade} and T. W. {Drummond}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Semi-automatic Annotations in Unknown Environments},
year={2007},
volume={},
number={},
pages={67-70},
abstract={Unknown environments pose a particular challenge for augmented reality applications because the 3D models required for tracking, rendering and interaction are not available ahead of time. Consequently, authoring of AR content must take place on-line. This work describes a set of techniques to simplify the online authoring of annotations in unknown environments using a simultaneous localisation and mapping (SLAM) system. The point-based SLAM system is extended to specifically track and estimate high-level features indicated by the user. The automatic estimation of these complex landmarks by the system relieves the user from the burden of manually specifying the full 3D pose of annotations while improving accuracy. These properties are especially interesting for remote collaboration applications where either user interfaces on handhelds or camera control by the remote expert are limited.},
keywords={augmented reality;computer vision;semiautomatic annotations;augmented reality;tracking;rendering;simultaneous localisation and mapping system;3D pose;remote collaboration;Simultaneous localization and mapping;Cameras;Application software;Layout;Augmented reality;Collaborative work;Geometry;Shape measurement;Humans;User interfaces;H.5.1 [Information Systems]: Multimedia Information Systems¿Augmented Reality;I.4.8 [Image Processing and Computer Vision]: Scene Analysis¿Tracking},
doi={10.1109/ISMAR.2007.4538827},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538828,
author={M. {Klopschitz} and D. {Schmalstieg}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Automatic Reconstruction of Wide-Area Fiducial Marker Models},
year={2007},
volume={},
number={},
pages={71-74},
abstract={We present an approach towards automatic reconstruction of large assemblies of fiducial markers scattered throughout a wide indoor area, using a computer vision based reconstruction approach. The data is acquired from a video stream captured with a monoscopic camera. The system is capable of creating markers models that are significantly larger in physical area and number of markers than with previous approaches.},
keywords={augmented reality;computer vision;image reconstruction;video streaming;automatic reconstruction;wide-area fiducial marker models;computer vision;video stream;monoscopic camera;Cameras;Streaming media;Assembly;Hardware;Image reconstruction;Computer vision;Augmented reality;Humans;Scattering;Pipelines},
doi={10.1109/ISMAR.2007.4538828},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538829,
author={J. {Platonov} and M. {Langer}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Automatic contour model creation out of polygonal CAD models for markerless Augmented Reality},
year={2007},
volume={},
number={},
pages={75-78},
abstract={We present a solution for automatic extraction of contour models out of polygonal CAD models. Such contour models can be used for markerless initialization and tracking purposes. To create a contour model we synthesize different views of the object using its CAD model. During the view generation we alternate the camera pose as well as light conditions in order to extract the most stable contours in terms of illumination invariance and view independence. We project the extracted 2D edges back into the 3D space and accumulate for every 3D point statistics over different views describing its visibility and stability under different illumination conditions. After filtering out all 3D points with probability below a certain threshold value we use the Euclidean Minimum Spanning Tree algorithm to get the connected contours out of the 3D point cloud. The result is a B-Spline or VRML representation of the most stable contours.},
keywords={augmented reality;CAD;computational geometry;solid modelling;splines (mathematics);trees (mathematics);automatic contour extraction model;industrial polygonal CAD model;markerless augmented reality;illumination invariance;2D edge extraction;euclidean minimum spanning tree algorithm;3D point cloud;B-spline contour model;VRML representation;Augmented reality;Cameras;Lighting;Layout;Costs;Statistics;Stability;Probability;Filtering algorithms;Clouds;I.4.8 [Image processing and computer vision]: Scene Analysis¿Tracking},
doi={10.1109/ISMAR.2007.4538829},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538830,
author={J. {Neubert} and J. {Pretlove} and T. {Drummond}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Semi-Autonomous Generation of Appearance-based Edge Models from Image Sequences},
year={2007},
volume={},
number={},
pages={79-89},
abstract={Many of the robust visual tracking techniques utilized by augmented reality applications rely on 3D models and information extracted from images. Models enhanced with image information make it possible to initialize tracking and detect poor registration. Unfortunately, generating 3D CAD models and registering them to image information can be a time consuming operation. Regularly the process requires multiple trips between the site being modeled and the workstation used to create the model. The system presented in this work eliminates the need for a separately generated 3D model by utilizing modern structure-from-motion techniques to extract the model and associated image information directly from an image sequence. The technique can be implemented on any handheld device instrumented with a camera and network connection. The process of creating the model requires minimal user interaction in the form of a few cues to identify planar regions on the object of interest. In addition the system selects a set of keyframes for each region to capture viewpoint based appearance changes. This work also presents a robust tracking framework to take advantage of these new edge models. Performance of both the modeling technique and the tracking system are verified on several different objects.},
keywords={CAD;edge detection;image registration;image sequences;semi-autonomous generation;appearance-based edge models;image sequences;robust visual tracking techniques;poor registration;3D CAD models;Image sequences;Robustness;Cameras;Rendering (computer graphics);Handheld computers;Image edge detection;Image reconstruction;Augmented reality;Data mining;Instruments},
doi={10.1109/ISMAR.2007.4538830},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538831,
author={M. {Tonnis} and C. {Lange} and G. {Klinker}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Visual Longitudinal and Lateral Driving Assistance in the Head-Up Display of Cars},
year={2007},
volume={},
number={},
pages={91-94},
abstract={Most car accidents occur due to longitudinal collisions or lane departure. We assume that the number of such accidents can be reduced, if the driver knows more precisely, where the car is heading and at which distance it can stop. To provide drivers with this kind of anticipation, we have developed two augmented reality based visualization schemes for longitudinal and lateral driver assistance in the head-up display (HUD) of cars. One presentation scheme indicates the braking distance by a virtual bar on the road. The second scheme adds the visualization of a drive-path between the car and the bar, zoning the entire region that the car will pass before coming to a complete halt. We have tested both schemes in a driving simulator in comparison to a baseline without visual assistance. Our results show, among other findings, that the bar is preferred, that it supports driving performance and that it does not increase mental workload.},
keywords={augmented reality;computer vision;data visualisation;driver information systems;lateral driving assistance;visual longitudinal driving assistance;head-up display;longitudinal collisions;augmented reality;visualization schemes;Displays;Road accidents;Driver circuits;Visualization;Augmented reality;Testing;Traffic control;Virtual reality;Usability;Human factors;Augmented Reality;Mixed Reality;Head-Up Display;Usability;Advanced Driver Assistance Systems;Human Factors},
doi={10.1109/ISMAR.2007.4538831},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538832,
author={J. {Wither} and S. {DiVerdi} and T. {Hollerer}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Evaluating Display Types for AR Selection and Annotation},
year={2007},
volume={},
number={},
pages={95-98},
abstract={This paper evaluates different display devices for selection or annotation tasks in augmented reality (AR). We compare three different display types - a head mounted display and two hand held displays. The first hand held display is configured as a magic lens where the user sees the augmented space directly behind the display. The second hand held display is configured to be used at waist level (as one would commonly hold a tablet computer) but the view is still of the scene in front of the user. Making a selection or annotation in AR requires two distinct tasks by the user. First, the user must find the real (or virtual) object they want to mark. Second, the user must move a cursor to the object's location. We test and compare our three representative displays with respect to both tasks. We found that using a hand held display in the magic lens configuration was faster for cursor movement than either of the other two displays. There was no significant difference among the displays regarding the amount of time it took users to search for either physical or virtual objects.},
keywords={augmented reality;computer graphics;computer vision;helmet mounted displays;augmented reality;head mounted display;annotation tasks;augmented space;representative displays;Computer displays;Lenses;Augmented reality;Layout;Testing;Application software;Personal digital assistants;Head;Cameras;Eyes;H.5.2 [Information Interfaces and Presentation]: User Interfaces¿Evaluation/methodology;I.3.6 [Computer Graphics]: Methodology and Techniques¿Interaction techniques},
doi={10.1109/ISMAR.2007.4538832},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538833,
author={C. M. {Robertson} and B. {Maclntyre}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={An Evaluation of Graphical Context as a Means for Ameliorating the Effects of Registration Error},
year={2007},
volume={},
number={},
pages={99-110},
abstract={An ongoing research problem in Augmented Reality (AR) is to improve tracking and display technology in order to minimize registration errors. However, perfect registration is not always necessary for users to understand the intent of an augmentation. This paper describes the results of an experiment to evaluate the effects of registration error in a Lego block placement task and the effectiveness of graphical context at ameliorating these effects. Three types of registration error were compared: no error, fixed error and random error. These three errors were evaluated with no context present and some graphical context present. The results of this experiment indicated that adding graphical context to a scene in which some registration error is present can allow a person to effectively operate in such an environment, in this case completing the Lego block placement task with a reduced number of errors made and in a shorter amount of time.},
keywords={augmented reality;computer displays;human computer interaction;image registration;augmented reality;registration error;Lego block placement task;human-computer interaction;computer displays;Graphics;Layout;Augmented reality;Visualization;Uncertainty;Computer displays;Multimedia systems;Back;Adaptive systems;Programming profession;augmented reality;communicative intent;augmented environments;human-computer interaction},
doi={10.1109/ISMAR.2007.4538833},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538834,
author={P. {Georgel} and P. {Schroeder} and S. {Benhimane} and S. {Hinterstoisser} and M. {Appel} and N. {Navab}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={An Industrial Augmented Reality Solution For Discrepancy Check},
year={2007},
volume={},
number={},
pages={111-115},
abstract={Construction companies employ CAD software during the planning phase, but what is finally built often does not match the original plan. The procedure of validating the model is called "discrepancy check". The system proposed here allows the user to easily obtain an augmentation in order to find differences between the planned 3D model and the built items. The main difference to previous body of work in this field is the emphasis on usability and acceptance of the solution. While standard image-based solutions use markers or rely on a "perfect" 3D model to find the pose of the camera, our software uses anchor-plates. Anchor-Plates are rectangular structures installed on walls and ceiling in the majority of industrial edifices. We are using them as landmarks because they are the most reliable components often used as reference coordinates by constructors. Furthermore, for real industrial applications, they are the most suitable solutions in terms of general applicability. Unfortunately, they have not been designed with computer vision applications in mind. On the contrary, they are often made or painted in such way that they are not easily popping out. They are therefore difficult targets to segment and to track. This paper proposes a solution to extract and match them to their 3D counterparts. We created a software that uses the detected structures for pose estimation and image augmentation. The software has been successfully employed to find discrepancies in several rooms of two industrial plants.},
keywords={augmented reality;CAD;computer vision;construction industry;production engineering computing;industrial augmented reality solution;discrepancy check;CAD software;construction companies;anchor-plates;computer vision;Augmented reality;Application software;Construction industry;Usability;Software standards;Cameras;Computer vision;Image segmentation;Target tracking;Industrial plants;H.5.1 [Multimedia Information Systems];H.5.2 [User Interfaces];I.4.6 [Segmentation];I.4.9 [Applications]},
doi={10.1109/ISMAR.2007.4538834},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538835,
author={M. {Tonnis} and R. {Lindl} and L. {Walchshausl} and G. {Klinker}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Visualization of Spatial Sensor Data in the Context of Automotive Environment Perception Systems},
year={2007},
volume={},
number={},
pages={115-124},
abstract={Spatial sensor systems in cars are gaining more and more importance. Such sensor systems are the foundation of future safety systems, such as automatic emergency brakes, as well as for interactive driver assistance systems. We have developed a system that can visualize such spatial sensor data. Two environments are supported: a laboratory setup for off-line experience and a car setup that enables live experience of spatially aligned laser scanner and video data in real traffic. We have used two visualization devices, a video see-through LCD flat panel (TFT) and an optical see-through head-mounted display (HMD) in both setups. For the laboratory setup, a back-projection table has been integrated as well. To present data in correct spatial alignment, we have installed tracking systems in both environments. Visualization schemes for spatial sensor data and for geometric models that outline recognized objects have been developed. We report on our system and discuss experiences from the development and realization phases. The system is not intended to be used as a component of real driver assistance systems. Rather, it can bridge the gap between human machine interface (HMI) designers and sensing engineers during the development phase. Furthermore, it can be both a debugging tool for the realization of environmental perception systems and an experimental platform for the design of presentation schemes for upcoming driver assistance systems.},
keywords={data visualisation;driver information systems;road safety;user interfaces;spatial sensor data visualization;automotive environment perception systems;automatic emergency brakes;interactive driver assistance systems;see-through LCD flat panel;optical see-through head-mounted display;object recognition;human machine interface;debugging tool;Data visualization;Sensor systems;Automotive engineering;Safety;Thin film transistors;Geometrical optics;Optical devices;Optical sensors;Flat panel displays;Liquid crystal displays;H.5.2 [Information Interfaces and Presentation]: User Interfaces¿Ergonomics;H.5.3 [Information Interfaces and Presentation]: Group and Organization Interfaces¿Evaluation/Methodology;H.1.2 [Models and Principles]: User/Machine Systems¿Human Factors},
doi={10.1109/ISMAR.2007.4538835},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538836,
author={C. {Bichlmeier} and S. M. {Heining} and M. {Rustaee} and N. {Navab}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Laparoscopic Virtual Mirror for Understanding Vessel Structure Evaluation Study by Twelve Surgeons},
year={2007},
volume={},
number={},
pages={125-128},
abstract={In this paper we present the evaluation of a virtual mirror used as a navigational tool within a medical augmented reality (AR) system for laparoscopy. 12 surgeons of our clinical partner participated in an experiment to evaluate whether laparoscope augmentation extended by a virtual mirror is useful for improved perception of complex structures. Such complex structures are encountered for instance in laparoscopic resection of tumor affected liver tissue. The blood vessels supplying the tumor have to be cut and closed before tumorous tissue can be removed. A laparoscopic camera and an optical tracking system allow for the visualization of visualized medical volumetric data registered with the real anatomy. Previously injected contrast agent provides an accentuation of blood vessels within the visualization. For evaluating the suitability of a virtual mirror to support the mentioned procedure, we designed a phantom consisting of wooden branches simulating the structure of blood vessel trees. Quantitative results of the experiment show the advantage of a mirror in certain cases, when blood vessels cannot be directly seen from the camera point of view due to self-occlusion of the structure. Results of a questionnaire filled out by the surgeons after the experiments confirm the acceptance of AR technology for particular medical procedures.},
keywords={augmented reality;blood vessels;liver;medical computing;tumours;laparoscopic virtual mirror;vessel structure evaluation;navigational tool;medical augmented reality system;laparoscopy;tumor;liver tissue;blood vessels;medical volumetric data;Laparoscopes;Mirrors;Surgery;Blood vessels;Data visualization;Liver neoplasms;Cameras;Navigation;Augmented reality;Biomedical optical imaging;Augmented reality;navigated surgery;medical visualization;user interaction},
doi={10.1109/ISMAR.2007.4538836},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538837,
author={C. {Bichlmeier} and F. {Wimmer} and S. M. {Heining} and N. {Navab}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Contextual Anatomic Mimesis Hybrid In-Situ Visualization Method for Improving Multi-Sensory Depth Perception in Medical Augmented Reality},
year={2007},
volume={},
number={},
pages={129-138},
abstract={The need to improve medical diagnosis and reduce invasive surgery is dependent upon seeing into a living human system. The use of diverse types of medical imaging and endoscopic instruments has provided significant breakthroughs, but not without limiting the surgeon's natural, intuitive and direct 3D perception into the human body. This paper presents a method for the use of augmented reality (AR) for the convergence of improved perception of 3D medical imaging data (mimesis) in context to the patient's own anatomy (in-situ) incorporating the physician's intuitive multi- sensory interaction and integrating direct manipulation with endoscopic instruments. Transparency of the video images recorded by the color cameras of a video see-through, stereoscopic head- mounted-display (HMD) is adjusted according to the position and line of sight of the observer, the shape of the patient's skin and the location of the instrument. The modified video image of the real scene is then blended with the previously rendered virtual anatomy. The effectiveness has been demonstrated in a series of experiments at the Chirurgische Klinik in Munich, Germany with cadaver and in-vivo studies. The results can be applied for designing medical AR training and educational applications.},
keywords={augmented reality;computer vision;data visualisation;endoscopes;helmet mounted displays;medical diagnostic computing;medical image processing;video signal processing;contextual anatomic mimesis hybrid in-situ visualization method;multi-sensory depth perception;medical augmented reality;medical diagnosis;invasive surgery;medical imaging;endoscopic instruments;3D medical imaging data;stereoscopic head- mounted-display;Visualization;Augmented reality;Biomedical imaging;Instruments;Medical diagnostic imaging;Humans;Anatomy;Medical diagnosis;Surgery;Surges;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems¿Artificial, augmented, and virtual realities;H.5.2 [Information Interfaces and Presentation]: User Interfaces¿Interaction styles;I.3.6 [Computer Graphics]: Methodology and Techniques¿Interaction techniques;J.3 [Life and Medical Sciences]},
doi={10.1109/ISMAR.2007.4538837},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538838,
author={G. {Takacs} and V. {Chandrasekhar} and B. {Girod} and R. {Grzeszczuk}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Feature Tracking for Mobile Augmented Reality Using Video Coder Motion Vectors},
year={2007},
volume={},
number={},
pages={141-144},
abstract={We propose a novel, low-complexity, tracking scheme that uses motion vectors directly from a video coder. We compare our tracking algorithm against ground truth data, and show that we can achieve a high level of accuracy, even though the motion vectors are rate-distortion optimized and do not represent true motion. We develop a framework for tracking in video sequences with various GOP structures. Such a scheme would find applications in the context of Mobile Augmented Reality. The proposed feature tracking algorithm can significantly reduce the required rate of feature extraction and matching.},
keywords={augmented reality;feature extraction;mobile handsets;feature tracking;video coder;motion vectors;mobile augmented reality;feature extraction;feature matching;Augmented reality;Feature extraction;Target tracking;Video sequences;Mobile handsets;Spatial databases;Personal digital assistants;Data mining;Displays;Image databases},
doi={10.1109/ISMAR.2007.4538838},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538839,
author={J. {Mooser} and S. {You} and U. {Neumann}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Real-Time Object Tracking for Augmented Reality Combining Graph Cuts and Optical Flow},
year={2007},
volume={},
number={},
pages={145-152},
abstract={We present an efficient and accurate object tracking algorithm based on the concept of graph cut segmentation. The ability to track visible objects in real-time provides an invaluable tool for the implementation of markerless Augmented Reality. Once an object has been detected, it's location in future frames can be used to position virtual content, and thus annotate the environment. Unlike many object tracking algorithms, our approach does not rely on a preexisting 3D model or any other information about the object or its environment. It takes, as input, a set of pixels representing an object in an initial frame and uses a combination of optical flow and graph cut segmentation to determine the corresponding pixels in each future frame. Experiments show that our algorithm robustly tracks objects of disparate shapes and sizes over hundreds of frames, and can even handle difficult cases where an object contains many of the same colors as its background. We further show how this technology can be applied to practical AR applications.},
keywords={augmented reality;graph theory;image segmentation;image sequences;real-time object tracking;augmented reality;optical flow;graph cut segmentation;Augmented reality;Image motion analysis;Pixel;Image segmentation;Image edge detection;Cameras;Navigation;Object detection;Robustness;Shape;I.4.8 [Image Processing and Computer Vision]: Scene Analysis¿Tracking;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems¿Augmented Reality},
doi={10.1109/ISMAR.2007.4538839},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538840,
author={D. {Chekhlov} and A. P. {Gee} and A. {Calway} and W. {Mayol-Cuevas}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Ninja on a Plane: Automatic Discovery of Physical Planes for Augmented Reality Using Visual SLAM},
year={2007},
volume={},
number={},
pages={153-156},
abstract={Most work in visual augmented reality (AR) employs predefined markers or models that simplify the algorithms needed for sensor positioning and augmentation but at the cost of imposing restrictions on the areas of operation and on interactivity. This paper presents a simple game in which an AR agent has to navigate using real planar surfaces on objects that are dynamically added to an unprepared environment. An extended Kalman filter (EKF) simultaneous localisation and mapping (SLAM) framework with automatic plane discovery is used to enable the player to interactively build a structured map of the game environment using a single, agile camera. By using SLAM, we are able to achieve real-time interactivity and maintain rigorous estimates of the system's uncertainty, which enables the effects of high quality estimates to be propagated to other features (points and planes) even if they are outside the camera's current field of view.},
keywords={augmented reality;Kalman filters;SLAM (robots);physical planes automatic discovery;augmented reality;visual SLAM;simultaneous localisation and mapping;sensor positioning;extended Kalman filter;Augmented reality;Simultaneous localization and mapping;Cameras;Sensor systems;Real time systems;Uncertainty;Layout;Surface fitting;Robustness;Interactive systems;H.5.1 [Information Systems]: Multimedia Information Systems¿Augmented Reality;I.4.8 [Image Processing and Computer Vision]: Scene Analysis¿Tracking, Surface Fitting},
doi={10.1109/ISMAR.2007.4538840},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538841,
author={R. M. {Freeman} and S. J. {Julier} and A. J. {Steed}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={A Method for Predicting Marker Tracking Error},
year={2007},
volume={},
number={},
pages={157-160},
abstract={Many augmented reality (AR) applications use marker-based vision tracking systems to recover camera pose by detecting one or more planar landmarks. However, most of these systems do not interactively quantify the accuracy of the pose they calculate. Instead, the accuracy of these systems is either ignored, assumed to be a fixed value, or determined using error tables (constructed in an off-line ground-truthed process) along with a run-time interpolation scheme. The validity of these approaches are questionable as errors are strongly dependent on the intrinsic and extrinsic camera parameters and scene geometry. In this paper we present an algorithm for predicting the statistics of marker tracker error in real-time. Based on the scaled spherical simplex unscented transform (SSSUT), the algorithm is applied to the augmented reality toolkit plus (ARToolKitPlus). The results are validated using precision off-line photogrammetric techniques.},
keywords={augmented reality;computer vision;marker tracking error prediction;vision tracking systems;run-time interpolation scheme;scaled spherical simplex unscented transform;augmented reality toolkit plus;Augmented reality;Cameras;Machine vision;Accuracy;Runtime;Interpolation;Layout;Geometry;Prediction algorithms;Error analysis;augmented reality;tracking;ARToolKit;unscented transforms},
doi={10.1109/ISMAR.2007.4538841},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538842,
author={G. {Reitmayr} and T. W. {Drummond}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Initialisation for Visual Tracking in Urban Environments},
year={2007},
volume={},
number={},
pages={161-172},
abstract={Outdoor augmented reality systems often rely on GPS to cover large environments. Visual tracking approaches can provide more accurate location estimates but typically require a manual initialisation procedure. This paper describes the combination of both techniques to create an accurate localisation system that does not require any additional input for (re-)initialisation. The 2D GPS position together with average user height is used as an initial estimate for the visual tracking. The large gap in available GPS accuracy versus required accuracy for initialisation is overcome through a search procedure that tries to minimise search time by improving the likelihood of finding the correct estimate early. Re-initialisation of the visual tracking system after catastrophic failures is further improved by modelling the GPS error with a Gaussian process to provide a better estimate of the current location, thereby decreasing search time.},
keywords={augmented reality;Global Positioning System;visual tracking initialisation;urban environments;augmented reality systems;GPS;Gaussian process;Global Positioning System;Cameras;Augmented reality;Robustness;Satellites;Sensor systems;Wearable sensors;Magnetic sensors;Position measurement;Error correction;H.5.1 [Information Systems]: Multimedia Information Systems¿Augmented Reality;I.4.8 [Image Processing and Computer Vision]: Scene Analysis¿Tracking, Sensor Fusion},
doi={10.1109/ISMAR.2007.4538842},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538843,
author={R. W. {Lindeman} and H. {Noma} and P. G. {de Barros}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Hear-Through and Mic-Through Augmented Reality: Using Bone Conduction to Display Spatialized Audio},
year={2007},
volume={},
number={},
pages={173-176},
abstract={We present a novel approach for mixing real and computer-generated audio for augmented reality (AR) applications. Analogous to optical-see-through and video-see-through techniques in the visual domain, we present Hear-Through and Mic-Through audio AR. Hear-Through AR uses a bone-conduction headset to deliver computer-generated audio, while leaving the ear canals free to receive audio from the surrounding environment. Mic-Through AR allows audio signals captured from ear-worn microphones to be mixed with computer-generated audio in the computer, and delivered to the user over headphones. We present preliminary results from an empirical user study conducted to compare a bone-conduction device, headphones, and a speaker array. The results show that subjects achieved the best accuracy using an array of speakers physically located around the listener when stationary sounds were played, but that there was no difference in accuracy between the speaker array and the bone-conduction device for sounds that were moving, and that both devices outperformed standard headphones for moving sounds.},
keywords={augmented reality;headphones;microphones;hear-through augmented reality;mic-through augmented reality;bone conduction headset;computer-generated audio;speaker array;Augmented reality;Bones;Auditory displays;Headphones;Loudspeakers;Computer displays;Application software;Optical mixing;Optical computing;Ear;Augmented reality;audio;bone conduction},
doi={10.1109/ISMAR.2007.4538843},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538844,
author={R. {Grasset} and A. {Dunser} and M. {Billinghurst}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Human-Centered Development of an AR Handheld Display},
year={2007},
volume={},
number={},
pages={177-180},
abstract={In this paper we present the process of designing and developing a fully functional and interactive AR handheld device. Based on a human-centered approach we describe the results of our exploration; from the contextual usage to the different rapid prototyped models and the evaluation activities. We also discuss some recommendations for how to use similar methods for developing future new AR physical interfaces.},
keywords={augmented reality;software prototyping;user interface management systems;human-centered development;AR handheld display;contextual usage;rapid prototyped models;Displays;Prototypes;Design engineering;Guidelines;Handheld computers;Usability;Design methodology;Augmented reality;Humans;Ergonomics;B.4.2 [Input/Output and data communications]: Input/Output Devices¿Image display;H.5.2 [Information interfaces and presentation]: User Interfaces¿User-centered design},
doi={10.1109/ISMAR.2007.4538844},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538845,
author={A. {Grundhofer} and M. {Seeger} and F. {Hantsch} and O. {Bimber}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Dynamic Adaptation of Projected Imperceptible Codes},
year={2007},
volume={},
number={},
pages={181-190},
abstract={In this paper we present an innovative adaptive imperceptible pattern projection technique that takes into account parameters of human visual perception. A coded image is temporally integrated into the projected image, which is invisible to the human eye but can be reconstructed by a synchronized camera. The embedded code is dynamically adjusted on the fly to guarantee its imperceptibility and to adapt it to the current camera pose. Linked with real-time flash keying, for instance, this enables in-shot optical tracking using a dynamic multi-resolution marker technique. A sample prototype has been realized that demonstrates the application of our method in the context of augmentations in television studios.},
keywords={image coding;image resolution;visual perception;projected imperceptible codes;dynamic adaptation;pattern projection technique;human visual perception;coded image;real-time flash keying;multi-resolution marker technique;Cameras;TV;Image coding;Humans;Prototypes;Multiresolution analysis;Visual perception;Image reconstruction;Image analysis;High speed optical techniques;H.5.1 [INFORMATION INTERFACES AND PRESENTATION]: Multimedia Information Systems¿Artificial, augmented, and virtual realities;I.4.8 [IMAGE PROCESSING AND COMPUTER VISION]: Scene Analysis¿Tracking},
doi={10.1109/ISMAR.2007.4538845},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538846,
author={D. {Kalkofen} and E. {Mendez} and D. {Schmalstieg}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Interactive Focus and Context Visualization for Augmented Reality},
year={2007},
volume={},
number={},
pages={191-201},
abstract={In this article we present interactive focus and context (F+C) visualizations for augmented reality (AR) applications. We demonstrate how F+C visualizations are used to affect the user's perception of hidden objects by presenting contextual information in the area of augmentation. We carefully overlay synthetic data on top of the real world imagery by taking into account the information that is about to be occluded. Furthermore, we present operations to control the amount of augmented information. Additionally, we developed an interaction tool, based on the magic lens technique, which allows for interactive separation of focus from context. We integrated our work into a rendering framework developed on top of the Studierstube augmented reality system. We finally show examples to demonstrate how our work benefits AR.},
keywords={augmented reality;computer vision;data visualisation;rendering (computer graphics);interactive focus;context visualization;augmented reality;contextual information;magic lens technique;rendering framework;augmented reality system;Augmented reality;Focusing;Rendering (computer graphics);Computer graphics;Layout;Data visualization;Computer displays;Data structures;X-ray imaging;Needles;Object overlay and spatial layout techniques;real-time rendering;interaction techniques for MR/AR;mediated and diminished reality},
doi={10.1109/ISMAR.2007.4538846},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538847,
author={K. {Higa} and T. {Nishiura} and A. {Kimura} and F. {Shibata} and H. {Tamura}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={A Two-by-Two Mixed Reality System That Merges Real and Virtual Worlds in Both Audio and Visual Senses},
year={2007},
volume={},
number={},
pages={203-206},
abstract={There have been many implementations of virtual reality, using audio and visual senses. However, implementations of mixed reality (MR) have thus far only dealt with the visual sense. We have developed an MR system that merges real and virtual worlds in both the audio and visual senses, wherein the geometric consistency of the audio sense was fully coordinated with the visual sense. We tried two approaches for merging real and virtual worlds in the audio sense, using open-air and closed-air headphones.},
keywords={virtual reality;two-by-two mixed reality system;virtual reality;geometric consistency;audio sense;closed-air headphones;Virtual reality;Headphones;Auditory displays;Real time systems;Optical sensors;Computer displays;Merging;Layout;Humans;Chromium;Mixed Reality;Audio and Visual Senses;Geometric Consistency;Open-Air Headphones;Closed-Air Headphones},
doi={10.1109/ISMAR.2007.4538847},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538848,
author={K. {Kiyokawa}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={A Wide Field-of-view Head Mounted Projective Display using Hyperbolic Half-silvered Mirrors},
year={2007},
volume={},
number={},
pages={207-210},
abstract={The development of a wide field-of-view (FOV) head mounted display (HMD) has been a technological challenge for decades. Previous HMDs tackled this problem using multiple display units (tiling) or multiple curved mirrors. The former approach tends to be expensive and heavy, whereas the latter approach tends to suffer from image distortion and a small exit pupil. In order to provide a wide FOV image with a large exit pupil, the present paper proposes a novel head mounted projective display (HMPD) using a hyperbolic half-silvered mirror, rather than a conventional planar mirror. The first bench-top prototype has successfully shown wide field-of-view projection capability.},
keywords={augmented reality;computer graphics;computer vision;helmet mounted displays;wide field-of-view head mounted projective display;hyperbolic half-silvered mirrors;multiple display units;tiling;multiple curved mirrors;Mirrors;Head;Three dimensional displays;Virtual reality;Eyes;Optical distortion;Optical refraction;Optical imaging;Optical design;Augmented reality;Virtual Reality;Augmented Reality;Head Mounted Projective Display;Hyperbolic Mirror;Wide Field-of-view},
doi={10.1109/ISMAR.2007.4538848},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538849,
author={M. {Huber} and D. {Pustka} and P. {Keitler} and F. {Echtler} and G. {Klinker}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={A System Architecture for Ubiquitous Tracking Environments},
year={2007},
volume={},
number={},
pages={211-214},
abstract={Ubiquitous tracking setups, covering large tracking areas with many heterogeneous sensors of varying accuracy, require dedicated middleware to facilitate development of stationary and mobile applications by providing a simple interface and encapsulating the details of sensing, calibration and sensor fusion. In this paper we present a centrally coordinated peer-to-peer architecture for ubiquitous tracking, where a server computes optimal data flow configurations for sensor and application clients, which are directly exchanging tracking data with low latency using a light-weight data flow framework. The server's decisions are inferred from an actively maintained central spatial relationship graph (SRG) using spatial relationship patterns. The system is compared to a previous Ubitrack implementation using the highly distributed DWARF middleware. It exhibits significantly better performance in a reference scenario.},
keywords={augmented reality;middleware;peer-to-peer computing;software architecture;ubiquitous computing;system architecture;ubiquitous tracking environments;heterogeneous sensors;coordinated peer-to-peer architecture;central spatial relationship graph;spatial relationship patterns;DWARF middleware;Middleware;Sensor fusion;Intelligent sensors;Calibration;Peer to peer computing;Pervasive computing;Virtual reality;Optical sensors;Runtime;Application software;I.4.8 [Image Processing and Computer Vision]: Scene Analysis¿Tracking;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems¿Artificial, augmented, and virtual realities},
doi={10.1109/ISMAR.2007.4538849},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538850,
author={T. {Sielhorst} and W. {Sa} and A. {Khamene} and F. {Sauer} and N. {Navab}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Measurement of absolute latency for video see through augmented reality},
year={2007},
volume={},
number={},
pages={215-220},
abstract={Latency is a key property of video see through AR systems since users' performance is strongly related to it. However, there is no standard way of latency measurement of an AR system in the literature. We have created a stable and comparable way of estimating the latency in a video see through AR system. The latency is estimated by encoding the time in the image and decoding the time after camera feedback. We have encoded the time as a translation of a circle in the image. The cross ratio has been used as an image feature that is preserved in a projective transformation. The encoding allows for a simple but accurate way of decoding. We show that this way of encoding has an adequate accuracy for latency measurements. As the method allows for a series of automatic measurements we propose to visualize the measurements in a histogram. This histogram reveals meaningful information about the system other than the mean value and standard deviation of the latency. The method has been tested on four different AR systems that use different camera technology, resolution and frame rates.},
keywords={augmented reality;video coding;video see through augmented reality;absolute latency;camera feedback;image coding;Delay;Augmented reality;Decoding;Cameras;Encoding;Histograms;Measurement standards;Image coding;Feedback;Visualization;H.5.2 [Information Interfaces And Presentation]: User Interfaces¿Standardization, Benchmarking},
doi={10.1109/ISMAR.2007.4538850},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538851,
author={B. {Okumura} and M. {Kanbara} and N. {Yokoya}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Precise Geometric Registration by Blur Estimation for Vision-based Augmented Reality},
year={2007},
volume={},
number={},
pages={221-224},
abstract={This paper proposes an accurate geometric registration method by estimating blur effects from a degraded image with image markers for augmented reality. A small and inexpensive camera used in augmented reality systems sometimes captures degraded images because its focus and/or iris are fixed. This degradation of a captured image affects the accuracy of the detected positions of feature points in the image. The proposed method improves the accuracy of the estimated camera position and posture by estimating blur effects from the captured image, and by correcting the detected positions of feature points through the results. The effectiveness of the method is confirmed through experiments of corner estimation from simulated images and extrinsic camera parameter estimation from real images.},
keywords={augmented reality;computer vision;image registration;image sensors;geometric registration;vision-based augmented reality;image markers;blur effects;Augmented reality;Cameras;Robustness;Degradation;Computer vision;Motion detection;Motion estimation;Focusing;Iris;Layout;augmented reality;geometric registration;blur estimation;point spread function},
doi={10.1109/ISMAR.2007.4538851},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538852,
author={G. {Klein} and D. {Murray}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Parallel Tracking and Mapping for Small AR Workspaces},
year={2007},
volume={},
number={},
pages={225-234},
abstract={This paper presents a method of estimating camera pose in an unknown scene. While this has previously been attempted by adapting SLAM algorithms developed for robotic exploration, we propose a system specifically designed to track a hand-held camera in a small AR workspace. We propose to split tracking and mapping into two separate tasks, processed in parallel threads on a dual-core computer: one thread deals with the task of robustly tracking erratic hand-held motion, while the other produces a 3D map of point features from previously observed video frames. This allows the use of computationally expensive batch optimisation techniques not usually associated with real-time operation: The result is a system that produces detailed maps with thousands of landmarks which can be tracked at frame-rate, with an accuracy and robustness rivalling that of state-of-the-art model-based systems.},
keywords={augmented reality;robot vision;SLAM (robots);parallel tracking;parallel mapping;SLAM algorithms;augmented reality;robotic exploration;hand-held camera;batch optimisation techniques;Robot vision systems;Cameras;Tracking;Yarn;Robustness;Layout;Simultaneous localization and mapping;Algorithm design and analysis;Concurrent computing;Handheld computers},
doi={10.1109/ISMAR.2007.4538852},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538853,
author={V. {Gay-Bellile} and A. {Bartoli} and P. {Sayd}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Deformable Surface Augmentation in Spite of Self-Occlusions},
year={2007},
volume={},
number={},
pages={235-238},
abstract={The augmentation problem for images of a deforming surface has been studied since recently. The surface is usually assumed not to be self-occluding. Two dimensional deformation estimation in the presence of self-occlusions is very challenging. This paper proposes a specific framework explicitly modeling self-occlusions for augmented reality applications. The basic idea is to detect self-occlusions as warp shrinkage areas. Deformations are initially estimated via direct non-rigid image registration. Temporal smoothness is then used to refine the warps and the image are augmented. Experimental results on several challenging datasets show that our approach convincingly augments self-occluded surfaces. Associated videos are available on the first author's Web homepage.},
keywords={augmented reality;deformation;hidden feature removal;image registration;deformable surface augmentation;2D self-occlusion modeling;image augmentation;augmented reality application;warp shrinkage area;direct nonrigid image registration;Image registration;Cameras;Deformable models;Augmented reality;Erbium;Video sequences},
doi={10.1109/ISMAR.2007.4538853},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538854,
author={D. {Kotake} and K. {Satoh} and S. {Uchiyama} and H. {Yamamoto}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={A Fast Initialization Method for Edge-based Registration Using an Inclination Constraint},
year={2007},
volume={},
number={},
pages={239-248},
abstract={We propose a hybrid camera pose estimation method using an inclination sensor value and correspondence-free line segments. In this method, possible azimuths of the camera pose are hypothesized by a voting method under an inclination constraint. Then some camera positions for each possible azimuth are calculated based on the detected line segments that affirmatively voted for the azimuth. Finally, the most consistent one is selected as the camera pose out of the multiple sets of the camera positions and azimuths. Unlike many other tracking methods, our method does not use past information but rather estimates the camera pose using only present information. This feature is useful for an initialization measure of registration in augmented reality (AR) systems. This paper describes the details of the method and shows its effectiveness with experiments in which the method is actually used in an AR application.},
keywords={augmented reality;edge detection;image registration;image sensors;pose estimation;fast initialization method;edge-based registration;inclination constraint;hybrid camera pose estimation method;inclination sensor value;augmented reality;Cameras;Azimuth;Layout;Image edge detection;Humans;Laboratories;Image segmentation;Space technology;Robustness;Voting;inclination constraint;edge-based registration;initialization;line segments;pose estimation;correspondence-free},
doi={10.1109/ISMAR.2007.4538854},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538855,
author={J. {Pilet} and V. {Lepetit} and P. {Fua}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Retexturing in the Presence of Complex Illumination and Occlusions},
year={2007},
volume={},
number={},
pages={249-258},
abstract={We present a nonrigid registration technique that achieves spatial, photometric, and visibility accuracy. It lets us photo-realistically augment 3D deformable surfaces under complex illumination conditions and in spite of severe occlusions. There are many approaches that address some of these issues but very few that simultaneously handle all of them as we do. We use triangulated meshes to model the geometry and introduce explicit visibility maps as well as separate illumination parameters for each mesh vertex. We cast our registration problem in an expectation maximization framework that allows robust and fully automated operation. It provides explicit illumination and occlusion models that can be used for rendering purposes.},
keywords={augmented reality;computer graphics;computer vision;expectation-maximisation algorithm;lighting;retexturing;complex illumination;occlusions;nonrigid registration technique;mesh vertex;expectation maximization framework;rendering;Lighting;Robustness;Augmented reality;Shape;Fingers;Deformable models;Image databases;Principal component analysis;Solid modeling;Surface texture;Occlusion;Augmented Reality;Shadows},
doi={10.1109/ISMAR.2007.4538855},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538856,
author={T. {Lee} and T. {Hollerer}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Initializing Markerless Tracking Using a Simple Hand Gesture},
year={2007},
volume={},
number={},
pages={259-260},
abstract={We introduce a technique to establish a coordinate system for augmented reality (AR) on tabletop environments. A user's hand is tracked and the fingertips on the outstretched hand are detected, providing a camera pose estimation relative to the hand. As a user places the hand on the surface of a tabletop environment, the hand's coordinate system is propagated to the environment, detecting distinctive image features in the scene. The features are tracked fast and robustly using optical flow. In this way, a new tabletop AR environment is set up without having to carry a marker or a sophisticated tracking system to the environment itself. We also demonstrate a proof-of-concept application for establishing a tabletop AR environment and recognizing a scene when detecting its features.},
keywords={augmented reality;feature extraction;gesture recognition;pose estimation;markerless tracking;hand gesture;augmented reality;camera pose estimation;distinctive image features detection;Cameras;Computer vision;Layout;Optical propagation;Augmented reality;Image motion analysis;Computer graphics;Personal digital assistants;Wearable computers;Eyes;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism¿Virtual Reality;I.4.8 [Image Processing and Computer Vision]: Scene Analysis},
doi={10.1109/ISMAR.2007.4538856},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538857,
author={R. {Tenmoku} and Y. {Yoshida} and F. {Shibata} and A. {Kimura} and H. {Tamura}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Visually Elegant and Robust Semi-Fiducials for Geometric Registration in Mixed Reality},
year={2007},
volume={},
number={},
pages={261-262},
abstract={This paper describes a novel image-based geometric registration method using visually elegant and robust semi-fiducial markers in mixed reality (MR). Most traditional visual markers stand out against the background, i.e., they can be recognized and identified easily. Here, we try to construct new visual markers for MR registration, which achieve a good balance between visual elegance and robustness. As the first step, we propose "two-tone colored markers". These markers have the following characteristics: 1) color similar to that of the background object, 2) placed at the corners of real objects in an inconspicuous manner. This paper describes the registration method using two- tone colored markers and a few experiments.},
keywords={computer graphics;image colour analysis;image registration;virtual reality;geometric registration;mixed reality;two-tone colored markers;Robustness;Virtual reality;Cameras;Brightness;Chromium;Multimedia systems;Information systems;Layout;Image sensors;Stability;Mixed reality;Visual marker;Geometric registration;Semi-fiducials},
doi={10.1109/ISMAR.2007.4538857},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538858,
author={M. {Fiala}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Webtag: A World Wide Internet Based AR System},
year={2007},
volume={},
number={},
pages={263-264},
abstract={Webtag is a marker based system where every marker contains an internationally unique identifier which links to 2D or 3D content from a user's Website. A two stage design combines the basic 10-bit ID (tier-1 ID) of the current ARTag system which is used for pose tracking when the marker is far away, and a smaller and dense array of extra bits allow an additional 32 bits (tier-2 ID) to be recognized when the camera is close. A user brings his/her AR device close to the marker such that the tier-2 ID is read, and then the IDs are mapped to a web address from which the augmenting model, image, or animation is loaded. Thereafter the content is rendered relative to the tier-1 ID which can be seen at greater distances. This allows a user anywhere on internet to access augmented content printed in magazines, seen on posters, etc. Webtag is a prototype system that may allow large scale acceptance of AR by the public.},
keywords={augmented reality;computer graphics;computer vision;Internet;Webtag;Internet;pose tracking;augmented reality;Internet;Cameras;Intrusion detection;Animation;Robustness;Computer vision;Cellular phones;Web sites;Uniform resource locators;Councils;fiducial markers;marker detection;self-identifying patterns},
doi={10.1109/ISMAR.2007.4538858},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538859,
author={S. {Jeon} and G. J. {Kim}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Mosaicing a Wide Geometric Field of View for Effective Interaction in Augmented Reality},
year={2007},
volume={},
number={},
pages={265-266},
abstract={In this paper, we have addressed the usability issue of augmented reality systems. In order to overcome the lack of view into the interaction space in nominal AR systems, we proposed to create and use a mosaiced image as a cost effective and practical solution. One of the obvious problem is that even if the view span is increased by the mosaicing, because of the fixed and limited resolution/size of the display(either HMD or upright display), the content must be viewed in smaller scale. Yang et al. has found out that increasing the geometric field of view can be tolerated to some degree without much degradation in task performance [3]. In our pilot experiment, a comparison of our mosaiced AR display to the nominal set up improved task performance when the task involved manipulation of many tangible props in a relatively wide area. Our future work is to conduct a more formal usability study and address other AR usability issues.},
keywords={augmented reality;helmet mounted displays;human computer interaction;image segmentation;video cameras;augmented reality system;usability issue;image mosaicing;augmented reality display resolution;head mounted display;Augmented reality;Cameras;Head;Displays;Transmission line matrix methods;Virtual reality;Computer science;Tracking;Equations;Haptic interfaces;Mosaicing;Augmented Reality;Spatial Awareness},
doi={10.1109/ISMAR.2007.4538859},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538860,
author={E. {Mendez} and D. {Schmalstieg}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Adaptive Augmented Reality Using Context Markup and Style Maps},
year={2007},
volume={},
number={},
pages={267-268},
abstract={Augmented reality (AR) enables users to visualize synthetic information overlaid on top of real imagery. Such visualization may be achieved by tools that distort, filter or enhance the explored information. However, little to no work has focused on the separation of style definitions and their mapping to scene objects. We target this separation based on a context rich scenegraph. Our research allows the definition of visualization styles independent on the data to be visualized.},
keywords={augmented reality;data visualisation;realistic images;adaptive augmented reality;context markup;style maps;visualize synthetic information;scenegraph;Augmented reality;Layout;Information filtering;Data visualization;Lenses;Computer graphics;Rendering (computer graphics);Data structures;Displays;Information filters;Object overlay and spatial layout techniques;real-time rendering;interaction techniques for MR/AR},
doi={10.1109/ISMAR.2007.4538860},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538861,
author={J. {Lugrin} and R. {Chaignon} and M. {Cavazza}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={A High-level Event System for Augmented Reality},
year={2007},
volume={},
number={},
pages={269-270},
abstract={3D graphics systems increasingly rely on sophisticated event systems derived from collision detection mechanisms, which support the discretisation of Physics as well as high-level programming and scripting. By contrast, augmented reality systems have not yet adopted this approach. We describe the development of a high-level event system on top of the ARToolkit environment incorporating the ODE Physics engine. We first define a typology of events encompassing interactions between virtual objects as well as interactions involving markers. We then describe how these events can be recognised in real-time from elementary collisions detected by the ODE Physics engine. We conclude by discussing examples of high-level event recognitions and how they can support the development of applications.},
keywords={augmented reality;computer graphics;computer vision;high-level event system;augmented reality;3D graphics systems;high-level programming;scripting;augmented reality systems;Augmented reality;Engines;Virtual reality;Application software;Physics computing;Event detection;Sampling methods;Graphics;Chromium;Multimedia systems;Augmented Reality;Event Systems;ARToolKit},
doi={10.1109/ISMAR.2007.4538861},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538862,
author={Y. {Allusse} and R. {Grasset} and M. {Billinghurst}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Accelerating Template-Based Matching on the GPU for AR Applications},
year={2007},
volume={},
number={},
pages={271-272},
abstract={Recently researchers have shown that it is possible to use GPU hardware for image processing and computer vision algorithms. We have been exploring how to use GPU hardware to improve marker- based tracking for AR Applications. In this paper we describe our findings and explored issues in the context of a standard fiducial tracking pipeline. We demonstrate the implementation of a template matching process on the GPU and the performance improvement gained in comparison to a traditional CPU implementation.},
keywords={augmented reality;computer graphics;computer vision;image matching;template-based matching;AR applications;GPU hardware;marker-based tracking;fiducial tracking pipeline;augmented reality;image processing;computer vision;Acceleration;Pipelines;Pattern matching;Computer vision;Hardware;Libraries;Application software;Image processing;Robustness;Costs;I.4.8 [Computing Methodologies]: Image Processing and Computer Vision¿Scene Analysis},
doi={10.1109/ISMAR.2007.4538862},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538863,
author={Y. {Jin} and Y. {Kim} and J. {Park}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={ARMO: Augmented Reality based Reconfigurable MOck-up},
year={2007},
volume={},
number={},
pages={273-274},
abstract={Rapid prototyping allows for evaluation of design product in a short period of time. Designers or CEOs may perceive the size and appearance of the product by touching the automatically constructed physical objects produced through rapid prototyping. In this paper, we introduce 'augmented reality based reconfigurable mock-up', which enables interactive changes of shapes of products as well as colors, textures, and user interfaces. The shapes of the physical objects are reconfigurable by assembling different parts to the main body of the mock-up. Augmented reality technologies were used to alter the appearance of the mock-up by rendering the 3D models, colors, and interfaces accordingly. Developed AR-based reconfigurable mock-up is expected to be used for realistic design evaluation and CEO presentations.},
keywords={augmented reality;computer graphics;computer vision;software prototyping;augmented reality;rapid prototyping;reconfigurable mock-up;interactive changes;Augmented reality;Shape;User interfaces;Product design;Rendering (computer graphics);Prototypes;Assembly;Computer graphics;Design automation;Process design;Augmented Reality;mock-up;design evaluation},
doi={10.1109/ISMAR.2007.4538863},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538864,
author={K. {Kobayashi} and K. {Nishiwaki} and S. {Uchiyama} and H. {Yamamoto} and S. {Kagami} and T. {Kanade}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Overlay what Humanoid Robot Perceives and Thinks to the Real-world by Mixed Reality System},
year={2007},
volume={},
number={},
pages={275-276},
abstract={One of the problems in developing a humanoid robot is caused by the fact that intermediate results, such as what the robot perceives the environment, and how it plans its moving path are hard to be observed online in the physical environment. What developers can see is only the behavior. Therefore, they usually investigate logged data afterwards, to analyze how well each component worked, or which component was wrong in the total system. In this paper, we present a novel environment for robot development, in which intermediate results of the system are overlaid on physical space using mixed reality technology. Real-time observation enables the developers to see intuitively, in what situation the specific intermediate results are generated, and to understand how results of a component affected the total system. This feature makes the development efficient and precise. This environment also gives a human-robot interface that shows the robot internal state intuitively, not only in development, but also in operation.},
keywords={control engineering computing;humanoid robots;man-machine systems;virtual reality;humanoid robot;mixed reality system;robot development;human-robot interface;Humanoid robots;Virtual reality;Space technology;Humans;Orbital robotics;Robot sensing systems;Intelligent robots;RNA;Independent component analysis;Tracking;Humanoid robot},
doi={10.1109/ISMAR.2007.4538864},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538865,
author={E. {Kruijff} and E. {Veas}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Vesp'R - Transforming Handheld Augmented Reality},
year={2007},
volume={},
number={},
pages={277-278},
abstract={This paper presents first results of an interaction design study performed for a novel handheld interaction system. Human factors of mid-size, self-containing wearable computer systems are explored.},
keywords={augmented reality;wearable computers;augmented reality;handheld interaction system;human factors;wearable computer systems;Augmented reality;Ergonomics;Fatigue;Computer graphics;Human factors;Personal digital assistants;Cameras;Displays;Wearable computers;Cellular phones;Handheld AR;spatial user interface},
doi={10.1109/ISMAR.2007.4538865},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538866,
author={T. {Okuma} and M. {Kourogi} and N. {Sakata} and T. {Kurata}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Reliving Museum Visiting Experiences on-and-off the Spot},
year={2007},
volume={},
number={},
pages={279-280},
abstract={We have developed a GUI tool and a mobile MR system for reliving experiences at museums. The GUI tool was developed to provide services based on content made from activity logs and to enable effective analysis of visitors' behaviors using the logs. In addition, by running the GUI tool on a mobile MR system, users can browse visitors' activities with a sensation of realism on the spot. This paper describes the GUI tool and a pilot user study that was conducted to evaluate it.},
keywords={computer graphics;humanities;mobile computing;museum visiting experiences;on-and-off the spot;GUI tool;mobile MR system;mixed reality system;Graphical user interfaces;Layout;Data visualization;Virtual reality;Mice;Control systems;Multimedia systems;Bicycles;Information technology;Chromium;wearable mixed reality system;reliving experience;museum guide;GIS},
doi={10.1109/ISMAR.2007.4538866},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538867,
author={S. {Webel} and M. {Becker} and D. {Stricker} and H. {Wuest}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Identifying differences between CAD and physical mock-ups using AR},
year={2007},
volume={},
number={},
pages={281-282},
abstract={Since the last ten years product development in automotive industry is changing radically. Most physical mock-ups have vanished and are now replaced by digital ones. But they are still needed for final evaluations or issues, which cannot be adequately simulated. During their production, deviations from the CAD model may be made. Since digital and real mock-up must match for the further product development, the transfer of differences between physical and digital mock-up to the CAD format is a crucial issue. In this paper an augmented reality (AR) based tool-chain is presented, which allows matching the CAD data with real mock-ups and documents the differences between them. Essential functions like measurement and online construction are provided, allowing the end-users to create information in AR space and feeding them back into the CAD model.},
keywords={augmented reality;automobile industry;CAD;data visualisation;machine tools;product design;product development;CAD model;physical mock-ups;automotive industry product development;augmented reality;AR based tool-chain;visualization system;Cameras;Solid modeling;Light emitting diodes;Design automation;Laser modes;Surface emitting lasers;Switches;Product development;Augmented reality;Extraterrestrial measurements;mixed reality application;tracking;rendering},
doi={10.1109/ISMAR.2007.4538867},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538868,
author={Y. {Yuan} and X. {Yang} and S. {Xiao}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={A Framework for Tangible User Interfaces within Projector-based Mixed Reality},
year={2007},
volume={},
number={},
pages={283-284},
abstract={This paper proposes a framework named TIPMR, for designing tangible user interfaces (TUIs) within projector-based mixed reality applications. The framework divides the target application into three parts: GUI-based application, TUI and an assistant. The assistant is employed as an adapter to translate between TUI operations and general GUI commands like mouse or keyboard events. This architecture makes it easier to focus on designing GUI-based applications and TUI separately. We built a tourist guidance system with two different tangible interaction modes based on TIPMR to demonstrate its usefulness and efficiency.},
keywords={graphical user interfaces;user interfaces;virtual reality;tangible user interfaces;projector-based mixed reality;GUI commands;graphical user interfaces;User interfaces;Virtual reality;Graphical user interfaces;Application software;Mice;Electronic mail;Keyboards;Computer architecture;Computer interfaces;Joining processes;tangible user interface;graphical user interface;projector-based mixed reality;extended MCRpd model},
doi={10.1109/ISMAR.2007.4538868},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538869,
author={B. {Avery} and W. {Piekarski} and B. H. {Thomas}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Visualizing Occluded Physical Objects in Unfamiliar Outdoor Augmented Reality Environments},
year={2007},
volume={},
number={},
pages={285-286},
abstract={This paper describes techniques to allow both the visualization of hidden objects, and removal of real objects, for a mobile augmented reality user. A gesture based technique is also described which allows the user to select when to view occluded objects. By using the real-time modeling techniques of the Tinmith system, the user is able to create the required geometry to allow image based rendering techniques to be used to render corrected images on the user's display. The images of the hidden objects are captured by a mobile robot platform that is controlled by the mobile user.},
keywords={augmented reality;computer vision;data visualisation;rendering (computer graphics);occluded physical objects;unfamiliar outdoor augmented reality environments;hidden objects visualization;gesture based technique;Tinmith system;real-time modeling techniques;mobile robot platform;Visualization;Augmented reality;Rendering (computer graphics);Wearable computers;Solid modeling;Computer displays;Computer graphics;Mobile computing;Mobile robots;Navigation;Outdoor Augmented Reality;Wearable Computers;Telepresence;Image-based Rendering;3D Modeling},
doi={10.1109/ISMAR.2007.4538869},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4538870,
author={W. {Chen} and Y. {Xiong} and J. {Gao} and N. {Gelfand} and R. {Grzeszczuk}},
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, title={Efficient Extraction of Robust Image Features on Mobile Devices},
year={2007},
volume={},
number={},
pages={287-288},
abstract={Recent convergence of imaging sensors and general purpose processors on mobile phones creates an opportunity for a new class of augmented reality applications. Robust image feature extraction is a crucial enabler of this type of systems. In this article, we discuss an efficient mobile phone implementation of a state-of-the-art algorithm for computing robust image features called SURF. We implement several improvements to the basic algorithm that significantly improve its performance and reduce its memory footprint making the use of this algorithm on the mobile phone practical. Our prototype implementation has been applied to several practical applications such as image search, object recognition and augmented reality applications.},
keywords={augmented reality;computer graphics;computer vision;feature extraction;mobile handsets;robust image feature extraction;mobile phones;augmented reality;memory footprint;object recognition;image search;Robustness;Mobile handsets;Augmented reality;Application software;Cameras;Image matching;Convergence;Image sensors;Sensor phenomena and characterization;Feature extraction},
doi={10.1109/ISMAR.2007.4538870},
ISSN={},
month={Nov},}