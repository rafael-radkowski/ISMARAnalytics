@INPROCEEDINGS{1115054,
author={},
booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, title={Proceedings International Symposium On Mixed And Augmented Reality [front matter]},
year={2002},
volume={},
number={},
pages={i-ix},
abstract={Conference proceedings front matter may contain various advertisements, welcome messages, committee or program information, and other miscellaneous conference information. This may in some cases also include the cover art, table of contents, copyright statements, title-page or half title-pages, blank pages, venue maps or other general information relating to the conference that was part of the original conference proceedings.},
keywords={},
doi={10.1109/ISMAR.2002.1115054},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1115059,
author={W. {Friedrich}},
booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, title={ARVIKA-augmented reality for development, production and service},
year={2002},
volume={},
number={},
pages={3-4},
abstract={Augmented reality (AR) is a form of human-machine interaction where information is presented in the field of view of an individual. ARVIKA, funded by the German Ministry of Education and Research, develops this technology and applications in the fields of development, production, and service in the automotive and aerospace industries, for power and processing plants and for machine tools and production machinery. Up to now, AR has only been a subject of individual research projects and a small number of application-specific industrial projects on a global scale. The current state of the art and the available appliances do not yet permit a product-oriented application of the technology. However, AR enables a new, innovative form of human-machine interaction that not only places the individual in the center of the industrial workflow, but also offers a high potential for process and quality improvements in production and process workflows. ARVIKA is primarily designed to implement an augmented reality system for mobile use in industrial applications. The report presents the milestones that have been achieved after a project duration of a full three years.},
keywords={augmented reality;engineering graphics;user interfaces;mobile computing;ARVIKA;augmented reality;human-machine interaction;development;production;service;aerospace industries;automotive industries;power plants;processing plants;machine tools;production machinery;Augmented reality;Batch production systems;Man machine systems;Home appliances;Ergonomics;Vehicle crash testing;Assembly systems;Automation;Educational products;Educational technology},
doi={10.1109/ISMAR.2002.1115059},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1115062,
author={S. {Prince} and A. D. {Cheok} and F. {Farbiz} and T. {Williamson} and N. {Johnson} and M. {Billinghurst} and H. {Kato}},
booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, title={3D live: real time captured content for mixed reality},
year={2002},
volume={},
number={},
pages={7-317},
abstract={We present a complete system for live capture of 3D content and simultaneous presentation in augmented reality. The user sees the real world from his viewpoint, but modified so that the image of a remote collaborator is rendered into the scene. Fifteen cameras surround the collaborator, and the resulting video streams are used to construct a three-dimensional model of the subject using a shape-from-silhouette algorithm. Users view a two-dimensional fiducial marker using a video-see-through augmented reality interface. The geometric relationship between the marker and head-mounted camera is calculated, and the equivalent view of the subject is computed and drawn into the scene. Our system can generate 384 /spl times/ 288 pixel images of the models at 25 fps, with a latency of < 100 ms. The result gives the strong impression that the subject is a real part of the 3D scene. We demonstrate applications of this system in 3D videoconferencing and entertainment.},
keywords={augmented reality;teleconferencing;rendering (computer graphics);video signal processing;helmet mounted displays;live 3D content capture;simultaneous presentation;augmented reality;mixed reality;real time captured content;remote collaborator image rendering;cameras;video streams;3D model;shape-from-silhouette algorithm;2D fiducial marker;video-see-through augmented reality interface;geometric relationship;head-mounted camera;3D videoconferencing;3D entertainment;Virtual reality;Layout;Augmented reality;Collaboration;Cameras;Rendering (computer graphics);Streaming media;Pixel;Delay;Teleconferencing},
doi={10.1109/ISMAR.2002.1115062},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1115063,
author={G. {Gordon} and M. {Billinghurst} and M. {Bell} and J. {Woodfill} and B. {Kowalik} and A. {Erendi} and J. {Tilander}},
booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, title={The use of dense stereo range data in augmented reality},
year={2002},
volume={},
number={},
pages={14-23},
abstract={This paper describes an augmented reality system that incorporates a real-time dense stereo vision system. Analysis of range and intensity data is used to perform two functions: 1) 3D detection and tracking of the user's fingertip or a pen to provide natural 3D pointing gestures, and 2) computation of the 3D position and orientation of the user's viewpoint without the need for fiducial mark calibration procedures, or manual initialization. The paper describes the stereo depth camera, the algorithms developed for pointer tracking and camera pose tracking, and demonstrates their use within an application in the field of oil and gas exploration.},
keywords={optical tracking;stereo image processing;augmented reality;gesture recognition;distance measurement;real-time systems;pointing systems;oil technology;natural gas technology;real-time dense stereo vision system;dense stereo range data;augmented reality system;3D detection;3D tracking;user fingertip;pen;natural 3D pointing gestures;3D position;3D orientation;user viewpoint;stereo depth camera;algorithms;pointer tracking;camera pose tracking;gas exploration;oil exploration;Augmented reality;Cameras;Real time systems;Petroleum;Stereo vision;Calibration;Hardware;Target tracking;Humans;Laboratories},
doi={10.1109/ISMAR.2002.1115063},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1115065,
author={L. {Naimark} and E. {Foxlin}},
booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, title={Circular data matrix fiducial system and robust image processing for a wearable vision-inertial self-tracker},
year={2002},
volume={},
number={},
pages={27-36},
abstract={A wearable low-power hybrid vision-inertial tracker has been demonstrated based on a flexible sensor fusion core architecture, which allows easy reconfiguration by plugging-in different kinds of sensors. A particular prototype implementation consists of one inertial measurement unit and one out-ward-looking wide-angle smart camera, with a built-in DSP to run all required image-processing tasks. The smart camera operates on newly designed 2D bar-coded fiducials printed on a standard black-and-white printer. The fiducial design allows having thousands of different codes, thus enabling uninterrupted tracking throughout a large building or even a campus at very reasonable cost. The system operates in various real-world lighting conditions without user intervention due to homomorphic image processing algorithms for extracting fiducials in the presence of very non-uniform lighting.},
keywords={computer vision;optical tracking;augmented reality;sensor fusion;circular data matrix fiducial system;robust image processing;wearable low-power hybrid vision-inertial self-tracker;flexible sensor fusion core architecture;reconfiguration;inertial measurement unit;outward-looking wide-angle smart camera;built-in DSP;2D bar-coded fiducials;black-and-white printer;uninterrupted tracking;real-world lighting conditions;large building;campus;homomorphic image processing algorithms;nonuniform lighting;Robustness;Image processing;Sensor fusion;Smart cameras;Intelligent sensors;Wearable sensors;Prototypes;Measurement units;Digital signal processing;Printers},
doi={10.1109/ISMAR.2002.1115065},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1115068,
author={S. {Gibson} and J. {Cook} and T. {Howard} and R. {Hubbold} and D. {Oram}},
booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, title={Accurate camera calibration for off-line, video-based augmented reality},
year={2002},
volume={},
number={},
pages={37-46},
abstract={Camera tracking is a fundamental requirement for video-based augmented reality applications. The ability to accurately calculate the intrinsic and extrinsic camera parameters for each frame of a video sequence is essential if synthetic objects are to be integrated into the image data in a believable way. In this paper, we present an accurate and reliable approach to camera calibration for off-line video-based augmented reality applications. We first describe an improved feature tracking algorithm, based on the widely used Kanade-Lucas-Tomasi tracker. Estimates of inter-frame camera motion are used to guide tracking, greatly reducing the number of incorrectly tracked features. We then present a robust hierarchical scheme that merges sub-sequences together to form a complete projective reconstruction. Finally, we describe how RANSAC-based random sampling can be applied to the problem of self-calibration, allowing for more reliable upgrades to metric geometry. Results of applying our calibration algorithms are given for both synthetic and real data.},
keywords={optical tracking;augmented reality;calibration;image sequences;video signal processing;motion estimation;off-line video-based augmented reality;accurate camera calibration;camera tracking;extrinsic camera parameters;intrinsic camera parameters;video sequence;synthetic objects;feature tracking algorithm;Kanade-Lucas-Tomasi tracker;inter-frame camera motion estimation;robust hierarchical scheme;sub-sequences;projective reconstruction;RANSAC-based random sampling;self-calibration;metric geometry;Cameras;Calibration;Augmented reality;Tracking;Video sequences;Motion estimation;Robustness;Image reconstruction;Sampling methods;Geometry},
doi={10.1109/ISMAR.2002.1115068},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1115069,
author={ {Jong Weon Lee} and {Suya You} and U. {Neumann}},
booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, title={Tracking with omni-directional vision for outdoor AR systems},
year={2002},
volume={},
number={},
pages={47-56},
abstract={Most pose (3D position and 3D orientation) tracking methods using vision require a priori knowledge about the environment and correspondences between 3D environment features and 2D images. This environmental information is difficult to acquire accurately for large working volumes or may not be available at all, especially for outdoor environments. As a result, most pose tracking methods using vision are designed for small indoor working spaces. We track the pose of a moving camera from 2D images of the world. The pose of a camera is tracked through two 5 degree-of-freedom (DOF) motion estimations, which requires only 2D-to-2D correspondences. Therefore, the presented method can be applied to varied working space sizes including outdoor environments.},
keywords={augmented reality;computer vision;optical tracking;motion estimation;pose tracking;3D orientation;3D position;omni-directional vision;outdoor augmented reality systems;moving camera;2D images;5 degree-of-freedom motion estimation;2D-to-2D correspondences;Cameras;Motion estimation;Tracking;Augmented reality;Space technology;Graphics;Layout;Calibration;Design methodology;Image databases},
doi={10.1109/ISMAR.2002.1115069},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1115073,
author={A. D. {Cheok} and {Wang Weihua} and {Xubo Yang} and S. {Prince} and {Fong Siew Wan} and M. {Billinghurst} and H. {Kato}},
booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, title={Interactive theatre experience in embodied + wearable mixed reality space},
year={2002},
volume={},
number={},
pages={59-317},
abstract={This paper presents an interactive theatre based on an embodied mixed reality space and wearable computers. Embodied computing mixed reality spaces integrate ubiquitous computing, tangible interaction and social computing within a mixed reality space, which enables intuitive interaction with physical world and virtual world. We believe it has potential advantages to support novel interactive theatre experiences. Therefore, we explored the novel interactive theatre experience supported in the embodied mixed reality space, and implemented live 3D characters to interact with user in such a system.},
keywords={augmented reality;ubiquitous computing;wearable computers;humanities;interactive theatre experience;embodied mixed reality space;wearable computers;ubiquitous computing;tangible interaction;social computing;intuitive interaction;physical world;virtual world;live 3D characters;Virtual reality;Pervasive computing;Space technology;Augmented reality;Wearable computers;Physics computing;Ubiquitous computing;Social network services;Humans;Layout},
doi={10.1109/ISMAR.2002.1115073},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1115075,
author={M. {Figl} and W. {Birkfellner} and C. {Ede} and J. {Hummel} and R. {Hanel} and F. {Watzinger} and F. {Wanschitz} and R. {Ewers} and H. {Bergmann}},
booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, title={The control unit for a head mounted operating microscope used for augmented reality visualization in computer aided surgery},
year={2002},
volume={},
number={},
pages={69-75},
abstract={Two main concepts of head mounted displays (HMD) for augmented reality (AR) visualization exist, the optical and video-see through type. Several research groups have pursued both approaches for utilizing HMDs for computer aided surgery. While the hardware requirements for a video see through HMD to achieve acceptable time delay and frame rate seem to be enormous the clinical acceptance of such a device is doubtful from a practical point of view. Starting from previous work in displaying additional computer-generated graphics in operating microscopes, we have adapted a miniature head mounted operating microscope for AR by integrating two very small computer displays. To calibrate the projection parameters of this so called varioscope AR we have used Tsai's (1987) algorithm for camera calibration. Connection to a surgical navigation system was performed by defining an open interface to the control unit of the varioscope AR. The control unit consists of a standard PC with an dual head graphics adapter to render and display the desired augmentation of the scene. We connected this control unit to an computer aided surgery (CAS) system by the TCP/IP interface. In this paper we present the control unit for the HMD and its software design. We tested two different optical tracking systems, the Flash-point (Image Guided Technologies, Boulder, CO), which provided about 10 frames per second, and the Polaris (Northern Digital, Ontario, Can) which provided at least 30 frames per second, both with a time delay of one frame.},
keywords={surgery;microcomputer applications;calibration;helmet mounted displays;augmented reality;data visualisation;optical tracking;medical image processing;biomedical optical imaging;optical microscopes;biomedical equipment;computer displays;head mounted displays;augmented reality visualization;computer aided surgery;control unit;head mounted operating microscope;computer-generated graphics;computer displays;projection parameter calibration;varioscope AR;surgical navigation system;open interface;PC;dual head graphics adapter;rendering;TCP/IP interface;software design;optical tracking systems;Flashpoint;Polaris;time delay;Microscopy;Augmented reality;Visualization;Surgery;Magnetic heads;Computer displays;Control systems;Biomedical optical imaging;Delay effects;Computer graphics},
doi={10.1109/ISMAR.2002.1115075},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1115076,
author={G. {Klinker} and A. H. {Dutoit} and M. {Bauer} and J. {Bayer} and V. {Novak} and D. {Matzke}},
booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, title={Fata Morgana - a presentation system for product design},
year={2002},
volume={},
number={},
pages={76-85},
abstract={Mobile augmented reality applications promise substantial savings in time and costs for product designers, in particular, for large products requiring scale models and expensive clay mockups (e.g., cars). Such applications are novel and introduce interesting challenges when attempting to describe them to potential users and stakeholders. For example, it is difficult, a priori, to assess the nonfunctional requirements of such applications and anticipate the usability issues that the product designers are likely to raise. In this paper, we describe our efforts to develop a proof-of-concept AR system for car designers. Over the course of a year, we developed two prototypes, one within a university context, the other at a car manufacturer. The lessons learned from both efforts illustrate the technical and human challenges encountered when closely collaborating with the end user in the design of a novel application.},
keywords={augmented reality;engineering graphics;product development;helmet mounted displays;computer displays;automobiles;CAD;technical presentation;Fata Morgana project;presentation system;product design;mobile augmented reality applications;large products;car design;end user collaboration;Product design;Head;Augmented reality;Usability;Prototypes;Virtual reality;Online Communities/Technical Collaboration;Application software;Cameras;Rendering (computer graphics)},
doi={10.1109/ISMAR.2002.1115076},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1115077,
author={M. {Fiorentino} and R. {de Amicis} and G. {Monno} and A. {Stork}},
booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, title={Spacedesign: a mixed reality workspace for aesthetic industrial design},
year={2002},
volume={},
number={},
pages={86-318},
abstract={Spacedesign is an innovative mixed reality (MR) application addressed to aesthetic design of free form curves and surfaces. It is a unique and comprehensive approach which uses task-specific configurations to support the design workflow from concept to mock-up evaluation and review. The first-phase conceptual design benefits from a workbench-like 3-D display for free hand sketching, surfacing and engineering visualization. Semitransparent stereo glasses augment the pre-production physical prototype by additional shapes, textures and annotations. Both workspaces share a common interface and allow collaboration and cooperation between different experts, who can configure the system for the specific task. A faster design workflow and CAD data consistency can be thus naturally achieved. Tests and collaborations with designers, mainly from automotive industry, are providing systematic feedback for this ongoing research. As far as the authors are concerned, there is no known similar approach that integrates the creation and editing phase of 3D curves and surfaces in virtual and augmented reality (VR/AR). Herein we see the major contribution of our new application.},
keywords={augmented reality;user interfaces;image processing;engineering graphics;helmet mounted displays;automobile industry;CAD/CAM;Spacedesign;mixed reality workspace;aesthetic industrial design;free form curves;free form surfaces;design workflow;3D display;free hand sketching;engineering visualization;semitransparent stereo glasses;user interface;CAD;data consistency;automotive industry;virtual realty;augmented reality;Virtual reality;Aerospace industry;Collaborative work;Three dimensional displays;Design engineering;Data visualization;Glass;Prototypes;Shape;Design automation},
doi={10.1109/ISMAR.2002.1115077},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1115078,
author={ {Xiang Zhang} and S. {Fronz} and N. {Navab}},
booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, title={Visual marker detection and decoding in AR systems: a comparative study},
year={2002},
volume={},
number={},
pages={97-106},
abstract={Visual markers are widely used in existing augmented reality (AR) applications. In most of such applications, the performance of an AR system depends highly on the tracking system for visual marker detection, tracking, and pose estimation. Currently, there are more than one marker based tracking/calibration systems available. It is thus desirable for the user to know which marker tracking system is likely to perform the best for a specific AR application. For this purpose, we compare several marker systems all using planar square coded visual markers. We present the evaluation results, both qualitatively and quantitatively, for the usability, efficiency, accuracy, and reliability. For a particular AR application, there are different marker detection and tracking requirements. Therefore, the purpose of this work is not to rank existing marker systems; instead, we try to analyze the strength and weakness of various aspects of the marker tracking systems and provide AR application developers with this information.},
keywords={augmented reality;optical tracking;feature extraction;visual marker detection;visual marker decoding;augmented reality;visual marker pose estimation;planar square coded visual markers;usability;efficiency;accuracy;reliability;Decoding;Augmented reality;Tracking;Calibration;Computer aided manufacturing;Military computing;Computer displays;Cameras;Thyristors;Performance evaluation},
doi={10.1109/ISMAR.2002.1115078},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1115079,
author={G. {Baratoff} and A. {Neubeck} and H. {Regenbrecht}},
booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, title={Interactive multi-marker calibration for augmented reality applications},
year={2002},
volume={},
number={},
pages={107-116},
abstract={Industrial augmented reality (AR) applications require fast, robust, and precise tracking. In environments where conventional high-end tracking systems cannot be applied for certain reasons, marker-based tracking can be used with success as a substitute if care is taken about (1) calibration and (2) run-time tracking fidelity. In out-of-the-laboratory environments multi-marker tracking is needed because the pose estimated from a single marker is not stable enough. The overall pose estimation can be dramatically improved by fusing information from several markers fixed relative to each other compared to a single marker only. To achieve results applicable in an industrial context relative marker poses need to be properly calibrated. We propose a semiautomatic image-based calibration method requiring only minimal interaction within the workflow. Our method can be used off-line, or preferably incrementally online. When used online, our method shows reasonably good accuracy and convergence with workflow interruption of less than one second per incremental step. Thus, it can be interactively used. We illustrate our method with an industrial application scenario.},
keywords={calibration;augmented reality;computer vision;optical tracking;interactive multi-marker calibration;augmented reality;marker-based tracking;run-time tracking fidelity;pose estimation;semiautomatic image-based calibration;accuracy;convergence;workflow interruption;Calibration;Augmented reality;Instruments;Computational fluid dynamics;Robustness;Airplanes;Magnetic heads;Virtual reality;Cameras;Manufacturing industries},
doi={10.1109/ISMAR.2002.1115079},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1115080,
author={S. {Malik} and C. {McDonald} and G. {Roth}},
booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, title={Hand tracking for interactive pattern-based augmented reality},
year={2002},
volume={},
number={},
pages={117-126},
abstract={Pattern-based augmented reality systems are considered the most promising approach for accurately registering virtual objects with real-time video feeds. The problem with existing solutions is the lack of robustness to partial occlusions of the pattern, which is important when attempting natural interactions with virtual objects. This paper describes a fast and accurate vision-based pattern tracking system that allows for autocalibrated 3D augmentation of virtual objects onto known planar patterns. The tracking system is shown to be robust to changes in pattern scale, orientation and, most importantly, partial occlusions. A method to detect a hand on top of the pattern is then described, along with a method to render the hand on top of the virtual objects.},
keywords={augmented reality;rendering (computer graphics);optical tracking;gesture recognition;video signal processing;hand tracking;interactive pattern-based augmented reality;virtual object registration;real-time video feeds;partial occlusion robustness;vision-based pattern tracking system;autocalibrated 3D augmentations;planar patterns;rendering;Augmented reality;Robustness;Computer science;Real time systems;Object detection;Computer displays;Magnetic separation;Councils;Feeds;Rendering (computer graphics)},
doi={10.1109/ISMAR.2002.1115080},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1115082,
author={S. {Vogt} and A. {Khamene} and F. {Sauer} and H. {Niemann}},
booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, title={Single camera tracking of marker clusters: multiparameter cluster optimization and experimental verification},
year={2002},
volume={},
number={},
pages={127-136},
abstract={We have built a system for augmented reality visualization based on a single head mounted tracking camera. The camera includes an infrared illuminator and works in conjunction with a set of retro-reflective markers that are placed around the workspace. This marker frame configuration delivers excellent pose information, which translates to stable, jitter-free augmentation. In this article, we describe using the same single camera system for tracking relatively small marker clusters, which can be used for tool or instrument tracking. Tracking of such a marker cluster is more susceptible to noise compared to tracking of a marker frame, mainly due to its small image coverage. The sensitivity to noise is studied using Monte Carlo simulations and verified in an experimental setup. We achieved jitter-free augmentation with an optimized cluster design.},
keywords={optical tracking;augmented reality;data visualisation;Monte Carlo methods;augmented reality visualization;single head mounted tracking camera;infrared illuminator;retro-reflective markers;marker frame configuration;pose information;stable jitter-free augmentation;single camera tracking;marker cluster tracking;tool tracking;instrument tracking;noise;Monte Carlo simulations;optimized cluster design;multiparameter cluster optimization;Cameras;Augmented reality;Magnetic heads;Optical sensors;Optical noise;Visualization;Optical imaging;Educational institutions;Instruments;Magnetic noise},
doi={10.1109/ISMAR.2002.1115082},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1115083,
author={K. {Kiyokawa} and M. {Billinghurst} and S. E. {Hayes} and A. {Gupta} and Y. {Sannohe} and H. {Kato}},
booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, title={Communication behaviors of co-located users in collaborative AR interfaces},
year={2002},
volume={},
number={},
pages={139-148},
abstract={We conducted two experiments comparing communication behaviors of co-located users in collaborative augmented reality (AR) interfaces. In the first experiment, we compared optical, stereo- and mono-video, and immersive head mounted displays (HMDs) using a target identification task. It was found that differences in the real world visibility severely affect communication behaviors. The optical see-through case produced the best results with the least extra communication needed. Generally, the more difficult it was to use non-verbal communication cues, the more people resorted to speech cues to compensate. In the second experiment, we compared three different combinations of task and communication spaces using a 2D icon design task with optical see-through HMDs. It was found that the spatial relationship between the task and communication spaces also severely affected communication behaviors. Placing the task space between the subjects produced the most active behaviors in terms of initiatory body languages and utterances with least miscommunications.},
keywords={helmet mounted displays;augmented reality;groupware;stereo image processing;video signal processing;interactive devices;user interfaces;communication behaviors;co-located users;collaborative augmented reality interfaces;immersive head mounted displays;mono-video head mounted displays;stereo-video head mounted displays;optical head mounted displays;target identification task;real world visibility;optical see-through case;nonverbal communication cues;speech cues;2D icon design task;spatial relationship;communication spaces;task spaces;initiatory body languages;utterances;Collaboration;Collaborative work;Augmented reality;Videoconference;Space technology;Time measurement;Displays;Speech;Video sharing;Visual communication},
doi={10.1109/ISMAR.2002.1115083},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1115084,
author={M. {Takemura} and Y. {Ohta}},
booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, title={Diminishing head-mounted display for shared mixed reality},
year={2002},
volume={},
number={},
pages={149-156},
abstract={We propose a new scheme to recover the eye-contact between multiple users in a shared mixed-reality space. The eye-contact in a shared mixed-reality space is lost as the side effect of wearing head-mounted displays. We synthesize facial images in real-time with arbitrary poses and eye expressions by using several photographs of the user. The face images are overlaid in order to diminish the HMD in his partner's view for the recovery of eye-contact. The basic idea, facial image synthesis, and an experimental system to diminish HMD are presented in this paper.},
keywords={helmet mounted displays;virtual reality;computer vision;diminishing head-mounted display;shared mixed reality;eye contact recovery;multiple users;real-time facial image synthesis;arbitrary poses;arbitrary eye expressions;user photographs;Displays;Virtual reality;Image generation;Rendering (computer graphics);Head;Space technology;Merging;Graphics;Shape;Systems engineering and theory},
doi={10.1109/ISMAR.2002.1115084},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1115085,
author={M. {Fjeld} and S. G. {Schar} and D. {Signorello} and H. {Krueger}},
booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, title={Alternative tools for tangible interaction: a usability evaluation},
year={2002},
volume={},
number={},
pages={157-318},
abstract={In this work we compare an in-house designed tangible user interface (TUI) with three alternative single-user tools through an empirical investigation. These three alternative tools are a 3D physical, a 2D cardboard, and a mathematical tool. We expected the 3D physical to perform best, followed by the TUI, the 2D cardboard, and the mathematical tool. A pilot study was first carried out, the results of which were used to design a major experiment. Participants solved the same positioning problem, each using one of the four tools. The mathematical tool was not used in the experiment. In the experiment, trial time, number of user operations, learning effect in both, and user satisfaction were measured. The TUI significantly outperformed the 2D cardboard tool. However, there was no significant difference between the TUI and the 3D physical tool. This justifies the value of researching TUI systems and carrying out usability studies with such systems.},
keywords={augmented reality;user interfaces;human factors;interactive devices;tangible interaction;usability evaluation;tangible user interface;single-user tools;3D physical tool;2D cardboard tool;mathematical tool;experiment;positioning;user satisfaction;augmented reality;input devices;direct manipulation;Usability;Augmented reality;Time measurement;User interfaces;Problem-solving;Layout;Virtual environment;Technology planning;Electrical capacitance tomography;Educational institutions},
doi={10.1109/ISMAR.2002.1115085},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1115086,
author={Y. {Genc} and M. {Tuceryan} and N. {Navab}},
booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, title={Practical solutions for calibration of optical see-through devices},
year={2002},
volume={},
number={},
pages={169-175},
abstract={Registration is a crucial task in a see-through augmented reality (AR) system. The importance stems not only from the fact that registration requires careful calibration but also from the necessity that any calibration procedure should take users into account. Tuceryan et al. (2002) proposed a general method for calibrating a see-through device based on dynamic alignment of virtual and real points. Although a powerful tool, our experiments showed that users find alignment of many points overwhelming. We introduce improvements to simplify the calibration process and increase the success rate. We first identified why calibration parameters differ from user to user and how this can be prevented by adopting particular configurations for the tracker sensor and display. This allowed us to re-use the existing calibrations. Furthermore, we have introduced a simpler model for the calibration that requires fewer user inputs, typically four, to calibrate the system.},
keywords={calibration;augmented reality;image registration;computer vision;optical tracking;optical see-through devices;calibration;see-through augmented reality system;registration;dynamic virtual point alignment;dynamic real point alignment;tracker sensor;display;Optical devices;Calibration;Cameras;Computer displays;Augmented reality;Active appearance model;Computer errors;Lighting;Computer vision;Virtual reality},
doi={10.1109/ISMAR.2002.1115086},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1115087,
author={ {Hong Hua} and {Chunyu Gao} and N. {Ahuja}},
booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, title={Calibration of a head-mounted projective display for augmented reality systems},
year={2002},
volume={},
number={},
pages={176-185},
abstract={In augmented reality (AR) applications, registering a virtual object with its real counterpart accurately and comfortably is one of the basic and challenging issues in the sense that the size, depth, geometry, as well as physical attributes of the virtual objects have to be rendered precisely relative to a physical reference, which is well known as the calibration or registration problem. This paper presents a systematic calibration process to address static registration in a custom-designed augmented reality system, which is based upon the recent advancement of head-mounted projective display (HMPD) technology. Following a concise review of the HMPD concept and system configuration, we present in detail a computational model for system calibration, describe the calibration procedures to obtain estimations of the unknown transformations, and include the calibration results, evaluation experiments and results.},
keywords={calibration;augmented reality;rendering (computer graphics);helmet mounted displays;image registration;augmented reality;virtual object registration;calibration;head-mounted projective display;static registration;computational model;virtual object geometry;virtual object depth;virtual object size;Calibration;Displays;Augmented reality},
doi={10.1109/ISMAR.2002.1115087},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1115088,
author={O. {Bimber} and B. {Frohlich}},
booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, title={Occlusion shadows: using projected light to generate realistic occlusion effects for view-dependent optical see-through displays},
year={2002},
volume={},
number={},
pages={186-319},
abstract={This paper presents projector-based illumination techniques for creating correct occlusion effects for optical see-through setups. We project view-dependent occlusion shadows onto the real surfaces that are located behind virtual objects. This results in a perfect occlusion of real objects by virtual ones. We have implemented and tested our approach in the context of the Virtual Showcase display. We describe a hardware extension for projecting light into the showcase and present our rendering techniques for displaying occlusion shadows for single and multi-user environments as well as for single and multi-light-projector configurations. We also report on the limitations of our system for multi-user situations and describe our experiences with a first experimental prototype.},
keywords={augmented reality;hidden feature removal;rendering (computer graphics);computer displays;helmet mounted displays;projector-based illumination techniques;view-dependent optical see-through displays;realistic occlusion effect generation;projected light;view-dependent occlusion shadow projection;virtual objects;real surfaces;Virtual Showcase display;hardware extension;rendering techniques;multi-user environments;single-user environments;multi light projector configurations;single light projector configurations;Mirrors;Optical sensors;Computer displays;Hardware;Augmented reality;Virtual reality;Lighting control;Computer graphics;Testing;Prototypes},
doi={10.1109/ISMAR.2002.1115088},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1115089,
author={H. {Ishii}},
booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, title={Tangible bits: designing the seamless interface between people, bits, and atoms},
year={2002},
volume={},
number={},
pages={199-199},
abstract={},
keywords={USA Councils;Art;Human computer interaction;Collaborative work;Laboratories;Augmented reality},
doi={10.1109/ISMAR.2002.1115089},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1115090,
author={H. {Ishii} and J. {Underkoffler} and D. {Chak} and B. {Piper} and E. {Ben-Joseph} and L. {Yeung} and Z. {Kanji}},
booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, title={Augmented urban planning workbench: overlaying drawings, physical models and digital simulation},
year={2002},
volume={},
number={},
pages={203-211},
abstract={There is a problem in the spatial and temporal separation between the varying forms of representation used in urban design. Sketches, physical models, and more recently computational simulation, while each serving a useful purpose, tend to be incompatible forms of representation. The contemporary designer is required assimilate these divergent media into a single mental construct and in so doing is distracted from the central process of design. We propose an augmented reality workbench called "Luminous Table" that attempts to address this issue by integrating multiple forms of physical and digital representations. 2D drawings, 3D physical models, and digital simulation are overlaid into a single information space in order to support the urban design process. We describe how the system was used in a graduate design course and discuss how the simultaneous use of physical and digital media allowed for a more holistic design approach. We also discuss the need for future technical improvements.},
keywords={town and country planning;augmented reality;digital simulation;engineering graphics;augmented urban planning workbench;digital simulation;spatial separation;temporal separation;augmented reality workbench;Luminous Table;2D drawings;3D physical models;graduate design course;Urban planning;Digital simulation;Process design;Physics computing;Buildings;Analytical models;Augmented reality;Satellites;Laboratories;Traffic control},
doi={10.1109/ISMAR.2002.1115090},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1115091,
author={C. {Furmanski} and R. {Azuma} and M. {Daily}},
booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, title={Augmented-reality visualizations guided by cognition: perceptual heuristics for combining visible and obscured information},
year={2002},
volume={},
number={},
pages={215-320},
abstract={One unique feature of mixed and augmented reality (MR/AR) systems is that hidden and occluded objects an be readily visualized. We call this specialized use of MR/AR, obscured information visualization (OIV). In this paper, we describe the beginning of a research program designed to develop such visualizations through the use of principles derived from perceptual psychology and cognitive science. In this paper we surveyed the cognitive science literature as it applies to such visualization tasks, described experimental questions derived from these cognitive principles, and generated general guidelines that can be used in designing future OIV systems (as well improving AR displays more generally). We also report the results from an experiment that utilized a functioning AR-OIV system: we found that in relative depth judgment, subjects reported rendered objects as being in front of real-world objects, except when additional occlusion and motion cues were presented together.},
keywords={augmented reality;data visualisation;hidden feature removal;psychology;visual perception;cognition-guided augmented reality visualizations;perceptual heuristics;visible/obscured information combination;mixed reality;augmented reality;hidden object visualization;occluded object visualization;obscured information visualization;perceptual psychology;cognitive science;relative depth judgment;rendered objects;motion cues;Augmented reality;Visualization;Displays;Virtual reality;Cognitive science;Guidelines;Application software;Laboratories;Psychology;Rendering (computer graphics)},
doi={10.1109/ISMAR.2002.1115091},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1115092,
author={T. {Tanikawa} and K. {Hirota} and M. {Hirose}},
booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, title={A study for image-based integrated virtual environment},
year={2002},
volume={},
number={},
pages={225-233},
abstract={In this paper, we propose a new approach to building a virtual environment by using raw data from a real environment. To manage and analyze different types of datasets from many sensors and databases, we build and integrate different types of environments with different types of datasets, user interactions and rendering methods. To integrate the environments, we merge rendered images of each virtual environment according to the user's viewpoint, depth information, and reliability of rendered images, and display the resultant image to the user. In addition, we propose an integration environment by using an environment manager, which determines the user's viewpoint with respect to the integrated environments according to the user's input, and compose a display image from rendered images of the integrant environments. The reliability of the rendered images is determined by data accuracy and resolution, display resolution and rendering errors from rendering methodologies. By integrating four constructed environments according to the user's viewpoint, we can construct a photorealistic large-scale environment from different types of datasets. Furthermore, we implemented the prototype system in CABIN and demonstrated the scalability of this architecture.},
keywords={rendering (computer graphics);virtual reality;image processing;image-based integrated virtual environment;sensors;databases;user interactions;rendering methods;user viewpoint;depth information;rendered image reliability;environment manager;display image;data accuracy;data resolution;display resolution;rendering errors;photorealistic large-scale environment;CABIN;scalability;Virtual environment;Rendering (computer graphics);Displays;Virtual reality;Solid modeling;Image resolution;Prototypes;Buildings;Power system modeling;Data visualization},
doi={10.1109/ISMAR.2002.1115092},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1115093,
author={M. {Haringer} and H. T. {Regenbrecht}},
booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, title={A pragmatic approach to augmented reality authoring},
year={2002},
volume={},
number={},
pages={237-245},
abstract={In this paper we describe the augmented reality (AR) authoring system "PowerSpace" which allows fast and comfortable generation of AR worlds. The system presented uses the functionality of a 2D presentation program (Microsoft PowerPoint) as the basis for the composition of 3D content. An MS PowerPoint export is used to generate an XML-based extensible description of a presentation. This description is enriched by 3D content with the help of an editor, which is also part of the PowerSpace system. The content of this presentation is finally converted into 3D scenes and used in an AR-viewer.},
keywords={augmented reality;authoring systems;hypermedia markup languages;augmented reality authoring;PowerSpace;augmented reality world generation;2D presentation program;Microsoft PowerPoint;3D content composition;XML-based extensible description;editor;3D scenes;augmented reality viewer;Augmented reality;Layout;Power generation;Rendering (computer graphics);Geometry;Engines;Switches;Authoring systems;Power system modeling;Graphics},
doi={10.1109/ISMAR.2002.1115093},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1115095,
author={S. {Uchiyama} and K. {Takemoto} and K. {Satoh} and H. {Yamamoto} and H. {Tamura}},
booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, title={MR Platform: a basic body on which mixed reality applications are built},
year={2002},
volume={},
number={},
pages={246-320},
abstract={This paper describes a platform package, called "MR Platform," which we have been implementing for research and development of augmented reality technology and applications. This package includes a parallax-less stereo video see-through HMD and a software development kit (SDK) for a Linux PC environment. The SDK is composed of a C++ class library for making runtime MR applications and related utilities such as a camera calibration tool. By using the SDK, the following functions are available: capturing video, handling a six degree-of-freedom (DOF) sensor, image processing such as color detection, estimating head position and orientation, displaying the real world image, and calibrating sensor placement and camera parameters of two cameras mounted on the HMD.},
keywords={augmented reality;image processing;helmet mounted displays;programming environments;Unix;C++ language;software libraries;calibration;object-oriented programming;MR Platform;mixed reality applications;platform package;research and development;augmented reality;parallax-less stereo video see-through HMD;software development kit;Linux;PC environment;C++ class library;camera calibration tool;video capture;six degree-of-freedom sensor;image processing;color detection;head position estimation;Virtual reality;Application software;Cameras;Packaging;Image sensors;Research and development;Augmented reality;Software packages;Programming;Linux},
doi={10.1109/ISMAR.2002.1115095},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1115098,
author={P. {Tschirner} and B. {Hillers} and A. {Graser}},
booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, title={A concept for the application of augmented reality in manual gas metal arc welding},
year={2002},
volume={},
number={},
pages={257-258},
abstract={The problem of creating manual welds of constant high quality results from missing optical information during the actual welding process. Due to the extreme brightness conditions in arc welding and the use of protective glasses, even experienced welders can hardly recognize details of the welding process and the environment. This paper describes a new research project for the development of a support system for the welder.},
keywords={production engineering computing;user interfaces;augmented reality;image processing;helmet mounted displays;arc welding;augmented reality;manual gas metal arc welding;missing optical information;extreme brightness conditions;protective glasses;head-mounted display;Augmented reality;Welding;Portable computers;Protection;Glass;Cameras;Computer displays;Data mining;Brightness;Image quality},
doi={10.1109/ISMAR.2002.1115098},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1115100,
author={M. {Fjeld} and B. M. {Voegtli}},
booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, title={Augmented Chemistry: an interactive educational workbench},
year={2002},
volume={},
number={},
pages={259-321},
abstract={We report on some of the advantages tangible interaction can bring to chemistry education. We describe the realisation of a tangible user interface (TUI) called Augmented Chemistry (AC). A set of interactive tools work within this system. Using these tools, elements can be chosen from a booklet menu and composed into 3D molecular models. The tools are one way towards seamless integration of the physical and digital realms. Since multiple tools can be used concurrently, single and multiple users can use the system at a time. To use the system in an educational context, it was extended into an educational workbench drawing on haptic and aural augmentation. The design and implementation of our system required contributions from optics, mathematics, molecular chemistry, software engineering, and 3D programming, making it a truly interdisciplinary project. Future challenges lie in user acceptance, educational effect, and further system development.},
keywords={chemistry computing;computer aided instruction;augmented reality;user interfaces;image processing;Augmented Chemistry;interactive educational workbench;chemistry education;tangible user interface;booklet menu;3D molecular models;aural augmentation;haptic augmentation;user acceptance;augmented reality;Chemistry;User interfaces;Chemical elements;Engineering drawings;Haptic interfaces;Optical design;Mathematics;Software engineering;Programming profession;Mathematical programming},
doi={10.1109/ISMAR.2002.1115100},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1115102,
author={B. {Schwald} and H. {Seibert} and T. {Weller}},
booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, title={A flexible tracking concept applied to medical scenarios using an AR window},
year={2002},
volume={},
number={},
pages={261-262},
abstract={This paper presents an approach to use a semitransparent display as a kind of window into a patient in the context of medical augmented reality (AR) applications. Besides the presentation of the non-off-the-shelf display, the tracking aspects of such an application are the focus of the work presented. In order to allow augmentations of real objects by virtual ones on the display, the user (i.e. physician), the display, the object (i.e. patient) and optional instruments have to be tracked. If required, a tracking system consisting of more than one subsystem, e.g. optical tracking combined with electromagnetic tracking, is used to satisfy all the needs of such a medical application.},
keywords={tracking;augmented reality;user interfaces;computer displays;medical image processing;flexible tracking concept;semitransparent display;medical augmented reality applications;tracking;optical tracking;electromagnetic tracking;computer display;image processing;Liquid crystal displays;Augmented reality;Flat panel displays;Instruments;Biomedical imaging;Biomedical optical imaging;Medical services;Biomedical equipment;Head;Holography},
doi={10.1109/ISMAR.2002.1115102},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1115103,
author={P. {Dahne} and J. N. {Karigiannis}},
booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, title={Archeoguide: system architecture of a mobile outdoor augmented reality system},
year={2002},
volume={},
number={},
pages={263-264},
abstract={We present the system architecture of a mobile outdoor augmented reality system for the Archeoguide project. We begin with a short introduction to the project. Then we present the hardware we chose for the mobile system and we describe the system architecture we designed for the software implementation. We conclude this paper with the first results obtained from experiments we made during our trials at ancient Olympia in Greece.},
keywords={archaeology;software architecture;mobile computing;augmented reality;user interfaces;image processing;wearable computers;Archeoguide;system architecture;mobile outdoor augmented reality system;experiments;ancient Olympia;wearable computers;cultural heritage;historical sites;archaeology;software architecture;Augmented reality;Rendering (computer graphics);Computer architecture;Hardware;Wearable computers;Image reconstruction;Batteries;Cameras;Computer graphics;Software design},
doi={10.1109/ISMAR.2002.1115103},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1115105,
author={A. {Tang} and C. {Owen} and F. {Biocca} and {Weimin Mou}},
booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, title={Experimental evaluation of augmented reality in object assembly task},
year={2002},
volume={},
number={},
pages={265-266},
abstract={This study evaluated the effectiveness of spatially overlaid instructions using augmented reality (AR) in an assembly task compared with other traditional media. Results indicate that overlaying 3D instructions on the workspace reduce error rate by 82%, particularly cumulative errors. Measurement of mental effort also suggests some of the mental workload is offloaded to the computer.},
keywords={augmented reality;user interfaces;computer based training;assembling;human factors;helmet mounted displays;image processing;experimental evaluation;augmented reality;object assembly task;spatially overlaid instructions;head mounted display;3D instructions;error rate;mental workload;human computer interaction;training;Augmented reality;Assembly;Error analysis;NASA;Humans;Computer aided instruction;Time measurement;Calibration;Computer errors;Computer interfaces},
doi={10.1109/ISMAR.2002.1115105},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1115106,
author={F. {Tang} and C. {Aimone} and J. {Fung} and A. {Marjan} and S. {Mann}},
booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, title={Seeing eye to eye: a shared mediated reality using EyeTap devices and the VideoOrbits gyroscopic head tracker},
year={2002},
volume={},
number={},
pages={267-268},
abstract={We present a system which allows wearable computer users to share their views of their current environments with each other. Our system uses an EyeTap: a device which allows the eye of the wearer to function both as a camera and a display. A wearer, by looking around his/her environment, "paints" or "builds" an environment map composed of images from the EyeTap device, along with head-tracking information recording the orientation of each image. The head-tracking algorithm uses a featureless image motion estimation algorithm coupled with a head mounted gyroscope. The environment map is then transmitted to another user, who, through their own head-tracking EyeTap system, browses the first user's environment solely by head motion, seeing the environment as though it were their own. As a result of browsing the transmitted environment map, the viewer builds and extends his/her own environment map, and thus this is a data-producing head-tracking system. These environment maps can then be shared reciprocally between wearers.},
keywords={augmented reality;user interfaces;helmet mounted displays;wearable computers;tracking;gyroscopes;motion estimation;shared mediated reality;EyeTap devices;VideoOrbits;gyroscopic head tracker;wearable computer;eye;camera;head-tracking algorithm;featureless image motion estimation;head mounted gyroscope;browsing;environment map;Cameras;Image databases;Wearable computers;Magnetic heads;Gyroscopes;Video sharing;Educational institutions;Computer displays;Motion estimation;Computer vision},
doi={10.1109/ISMAR.2002.1115106},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1115107,
author={W. {Piekarski} and B. H. {Thomas}},
booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, title={Bread Crumbs: a technique for modelling large outdoor ground features},
year={2002},
volume={},
number={},
pages={269-321},
abstract={This paper presents a new technique we have created, known as Bread Crumbs, which allows the modelling of large outdoor ground features using a mobile augmented reality system and a user's physical presence. Using this technique, we demonstrate an example of modelling a large grassy area on campus. This technique is one component of a larger set of tools which allow users to capture and work with complex outdoor geometry interactively, and to view this geometry in new and interesting ways.},
keywords={augmented reality;mobile computing;Bread Crumbs;large outdoor ground feature modelling;mobile augmented reality system;user physical presence;on-campus large grassy area;complex outdoor geometry;Augmented reality;Geometry;Fingers;Solid modeling;Wearable computers;Lakes;Mobile computing;Cameras;User interfaces;Control systems},
doi={10.1109/ISMAR.2002.1115107},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1115108,
author={S. {Noelle}},
booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, title={Stereo augmentation of simulation results on a projection wall by combining two basic ARVIKA systems},
year={2002},
volume={},
number={},
pages={271-322},
abstract={The article describes the potential for cost-reduction by using augmented reality (AR) in the automotive industry. AR allows for the evaluation of computer-generated simulations with physically crashed cars. The augmented simulation results are displayed on a projection wall in stereo by simply combining two basic ARVIKA systems.},
keywords={augmented reality;digital simulation;automobiles;engineering graphics;stereo image processing;video signal processing;stereo augmentation;computer generated simulation evaluation;cost reduction;augmented reality;automotive industry;physically crashed cars;projection wall;ARVIKA systems;Visualization;Vehicle crash testing;Computational modeling;Augmented reality;Cams;Automotive engineering;Computer simulation;Computer crashes;Animation;Virtual reality},
doi={10.1109/ISMAR.2002.1115108},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1115109,
author={A. D. {Cheok} and E. {Neo Weng Chuen} and {Ang Wee Eng}},
booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, title={Inexpensive non-sensor based augmented reality modeling of curves and surfaces in physical space},
year={2002},
volume={},
number={},
pages={273-274},
abstract={Previous work in modeling curves and surfaces in augmented reality (AR) space has used expensive sensors such as magnetic sensors. In this work, we propose an augmented reality system where a user can model interesting surfaces with her hands, without expensive sensing systems. The system uses computer vision based methods for tracking the user's head and hand position. Using a glove and the tracking system, the user can draw smooth lines or surfaces with her hands in a physical space. The user can also intuitively modify the lines or surface created by pushing or pulling at the control points of lines or curves in a tangible manner.},
keywords={augmented reality;data gloves;computer vision;optical tracking;CAD;engineering graphics;inexpensive nonsensor based augmented reality modeling;curves;surfaces;augmented reality;computer vision based methods;head position tracking;hand position tracking;glove;Augmented reality;Fingers;Magnetic sensors;Virtual reality;Sensor systems;Shape control;Color;Computer vision;Magnetic heads;Automatic control},
doi={10.1109/ISMAR.2002.1115109},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1115110,
author={J. {Fung} and S. {Mann}},
booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, title={Exploring humanistic intelligence through physiologically mediated reality},
year={2002},
volume={},
number={},
pages={275-276},
abstract={We present a way of making the wearing of a lifelong electrocardiographic health monitor fun for a user. The health monitor is coupled with a reality mediator device to create physiologically mediated reality, i.e. mediated reality which alters a user's audiovisual perception of the world based upon their own electrocardiographic waveform. This creates an interesting audiovisual experience for the user, playing upon the poetic narrative of combining cardio-centric metaphors pervasive in everyday life (the heart as a symbol of love and centrality, e.g. "get to the heart of the matter") with ubiquitous occular-centric metaphors such as "see the world from my point of view". This audiovisual experience is further enhanced by combining music which alters the visual perception and also heightens the user's emotional response to their experience and, in doing so, further affects their heart(beat).},
keywords={electrocardiography;medical signal processing;music;patient monitoring;virtual reality;audio-visual systems;spectral analysis;video signal processing;physiology;physiologically mediated reality;lifelong electrocardiographic health monitor;reality mediator device;audiovisual perception;humanistic intelligence;electrocardiographic waveform;audiovisual experience;poetic narrative;cardio-centric metaphors;occular-centric metaphors;emotional response;Electrocardiography;Biomedical monitoring;Heart beat;Cardiology;Spectral analysis;Low pass filters;Educational institutions;Visual perception;Bars;Frequency},
doi={10.1109/ISMAR.2002.1115110},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1115111,
author={R. {Behringer} and {Jun Park} and V. {Sundareswaran}},
booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, title={Model-based visual tracking for outdoor augmented reality applications},
year={2002},
volume={},
number={},
pages={277-322},
abstract={Outdoor augmented reality (AR) applications rely on hybrid tracking (GPS, digital compass, visual) for registration. RSC has developed a real-time visual tracking system that uses visual cues of buildings in an urban environment for correcting the results of a conventional tracking system. This approach relies on knowledge of a CAD model of the building. It not only provides motion estimation, but also absolute orientation/position. It is based on the "visual servoing" approach, originally developed for robotics tasks. We have demonstrated this approach in real-time at a building on the NRL campus This poster shows the approach and results. The concept can be generalized to any scenario where a CAD model is available. This system is being prepared for integration into the NRL system BARS (Battlefield Augmented Reality System).},
keywords={augmented reality;optical tracking;CAD;motion estimation;real-time systems;military computing;military systems;model-based visual tracking;outdoor augmented reality applications;hybrid tracking;digital compass;GPS;registration;real-time visual tracking system;visual cues;buildings;urban environment;CAD model;motion estimation;absolute orientation;absolute position;visual servoing;Battlefield Augmented Reality System;Augmented reality;Cameras;Image processing;Global Positioning System;Bars;Visual servoing;Real time systems;Motion estimation;Robots;Minimization methods},
doi={10.1109/ISMAR.2002.1115111},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1115112,
author={M. {Kanbara} and N. {Yokoya}},
booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, title={Geometric and photometric registration for real-time augmented reality},
year={2002},
volume={},
number={},
pages={279-280},
abstract={This paper proposes an augmented reality system with correct representation of shading and shadow. To realize a seamless augmented reality system, we need to resolve certain problems. The geometric and photometric registration problems are particularly important. These problems require the position of light sources and user's viewpoint. The proposed system resolves these problems using a 3D marker which combines a 2D square marker and a mirror ball. The 2D marker and the ball are used to estimate the relationship between the real and virtual worlds and the positions of light sources in the real world, respectively.},
keywords={augmented reality;image registration;computational geometry;photometry;light sources;real-time systems;real-time augmented reality;photometric registration;geometric registration;shading;shadow;light source position;user viewpoint;3D marker;2D square marker;mirror ball;real/virtual world relationship;Photometry;Augmented reality;Mirrors;Light sources;Cameras;Layout;Information science;Virtual environment;Lighting;Rendering (computer graphics)},
doi={10.1109/ISMAR.2002.1115112},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1115113,
author={C. {Geiger} and V. {Paelke} and C. {Reimann} and W. {Rosenbach} and J. {Stoecklein}},
booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, title={Testable design representations for mobile augmented reality authoring},
year={2002},
volume={},
number={},
pages={281-282},
abstract={This paper applies the idea of a continuously testable design representation to authoring of augmented realities for mobile devices.},
keywords={augmented reality;user interfaces;mobile computing;authoring systems;testable design representations;mobile augmented reality authoring;mobile devices;prototyping;mobile computing;Testing;Augmented reality;Prototypes;Automata;Iterative methods;Mobile computing;Wireless communication;Communications technology;Hardware;Personal digital assistants},
doi={10.1109/ISMAR.2002.1115113},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1115118,
author={G. {Simon} and M. -. {Berger}},
booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, title={Reconstructing while registering: a novel approach for markerless augmented reality},
year={2002},
volume={},
number={},
pages={285-293},
abstract={This paper addresses the registration problem for unprepared multi-planar scenes. An interactive process is proposed to obtain accurate results using only the texture information of planes. In particular, classical preparation steps (camera calibration, scene acquisition) are greatly simplified, since they are included in the on-line registration process. Results are shown on indoor and outdoor scenes. Videos are available at url http://www.loria.fr//spl tilde/gsimon/Ismar.},
keywords={augmented reality;image registration;image reconstruction;calibration;unprepared multi-planar scenes;markerless augmented reality;interactive process;planar texture information;camera calibration;scene acquisition;on-line registration process;outdoor scenes;indoor scenes;reconstruction;Augmented reality;Layout;Cameras;Calibration;Magnetic sensors;Uniform resource locators;Video sequences;Application software;Learning systems;Multimedia communication},
doi={10.1109/ISMAR.2002.1115118},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1115122,
author={Y. {Genc} and S. {Riedel} and F. {Souvannavong} and C. {Akinlar} and N. {Navab}},
booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, title={Marker-less tracking for AR: a learning-based approach},
year={2002},
volume={},
number={},
pages={295-304},
abstract={Estimating the pose of a camera (virtual or real) in which some augmentation takes place is one of the most important parts of an augmented reality (AR) system. The availability of powerful processors and fast frame grabbers has made the use of vision-based trackers commonplace due to their accuracy as well as flexibility and ease of use. Current vision-based trackers are based on tracking of markers. The use of markers increases robustness and reduces computational requirements. However, their use can be very complicated, as they require maintenance. Direct use of scene features for tracking, therefore, is desirable. To this end, we describe a general system that tracks the position and orientation of a camera observing a scene without visual markers. Our method is based on a two-stage process. In the first stage, a set of features is learned with the help of an external tracking system during use. The second stage uses these learned features for camera tracking when the system in the first stage decides that it is possible to do so. The system is very general so that it can employ any available feature tracking and pose estimation system for learning and tracking. We experimentally demonstrate the viability of the method in real-life examples.},
keywords={optical tracking;augmented reality;computer vision;markerless tracking;augmented reality;camera pose estimation;vision-based trackers;scene features;camera position tracking;camera orientation tracking;feature learning;external tracking system;Layout;Cameras;Computer vision;Robustness;Augmented reality;Streaming media;Application software;Computer displays;Video sequences;Ultrasonic imaging},
doi={10.1109/ISMAR.2002.1115122},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1115123,
author={ {Kar Wee Chia} and A. D. {Cheok} and S. J. D. {Prince}},
booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, title={Online 6 DOF augmented reality registration from natural features},
year={2002},
volume={},
number={},
pages={305-313},
abstract={We present a complete scalable system for 6 DOF camera tracking based on natural features. Crucially, the calculation is based only on pre-captured reference images and previous estimates of the camera pose and is hence suitable for online applications. We match natural features in the current frame to two spatially separated reference images. We overcome the wide baseline matching problem by matching to the previous frame and transferring point positions to the reference images. We then minimize deviations from the two-view and three-view constraints between the reference images and the current frame as a function of camera position parameters. We stabilize this calculation using a recursive form of temporal regularization that is similar in spirit to the Kalman filter. We can track camera pose over hundreds of frames and realistically integrate virtual objects with only slight jitter.},
keywords={augmented reality;optical tracking;image registration;image matching;online 6 DOF augmented reality registration;natural features;scalable system;pre-captured reference images;camera pose estimation;spatially separated reference images;wide baseline matching problem;point position transfer;three-view constraints;two-view constraints;camera position parameters;recursive temporal regularization;camera pose tracking;virtual objects;jitter;Augmented reality;Cameras;Layout;Motion estimation;Position measurement;Jitter;Shape;Two dimensional displays;Labeling;Fluid flow measurement},
doi={10.1109/ISMAR.2002.1115123},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1115124,
author={},
booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, title={Author index},
year={2002},
volume={},
number={},
pages={323-324},
abstract={The author index contains an entry for each author and coauthor included in the proceedings record.},
keywords={},
doi={10.1109/ISMAR.2002.1115124},
ISSN={},
month={Oct},}