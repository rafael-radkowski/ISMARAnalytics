@INPROCEEDINGS{8699315,
author={},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={2018 IEEE International Symposium on Mixed and Augmented Reality},
year={2018},
volume={},
number={},
pages={1-1},
abstract={2018 IEEE International Symposium on Mixed and Augmented Reality},
keywords={},
doi={10.1109/ISMAR-Adjunct.2018.00001},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699199,
author={},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={ISMAR 2018 Conference Committee Members},
year={2018},
volume={},
number={},
pages={xxiv-xxiv},
abstract={ISMAR 2018 Conference Committee Members},
keywords={},
doi={10.1109/ISMAR-Adjunct.2018.00010},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699269,
author={D. {Shor} and B. {Zaaijer} and L. {Ahsmann} and S. {Immerzeel} and M. {Weetzel} and D. {Eikelenboom} and J. {Hartcher-O'Brien} and D. {Aschenbrenner}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Designing Haptics: Comparing Two Virtual Reality Gloves with Respect to Realism, Performance and Comfort},
year={2018},
volume={},
number={},
pages={318-323},
abstract={This paper describes the development of custom built interface between a force-replicating Virtual Reality (VR) haptic glove, and a user. The ability to convey haptic information - both kinesthetic and tactile - is a critical barrier in creating comprehensive simulations. Haptic interface gloves can convey haptic information, but often the haptic signal is diluted by sensory noise, miscuing the users brain. Our goal is to convey compelling interactions with virtual objects, such as grasping, squeezing, and pressing by improving one such haptic interface glove - the Sense Glove - through a redesign of the user-glove interface - Soft Glove. The redesign revolves around three critical design factors - comfort, realism, and performance - and three critical design areas - thimble/fingertip, palm, and haptic feedback. This paper introduces the redesign method and compares the two designs with a quantitative user study. The benefit of the Improved Soft Glove can be shown by a significant improvement of the design factors.},
keywords={data gloves;ergonomics;haptic interfaces;virtual reality;custom built interface;haptic information;haptic interface glove;virtual objects;Sense Glove;user-glove interface;haptic feedback;design factors;haptics design;virtual reality gloves;Soft Glove;Haptic interfaces;Task analysis;Headphones;Virtual environments;Prototypes;Welding;Visualization;Human-Centered Computing—Human Computer Interaction—Interaction Paradigms—Virtual Reality;Human-Centered Computing—Human Computer Interaction—Interaction Devices—Haptic Devices;Human-Centered Computing—Interaction Design—Interaction Design Theory;Concepts and Paradigms},
doi={10.1109/ISMAR-Adjunct.2018.00095},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699264,
author={M. {Engberg} and J. {David Bolter} and B. {Maclntyre}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={RealityMedia: An Experimental Digital Book in WebXR},
year={2018},
volume={},
number={},
pages={324-327},
abstract={This paper presents an ongoing experiment using WebXR to create something analogous to a non-fiction book in AR and VR; an immersive, interactive experience that stands on its own, rather than merely complementing a traditional book. The book introduces the reader/user to AR and VR both as technologies and as media. The printed book is one of the more influential communicative interfaces in history. AR and VR have the potential to remediate several genres of printed books, but somewhat different conventions may need to be developed for different combinations of genre and modality. The lessons learned through this experiment should contribute to the establishment of guidelines for this new form of multimedia, in particular conventions that facilitate the reader/user's transition from discursive to immersive modes and back.},
keywords={augmented reality;electronic publishing;experimental digital book;WebXR;ongoing experiment;nonfiction book;immersive experience;interactive experience;traditional book;printed book;influential communicative interfaces;AR;VR;Media;Portals;Three-dimensional displays;History;Two dimensional displays;Augmented reality;Writing;Augmented reality;virtual reality;world wide web;UX design;• Human-centered computing∼Mixed / augmented reality;• Human-centered computing∼Virtual reality;• Human-centered computing∼Interface design prototyping},
doi={10.1109/ISMAR-Adjunct.2018.00096},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699188,
author={G. {Speiginer} and B. {Maclntyre}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Rethinking Reality: A Layered Model of Reality for Immersive Systems},
year={2018},
volume={},
number={},
pages={328-332},
abstract={We introduce the Environment-Augmentation framework and explore how this framework can inform the design of extended reality (i.e., “immersive”) systems and experiences.},
keywords={virtual reality;immersive systems;Environment-Augmentation framework;extended reality;Virtual environments;Augmented reality;Security;Software architecture;Visualization;Sensors;• Human-centered computing∼Mixed / augmented reality;• Human-centered computing∼Virtual reality;• Software and its engineering∼Layered systems},
doi={10.1109/ISMAR-Adjunct.2018.00097},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699236,
author={M. {Nebeling} and M. {Speicher}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={The Trouble with Augmented Reality/Virtual Reality Authoring Tools},
year={2018},
volume={},
number={},
pages={333-337},
abstract={There are many technical and design challenges in creating new, usable and useful AR/VR applications. In particular, non-technical designers and end-users are facing a lack of tools to quickly and easily prototype and test new AR/VR user experiences. We review and classify existing AR/VR authoring tools and characterize three primary issues with these tools based on our review and a case study. To address the issues, we discuss two new tools we designed with support for rapid prototyping of new AR/VR content and gesture-based interactions geared towards designers without technical knowledge in gesture recognition, 3D modeling, and programming.},
keywords={augmented reality;authoring systems;gesture recognition;human computer interaction;software prototyping;user experience;rapid prototyping;gesture-based interactions;virtual reality authoring tools;augmented reality authoring tools;VR authoring tools;AR authoring tools;VR user experiences;AR user experiences;Tools;Three-dimensional displays;Solid modeling;Animation;Programming;Authoring systems;Tracking;augmented reality;virtual reality;authoring;design;rapid prototyping;3D modeling;gestures;Wizard of Oz;Human-centered computing;Interaction paradigms;Mixed / augmented reality},
doi={10.1109/ISMAR-Adjunct.2018.00098},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699303,
author={B. {Maclntyre} and T. F. {Smith}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Thoughts on the Future of WebXR and the Immersive Web},
year={2018},
volume={},
number={},
pages={338-342},
abstract={The web has a long history as a platform for design and creativity, and new standards aim to bring a wide range of emerging technologies (AR, VR, voice, IoT, etc) to the web. The WebXR Device API hopes to bring AR and VR capabilities to the web and allow these technologies to be added to new or existing web sites. How these technologies are exposed, and what capabilities they have, needs to be informed by designers and others with real use-cases in mind. This position paper outlines some of the questions and ideas we have on future directions for the immersive web. In addition to expanding the initial WebXR Device API to support a more comprehensive set of capabilities, additional web APIs and frameworks will need to be created or expanded to fulfill the promised of Web-based AR and VR. As part of charting this path forward, the community will also need to decide what will and will not be possible on the web, and in what areas web-based AR and VR will excel.},
keywords={application program interfaces;augmented reality;Internet;Web sites;design;Web sites;Web-based AR;immersive Web;WebXR Device API;Web-based VR;Sensors;Geospatial analysis;Browsers;Standards;Computer vision;Visualization;Face;Augmented reality;virtual reality;world wide web;webxr;immersive web},
doi={10.1109/ISMAR-Adjunct.2018.00099},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699182,
author={M. {Kanbara} and I. {Kitahara} and K. {Kiyokawa}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={International Workshop on Comfort Intelligence with AR for Autonomous Vehicle 2018},
year={2018},
volume={},
number={},
pages={343-343},
abstract={Many researchers and companies have been developing technologies for autonomous vehicle. Most technologies are focused on safety control and efficient path planning of autonomous driving. To accept autonomous vehicle socially, a comfort of passenger who is driver being free from driving is one of important issues. Passengers of autonomous vehicle feel discomfort when the vehicle behaves unexpectedly or moves on unexpected path. In addition, the problem of motion sickness will be increased in autonomous vehicle because the passenger will not be able to understand the behavior of the vehicle. In near future, since a windscreen of autonomous vehicle will become an AR display, it is expected that the problems of a VR sickness will be increased too. For these reasons, there will be many discomfort factors of passengers in autonomous vehicle. This workshop focuses on technologies for improvement of passenger's comfort in autonomous vehicle, such as sensing methods of passengers and environment, AR technology, AR user interface and AR display in autonomous vehicle.},
keywords={augmented reality;control engineering computing;mobile robots;road traffic control;road vehicles;traffic engineering computing;user interfaces;passenger comfort;comfort intelligence;autonomous vehicle;motion sickness;AR display;VR sickness;discomfort factors;AR technology;AR user interface;Autonomous vehicles;Conferences;Augmented reality;Companies;Safety;Path planning},
doi={10.1109/ISMAR-Adjunct.2018.00100},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699226,
author={Y. {Sakamura} and A. {Tomita} and H. {Shishido} and T. {Mizunami} and K. {Inoue} and Y. {Kameda} and E. T. {Harada} and I. {Kitahara}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={A Virtual Boarding System of an Autonomous Vehicle for Investigating the Effect of an AR Display on Passenger Comfort},
year={2018},
volume={},
number={},
pages={344-349},
abstract={This paper introduces our attempt to investigate the relationship between an augmented reality display and the sense of comfort of a passenger on board an autonomous vehicle, using an omnidirectional video display system. An omnidirectional camera is located above the passenger seat of a moving vehicle and captures omnidirectional scenes from in and around the vehicle. A virtual environment, specifically a spherical CG model placed in a virtual space, is constructed in a computer, and a panoramic video generated from the omnidirectional video is mapped on to it. A virtual camera that captures the passenger's direction of view is set at the centre of the CG sphere. The orientation of the virtual camera is set in line with the orientation of the passenger's view, which is acquired by an inertia sensor in a head-mounted display (HMD), so that the passenger can look around at the omnidirectional scene. We control the vehicle speed in the virtual space by changing the video replay speed in our developed system. Using this system, we conducted experiments to investigate the relationship between an augmented reality display and a passenger's sense of comfort, by superimposing the virtual pattern (depicting the vehicle's direction) generated using vehicle information recorded along with the video recording.},
keywords={augmented reality;cameras;helmet mounted displays;video recording;video signal processing;vehicle information;video recording;virtual boarding system;autonomous vehicle;AR display;passenger comfort;augmented reality display;omnidirectional video display system;omnidirectional camera;passenger seat;moving vehicle;virtual environment;spherical CG model;virtual space;panoramic video;virtual camera;CG sphere;head-mounted display;omnidirectional scene;vehicle speed;video replay speed;developed system;virtual pattern;omnidirectional scenes;Augmented reality;Augmented Reality;Sense of Comfort;Virtual Pattern;Autonomous Vehicle;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities},
doi={10.1109/ISMAR-Adjunct.2018.00101},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699194,
author={T. {Sawabe} and M. {Kanbara} and N. {Hagita}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Comfort Intelligence for Autonomous Vehicles},
year={2018},
volume={},
number={},
pages={350-353},
abstract={Variety of different autonomous vehicles have been developed for different use cases. Previous studies mostly focus on safety and efficiency for a realization of autonomous vehicles, however, it is also necessary to consider passenger comfort in autonomous vehicles for the real use case of autonomous vehicles in the human society. In this paper, we explain and define the intelligence system that considers passengers comfort inside the moving autonomous vehicles is called comfort intelligence (CI). The concept of comfort intelligence includes anxiety (negative state) to the entertainments (positive state) in the real situation of using autonomous vehicles with passengers feeling. In this research field, anxiety reduction is mainly targeted. The anxiety comes from the variety of stress factors. In the field of vehicles, we defined stress in autonomous vehicles as Autonomous Vehicle Stress (AVS) to estimate, classify and consider reduction methods for reducing anxiety in the autonomous vehicles. Moreover, anxiety is not only about stress but also have to consider motion sickness. Since many people release from driving, it is difficult to understand vehicles behavior. In addition, people tend to spend more time on entertainments that will increase car sickness more often. Furthermore, many car accessory companies tend to develop HUD by using VR or AR for better navigation in the future. This trends, however, also increase VR sickness. In our ideas, there will be new motion sickness called Autonomous Vehicle Motion Sickness (AVMS) that is mixed with car sickness and VR sickness in the future autonomous vehicle environment. Therefore, in this paper, explain the importance of considering comfort intelligence (CI) by looking at previous researches of anxiety, as well as discussing stress and motion sickness perspective.},
keywords={human factors;mobile robots;road vehicles;comfort intelligence;passenger comfort;autonomous vehicle motion sickness;autonomous vehicle stress;anxiety reduction;VR sickness;car sickness;Stress;Autonomous vehicles;Physiology;Indexes;Stress measurement;Wheelchairs;Heart rate;Comfort intelligence;Autonomous vehicles;Stress;Motion sickness;Physiological indexes},
doi={10.1109/ISMAR-Adjunct.2018.00102},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699234,
author={S. {Ikeda} and I. {Takemura} and A. {Kimura} and F. {Shibata}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Diminished Reality System Based on Open-Source Software for Self-Driving Mobility},
year={2018},
volume={},
number={},
pages={354-357},
abstract={The diminished reality (DR) techniques that visualize blind areas in road environments are expected to prevent accidents and to reduce passengers' stress or anxiety. However, the feasibility of such techniques is still unclear because most researches on DR for road environments are based on the assumption of the availability of specific sensor arrangements and infrastructures, which are not guaranteed to spread in the future. In this research, we propose a novel design to implement a DR system for rendering ghosted hidden background areas using various sensor data for self-driving. Our major assumption is that a number of automotive vehicles run around the world in the near future and their sensors and program modules are available for other purposes. In our experiments, we confirmed that hidden area can be visualized by using such data and modules.},
keywords={automobiles;data visualisation;driver information systems;public domain software;road accidents;road safety;road vehicles;traffic engineering computing;virtual reality;DR system;diminished reality system;open-source software;self-driving mobility;diminished reality techniques;road environments;accident prevention;ghosted hidden background areas rendering;automotive vehicles;blind areas visualization;data visualization;Cameras;Three-dimensional displays;Laser radar;Vehicle dynamics;Roads;Rendering (computer graphics);Robot sensing systems;Human-centered computing—Human computer interaction—Interaction paradigms-Mixed/augmented reality;Modeling and simulation—Computer graphics—Graphics systems and interfaces—Mixed/augmented reality},
doi={10.1109/ISMAR-Adjunct.2018.00103},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699251,
author={},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={ISMAR 2018 Science and Technology Program Committee Members},
year={2018},
volume={},
number={},
pages={xxv-xxv},
abstract={ISMAR 2018 Science and Technology Program Committee Members},
keywords={},
doi={10.1109/ISMAR-Adjunct.2018.00011},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699235,
author={P. {Lindemann} and T. {Lee} and G. {Rigoll}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Supporting Driver Situation Awareness for Autonomous Urban Driving with an Augmented-Reality Windshield Display},
year={2018},
volume={},
number={},
pages={358-363},
abstract={Urban traffic environments can create unpredictable hazardous situations. Even future, highly reliable automated cars (ACs) could be forced to execute sudden, surprising maneuvers causing driver discomfort and possibly a loss of trust. A counter measure might be to support driver situation awareness (SA) in such scenarios. We simulated an explanatory windshield display (WSD) interface for ACs to increase driver SA in a mixed-reality driving simulation. The interface combines screen-fixed elements and world-registered augmented reality (AR) overlays. We conducted a user study to evaluate the interface in which participants encountered critical urban driving situations and answered SA test questions during on-line freeze probes. We found a significant improvement of driver SA with the WSD interface in clear weather conditions (medium effect) as well as in bad visibility conditions (large effect).},
keywords={augmented reality;automotive components;display devices;driver information systems;road safety;road traffic;road vehicles;driver situation awareness;autonomous urban driving;augmented-reality windshield display;urban traffic environments;unpredictable hazardous situations;highly reliable automated cars;sudden maneuvers;surprising maneuvers;driver discomfort;explanatory windshield display interface;driver SA;mixed-reality driving simulation;screen-fixed elements;critical urban driving situations;WSD interface;AC;Automobiles;Roads;Visualization;Automation;Urban areas;Bars;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Mixed / augmented reality;Human-centered computing—Interaction design},
doi={10.1109/ISMAR-Adjunct.2018.00104},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699256,
author={A. {Dey} and M. {Billinghurst} and G. {Welch} and E. {Rojas-Muñoz}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={3rd Virtual and Augmented Reality for Good (VAR4Good) Workshop},
year={2018},
volume={},
number={},
pages={364-364},
abstract={Virtual Reality (VR) and Augmented Reality (AR) are becoming mainstream. With the research and technological advances, it is now possible to use these technologies in almost all domains and places. This provides a bigger opportunity to create applications that intend to impact society in greater ways than beyond just entertainment. Today the world is facing different challenges including healthcare, environment, and education. Now is the time to explore how VR/AR might be used to solve widespread societal challenges. The third Virtual and Augmented Reality for Good (VAR4Good) workshop will bring together researchers, developers, and industry partners in presenting and promoting research that intends to solve real-world problems using VR/AR. The workshop will provide a platform to grow a research community that discusses challenges and opportunities to create Virtual and Augmented Reality for Good. We invite application and position papers (2-4 pages, excluding references), that address the way that VR/AR technologies can solve real-world problems in various application domains including, but not limited to, health, the environment, education, sports, the arts, and applications in support of special needs such as assistive, adaptive, and rehabilitative applications. Our focus and preference will be on applications that are beyond general uses of VR/AR. Please see full CFP on our website.},
keywords={augmented reality;human computer interaction;technology management;VAR4Good;technological advances;Virtual and Augmented Reality for Good workshop;VR/AR technologies;Augmented reality;Conferences;Australia;Education;Entertainment industry;Medical services},
doi={10.1109/ISMAR-Adjunct.2018.00105},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699186,
author={J. A. {Fisher} and J. {David Bolter}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Ethical Considerations for AR Experiences at Dark Tourism Sites},
year={2018},
volume={},
number={},
pages={365-369},
abstract={There are a number of Augmented Reality (AR) experiences that are situated or related to sites of pain, suffering, and death. The ethical design, development, and facilitation of these experiences has not been addressed by the current AR scholarship. To fill this knowledge gap, this paper presents foundational principles for how AR can be ethically designed to facilitate a respectful experience. A niche form of tourism studies, dark tourism is used as a universal term for any form of tourism that is related to death, suffering, atrocity, tragedy or crime. Based on this research, the paper proposes some suggestions for creating ethical AR experiences at dark tourism sites before, during, and after a visit. AR can become an ethical and powerful extension of reflecting upon mortality if spectacle is moderated at these dark sites. The paper concludes with design suggestions for ethical AR experiences at dark tourism sites already engaged in the commodification and consumption of the macabre.},
keywords={augmented reality;ethical aspects;travel industry;AR experiences;ethical considerations;ethical extension;dark tourism sites;tourism studies;ethical design;Intelligent agents;History;Visualization;Augmented reality;Pain;Industries;Media;Tourism;ethical design;augmented reality;[Social Implications of Technology]: Ethics—Technology Social Factors;[The Computing Profession]: Miscellaneous—Ethics},
doi={10.1109/ISMAR-Adjunct.2018.00106},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699276,
author={N. {Rodriguez}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Identifying Accessibility Conditions for Children with Multiple Disabilities: A Virtual Reality Wheelchair Simulator},
year={2018},
volume={},
number={},
pages={370-372},
abstract={Training is one of the main domain applications of Virtual Reality (VR). Simulation and visual realism provide training situations very close to practice with real systems while reducing cost and with greater safety. Furthermore, VR offers the possibility of change time or space scales, visualize from different perspectives, experience inaccessible real environments, all under the user's control, without risks, at her own pace. This allows to develop skills and to have confidence to work thereafter in real conditions with real equipment. Interaction technologies are now more widely available and affordable. But generally devices are conceived for “standard” people leaving behind people with impairments and further accentuating the digital gap. In this paper, we present our work in the development of an accessible wheelchair simulator designed to allow children with multiple disabilities to familiarize themselves with the wheelchair, and practitioners to better understand children capabilities.},
keywords={computer based training;computer simulation;handicapped aids;virtual reality;wheelchairs;accessibility conditions;VR;visual realism;interaction technologies;accessible wheelchair simulator;virtual reality wheelchair simulator;children with multiple disabilities;Wheelchairs;Virtual reality;Tools;Solid modeling;Visualization;Games;Adaptation models;virtual reality;simulator;disability;multiple disabilities;wheelchair;learning;augmented and alternative communication;interaction devices;I.3.1 [Computer Graphics]: Hardware Architecture — Input devices;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism – Virtual reality;H.5.1 [Information Interfaces And Presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities;H.5.2 [Information Interfaces And Presentation]: User Interfaces — Input devices and strategies},
doi={10.1109/ISMAR-Adjunct.2018.00107},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699184,
author={S. {Cöté} and J. {St-Pierre}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Interfacing with Global Collective Intelligence Using Virtual Assistants},
year={2018},
volume={},
number={},
pages={373-376},
abstract={The evolution of our society and of humans as individuals is hindered by the widely distributed aspect of human knowledge. Humans often need to re-invent the wheel, because they are unaware of existing solutions already found by many other humans. In this paper, we propose the concept of an Artificial Intelligence, interfaced through Intelligent Virtual Agents displayed in Augmented Reality, that would collect all human knowledge, and distribute it freely to all humans, when they need it. We argue that the proposed system would have significant very positive impacts on Humanity, and put it on a path of peaceful enhanced evolution.},
keywords={artificial intelligence;augmented reality;software agents;global collective intelligence;virtual assistants;artificial intelligence;intelligent virtual agents;augmented reality;Augmented reality;Intelligent Virtual Agents;Augmented Reality;Collective Intelligence;Human-centered computing-Mixed / augmented reality;Computing methodologies∼Intelligent agents},
doi={10.1109/ISMAR-Adjunct.2018.00108},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699167,
author={C. {Leuze} and G. {Yang} and B. {Hargreaves} and B. {Daniel} and J. A. {McNab}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Mixed-Reality Guidance for Brain Stimulation Treatment of Depression},
year={2018},
volume={},
number={},
pages={377-380},
abstract={Depression affects more than 16 million American adults and more than half do not respond to medication. Transcranial magnetic stimulation (TMS) is an important anti-depressant treatment that targets specific brain circuits responsible for mood and behavior. TMS efficacy and risk is strongly linked to correct TMS coil placement and can be significantly improved by accurate neuronavigation. In this paper, we present tools for the development of a novel mixed reality neuronavigation setup that allows the TMS operator to view the patient's brain anatomy directly overlaid on the head. This is performed by integrating patient tracking and visualization of brain magnetic resonance imaging (MRI) to provide a streamlined visualization of the patient's anatomy in a single immersive environment.},
keywords={biomedical MRI;brain;neurophysiology;patient treatment;transcranial magnetic stimulation;brain stimulation treatment;depression;medication;transcranial magnetic stimulation;specific brain circuits;mood;TMS coil placement;TMS operator;patient tracking;brain magnetic resonance imaging;antidepressant treatment;neuronavigation;mixed reality neuronavigation setup;Head;Cameras;Magnetic resonance imaging;Three-dimensional displays;Data visualization;Rendering (computer graphics);Brain;Depression;Hololens;Medicine;Mental Health},
doi={10.1109/ISMAR-Adjunct.2018.00109},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699173,
author={R. {Horst} and R. {Dörner}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Opportunities for Virtual and Mixed Reality Knowledge Demonstration},
year={2018},
volume={},
number={},
pages={381-385},
abstract={Universities are more and more expected to disseminate research findings to the general public and specific stakeholders especially in the economy (“third mission”). Since this third mission also affects third party research funding within recent decades, j scientists do need to be able to service both educational and marketing related purposes within their presentations. Moreover, scientists need to address heterogenous audiences. To meet such demanding requirements, Virtual Reality (VR) and Mixed Reality (MR) technology could serve as a valuable medium. In this position paper, we explore opportunities concerning VR and MR for what we refer to as `Knowledge Demonstration' (KD), based on a state of the art literature analysis. Furthermore, we propose aspects to consider when planning virtually augmented KD systems.},
keywords={educational institutions;technical presentation;virtual reality;universities;virtually augmented KD systems;virtual reality technology;marketing related purposes;educational related purposes;mixed reality knowledge demonstration;mixed reality technology;Education;Collaboration;Augmented reality;Face;Tools;Videos;Human-centered computing—Interaction paradigms—Mixed / augmented reality;Human-centered computing—Interaction paradigms—Virtual reality;Applied computing—Education—Computer-assisted instruction;Applied computing—Operations research—Marketing},
doi={10.1109/ISMAR-Adjunct.2018.00110},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699296,
author={H. {Engelbrecht} and S. G. {Lukosch}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Viability of Augmented Content for Field Policing},
year={2018},
volume={},
number={},
pages={386-389},
abstract={This paper describes the design and evaluation of a prototype for mobile information provisioning in augmented reality (AR) for field officers of the Dutch police. Five different fictional cases were constructed in cooperation with officers from the Dutch police. These cases were comprised of dynamic as well as static hotspots that would occur naturally during field work. Three different versions of the early prototype were tested using the method of heuristic evaluation. The application was shown to three experts from two police departments. Evaluation of the heuristics, possible future improvements as well as AR viability considerations for field policing are discussed.},
keywords={augmented reality;mobile computing;police data processing;static hotspots;dynamic hotspots;augmented reality;mobile information provisioning;augmented content;field policing;AR viability considerations;police departments;heuristic evaluation;Dutch police;Law enforcement;Cameras;Meters;Prototypes;Augmented reality;Smart phones;Collaboration;Human-centered computing—Interaction paradigms—Mixed/augmented reality—;Human-centered computing—HCI design and evaluation methods—Usability testing},
doi={10.1109/ISMAR-Adjunct.2018.00111},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699203,
author={C. {Zimmer} and N. {Ratz} and M. {Bertram} and C. {Geiger}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={War Children: Using AR in a Documentary Context},
year={2018},
volume={},
number={},
pages={390-394},
abstract={The goal of the project “War Children” is to tell stories of survivors of the Second World War by means of augmented reality. We want to make memories persistent, accessible and comprehensible to users that do not yet have access to these memories, e.g., digital natives. The application of immersive technologies provides us with new ways to tell stories about the past in an empathic way by augmenting the narration with audio-visual assets. In order to provide an immersive reference to the story, the user can visually place a contemporary witness in his environment using a mobile device. As a result, the user can listen to the respective witness sitting in front of him / her and beginning to describe their wartime experiences as a child. During the narration, the user's real environment is augmented by AR content that illustrates the narration. The project was developed as the result of an iterative design process for WDR, a German public broadcasting institution. An initial prototype was presented at the re:publica 2018 festival to the public and tested by more than 500 people. 160 of them were interviewed and provided feedback about their experience.},
keywords={augmented reality;history;mobile computing;multimedia computing;War Children;documentary context;augmented reality;memories;digital natives;immersive technologies;narration;audio-visual assets;immersive reference;contemporary witness;mobile device;Second World War survivors;wartime experiences;AR content;iterative design process;German public broadcasting institution;Cameras;Three-dimensional displays;Image color analysis;Visualization;Augmented reality;Calibration;Production;Augmented Reality—AR Documentation—Documentary Context;—Mixed Reality—Human-centered storytelling},
doi={10.1109/ISMAR-Adjunct.2018.00112},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699209,
author={},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={ISMAR 2018 Steering Committee Members},
year={2018},
volume={},
number={},
pages={xxvi-xxvi},
abstract={ISMAR 2018 Steering Committee Members},
keywords={},
doi={10.1109/ISMAR-Adjunct.2018.00012},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699185,
author={S. {Hegde} and G. {Garg} and R. {Perla} and R. {Hebbalaguppe}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={A Fingertip Gestural User Interface Without Depth Data for Mixed Reality Applications},
year={2018},
volume={},
number={},
pages={395-396},
abstract={Hand gestures are widely used in Human-Computer and Human Robotic Interfaces. Head mounted devices use gestures to communicate as evident on HoloLens, Meta, and ARCore/ARKit platform enabled smartphones. However, these devices are expensive mainly due to onboard powerful processors and sensors such as multiple cameras, depth and IR sensors that process hand gestures. To enable mass market reach via inexpensive MR headsets without built-in depth or IR sensors, we propose a real-time, in-air gestural framework that works on monocular RGB input alone. We use fingertip for writing in air analogous to a pen on paper. The major challenge in training egocentric gesture recognition models is in obtaining sufficient labeled data for end-to-end learning. Thus, we design a cascade of networks, consisting of a CNN with differentiable spatial to numerical transform (DSNT) layer, for fingertip regression, followed by a Bidirectional Long Short-Term Memory (Bi-LSTM), for a real-time pointing hand gesture classification. The framework takes 1.73s to run end-to-end and has a low memory footprint of 14MB facilitating easy portability on a smart-phone while achieving an accuracy of 88.0% on egocentric video dataset.},
keywords={convolutional neural nets;gesture recognition;image classification;image colour analysis;learning (artificial intelligence);numerical analysis;regression analysis;transforms;user interfaces;end-to-end learning;fingertip regression;real-time pointing hand gesture classification;fingertip gestural user interface;mixed reality applications;IR sensors;in-air gestural framework;monocular RGB input;training egocentric gesture recognition models;human-computer interface;cameras;human robotic interfaces;ARCore-ARKit platform;HoloLens;Meta platform;smartphones;depth sensor;MR headsets;CNN;differentiable spatial to numerical transform layer;DSNT layer;bidirectional long short-term memory;Bi-LSTM;egocentric video dataset;time 1.73 s;memory size 14.0 MByte;Heating systems;Sensors;Augmented reality;Head;Google;Real-time systems;Maintenance engineering;H.5.1 [Information Interfaces and Presentation]: Artificial, Augmented and Virtual Realities—;I.4.8 [Computing Methodologies]: Image Processing and Computer Vision—Scene Analysis},
doi={10.1109/ISMAR-Adjunct.2018.00113},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699170,
author={F. {Bérard} and T. {Louis}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={A Low-Latency, High-Precision Handheld Perspective Corrected Display},
year={2018},
volume={},
number={},
pages={397-398},
abstract={Perspective Corrected Displays (PCDs) show the perspective projection of a 3D scene computed from the viewer's position that is updated in real-time. By moving their head in front of the display, users perceive the motion parallax effect, a strong depth perception cue [6]. “fish-tank virtual reality” systems are well-known instantiation of PCDs involving a fixed planar display and stereo rendering is added to the motion parallax depth cue [5]. More recently, PCDs have been arranged to form small volumes such as a cube (i.e. gCu-bik [2] and pCubee [4]). This gives the illusion that the scene is contained inside the display. Because of the small size of the displays, users can rotate them in their hands, hence the Handheld in HPCD. This creates the strong illusion of holding the virtual scene in one's hands. In addition, rotating the display in hands requires much less effort than moving the whole body in front of the display. Hence, HPCDs allow users to experience more motion parallax than PCDs. However, the two previous implementations of cubic HPCDs required the presence of complex electronic on the display. As a result, they are connected with a thick wire to a rendering computer. This prevents to display on all faces as one of them is used for the connection. In addition, a significant part of the cube's sides is occluded by the displays' bezels.},
keywords={computer displays;human computer interaction;interactive devices;rendering (computer graphics);stereo image processing;three-dimensional displays;virtual reality;visual perception;perspective projection;motion parallax effect;strong depth perception cue;fish-tank virtual reality systems;fixed planar display;stereo rendering;motion parallax depth cue;strong illusion;virtual scene;rendering computer;low-latency;high-precision handheld perspective corrected display;HPCD;Three-dimensional displays;Rendering (computer graphics);Indexes;Games;Glass;Human factors;Computing methodologies;Computer graphics;Graphics systems and interfaces;Mixed / augmented reality Human-centered computing;Human computer interaction (HCI);Interaction devices;Displays and imagers},
doi={10.1109/ISMAR-Adjunct.2018.00114},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699282,
author={M. {Uimonen} and M. {Hakkarainen}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Accessing BIM-Related Information Through AR},
year={2018},
volume={},
number={},
pages={399-400},
abstract={The demonstration in hand is a facility management interface for handheld devices. It utilizes a combination of VTT's ALVAR point cloud tracking for accurate coupling of indoor location and information related to points of interest, and Google's ARCore for enhanced mobility and less restricted experience. The application can connect to a database of a Finnish commercial facility management operator Granlund Oy, allowing the user to instantly access and modify information related to a selected part of a Building Information Model (BIM). Such information includes temperature and flow rate of air conditioning, for example.},
keywords={augmented reality;building information modelling;Finnish commercial facility management operator Granlund Oy;BIM-related Information;facility management interface;handheld devices;building information model;Google ARCore;VTT ALVAR point cloud tracking;augmented reality;Augmented reality;Augmented and mixed reality;BIM;facility management},
doi={10.1109/ISMAR-Adjunct.2018.00115},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699297,
author={D. {Rumiński} and G. {Klinker}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Augmenting Mixed Reality Applications with the Vibro Motors Wearable},
year={2018},
volume={},
number={},
pages={401-402},
abstract={In this demo, we demonstrate the use of the mobile Vibro Motors Wearable device, and we explain how it can be used to enhance Mixed Reality applications with tactile human-machine hand interactions. The presented device is capable of providing tactile feedback while interacting with virtual objects, e.g., when elements of a 3D user interface are touched. The device enables to run vibro motors mounted on a user's fingers while interacting with synthetic objects. In order to control the vibe of a particular vibro motor in a wireless manner, we have developed an HTTP-based API. The API can be used to develop mobile-tactile Mixed Reality applications to enhance user experience by giving an impression of fading in/out effects when interacting with 3D objects. To show the device's capabilities, a 3D demo application has been developed in which a user can experience tactile feedback effects while interacting with a virtual object.},
keywords={application program interfaces;augmented reality;haptic interfaces;mobile computing;synthetic objects;particular vibro motor;HTTP-based API;user experience;tactile feedback effects;virtual object;human-machine hand interactions;mobile vibro motors wearable device;mobile-tactile mixed reality applications;augmenting mixed reality applications;tactile human-machine hand interactions;3D user interface;3D demo application;Tactile sensors;Three-dimensional displays;Augmented reality;Indexes;Hardware;Man-machine systems;Hardware [Communication hardware;interfaces and storage]: Wireless devices—Sensors and actuators;Human-centered computing [Mixed / augmented reality]},
doi={10.1109/ISMAR-Adjunct.2018.00116},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699221,
author={S. {Rothe} and T. {Höllerer} and H. {Hußmann}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={CVR-Analyzer: A Tool for Analyzing Cinematic Virtual Reality Viewing Patterns},
year={2018},
volume={},
number={},
pages={403-404},
abstract={Cinematic Virtual Reality (CVR) has been increasing in popularity over the last years. In Cinematic Virtual Reality (CVR) the viewer watches omnidirectional movies using a head-mounted display or other VR devices. Thus, the viewer is positioned inside the scene, and can freely choose the direction of view. Accordingly, the viewer determines the visible section of the movie, and so it may happen that details, important for the story, are missed. During our research on user attention in CVR, we encountered many analytic demands and documented potentially useful features. This led us to developing an analyzing tool for omnidirectional movies: the CVR-Analyzer.},
keywords={helmet mounted displays;humanities;virtual reality;CVR-Analyzer;omnidirectional movies;head-mounted display;cinematic virtual reality;VR devices;Heating systems;Motion pictures;Tools;Virtual reality;Spatiotemporal phenomena;Gaze tracking;Data visualization;Cinematic Virtual Reality;360° movie;omnidirectional movie;eye tracking;head pose;heatmaps;H.5.1 [Multimedia Information Systems]: Artificial, augmented, and virtual realities;I.2.10 [Vision and Scene Understanding]: Video analysis},
doi={10.1109/ISMAR-Adjunct.2018.00117},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699208,
author={J. D. {Hart} and T. {Piumsomboon} and L. {Lawrence} and G. A. {Lee} and R. T. {Smith} and M. {Billinghurst}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Demonstrating Emotion Sharing and Augmentation in Cooperative Virtual Reality Games},
year={2018},
volume={},
number={},
pages={405-406},
abstract={For our demonstration, we present a prototype system for sharing and augmenting facial expression in cooperative social Virtual Reality (VR) games. We created two social VR games, “Bomb Defusal” and “Island Survivor”, to demonstrate our system for capturing and sharing facial expression between VR players through their avatar.},
keywords={avatars;computer games;emotion recognition;groupware;social VR games;VR players;emotion sharing;facial expression;cooperative virtual reality games;Bomb Defusal;Island Survivor;avatar;cooperative social virtual reality games;Games;Weapons;Avatars;Portable computers;Face;Prototypes;Emotion sharing;emotion augmentation;facial expression;virtual reality;computer games;H.5.1. Information interfaces and presentation (e.g. HCI): Multimedia Information Systems—Artificial;augmented;and virtual realities},
doi={10.1109/ISMAR-Adjunct.2018.00118},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699189,
author={Y. {Yan} and B. {Bout} and A. {Berthelier} and X. {Naturel} and T. {Chateau}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Face Parsing for Mobile AR Applications},
year={2018},
volume={},
number={},
pages={407-408},
abstract={Face parsing is a segmentation task over facial components, which is very important for a lot of facial augmented reality applications. We present a demonstration of face parsing for mobile platforms such as iPhone and Android. We design an efficient fully convolutional neural network (CNN) in an hourglass form that is adapted to live face parsing. The CNN is implemented on the iPhone with the CoreML framework. In order to visualize the segmentation results, we superpose a mask with false colors so that the user can have an instant AR experience.},
keywords={augmented reality;convolutional neural nets;face recognition;image segmentation;mobile computing;face parsing;mobile AR applications;facial components;facial augmented reality applications;fully convolutional neural network;segmentation task;iPhone;CoreML framework;Face;Image segmentation;Semantics;Face detection;Hair;Conferences;Face Parsing;Mobile AR;Semantic Segmentation;Video Segmentation;Computer Vision},
doi={10.1109/ISMAR-Adjunct.2018.00119},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699298,
author={D. {Hompapas} and C. {Sandor} and A. {Plopski} and D. {Saakes} and D. H. {Yun} and T. {Taketomi} and H. {Kato}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={HoloRoyale: A Large Scale High Fidelity Augmented Reality Game},
year={2018},
volume={},
number={},
pages={409-410},
abstract={Recent years saw an explosion in Augmented Reality (AR) experiences for consumers. These experiences can be classified based on the scale of the interactive area (room vs city/global scale), or the fidelity of the experience (high vs low) [4]. Experiences that target large areas, such as campus or world scale [6], [7], commonly have only rudimentary interactions with the physical world, and suffer from registration errors and jitter. We classify these experiences as large scale and low fidelity. On the other hand, various room sized experiences [5], [8] feature realistic interaction of virtual content with the real world. We classify these experiences as small scale and high fidelity.},
keywords={Games;Augmented reality;Synchronization;Servers;Navigation;Three-dimensional displays;Drones;Human-centered Computing;Mixed/Augmented Reality},
doi={10.1109/ISMAR-Adjunct.2018.00120},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699278,
author={C. {Elvezio} and P. {Amelot} and R. {Boyle} and C. I. {Wes} and S. {Feiner}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Hybrid UIs for Music Exploration in AR and VR},
year={2018},
volume={},
number={},
pages={411-412},
abstract={We present hybrid user interfaces that facilitate interaction with music content in 3D, using a combination of 2D and 3D input and display devices. Participants will explore an online music library, some wearing AR or VR head-worn displays used alone or in conjunction with touch screens, and others using only touch screens. They will select genres, artists, albums and songs, interacting through a combination of 3D hand-tracking and 2D multi-touch technologies.},
keywords={augmented reality;human computer interaction;music;touch sensitive screens;user interfaces;2D multitouch technologies;music exploration;hybrid user interfaces;music content;display devices;online music library;touch screens;3D hand-tracking;hybrid UI;AR;VR;Three-dimensional displays;User interfaces;Touch sensitive screens;Two dimensional displays;Mobile handsets;Augmented reality;Libraries;Human-centered computing—Visualization—Visualization techniques—Treemaps;Human-centered computing—Visualization—Visualization design and evaluation methods},
doi={10.1109/ISMAR-Adjunct.2018.00121},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699310,
author={},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Reviewers},
year={2018},
volume={},
number={},
pages={xxvii-xxvii},
abstract={Reviewers},
keywords={},
doi={10.1109/ISMAR-Adjunct.2018.00013},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699243,
author={S. {Golodetz} and T. {Cavallari} and N. A. {Lord} and V. A. {Prisacariu} and D. W. {Murray} and P. H. S. {Torr}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Live Collaborative Large-Scale Dense 3D Reconstruction Using Consumer-Grade Hardware},
year={2018},
volume={},
number={},
pages={413-414},
abstract={We present a real-time system for collaboratively reconstructing dense volumetric models of large 3D scenes (see Figure 1). Reconstructing such models is important for many tasks – e.g. content creation for films and games [10], augmented reality [19], cultural heritage preservation [21] and building information modelling [14] – but capturing large scenes can take significant time, and the risk of transient changes to the scene (e.g. people moving around) goes up as the capture time increases, corrupting the model and forcing the user to restart the capture. There are thus good reasons to want instead to split the capture into several shorter sequences, which can be captured either over multiple sessions or in parallel (by multiple users) and then joined to make the whole scene.},
keywords={Three-dimensional displays;Buildings;Collaboration;Real-time systems;Cameras;Servers;Hardware},
doi={10.1109/ISMAR-Adjunct.2018.00122},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699237,
author={J. R. {Puigvert} and T. {Krempel} and A. {Fuhrmann}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Localization Service Using Sparse Visual Information Based on Recent Augmented Reality Platforms},
year={2018},
volume={},
number={},
pages={415-416},
abstract={The ability to localize a device or user precisely within a known space, would allow many use cases on the context of location-based augmented reality. We propose a localization service based on sparse visual information using ARCore [4], a state-of-the-art augmented reality platform for mobile devices. Our service is constituted by two components: front-end and back-end. On the front-end, using the point cloud generated by ARCore as feature points, a corresponding binary keypoint descriptor algorithm like ORB [6] or FREAK [1] is computed to describe the place. On the back-end, this binary descriptor is searched in a map using the bags of binary words technique [3], responding with the position of the recognized place.},
keywords={augmented reality;mobile computing;point cloud;feature points;binary descriptor;localization service;sparse visual information;location-based augmented reality;mobile devices;augmented reality platforms;ARCore;binary keypoint descriptor algorithm;Visualization;Augmented reality;Simultaneous localization and mapping;Tracking;Three-dimensional displays;Prototypes;Computer architecture;Augmented Reality;Localization;SLAM;Recognition},
doi={10.1109/ISMAR-Adjunct.2018.00123},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699220,
author={S. {Ekneling} and T. {Sonestedt} and A. {Georgiadis} and S. {Yousefi} and J. {Chana}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Magestro: Gamification of the Data Collection Process for Development of the Hand Gesture Recognition Technology},
year={2018},
volume={},
number={},
pages={417-418},
abstract={The work presented in this demo, explores the enhancement of the data collection and data annotation processes via gamification. For the use case of Hand Tracking (HT) and Gesture Recognition (GR) we have created an Augmented Reality (AR) and Virtual Reality (VR) application that implements both the collection and annotation task. Similar to other popular “Simon Says” games such as Guitar hero, the game versions of the app were easily understood and used by users. Based on previous results, the game versions were widely adopted by the users because of their novelty and entertainment value.},
keywords={augmented reality;computer games;gesture recognition;object tracking;gamification;data collection process;data annotation processes;hand tracking;augmented reality;virtual reality application;hand gesture recognition technology;Magestro;Games;Data collection;Augmented reality;Task analysis;Three-dimensional displays;Gesture recognition;Augmented Reality;Virtual Reality;Gamification;Hand Gestures;Gamification—Gesture Recognition—;Data Collection—Data Collection App—Game—Application;ManoMotion—Virtual Reality (VR)—Data Annotation—;Gesture Analysis—Data Labeling—},
doi={10.1109/ISMAR-Adjunct.2018.00124},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699293,
author={S. {Jiddi} and P. {Hobert} and A. {Laurent} and M. {Fradet} and P. {Jouet} and C. {Baillard} and E. {Marchand}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Probeless and Realistic Mixed Reality Application in Presence of Dynamic Light Sources},
year={2018},
volume={},
number={},
pages={419-420},
abstract={In this work, we consider the challenge of achieving a coherent blending between real and virtual worlds in the context of a Mixed Reality (MR) scenario. Specifically, we have designed and implemented an interactive demonstrator that shows a realistic MR application without using any light probe. The proposed system takes as input the RGB stream of the real scene, and uses these data to recover both the position and intensity of light sources. The lighting can be static and/or dynamic and the geometry of the scene can be partially altered. Our system is robust in presence of specular effects and handles both uniform and/or textured surfaces.},
keywords={augmented reality;geometry;image colour analysis;light sources;lighting;dynamic light sources;coherent blending;virtual worlds;interactive demonstrator;realistic MR application;light probe;RGB stream;mixed reality scenario;Lighting;Virtual reality;Light sources;Geometry;Cameras;Surface texture;Three-dimensional displays;Scene Analysis;Photometry;Modeling;Shadows;Textures;Lighting;Reflectance;Mixed Reality},
doi={10.1109/ISMAR-Adjunct.2018.00125},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699239,
author={S. {Tsunezaki} and R. {Nomura} and T. {Komuro} and S. {Yamamoto} and N. {Tsumura}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Reproducing Material Appearance of Real Objects Using Mobile Augmented Reality},
year={2018},
volume={},
number={},
pages={421-422},
abstract={In this paper, we propose a system that can reproduce the material appearance of real objects using mobile augmented reality (AR). Our proposed system allows a user to manipulate a virtual object, whose model is generated from the shape and reflectance of a real object, using the user's own hand. The shape of the real object is reconstructed by integrating depth images of the object, which are captured using an RGB-D camera from different directions. The reflectance of the object is obtained by estimating the parameters of a reflectance model from the reconstructed shape and color images, assuming that a single light source is attached to the camera. We measured the shape and reflectance of some real objects and presented the material appearance of the objects using mobile AR. It was confirmed that users were able to obtain the perception of materials from changes in gloss and burnish of the objects by rotating the objects using their own hand.},
keywords={augmented reality;cameras;image colour analysis;image reconstruction;mobile computing;reproducing material appearance;mobile augmented reality;virtual object;integrating depth images;reflectance model;reconstructed shape;color images;RGB-D camera;AR;Shape;Cameras;Augmented reality;Image reconstruction;Light sources;Shape measurement;Color;reflectance property;reflectance measurement;object manipulation;mobile display;RGB-D camera;Human-centered computing—Human conputer interaction (HCI)—Interaction paradigms—Mixed / augmented reality},
doi={10.1109/ISMAR-Adjunct.2018.00126},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699323,
author={T. {Kurahashi} and R. {Sakuma} and K. {Zempo} and K. {Mizutani} and N. {Wakatsuki}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Retrospective Speech Balloons on Speech-Visible AR via Head-Mounted Display},
year={2018},
volume={},
number={},
pages={423-424},
abstract={It was confirmed speech balloon captioning shown in 3 dimensional space (such as Augmented Reality) were better than caption shown in 2 dimension screen with the aim of information insurance for people with hearing impairment in previous research. In this research, we reconfirmed that multi line captions contributes to the user's comprehension of the speech balloon contents better than the single line caption. This research has also found that the system's ability which enables the user to look back on the previous parts of the conversation leads to the improvement of the user experience. In addition, when the amount of sentences increases, the captions presented not as a log, but as multi speech balloons like comics are more natural and do not obstruct the user's view.},
keywords={augmented reality;handicapped aids;hearing;helmet mounted displays;speech processing;user experience;information insurance;hearing impairment;multiline captions;user experience;multispeech balloons;retrospective speech balloons;speech-visible AR;head-mounted display;augmented reality;speech balloon captioning;Resists;Speech recognition;Auditory system;Face;Head-mounted displays;Cameras;Augmented reality;Support for person with hearing impairment;speech recognition;scroll type caption;speech Balloon;Human-centered computing—Accessibility—Accessibility systems and tools;Human-centered computing— Interaction paradigms—Mixed / augmented reality},
doi={10.1109/ISMAR-Adjunct.2018.00127},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699255,
author={J. H. {Mueller} and P. {Voglreiter} and M. {Dokter} and T. {Neff} and M. {Makar} and M. {Steinberger} and D. {Schmalstieg}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Shading Atlas Streaming Demonstration},
year={2018},
volume={},
number={},
pages={425-426},
abstract={Streaming high quality rendering for virtual reality applications requires minimizing perceived latency. We introduce Shading Atlas Streaming (SAS) [1], a novel object-space rendering framework suitable for streaming virtual reality content. SAS decouples server-side shading from client-side rendering, allowing the client to perform framerate upsampling and latency compensation autonomously for short periods of time. The shading information created by the server in object space is temporally coherent and can be efficiently compressed using standard MPEG encoding. Our results show that SAS compares favorably to previous methods for remote image-based rendering in terms of image quality and network bandwidth efficiency. SAS allows highly efficient parallel allocation in a virtualized-texture-like memory hierarchy, solving a common efficiency problem of object-space shading. With SAS, untethered virtual reality headsets can benefit from high quality rendering without paying in increased latency. Visitors will be able to try SAS by roaming the exhibit area wearing a Snapdragon 845 based headset.},
keywords={data compression;image coding;rendering (computer graphics);virtual reality;image quality;network bandwidth efficiency;virtualized-texture-like memory hierarchy;object-space shading;untethered virtual reality headsets;high quality rendering;Shading Atlas Streaming demonstration;client-side rendering;standard MPEG encoding;parallel allocation;remote image-based rendering;object-space rendering;Synthetic aperture sonar;Rendering (computer graphics);Graphics processing units;Servers;Transform coding;Headphones;Games},
doi={10.1109/ISMAR-Adjunct.2018.00128},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699312,
author={C. {Sandor} and H. {Nakamura}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={SoliScratch: A Radar Interface for Scratch DJs},
year={2018},
volume={},
number={},
pages={427-427},
abstract={Scratching is a DJ (Disk Jockey) technique for producing rhythmic sounds by moving a record back and forth. It requires a high level of manual dexterity and its difficulty has been likened to playing an instrument. Our goals are: first, ease of use, so that novice users can start scratching immediately; second, to enable users to scratch while walking. We employ millimeter wave radar sensing technology to replace record movements with mid-air gestures that are captured at 1 kHz. We developed our systems in several iterations with professional DJs. The accompanying movie shows that we could achieve our goals.},
keywords={gesture recognition;millimetre wave radar;music;user interfaces;SoliScratch;radar interface;scratch DJs;scratching;DJ technique;rhythmic sounds;manual dexterity;mid-air gestures;professional DJs;disk jockey;ease of use;millimeter wave radar sensing technology;Millimeter wave radar;Sensors;Google;Legged locomotion;Motion pictures;Augmented reality},
doi={10.1109/ISMAR-Adjunct.2018.00129},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699201,
author={H. {Kim} and J. {Lee} and H. {Yeo} and A. {Quigley} and W. {Woo}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={SWAG Demo: Smart Watch Assisted Gesture Interaction for Mixed Reality Head-Mounted Displays},
year={2018},
volume={},
number={},
pages={428-429},
abstract={In this demonstration, we will show a prototype system with sensor fusion approach to robustly track 6 degrees of freedom of hand movement and support intuitive hand gesture interaction and 3D object manipulation for Mixed Reality head-mounted displays. Robust tracking of hand and finger with egocentric camera remains a challenging problem, especially with self-occlusion - for example, when user tries to grab a virtual object in midair by closing the palm. Our approach leverages the use of a common smart watch worn on the wrist to provide a more reliable palm and wrist orientation data, while fusing the data with camera to achieve robust hand motion and orientation for interaction.},
keywords={augmented reality;gesture recognition;helmet mounted displays;human computer interaction;image motion analysis;object tracking;sensor fusion;wearable computers;SWAG demo;smart watch assisted gesture interaction;sensor fusion approach;3D object manipulation;robust tracking;robust hand motion;finger tracking;intuitive hand gesture interaction;mixed reality head-mounted displays;robust hand movement tracking;egocentric camera;self-occlusion;palm orientation data;wrist orientation data;Augmented reality;Augmented Reality;Wearable Computing;3D User Interfaces;Hand Interaction;Virtual 3D Object Manipulation;H.5.2 [Information Interfaces and Presentation]: User interfaces—Input interaction styles},
doi={10.1109/ISMAR-Adjunct.2018.00130},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699222,
author={D. {Eckhoff} and C. {Sandor} and D. {Kalkoten} and U. {Eck} and C. {Lins} and A. {Hein}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={TutAR: Semi-Automatic Generation of Augmented Reality Tutorials for Medical Education},
year={2018},
volume={},
number={},
pages={430-431},
abstract={With Augmented Reality (AR) on Optical-See-Through-Head-Mounted Displays (OST-HMD), users can observe the real world and computer graphics at the same time. In this work, we present TutAR, a pipeline that semi-automatically creates AR tutorials out of 2D RGB videos. TutAR extracts relevant 3D hand motion from the input video. The derived motion will be displayed as an animated 3D hand relative to the human body and plays synchronously with the motion in the video on an OST-HMD.},
keywords={augmented reality;biomedical education;computer animation;medical image processing;video signal processing;TutAR;semiautomatic generation;Augmented Reality tutorials;medical education;Optical-See-Through-Head-Mounted Displays;OST-HMD;computer graphics;2D RGB videos;animated 3D hand;3D hand motion;Three-dimensional displays;Tutorials;Two dimensional displays;Trajectory;Augmented reality;Image reconstruction;Animation;Augmented Reality;Video tutorials;Motion extraction},
doi={10.1109/ISMAR-Adjunct.2018.00131},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699307,
author={},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={The XR Future - The Coming Utopia or a Gamer's Plaything},
year={2018},
volume={},
number={},
pages={xxviii-xxix},
abstract={Augmented and Virtual Reality have been hailed as "the next big thing" several times in the past 30 years. Some predict that it will be the next computing platform after mobile, while for others it brings to mind the 1990s Apple Newton--intriguing, but not yet ready to appeal beyond enthusiasts. This talk will (1) begin with a short, personal history of the first 50 years of AR and VR; (2) survey selected research at UNC (egocentric 3D reconstruction of user and surroundings, and AR display systems for low latency, high dynamic range, wide field of view and depth accommodation); and (3) conclude with a discussion of remaining problems in XR that need to be solved urgently if the field is to avoid the trough of disillusion and instead to grow into widespread use in everyone's daily life.},
keywords={},
doi={10.1109/ISMAR-Adjunct.2018.00014},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699228,
author={},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Author index},
year={2018},
volume={},
number={},
pages={433-438},
abstract={Presents an index of the authors whose articles are published in the conference proceedings record.},
keywords={},
doi={10.1109/ISMAR-Adjunct.2018.00132},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699206,
author={},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={IEEE Computer Society Technical Conference Activities Board},
year={2018},
volume={},
number={},
pages={440-440},
abstract={The world-renowned IEEE Computer Society publishes, promotes, and distributes a wide variety of authoritative computer science and engineering texts. These books are available from most retail outlets. Visit the CS Store at http://www.computer.org/portal/site/store/index.jsp for a list of products.},
keywords={},
doi={10.1109/ISMAR-Adjunct.2018.00133},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699331,
author={},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Sponsors and Supporters},
year={2018},
volume={},
number={},
pages={xxx-xxxii},
abstract={Sponsors and Supporters},
keywords={},
doi={10.1109/ISMAR-Adjunct.2018.00015},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699302,
author={},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Cognitive Aspects of Interaction in Virtual and Augmented Reality Systems (CAIVARS)},
year={2018},
volume={},
number={},
pages={xxxiii-xxxiii},
abstract={Augmented (AR) and Virtual Reality (VR) systems are designed to immerse humans into rich and compelling simulated environments by leveraging our perceptual systems (i.e. vision, touch, sound). In turn, exposure to AR/VR can reshape and alter our perceptual processing by tapping into the brain's significant ability to adapt to changes in the environment through neural plasticity. Therefore, to design successful AR/VR systems, we must first understand the functioning and limitations of our perceptual and cognitive systems. We can then tailor AR/VR technology to optimally stimulate our senses and maximize user experience. Understanding how to wield AR/VR tools to reshape how we perceive the world also has incredible potential for societal and clinical applications.},
keywords={augmented reality;brain;cognitive systems;perceptual systems;perceptual processing;brain;neural plasticity;cognitive systems;augmented reality systems;virtual reality systems},
doi={10.1109/ISMAR-Adjunct.2018.00016},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699214,
author={M. {Vosmeer}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Storytelling for Cinematic Virtual Reality},
year={2018},
volume={},
number={},
pages={xxxiv-xxxiv},
abstract={In this tutorial, I will present the VR projects that I have worked on at the Amsterdam University of Applied Sciences, for our research project named `Storytelling for 360° Media'. There are still many gaps in the knowledge on how to produce good VR content. Also, we actually don't know that much yet on how audiences react to VR, and how we can measure concepts like presence, engagement and immersion. In collaboration with industry partners and students, we have therefore set up a series of VR projects that were intended to explore the new film language that needs to be developed to fully benefit the possibilities of Cinematic VR. In every project, an element of storytelling has been explored. By reflecting on the results of our projects, I will share what we have learned about VR storytelling and how VR may be validated and evaluated. This tutorial is not a static summing up of theoretical insights, but rather a discussion of a set of cases and issues that have led to insights into storytelling for VR. I will share our research questions, reflect on what went wrong - and eventually share the new questions that have arisen while doing it.},
keywords={computer aided instruction;virtual reality;Amsterdam University;Applied Sciences;VR projects;Cinematic VR;VR storytelling;Cinematic virtual reality;Storytelling for 360° Media},
doi={10.1109/ISMAR-Adjunct.2018.00017},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699241,
author={D. {Borrmann} and A. {Nuechter} and T. {Wiemann}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Large-Scale 3D Point Cloud Processing for Mixed and Augmented Reality},
year={2018},
volume={},
number={},
pages={xxxv-xxxv},
abstract={The rapid development of 3D scanning technology combined with state-of-the-art mapping algorithms allows to capture 3D point clouds with high resolution and accuracy. The high amount of data collected with LiDAR, RGB-D cameras or generated through SfM approaches makes the direct use of the recorded data for realistic rendering and simulation problematic. Therefore, these point clouds have to be transformed into representations that fulfill the computational requirements for VR and AR setups. In this tutorial participants will be introduced to state-of-the-art methods in point cloud processing and surface reconstruction with open source software to learn the benefits for AR and VR applications by interleaved presentations, software demonstrations and software trials. The focus lies on 3D point cloud data structures (range images, octrees, k-d trees) and algorithms, and their implementation in C/C++. Surface reconstruction using Marching Cubes and other meshing methods will play another central role. Reference material for subtopics like 3D point cloud registration and SLAM, calibration, filtering, segmentation, meshing, and large scale surface reconstruction will be provided. Participants are invited to bring their Linux, MacOS or Windows laptops to gain hands-on experience on practical problems occuring when working with large scale 3D point clouds in VR and AR applications.},
keywords={augmented reality;computer graphics;octrees;public domain software;rendering (computer graphics);surface reconstruction;3D scanning technology;realistic rendering;open source software;3D point cloud data structures;3D point cloud registration;large scale 3D point cloud processing;large scale surface reconstruction;Marching Cubes;octrees;k-d trees;meshing methods;augmented reality},
doi={10.1109/ISMAR-Adjunct.2018.00018},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699277,
author={J. E. {Swan}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={The Replication Crisis in Empirical Science: Implications for Human Subject Research in Mixed Reality},
year={2018},
volume={},
number={},
pages={xxxvi-xxxvi},
abstract={This tutorial will first discuss the replication crisis in empirical science. This term was coined to describe recent significant failures to replicate empirical findings in both medicine and psychology. In many cases, over 50% of previously reported results could not be replicated. This fact has shaken the foundations of both fields: Can empirical results really be believed? Should, for example, medical decisions really be based on empirical research? After describing the crisis, the tutorial will revisit enough of the basics of empirical science to explain the origins of the replication crisis. The key issue is that hypothesis testing, which in empirical science is used to establish truth, is the result of a probabilistic process. However, the human mind is wired to reason absolutely: Humans have a difficult time understanding probabilistic reasoning. The tutorial will discuss some of the ways that funding agencies, such as the US National Institutes of Health (NIH), have responded to the replication crisis, by, for example, funding replication studies, and requiring that grant recipients publically post anonymized data. Finally, the tutorial will consider how the Virtual Environments community might respond to the replication crisis. In particular, in our community the reviewing process often considers work that involves systems, architectures, or algorithms. In these cases, the reasoning behind the correctness of the results is usually absolute. Therefore, the standard for accepting papers is that the finding exhibits novelty - to some degree, the result should be surprising. However, this standard does not work for empirical studies (which, typically, involve human experimental subjects). Because empirical reasoning is probabilistic, important results need to be replicated, sometimes multiple times, and by different laboratories. As the replications mount, the field is justified in embracing increasing belief in the results. In other words, consider a field that, in order to accept a paper reporting empirical results, always requires surprise: This is a field that will not progress in empirical knowledge. The tutorial will end with a call for the community to be more accepting of replication studies. In addition, the tutorial will consider whether actions taken by other fields, in response to the replication crisis, might also be recommendable for the Virtual Environments community.},
keywords={behavioural sciences computing;cognition;cognitive systems;inference mechanisms;psychology;replication crisis;empirical science;tutorial;empirical research;empirical reasoning;empirical knowledge;human subject research;mixed reality;psychology;probabilistic process;probabilistic reasoning;US National Institutes of Health;anonymized data;virtual environments community},
doi={10.1109/ISMAR-Adjunct.2018.00019},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699204,
author={},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Adjunct Proceedings of the 2018 IEEE International Symposium on Mixed and Augmented Reality},
year={2018},
volume={},
number={},
pages={1-1},
abstract={Adjunct Proceedings of the 2018 IEEE International Symposium on Mixed and Augmented Reality},
keywords={},
doi={10.1109/ISMAR-Adjunct.2018.00002},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699212,
author={S. {Mori} and J. {Herling} and W. {Broll} and N. {Kawai} and H. {Saito} and D. {Schmalstieg} and D. {Kalkofen}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={3D PixMix: Image Inpainting in 3D Environments},
year={2018},
volume={},
number={},
pages={1-2},
abstract={State of the art methods for diminished reality (DR) propagate pixel information from keyframes to later frames to achieve real-time image inpainting in 3D spaces. However, this approach assumes a planar scene and produces artifacts, if the scene geometry is not sufficiently planar. In this paper, we present 3D PixMix, a new real-time inpainting method that addresses non-planar scenes by considering both color and depth information in the inpainting process. We define cost functions for both the color and the geometric appearance in the inpainting scheme and use an RGB-D sensor for depth fusion using a SLAM. Comparison results against the conventional PixMix show that 3D PixMix obtains the equivalent or even better quality in 3D scenes with additional depth information.},
keywords={image colour analysis;image reconstruction;image restoration;image texture;SLAM (robots);planar scene;scene geometry;3D PixMix;real-time inpainting method;nonplanar scenes;inpainting process;inpainting scheme;conventional PixMix show;additional depth information;diminished reality propagate pixel information;real-time image inpainting;3D environments;Three-dimensional displays;Image color analysis;Rendering (computer graphics);Visualization;Cameras;Color;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, Augmented, and Virtual Realities;I.4.5 [Image Processing and Computer Vision]: Reconstruction—},
doi={10.1109/ISMAR-Adjunct.2018.00020},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699305,
author={C. {Lin} and D. {Andersen} and V. {Popescu} and E. {Rojas-Muñoz} and M. E. {Cabrera} and B. {Mullis} and B. {Zarzaur} and K. {Anderson} and S. {Marley} and J. {Wachs}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={A First-Person Mentee Second-Person Mentor AR Interface for Surgical Telementoring},
year={2018},
volume={},
number={},
pages={3-8},
abstract={This application paper presents the work of a multidisciplinary group of designing, implementing, and testing an Augmented Reality (AR) surgical telementoring system. The system acquires the surgical field with an overhead camera, the video feed is transmitted to the remote mentor, where it is displayed on a touch-based interaction table, the mentor annotates the video feed, the annotations are sent back to the mentee, where they are displayed into the mentee's field of view using an optical see-through AR head-mounted display (HMD). The annotations are reprojected from the mentors second-person view of the surgical field to the mentee's first-person view. The mentee sees the annotations with depth perception, and the annotations remain anchored to the surgical field as the mentee moves their head. Average annotation display accuracy is 1.22cm. The system was tested in the context of a user study where surgery residents ( n = 20) were asked to perform a lower-leg fasciotomy on cadaver models. Participants who benefited from telementoring using our system received a higher Individual Performance Score, and they reported higher usability and self confidence levels.},
keywords={augmented reality;biomechanics;helmet mounted displays;surgery;telemedicine;display accuracy;first-person mentee;second-person mentor AR interface;application paper;surgical field;video feed;remote mentor;touch-based interaction table;annotations;AR head-mounted display;second-person view;augmented reality surgical telementoring system;Surgery;Resists;Cameras;Calibration;Feeds;Visualization;Augmented reality;Human-centered computing;Human computer interaction;Interaction paradigms;Mixed / augmented reality},
doi={10.1109/ISMAR-Adjunct.2018.00021},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699313,
author={T. {Michel} and P. {Genevès} and N. {Layaïda}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={A Method to Quantitatively Evaluate Geo Augmented Reality Applications},
year={2018},
volume={},
number={},
pages={9-14},
abstract={We propose a method for quantitatively assessing the quality of Geo AR browsers. Our method aims at measuring the impact of attitude and position estimations on the rendering precision of virtual features. We report on lessons learned by applying our method on various AR use cases with real data. Our measurement technique allows to shedding light on the limits of what can be achieved in Geo AR with current technologies. This also helps in identifying interesting perspectives for the further development of high-quality Geo AR applications.},
keywords={augmented reality;Geo AR browsers;position estimations;rendering precision;virtual features;measurement technique;high-quality Geo AR applications;Geo augmented reality applications;Estimation;Wireless fidelity;Magnetometers;Smart phones;Magnetic sensors;Position measurement},
doi={10.1109/ISMAR-Adjunct.2018.00022},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699178,
author={B. {Huang} and S. {Ozdemir} and Y. {Tang} and C. {Liao} and H. {Ling}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={A Single-Shot-Per-Pose Camera-Projector Calibration System for Imperfect Planar Targets},
year={2018},
volume={},
number={},
pages={15-20},
abstract={Existing camera-projector calibration methods typically warp feature points from a camera image to a projector image using estimated homographies, and often suffer from errors in camera parameters and noise due to imperfect planarity of the calibration target. In this paper we propose a simple yet robust solution that explicitly deals with these challenges. Following the structured light (SL) camera-project calibration framework, a carefully designed correspondence algorithm is built on top of the De Bruijn patterns. Such correspondence is then used for initial camera-projector calibration. Then, to gain more robustness against noises, especially those from an imperfect planar calibration board, a bundle adjustment algorithm is developed to jointly optimize the estimated camera and projector models. Aside from the robustness, our solution requires only one shot of SL pattern for each calibration board pose, which is much more convenient than multi-shot solutions in practice. Data validations are conducted on both synthetic and real datasets, and our method shows clear advantages over existing methods in all experiments.},
keywords={calibration;cameras;optical projectors;parameter estimation;pose estimation;multishot solutions;single-shot-per-pose camera-projector calibration system;structured light camera-project calibration framework;imperfect planar calibration board;homography estimation;camera parameter estimation;camera-projector calibration methods;projector imaging models;SL camera-project calibration framework;De Bruijn patterns;bundle adjustment algorithm;Calibration;Cameras;Image color analysis;Bundle adjustment;Robustness;Encoding;Computing methodologies—Camera calibration;Computing methodologies—3D imaging;Computing methodologies—Reconstruction},
doi={10.1109/ISMAR-Adjunct.2018.00023},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699304,
author={J. {Gimeno} and S. {Casas} and C. {Portalés} and M. {Fernádez}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Addressing the Occlusion Problem in Augmented Reality Environments with Phantom Hollow Objects},
year={2018},
volume={},
number={},
pages={21-24},
abstract={Occlusion handling is essential to provide a seamless integration of virtual and real objects in AR applications. Different approaches have been presented with a variety of technologies, environment conditions and methods. Among these methods, 3D model-based occlusion approaches have been extensively used. However, these solutions could be too time-consuming in certain situations, since they must render all the occlusion objects even though they are invisible. For this reason, we propose an inverse 3D model-based solution for handling occlusions, designed for those AR applications in which virtual objects are placed inside a real object with holes or windows. With this restriction, the occlusion problem could be solved by rendering the geometry of transparent/hollow objects instead of rendering the opaque geometry. The method has been tested in a real case study with an augmented car in which the virtual content is shown in the interior of the vehicle. Results show that our method outperforms the traditional method, proving that this approach is an efficient option for solving the occlusion problem in certain AR applications.},
keywords={augmented reality;rendering (computer graphics);solid modelling;inverse 3D model-based solution;AR applications;virtual objects;occlusion problem;virtual content;augmented reality environments;phantom hollow objects;occlusion handling;seamless integration;real objects;environment conditions;model-based occlusion approaches;transparent objects;rendering;Augmented reality;Augmented Reality;occlusion;hollow;inverse phantom objects;depth rendering;offline 3D reconstruction;K.5.1 [Multimedia Information Systems]: Artificial, augmented, and virtual realities},
doi={10.1109/ISMAR-Adjunct.2018.00024},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699274,
author={B. M. {Williamson} and A. {Vargas} and P. {Garrity} and R. {Sottilare} and J. J. {LaViola}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={AgileSLAM: A Localization Approach for Agile Head Movements in Augmented Reality},
year={2018},
volume={},
number={},
pages={25-30},
abstract={Realistic augmented reality systems require both accurate localization of the user and a mapping of the environment. In a markerless environment this is often done with SLAM algorithms which, for localization, pick out features in the environment and compare how they have changed from keyframe to current frame. However, human head agility, such as seen in video gaming tasks or training exercises, poses a problem; fast rotations will cause all previously tracked features to no longer be within the field of view and the system will struggle to localize accurately. In this paper we present an approach that is capable of tracking a human head's agile movements by using an array of RGB-D sensors and a reconstruction of this sensor data into 360 degrees of features that is fed into our SLAM algorithm. We run an experiment with pre-recorded agile movement scenarios that demonstrate the accuracy of our system. We also compare our approach against single sensor algorithms and show a significant improvement (up to 15 to 20 times better accuracy) in localization. The development of our sensor array and SLAM algorithm creates a novel approach to accurately localize extremely agile human head movements.},
keywords={augmented reality;computer games;image colour analysis;image motion analysis;SLAM (robots);solid modelling;sensor data;RGB-D sensors;tracked features;fast rotations;video gaming tasks;human head agility;current frame;markerless environment;realistic augmented reality systems;agile head movements;AgileSLAM;extremely agile human head movements;sensor array;single sensor algorithms;pre-recorded agile movement scenarios;SLAM algorithm;Sensor arrays;Simultaneous localization and mapping;Tracking;Head;Sensor phenomena and characterization;Cameras;Human-centered computing—Human Computer Interaction—Interaction Paradigms—Mixed / Augmented Reality;Computing Methodologies—Artificial Intelligence—Computer Vision—Tracking},
doi={10.1109/ISMAR-Adjunct.2018.00025},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699330,
author={M. {Prilla} and L. M. {Rühmann}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={An Analysis Tool for Cooperative Mixed Reality Scenarios},
year={2018},
volume={},
number={},
pages={31-35},
abstract={In this paper, we want to introduce a novel mixed reality (MR) analysis tool that provides 3D reproductions of multiple actors using head-mounted AR devices in a cooperative setting. The tool is motivated by a challenge we are faced with when analysing AR support for cooperative scenarios: For this analysis, one needs information on people's movements and behaviour as well as on their interaction with digital information and objects. As other means of analysis proved insufficient for these purposes, we created and applied a novel tool which shows wearers of Head Mounted Displays (HMD) devices, the digital objects they interacted with and the actual setting they were in. The tool features several features for the analysis of these settings, which were predominately based on requirements stemming from two cases studies. In this paper, we present the tool, its most prominent features, and its application in two cases. To our knowledge, there is no other tool that includes similar features and means for analysis available, and we would like to discuss its benefits and possible applications with interested colleagues.},
keywords={augmented reality;helmet mounted displays;human computer interaction;cooperative mixed reality scenarios;head-mounted AR devices;Head Mounted Displays devices;mixed reality analysis tool;HMD devices;Tools;Task analysis;Resists;Three-dimensional displays;Search problems;Augmented reality;Mixed Reality;Augmented Reality;Analysis Tool;Cooperation;Human-Centered computing—Interaction paradigms—Mixed / augmented reality;Human-Centered computing-collaborative and social computing—collaborative and social computing systems and tools—open source software},
doi={10.1109/ISMAR-Adjunct.2018.00026},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699300,
author={P. {Lindemann} and T. {Lee} and G. {Rigoll}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={An Explanatory Windshield Display Interface with Augmented Reality Elements for Urban Autonomous Driving},
year={2018},
volume={},
number={},
pages={36-37},
abstract={One of the challenges in reaching wide-spread autonomous driving is the establishment of driver trust in the technology. We suggest a windshield display interface showing the perceptive abilities and decision-making of an automated car while driving. We took a human-centered design approach to determine user expectations and requirements. We present our resulting interface prototype which runs in a mixed-reality environment. We plan to evaluate its impact on situation awareness and trust in hard-to-predict urban scenarios.},
keywords={augmented reality;automobiles;decision making;driver information systems;road vehicles;user centred design;user interfaces;human-centered design;user requirements;windshield display interface;mixed-reality environment;user expectations;automated car;decision-making;driver trust;urban autonomous driving;augmented reality elements;Meteorology;Automobiles;Hazards;Prototypes;Automation;Augmented reality;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Mixed / augmented reality;Human-centered computing—Interaction design},
doi={10.1109/ISMAR-Adjunct.2018.00027},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699225,
author={J. {Zillner} and E. {Mendez} and D. {Wagner}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Augmented Reality Remote Collaboration with Dense Reconstruction},
year={2018},
volume={},
number={},
pages={38-39},
abstract={This paper describes an Augmented Reality remote collaboration system leveraging high-fidelity, dense scene reconstruction for intuitive and precise remote guidance. A local worker in need of help can use our system to automatically generate a 3D mesh of the surrounding and stream it to a remote expert. The remote expert can navigate and explore the reconstructed environment independently of the local worker in six degrees of freedom. World-stabilized text- and image-annotations can be placed in the scene and strokes drawn on surfaces are intelligently positioned in the world. In addition, the reconstruction allows the remote expert to segment colored objects from the mesh and use the resulting 3D model to create simple animations in order to convey precise instructions.},
keywords={augmented reality;computer animation;image colour analysis;image reconstruction;image segmentation;mesh generation;solid modelling;remote expert;dense scene reconstruction;augmented reality remote collaboration system;3D mesh generation;colored objects segmentation;3D model;Image reconstruction;Three-dimensional displays;Augmented reality;Collaboration;Glass;Real-time systems;Tools;Augmented Reality;CSCW;Remote Collaboration;Telepresence;Dense Reconstruction;Human-centered computing;Human computer interaction;Interaction paradigms;Mixed / augmented reality},
doi={10.1109/ISMAR-Adjunct.2018.00028},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699192,
author={},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Copyright Page},
year={2018},
volume={},
number={},
pages={1-1},
abstract={Copyright and Reprint Permissions: Abstracting is permitted with credit to the source. Libraries may photocopy beyond the limits of US copyright law, for private use of patrons, those articles in this volume that carry a code at the bottom of the first page, provided that the per-copy fee indicated in the code is paid through the Copyright Clearance Center, 222 Rosewood Drive, Danvers, MA 01923.},
keywords={},
doi={10.1109/ISMAR-Adjunct.2018.00003},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699244,
author={R. {Dukalski} and D. {Aschenbrenner} and M. {Dieben} and M. {Jongbloed} and J. {Verlinden}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Augmenting a Cardiology-Patient Doctor-Dialogue Through Integrated Heartbeat-Activated Holographic Display},
year={2018},
volume={},
number={},
pages={40-44},
abstract={The causes and treatment solutions of congenital heart defects are difficult to address and discuss between patient and doctor. This is mainly due to the complex spatial nature of congenital cardiac defects, which makes it difficult for the patients to envision the defect without prior anatomical knowledge and renders the comprehension largely dependent on doctors' (variable) skills to describe the anomaly. To improve communication, 3D printed hearts have been developed, yet these are expensive, difficult to manage for the large collection of defects, and require substantial oral explanation. In addition, the correlation with cardiac function remains rather abstract. Instead, we propose an augmented reality solution, involving a see-through head-mounted display (HMD) extended with a built-in heart rate monitor. In order to increase the presence and the conversational power, the heartbeat of the patient is used to drive an animation of a supersized, floating heart visualisation; enabling the user to inspect a specific heart condition from all sides. To enable this, a universal add-on casing was developed for the HoloLens. Heuristic analysis and pilot tests with 6+15 participants reveal limitations of the implementation and show that the solution does increase comprehension, although more has to be done to enable a robust system.},
keywords={augmented reality;cardiology;data visualisation;diseases;helmet mounted displays;holography;medical signal processing;patient monitoring;cardiology-patient doctor-dialogue;integrated heartbeat-activated holographic display;treatment solutions;congenital heart defects;congenital cardiac defects;3D printed hearts;substantial oral explanation;cardiac function;augmented reality solution;specific heart condition;see-through head-mounted display;built-in heart rate monitor;floating heart visualisation;Heuristic analysis;pilot tests;HoloLens;Heart beat;Medical services;Bluetooth;Thumb;Augmented reality;Three-dimensional displays;Augmented Reality;heart;heartbeat;Human-centered computing ∼ Mixed / augmented reality;Applied computing ∼ Consumer health;Computing methodologies ∼ Simulation tools},
doi={10.1109/ISMAR-Adjunct.2018.00029},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699219,
author={J. {Collins} and H. {Regenbrecht} and T. {Lanalotz}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Back to the Future: Constructivist Learning in Virtual Reality},
year={2018},
volume={},
number={},
pages={45-46},
abstract={A proposal was first made in 1971 for a study attempting to investigate radical constructivism as a valid learning theory, though the study was never formally conducted. This work describes our Virtual Reality interactive four-dimensional Hypercube system used as our investigative medium, and our initial implementation of the historic study proposal for validation. Our lessons learned are leading to further experimentation and investigation into learning applications in Virtual Reality.},
keywords={computer aided instruction;virtual reality;constructivist learning;radical constructivism;valid learning theory;virtual reality interactive four-dimensional hypercube system;Hypercubes;Virtual reality;Three-dimensional displays;Proposals;Task analysis;Visualization;Quaternions;Virtual Reality;Learning;Education;Pedagogy;H.5.1 [Information Interfaces and Presentation (e.g. HCI)]: Multimedia Information Systems — Artificial, augmented, and virtual realities},
doi={10.1109/ISMAR-Adjunct.2018.00030},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699207,
author={F. {Niebling} and M. E. {Latoschik}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Browsing Spatial Photography Using Augmented Models},
year={2018},
volume={},
number={},
pages={47-48},
abstract={Both digital and physical 3D models of buildings as well as historical photographs of architecture are used for a wide range of needs, from research in humanities and information technologies, museum contexts and library studies, to touristic applications. Spatially oriented photographs play an important role in visualizing and browsing contemporary as well as historical architecture, starting with the ground-breaking Photo Tourism project [4]. We present a technique to combine physical 3D models of buildings with spatially registered historical photographic documents in a hand-held Augmented Reality (AR) environment. Users are enabled to spatially explore historical views of architecture by selecting photos from a collection of images which are then utilized as textures for the physical model rendered on their respective mobile device. We compare different methods to select photos registered to a physical model in hand-held AR.},
keywords={augmented reality;data visualisation;digital photography;history;mobile computing;museums;photography;solid modelling;travel industry;browsing spatial photography;information technologies;museum contexts;library studies;touristic applications;spatially oriented photographs;historical architecture;spatially registered historical photographic documents;physical model;augmented models;hand-held augmented reality environment;ground-breaking photo tourism project;hand-held AR;Solid modeling;Three-dimensional displays;Buildings;Photography;Visualization;Cameras;Augmented reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality Human-centered computing;Visualization;Visualization application domains;Geographic visualization},
doi={10.1109/ISMAR-Adjunct.2018.00031},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699171,
author={J. P. {Freiwald} and N. {Katzakis} and F. {Steinicke}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Camera Time Warp: Compensating Latency in Video See-Through Head-Mounted-Displays for Reduced Cybersickness Effects},
year={2018},
volume={},
number={},
pages={49-50},
abstract={Camera Time Warp or “CamWarp” is an extension of current reprojection techniques for video see-through augmented reality (AR), which significantly reduces the registration error between captured real-world videos and rendered virtual images. Experiment participants were asked to report discomfort while moving their head in a Fitts' Law inspired pattern. Results suggest that CamWarp can reduce discomfort and cybersickness symptoms for all tested camera configurations. In a second experiment participants were asked to move physical objects on a projected path as quickly and precisely as possible. CamWarp had a positive effect on speed and accuracy.},
keywords={augmented reality;cameras;helmet mounted displays;rendering (computer graphics);video signal processing;augmented reality;registration error;CamWarp;camera time warp;cybersickness effects;video see-through head-mounted-displays;Fitts' law inspired pattern;Cameras;Head;Augmented reality;Streaming media;Resists;Analysis of variance;Human-centered computing—Human computer interaction (HCI)—Epirical studies in HCI×Cyersickness;Human-centered computing—Interaction paradigms×Mixed / augmented reality},
doi={10.1109/ISMAR-Adjunct.2018.00032},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699217,
author={U. {Fontana} and F. {Cutolo} and N. {Cattari} and V. {Ferrari}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Closed – Loop Calibration for Optical See-Through Near Eye Display with Infinity Focus},
year={2018},
volume={},
number={},
pages={51-56},
abstract={In wearable augmented reality systems, optical see-through near-eye displays (OST NEDs) based on waveguides are becoming a standard as they are generally preferred over solutions based on semi-reflective curved mirrors. This is mostly due to their ability to ensure reduced image distortion and sufficiently wide eye motion box without the need for bulky optical and electronics components to be placed in front of the user's face and/or onto the user's line of sight. In OST head-mounted displays (HMDs) the user's own view is augmented by optically combining it with the virtual content rendered on a two-dimensional (2D) microdisplay. For achieving a perfect combination of the light field in the real 3D world and the computer-generated 2D graphics projected on the display, an accurate alignment between real and virtual content must be yielded at the level of the NED imaging plane. To this end, we must know the exact position of the user's eyes within the HMD reference system. State-of-the-art methods models the eye-NED system as an off-axis pinhole camera model, and therefore include the contribution of the eyes' positions into the modelling of the intrinsic matrix of the eye-NED. In this paper, we will describe a method for robustly calibrating OST NEDs that explicitly ignore this assumption. To verify the accuracy of our method, we conducted a set of experiments in a setup comprising a commercial waveguide-based OST NED and a camera in place of the user's eye. We tested a set of different camera (or eye) positions within the eye box of the NED. The obtained results demonstrate that the proposed method yields accurate results in terms of real-to-virtual alignment, regardless of the position of the eyes within the eye box of the NEDs (Figure 1). The achieved viewing accuracy was of 1.85 ±1.37 pixels.},
keywords={augmented reality;calibration;helmet mounted displays;pose estimation;rendering (computer graphics);wearable computers;optical see-through near eye display;optical see-through near-eye displays;image distortion;closed-loop calibration;virtual content rendering;eye positions;visual augmented reality systems;off-axis pinhole camera model;eye-NED system;computer-generated 2D graphics;OST head-mounted displays;wearable augmented reality systems;Augmented reality;Near-Eye Display;Optical See-through;Calibration;Eye Position;OST-HMD;NED;parallax;image warping;homography induced by a plane},
doi={10.1109/ISMAR-Adjunct.2018.00033},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699273,
author={J. {Wang} and H. {Liu} and L. {Cong} and Z. {Xiahou} and L. {Wang}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={CNN-MonoFusion: Online Monocular Dense Reconstruction Using Learned Depth from Single View},
year={2018},
volume={},
number={},
pages={57-62},
abstract={Online dense reconstruction is a major task of Augmented Reality (AR) applications, especially for realistic interactions like collisions and occlusions. Monocular cameras are most widely used on AR equipment, however existing monocular dense reconstruction methods have poor performance in practical applications due to the lack of true depth. This paper presents an online monocular dense reconstruction framework using learned depth, which overcomes the inherent difficulties of reconstruction for low-texture regions or pure rotational motions. Firstly, we design a depth prediction network combined with an adaptive loss, so that our network can be extended to train on mixed datasets with various intrinsic parameters. Then we loosely combine depth prediction with monocular SLAM and frame-wise point cloud fusion to build a dense 3D model of the scene. Experiments validate that our depth prediction from a single view reaches a state-of-the-art accuracy on different benchmarks, and the proposed framework can reconstruct smooth, surface-clear and dense models on various scenes with dedicated point cloud fusion scheme. Furthermore, collision and occlusion detection are tested using our dense model in an AR application, which demonstrates that the proposed framework is particularly suitable for AR scenarios. Our code will be publicly available along with our indoor RGB-D dataset at: https://github.com/NetEaseAI-CVLab/CNN-MonoFusion.},
keywords={augmented reality;convolutional neural nets;image colour analysis;image fusion;image reconstruction;image texture;learning (artificial intelligence);SLAM (robots);depth learning;augmented reality applications;CNN-MonoFusion;low-texture region reconstruction;pure rotational motion reconstruction;indoor RGB-D dataset;AR application;occlusion detection;collision;dedicated point cloud fusion scheme;dense 3D model;frame-wise point cloud fusion;monocular SLAM;depth prediction network;online monocular dense reconstruction framework;monocular cameras;Three-dimensional displays;Simultaneous localization and mapping;Cameras;Image reconstruction;Adaptation models;Augmented reality;Solid modeling;Augmented reality;online dense reconstruction;monocular SLAM;monocular depth prediction;fully convolutional network;Computing methodologies—Computer graphics—Graphics systems and interfaces—Mixed / augmented reality;Computing methodologies—Machine learning—Learning paradigms—Supervised learning;Computing methodologies—Artificial intelligence—Computer vision—Computer vision problems},
doi={10.1109/ISMAR-Adjunct.2018.00034},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699328,
author={T. {Wang} and X. {Qin} and F. {Zhong} and X. {Tong} and B. {Chen} and M. C. {Lin}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Compact Object Representation of a Non-Rigid Object for Real-Time Tracking in AR Systems},
year={2018},
volume={},
number={},
pages={63-68},
abstract={Detecting moving objects in the real world with reliability, robustness and efficiency is an essential but difficult task in AR applications, especially for interactions between virtual agents and real pedestrians, motorcycles and more, where the spatial occupancy of non-rigid objects should be perceived. In this paper, a novel object tracking method using visual cues with pre-training is proposed to track dynamic objects in 2D online videos robustly and reliably. The object's area in images can be transformed to 3D spatial area in the physical world with some simple, well-defined constraints and priors, thus spatial collision of agents and pedestrians can be avoided in AR environments. To achieve robust tracking in a markerless AR environment, we first create a novel representation of non-rigid objects, which is actually the manifold of normalized sub-images of all the possible appearances of the target object. These sub-images, captured from multiple views and under varying lighting conditions, are free from any occlusion and can be obtained from both video sequences and synthetic image generation. Then, from the instance pool made up of these sub-images, a compact set of templates which can well represent the manifold is learned by our proposed iterative method using sparse dictionary learning. We ensure that this template set is complete by using an SVM-based sparsity detection method. This compact, complete set of templates is then used to track the target trajectory online in video and augmented reality (AR) systems. Experiments demonstrate the robustness and efficiency of our method.},
keywords={augmented reality;image motion analysis;image representation;image sequences;iterative methods;learning (artificial intelligence);object detection;object tracking;support vector machines;video signal processing;nonrigid object;moving objects;robust tracking;object representation;object tracking method;AR systems;2D online videos;video sequences;synthetic image generation;iterative method;sparse dictionary learning;SVM;augmented reality;Augmented reality;Computing methodologies—Mixed / augmented reality;Computing methodologies—Appearance and texture representations;Computing methodologies—Tracking;Computing methodologies—Spatial and physical reasoning},
doi={10.1109/ISMAR-Adjunct.2018.00035},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699260,
author={D. {Aschenbrenner} and M. {Rojkov} and F. {Leutert} and J. {Verlinden} and S. {Lukosch} and M. E. {Latoschik} and K. {Schilling}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Comparing Different Augmented Reality Support Applications for Cooperative Repair of an Industrial Robot},
year={2018},
volume={},
number={},
pages={69-74},
abstract={Digitization and the growing capabilities of data networks enable companies to perform tasks via remote support, which previously required service personnel to travel. But which mixed reality method leads to better results regarding human factors, grounding and performance criteria? This paper reports on a collaborative user study, in which a local worker is guided by a remote expert with the help of different augmented reality methods, specifically see-through HMD, spatial projection, and video-mixing tablet. The task to perform is the exchange of a controller in a switch cabinet of an industrial robot, a task rather typical for failure detection within the field. Our study was conducted in collaboration with a technician school, where 50 technician apprentices participated in our study. Our results show clear advantages of using augmented reality (AR) versus traditional conditions (audio, video, screenshot) to enable remote support. It further gives significant indications for using a projection based AR method.},
keywords={augmented reality;control engineering computing;helmet mounted displays;human factors;industrial robots;maintenance engineering;production engineering computing;robot vision;video signal processing;industrial robot;digitization;data networks;remote support;service personnel;human factors;grounding;performance criteria;collaborative user study;remote expert;video-mixing tablet;cooperative repair;mixed reality;augmented reality support;HMD;Task analysis;Maintenance engineering;Resists;Visualization;Augmented reality;Service robots;Human-centered computing—Visualization— Visualization techniques— Treemaps;Human-centered computing—Visualization—Visualization design and evaluation methods},
doi={10.1109/ISMAR-Adjunct.2018.00036},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699263,
author={N. {Phillips} and K. {Massey} and M. S. {Arefin} and J. E. {Swan}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Design and Calibration of an Augmented Reality Haploscope},
year={2018},
volume={},
number={},
pages={75-76},
abstract={Most augmented reality (AR) research is performed with commercially-available displays. However, these displays have unadjustable mechanical and optical properties, which limit the experimental questions that can be asked. In order to ask certain questions, it becomes necessary to build a custom display, using off-the-shelf optical components. In the field of visual perception, such devices are often developed, and are called haploscopes. In this paper, we describe the mechanical design of an AR haploscope, which can present virtual objects seen in augmented reality. In order to make accurate measurements, the haploscope must be carefully calibrated, but this calibration is quite difficult. Therefore, this abstract contributes a description of an AR haploscope, and outlines calibration procedures.},
keywords={augmented reality;calibration;optical engineering computing;visual perception;optical properties;experimental questions;custom display;off-the-shelf optical components;visual perception;mechanical design;AR haploscope;augmented reality haploscope;commercially-available displays;unadjustable mechanical properties;calibration procedures;virtual objects;Optical imaging;Calibration;Augmented reality;Lenses;Adaptive optics;Monitoring},
doi={10.1109/ISMAR-Adjunct.2018.00037},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699227,
author={P. {Wang} and S. {Zhang} and X. {Bai} and M. {Billinghurst} and W. {He} and L. {Zhang} and J. {Du} and S. {Wang}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Do You Know What I Mean? An MR-Based Collaborative Platform},
year={2018},
volume={},
number={},
pages={77-78},
abstract={The Mixed Reality (MR) technology can be used to create unique collaborative experiences. In this paper, we propose a new remote collaboration platform using MR and eye-tracking that enables a remote helper to assist a local worker in an assembly task. We present results from research exploring the effect of sharing virtual gaze and annotations cues in an MR-based projector interface for remote collaboration. The key advantage compared to other remote collaborative MR interfaces is that it projects the remote expert's eye gaze into the real worksite to improve co-presence. The prototype system was evaluated with a pilot study comparing two conditions: POINTER and ET (eye-tracker cues). We observed that the task completion performance was better in the ET condition. And that sharing gaze significantly improved the awareness of each other's focus and co-presence.},
keywords={augmented reality;gaze tracking;groupware;object tracking;user interfaces;video signal processing;virtual reality;collaborative platform;unique collaborative experiences;remote collaboration platform;eye-tracking;remote helper;local worker;assembly task;virtual gaze;annotations cues;projector interface;remote collaborative MR interfaces;remote expert;eye-tracker cues;task completion performance;mixed reality technology;ET;POINTER;MR;Augmented reality;Remote collaboration;Eye gaze;Augmented Reality;Mixed Reality;H.5.1 [User Experience Design]: Collaborative interfaces—Collaboration, augmented and virtual realities},
doi={10.1109/ISMAR-Adjunct.2018.00038},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699253,
author={},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Table of Contents},
year={2018},
volume={},
number={},
pages={v-xvii},
abstract={Table of Contents},
keywords={},
doi={10.1109/ISMAR-Adjunct.2018.00004},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699248,
author={P. {Mohan} and W. B. {Goh} and C. {Fu} and S. {Yeung}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={DualGaze: Addressing the Midas Touch Problem in Gaze Mediated VR Interaction},
year={2018},
volume={},
number={},
pages={79-84},
abstract={With the increasing acceptance of eye tracking as a viable interaction method for Virtual Reality (VR) headsets, thoughtful gaze interaction methods need to be carefully designed to meet common challenges such as the Midas Touch problem, where users unintentionally select onscreen objects by gazing upon them. This paper presents DualGaze, a novel interaction method in which users perform a distinctive two-step gaze gesture for object selection. Once users gaze upon an object that they wish to select, a confirmation flag pops up next to the object at a location where the users' gaze just passed through. This trajectory-adaptive flag placement strategy reduces the chance of unintentional confirmation by requiring a returning gaze back to the flag. We conducted a user study to compare the accuracy and selection speed of DualGaze and the popular gaze fixation method on a simple gaze-typing task. Our results show that DualGaze is significantly more accurate while maintaining a comparable selection speed that was observed to improve with familiarity of use.},
keywords={gaze tracking;human computer interaction;virtual reality;DualGaze;Midas Touch problem;gaze mediated VR interaction;eye tracking;onscreen objects;two-step gaze gesture;object selection;trajectory-adaptive flag placement strategy;virtual reality headsets;gaze fixation method;gaze interaction methods;Gaze tracking;Headphones;Task analysis;Virtual reality;Trajectory;Back;Reliability engineering;virtual reality;interaction methods;gaze interaction;Midas touch;eye tracking;H.5.2 [Information Interfaces and Presentation]: User Interfaces—Interaction styles},
doi={10.1109/ISMAR-Adjunct.2018.00039},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699299,
author={C. {Chen} and E. S. {Rosenberg}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Dynamic Omnidirectional Texture Synthesis for Photorealistic Virtual Content Creation},
year={2018},
volume={},
number={},
pages={85-90},
abstract={We present a dynamic omnidirectional texture synthesis (DOTS) approach for generating real-time virtual reality content captured using a consumer-grade RGB-D camera. Compared to a single fixed-viewpoint color map, view-dependent texture mapping (VDTM) techniques can reproduce finer detail and replicate dynamic lighting effects that become especially noticeable with head tracking in virtual reality. However, VDTM is very sensitive to errors such as missing data or inaccurate camera pose estimation, both of which are commonplace for objects captured using consumer-grade RGB-D cameras. To overcome these limitations, our proposed optimization can synthesize a high resolution view-dependent texture map for any virtual camera location. Synthetic textures are generated by uniformly sampling a spherical virtual camera set surrounding the virtual object, thereby enabling efficient real-time rendering for all potential viewing directions.},
keywords={cameras;image colour analysis;image reconstruction;image texture;rendering (computer graphics);virtual reality;dynamic omnidirectional texture synthesis;photorealistic virtual content creation;view-dependent texture mapping techniques;VDTM;virtual camera location;virtual object;virtual reality content;RGB-D camera;Cameras;Image color analysis;Solid modeling;Rendering (computer graphics);Trajectory;Image reconstruction;Three-dimensional displays;virtual reality;view-dependent texture mapping;virtual content creation;Computing methodologies—Computer Graphics—Graphics systems and interfaces—Virtual reality;Computing methodologies—Computing graphics—Image manipulation—Appearance and texture representations;Computing methodologies—Computer Graphics—Image manipulation—Image-based rendering},
doi={10.1109/ISMAR-Adjunct.2018.00040},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699319,
author={K. K. K. {Kwok} and A. K. T. {Ng} and H. Y. K. {Lau}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Effect of Navigation Speed and VR Devices on Cybersickness},
year={2018},
volume={},
number={},
pages={91-92},
abstract={“Cybertravel” in virtual reality (VR) system can easily provoke cyber-sickness as there are no vestibular cues available during visual optic flow. This study examined the effect of navigation speed as well as the use of different VR devices for navigation on cybersickness. Participants experience street navigation while they are standing still. Four conditions: CAVE (cave automatic virtual environment) and HMD (head-mounted display) x 10 m/s and 24 m/s, were tested while participants perform a counting task. Results showed that higher navigation speed leads to increase in ratings of severity of cybersickness measured by simulator sickness questionnaire (SSQ) and miserable score (MISC). A difference in cybersickness ratings between VR devices is also observed with experimental order effect.},
keywords={human computer interaction;virtual reality;head-mounted display;cybersickness ratings;virtual reality system;vestibular cues;visual optic flow;street navigation;cave automatic virtual environment;navigation speed;cybertravel;Navigation;Resists;Visualization;Virtual environments;Task analysis;Optical sensors;Motion sickness;VIMS;CAVE;imseCAVE;HMD;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Virtual reality;Computing methodologies—Computer graphics—Graphics systems and interfaces—Perception},
doi={10.1109/ISMAR-Adjunct.2018.00041},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699249,
author={X. {Yu} and D. {Weng} and J. {Guo} and H. {Jiang} and Y. {Bao}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Effect of Using HMDs for One Hour on Preteens Visual Fatigue},
year={2018},
volume={},
number={},
pages={93-96},
abstract={We designed a within-subject experiment to compare visual discomfort to preteen users caused by using head-mounted displays (HMD) and tablet computers for an hour. 18 participants younger than 13 years old were recruited to fulfill a series of similar painting tasks under both display conditions. Visual fatigue was measured with visual analog scale before and after experiment and during the break of experiment. The results indicated that HMD had a trend to bring higher visual fatigue than tablet computer during the exposure of 1 hour. Although the symptoms of visual discomfort disappeared after resting, there is need for preteen-specific head-mounted displays.},
keywords={age issues;ergonomics;helmet mounted displays;human factors;notebook computers;HMD;visual discomfort;preteen users;tablet computer;visual fatigue;head-mounted displays;Visualization;Fatigue;Task analysis;Resists;Tablet computers;Atmospheric measurements;Particle measurements;vision;head-mounted display;virtual reality;visual fatigue},
doi={10.1109/ISMAR-Adjunct.2018.00042},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699200,
author={L. {Wang} and A. {Cao} and Z. {Li} and X. {Yang} and V. {Popescu}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Effective Free Field of View Scene Exploration in VR and AR},
year={2018},
volume={},
number={},
pages={97-102},
abstract={We propose to improve virtual reality (VR) and optical see-through augmented reality (AR) head-mounted display scene exploration efficiency by allowing the user to adapt the field of view interactively. This way the user can zoom in to examine parts of the scene in more detail without having to translate the viewpoint forward, as would be required in conventional fixed field of view scene exploration. The user can also zoom out, to gain a more comprehensive view of the scene and to examine distant parts of the scene in parallel, without the need to translate the viewpoint backward. Zooming in is supported with a focus+context visualization approach that integrates a distortion-free magnified focus region seamlessly into context. For AR, the higher resolution focus region is resampled from the video feed acquired by a head-mounted high-resolution camera. We demonstrate the benefits of our free field of view scene exploration in the context of VR and AR tasks, where it brings a substantial reduction of viewpoint translation, view direction rotation, and task completion time.},
keywords={augmented reality;data visualisation;helmet mounted displays;image resolution;lenses;view direction rotation;head-mounted high-resolution camera;distortion-free magnified focus region;comprehensive view;conventional fixed field;augmented reality head-mounted display scene exploration efficiency;VR;view scene exploration;effective free field;Visualization;Cameras;Resists;Distortion;Augmented reality;Legged locomotion;Augmented reality;Virtual reality;Navigation;Field of view},
doi={10.1109/ISMAR-Adjunct.2018.00043},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699286,
author={D. {Roth} and P. {Kullmann} and G. {Bente} and D. {Gall} and M. E. {Latoschik}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Effects of Hybrid and Synthetic Social Gaze in Avatar-Mediated Interactions},
year={2018},
volume={},
number={},
pages={103-108},
abstract={Human gaze is a crucial element in social interactions and therefore an important topic for social Augmented, Mixed, and Virtual Reality (AR, MR, VR) applications. In this paper we systematically compare four modes of gaze transmission: (1) natural gaze, (2) hybrid gaze, which combines natural gaze transmission with a social gaze model, (3) synthesized gaze, which combines a random gaze transmission with a social gaze model, and (4) purely random gaze. Investigating dyadic interactions, results show a linear trend for the perception of virtual rapport, trust, and interpersonal attraction, suggesting that these measures increase with higher naturalness and social adequateness of the transmission mode. We further investigated the perception of realism as well as the resulting gaze behavior of the avatars and the human participants. We discuss these results and their implications.},
keywords={augmented reality;avatars;human computer interaction;human factors;human-robot interaction;synthetic social gaze;avatar-mediated interactions;human gaze;social interactions;natural gaze transmission;social gaze model;random gaze transmission;dyadic interactions;social adequateness;transmission mode;gaze synthesis;gaze behavior;hybrid gaze;social augmented reality;virtual reality;virtual rapport perception;trust perception;interpersonal attraction perception;Data models;Avatars;Analytical models;Computational modeling;Human computer interaction;Animation;Artificial intelligence;Human-centered computing;Visualization;Visualization techniques},
doi={10.1109/ISMAR-Adjunct.2018.00044},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699166,
author={T. {Chalasani} and J. {Ondrej} and A. {Smolic}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Egocentric Gesture Recognition for Head-Mounted AR Devices},
year={2018},
volume={},
number={},
pages={109-114},
abstract={Natural interaction with virtual objects in AR/VR environments makes for a smooth user experience. Gestures are a natural extension from real world to augmented space to achieve these interactions. Finding discriminating spatio-temporal features relevant to gestures and hands in ego-view is the primary challenge for recognising egocentric gestures. In this work we propose a data driven end-to-end deep learning approach to address the problem of egocentric gesture recognition, which combines an ego-hand encoder network to find ego-hand features, and a recurrent neural network to discern temporally discriminating features. Since deep learning networks are data intensive, we propose a novel data augmentation technique using green screen capture to alleviate the problem of ground truth annotation. In addition we publish a dataset of 10 gestures performed in a natural fashion in front of a green screen for training and the same 10 gestures performed in different natural scenes without green screen for validation. We also present the results of our network's performance in comparison to the state-of-the-art using the AirGest dataset.},
keywords={augmented reality;gesture recognition;helmet mounted displays;human computer interaction;learning (artificial intelligence);recurrent neural nets;user experience;egocentric gesture recognition;head-mounted AR devices;AR/VR environments;smooth user experience;ego-hand encoder network;ego-hand features;recurrent neural network;deep learning networks;green screen capture;deep learning;data augmentation technique;Green products;Databases;Gesture recognition;Cameras;Deep learning;Training;Recurrent neural networks;Egocentric Gesture Recognition—Deep Learning—LSTMs—;Human Computer Interfaces—Natural Gestures},
doi={10.1109/ISMAR-Adjunct.2018.00045},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699311,
author={M. {Miyazaki} and T. {Komuro}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Extended Workspace Using a Smartphone with a Depth Camera},
year={2018},
volume={},
number={},
pages={115-116},
abstract={In this paper, we propose a user interface which expands the workspace of a smartphone by superimposing a virtual touchscreen on a flat surface such as a desk and the user's lap and allowing the user to perform touch operation on the surface. Information is displayed not only on the flat surface but also outside and above the surface and is fixed in real space, which enables the user to effectively utilize the space and change the area to be displayed quickly by moving the device. The user can drag the entire workspace by swiping the surface or performing gesture operation in the air. This enables the user to move the objects outside the surface or out of the user's reach toward the user so that the user can touch the objects. We implemented a map application to confirm the effectiveness of the proposed interface.},
keywords={gesture recognition;human computer interaction;smart phones;touch sensitive screens;extended workspace;smartphone;depth camera;user interface;virtual touchscreen;gesture operation;Cameras;Performance evaluation;Augmented reality;Mobile handsets;Haptic interfaces;User interfaces;Pins;Mobile device;peephole interaction;touch input;midair gesture;H.5.2[User Interfaces]: augmented and virtual reality},
doi={10.1109/ISMAR-Adjunct.2018.00046},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699247,
author={A. {Korinevskaya} and I. {Makarov}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Fast Depth Map Super-Resolution Using Deep Neural Network},
year={2018},
volume={},
number={},
pages={117-122},
abstract={Depth map super-resolution is a challenging computer vision problem. In this paper, we present two deep convolutional neural networks solving the problem of single depth map super-resolution. Both networks learn residual decomposition and trained with specific perceptual loss improving sharpness and perceptive quality of the upsampled depth map. Several experiments on various depth super-resolution benchmark datasets show state-of-art performance in terms of RMSE, SSIM, and PSNR metrics while allowing us to process depth super-resolution in real time with over 25-30 frames per second rate.},
keywords={computer vision;convolutional neural nets;image resolution;deep neural network;deep convolutional neural networks;single depth map super-resolution;upsampled depth map;depth super-resolution benchmark datasets;fast depth map super-resolution;computer vision problem;residual decomposition;perceptual loss;perceptive quality;Signal resolution;Spatial resolution;Artificial neural networks;Gallium nitride;Training;Generators;Depth Map Super Resolution;Depth Reconstruction;Semi-Dense Depth Map Interpolation;Deep Convolutional Neural Networks;Mixed Reality;Perceptual Loss;K.10.2 [Human-centered computing]: HCI—Mixed/augmented reality;K.11.3 [Computing methodologies]: AI—Computer vision problems: Reconstruction},
doi={10.1109/ISMAR-Adjunct.2018.00047},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699336,
author={A. {Nassani} and H. {Bai} and G. {Lee} and T. {Langlotz} and M. {Billinghurst} and R. W. {Lindeman}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Filtering 3D Shared Surrounding Environments by Social Proximity in AR},
year={2018},
volume={},
number={},
pages={123-124},
abstract={In this poster, we explore the social sharing of surrounding environments on wearable Augmented Reality (AR) devices. In particular, we propose filtering the level of detail of sharing the surrounding environment based on the social proximity between the viewer and the sharer. We test the effect of having the filter (varying levels of detail) on the shared surrounding environment on the sense of privacy from both viewer and sharer perspectives and conducted a pilot study using HoloLens. We report on semi-structured questionnaire results and suggest future directions in the social sharing of surrounding environments.},
keywords={augmented reality;data visualisation;image filtering;stereo image processing;social proximity;social sharing;3D shared surrounding environment filtering;wearable augmented reality devices;visualization;Privacy;Three-dimensional displays;Avatars;Augmented reality;Filtering;Social networking (online);Two dimensional displays;Human-centered computing—Visualization—Visualization techniques—Treemaps;Human-centered computing—Visualization—Visualization design and evaluation methods},
doi={10.1109/ISMAR-Adjunct.2018.00048},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699252,
author={},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Message from the ISMAR 2018 General Chairs},
year={2018},
volume={},
number={},
pages={xviii-xviii},
abstract={The following topics are dealt with: augmented reality; virtual reality; helmet mounted displays; human computer interaction; image colour analysis; cameras; mobile computing; rendering (computer graphics); user interfaces; gesture recognition.},
keywords={augmented reality;helmet mounted displays;human computer interaction;image colour analysis;virtual reality;augmented reality;virtual reality;helmet mounted displays;human computer interaction;image colour analysis;cameras;mobile computing;rendering (computer graphics);user interfaces;gesture recognition},
doi={10.1109/ISMAR-Adjunct.2018.00005},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699318,
author={A. {Langbein} and D. A. {Plecher} and F. {Pankratz} and C. {Eghtebas} and F. {Palmas} and G. {Klinker}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Gamifying Stereo Camera Registration for Augmented Reality},
year={2018},
volume={},
number={},
pages={125-126},
abstract={In this paper, we propose a solution to ease registrational tasks on users and keep them motivated during long lasting procedures using gamification elements. Augmented Reality (AR) technology appears in many different hardware setups ranging from mobile applications to full scale room tracking constructions. Many of which contain non-rigid sensors that require re-registrational maintenance to preserve satisfactory tracking capabilities. While important for a functioning setup, these tasks can become tiring over time leading to less care spent on a qualitative registration. We report on our preliminary study results, showing that a gami-fied registration routine incites participants to perform these tasks for a longer amount of time with up to four times as many measurements taken as the non-gamified version.},
keywords={augmented reality;computer games;image registration;mobile computing;stereo camera registration;registrational tasks;gamification elements;mobile applications;scale room tracking constructions;nonrigid sensors;re-registrational maintenance;functioning setup;qualitative registration;augmented reality technology;hardware setups;tracking capabilities;gamified registration routine;Games;Cameras;Task analysis;Calibration;Sensors;Atmospheric measurements;Particle measurements;I.4.1 [Image Processing and Computer Vision]: Camera Calibration;H.5.2 [Information Interfaces and Presentation]: User Interfaces},
doi={10.1109/ISMAR-Adjunct.2018.00049},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699262,
author={U. {Gruenefeld} and T. C. {Stratmann} and J. {Jung} and H. {Lee} and J. {Choi} and A. {Nanda} and W. {Heuten}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Guiding Smombies: Augmenting Peripheral Vision with Low-Cost Glasses to Shift the Attention of Smartphone Users},
year={2018},
volume={},
number={},
pages={127-131},
abstract={Over the past few years, playing Augmented Reality (AR) games on smartphones has steadily been gaining in popularity (e.g., Pokémon Go). However, playing these games while navigating traffic is highly dangerous and has led to many accidents in the past. In our work, we aim to augment peripheral vision of pedestrians with low-cost glasses to support them in critical traffic encounters. Therefore, we developed a 10-fi prototype with peripheral displays. We technically improved the prototype with the experience of five usability experts. Afterwards, we conducted an experiment on a treadmill to evaluate the effectiveness of collision warnings in our prototype. During the experiment, we compared three different light stimuli (instant, pulsing and moving) with regard to response time, error rate, and subjective feedback. Overall, we could show that all light stimuli were suitable for shifting the users' attention (100% correct). However, moving light resulted in significantly faster response times and was subjectively perceived best.},
keywords={augmented reality;computer games;smart phones;low-cost glasses;smartphones;peripheral vision;10-fi prototype;peripheral displays;guiding smombies;augmented reality games;Prototypes;Light emitting diodes;Visualization;Games;Glass;Strips;Interviews;Human-centered computing—Visualization—Visualization techniques;Human-centered computing—Visualization—Visualization design and evaluation methods},
doi={10.1109/ISMAR-Adjunct.2018.00050},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699306,
author={H. {Jiang} and D. {Weng} and Z. {Zhang} and Y. {Bao} and Y. {Jia} and M. {Nie}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={HiKeyb: High-Efficiency Mixed Reality System for Text Entry},
year={2018},
volume={},
number={},
pages={132-137},
abstract={Text entry is an imperative issue to be addressed in current entry systems for virtual environments (VEs). The entry method using a physical keyboard is still the most dominant choice for an efficient interaction regarding text entry. In this paper, we propose a typing system with a style of mixed reality, which is called HiKeyb, and it possesses a similar high-efficiency with the single physical keyboard in the real environment. The HiKeyb system consists of a depth camera, a pose tracking module, a head-mounted display (HMD), a QWERTY keyboard and a black table mat. This system can guarantee the entry efficiency and the amenity by not only introducing the force feedback from a movable physical keyboard, but also improving the immersion with the real hand image. In addition, the infrared absorption material helps improve the robustness of the system against different lighting environments. Experiments have proved that users wearing HMDs in Virtual Phrases session can achieve an entry rate of 23.1 words per minute and an error rate of 2.76%, and the rate ratio of virtual reality to real world is 78% when typing phrases. Besides, we find that the proposed system can provide a relatively close entry efficiency to that using a pure physical keyboard in the real environment.},
keywords={helmet mounted displays;keyboards;virtual reality;virtual reality;high-efficiency mixed reality system;text entry;virtual environments;typing system;HiKeyb system;QWERTY keyboard;movable physical keyboard;depth camera;pose tracking module;head-mounted display;force feedback;infrared absorption material;Augmented reality;Human-centered computing—Human computer interaction—Interaction paradigms—Virtual reality;Human-centered computing—Human computer interaction—HCI design and evaluation methods—Usability testing},
doi={10.1109/ISMAR-Adjunct.2018.00051},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699196,
author={R. {Radkowski}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={HoloLens Integration into a Multi-Kinect Tracking Environment},
year={2018},
volume={},
number={},
pages={138-142},
abstract={The Microsoft HoloLens is one of the latest headsets that facilitates mixed and augmented reality (AR) applications. It has a high potential to leverage AR applications in many domains. However, the first version also comes with several limitations such as the limited battery lifetime and the small field of view. An important one is the lack of high-fidelity depth data which would facilitate object detection and tracking research; a capability imperative for many applications. To mitigate this limitation, we integrated the HoloLens into a point cloud-based tracking system. Our system uses several Kinect range cameras to obtain a point cloud and to detect and track real assets of interest in this point cloud. The pose data for all objects is forwarded to the HoloLens, which can then render 3D models from the right perspective. However, this system is not free of tolerances and tracking errors, which mandates calibration. This poster explains how the system was set-up and verifies the feasibility of a system such as this for 3D model registration using the HoloLens as a display device.},
keywords={augmented reality;calibration;cameras;image registration;image sensors;object detection;rendering (computer graphics);solid modelling;stereo image processing;Microsoft HoloLens;augmented reality applications;battery lifetime;high-fidelity depth data;object detection;tracking research;tracking errors;HoloLens integration;kinect range cameras;point cloud;tracking system;multikinect tracking environment;3D model registration;Cameras;Three-dimensional displays;Calibration;Solid modeling;Databases;Hardware;Tracking},
doi={10.1109/ISMAR-Adjunct.2018.00052},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699232,
author={S. {Shoman} and T. {Mashita} and A. {Plopski} and P. {Ratsamee} and Y. {Uranishi} and H. {Takemura}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Illumination Invariant Camera Localization Using Synthetic Images},
year={2018},
volume={},
number={},
pages={143-144},
abstract={Accurate camera localization is an essential part of tracking systems. However, localization results are greatly affected by illumination. Including data collected under various lighting conditions can improve the robustness of the localization algorithm to lighting variation. However, this is very tedious and time consuming. By using synthetic images, it is possible to easily accumulate a large variety of views under varying illumination and weather conditions. Despite continuously improving processing power and rendering algorithms, synthetic images do not perfectly match real images of the same scene, i.e., there exists a gap between real and synthetic images that also affects the accuracy of camera localization. To reduce the impact of this gap, we introduce “REal-to-Synthetic Transform (REST).” REST is an autoencoder-like network that converts real features to their synthetic counterpart. The converted features can then be matched against the accumulated database for robust camera localization. Our results shows that REST improves matching accuracy by approximately 30%.},
keywords={cameras;computer vision;image matching;image sensors;rendering (computer graphics);transforms;autoencoder-like network;REST;tracking systems;real-to-synthetic transform;robust camera localization;rendering algorithms;processing power;localization algorithm;lighting conditions;accurate camera localization;synthetic images;illumination invariant camera localization;Cameras;Feature extraction;Lighting;Transforms;Three-dimensional displays;Databases;Computational modeling;Camera Localization;Variable Lighting;Synthetic View;Autoencoder},
doi={10.1109/ISMAR-Adjunct.2018.00053},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699275,
author={P. {Skinner} and J. {Ventura} and S. {Zollmann}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Indirect Augmented Reality Browser for GIS Data},
year={2018},
volume={},
number={},
pages={145-150},
abstract={In Augmented Reality applications, user experience is highly dependent on the accuracy of registration between digital content and the real world. Errors in tracking and registration can arise due to inaccuracy of sensors or challenging conditions such as urban canyon effects or magnetic distortions. Indirect augmented reality is an approach that avoid these issues by using precaptured and preregistered images instead of a live video feed. However, indirect augmented reality highly depends on the availability of those preregistered images. In particular, when being used for browsing geographic information, it is important to access data in an omnipresent way. In this work, we propose an Indirect Augmented Reality browser that aims to address these availability problems by combining indirect Augmented Reality with crowd sourced precaptured street level imagery with geospatial data. We demonstrate how our indirect augmented reality browser annotates buildings and landmarks in the users' environment and investigate the feasibility by analysing the performance of such an approach. In addition, we investigate issues of visibility and legibility when labelling the environment.},
keywords={augmented reality;geographic information systems;image registration;preregistered images;indirect augmented reality browser;GIS data;precaptured images;geographic information;crowd sourced precaptured street level imagery;geospatial data;Browsers;Augmented reality;Buildings;Three-dimensional displays;Cameras;User experience;Databases;Human-centered computing—Visualization—Visualization application domains—Geographic visualization;Human-centered computing—Human computer interaction (HCI)— Interaction paradigms—Mixed / augmented reality},
doi={10.1109/ISMAR-Adjunct.2018.00054},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699183,
author={M. {Lorenz} and S. {Knopp} and P. {Klimant}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Industrial Augmented Reality: Requirements for an Augmented Reality Maintenance Worker Support System},
year={2018},
volume={},
number={},
pages={151-153},
abstract={Supporting maintenance workers with Augmented Reality (AR) applications has always been one key use case to demonstrate the advantages of AR. However, such AR applications are still not widely used in industry, which may lay at the complex industrial requirements. We present the user, technical, environmental and regulative requirements for an AR maintenance worker support system, which were gathered by analyzing three diverse production sites.},
keywords={augmented reality;maintenance engineering;personnel;production engineering computing;AR applications;complex industrial requirements;technical requirements;environmental requirements;regulative requirements;AR maintenance worker support system;maintenance workers;augmented reality applications;augmented reality maintenance worker support system;industrial augmented reality;production sites;Maintenance engineering;Augmented reality;Task analysis;Safety;Gears;Glass;Augmented Reality;Requirements;Maintenance;Service;[Human-centered computing]: Mixed / augmented reality;[Computer systems organization]: Embedded and cyber-physical systems},
doi={10.1109/ISMAR-Adjunct.2018.00055},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699316,
author={Z. {Zhang} and D. {Weng} and H. {Jiang} and Y. {Liu} and Y. {Wang}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Inverse Augmented Reality: A Virtual Agent's Perspective},
year={2018},
volume={},
number={},
pages={154-157},
abstract={We propose a framework called inverse augmented reality (IAR) which describes the scenario that a virtual agent living in the virtual world can observe both virtual objects and real objects. This is different from the traditional augmented reality. The traditional virtual reality, mixed reality and augmented reality are all generated for humans, i.e., they are human-centered frameworks. On the contrary, the proposed inverse augmented reality is a virtual agent-centered framework, which represents and analyzes the reality from a virtual agent's perspective. In this paper, we elaborate the framework of inverse augmented reality to argue the equivalence of the virtual world and the physical world regarding the whole physical structure.},
keywords={augmented reality;human computer interaction;multi-agent systems;inverse augmented reality;virtual agent-centered framework;virtual world;virtual objects;traditional augmented reality;traditional virtual reality;mixed reality;IAR;human-centered frameworks;Augmented reality;Physics;Virtual environments;Evolution (biology);Augmented virtuality;Mathematical model;augmented reality;inverse;virtual agent},
doi={10.1109/ISMAR-Adjunct.2018.00056},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699238,
author={C. {Nimcharoen} and S. {Zollmann} and J. {Collins} and H. {Regenbrecht}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Is That Me?—Embodiment and Body Perception with an Augmented Reality Mirror},
year={2018},
volume={},
number={},
pages={158-163},
abstract={Virtual reality has been used intensively to study embodiment and body perception, in particular for research purposes in psychological domains. Virtual avatars are used to resemble users' appearance and to implement interactively simulated behaviour. To make this a realistic and believable experience users should feel embodiment, i.e. ownership, agency, and self-location/presence. State-of-the-art capture and display technologies allow for extending virtual reality embodiment to the realm of augmented reality for higher efficacy-instead of seeing a virtual reality body one would see a captured, 3D representation of their own body naturally controlled by their real body movements within the context of the present real environment. However, it is unclear whether users would experience embodiment with their augmented reality avatar and whether findings from virtual reality targeting body perception can be replicated. Here we present an augmented reality system comprising a 3D point cloud capturing system (Microsoft Kinect) and an optical see-through head-mounted display (Microsoft HoloLens), both connected to a purpose-developed application displaying a user's body in a virtual 3D mirror embedded into the real environment. In a study with 24 participants, we evaluated embodiment and body weight perception as a proof of concept. This is based on a similar study conducted in Virtual Reality. Our findings show that users experience ownership and agency with the mirrored body and that body weight perception in virtual and augmented reality systems is similar.},
keywords={augmented reality;avatars;helmet mounted displays;psychology;virtual reality body;head-mounted display;3D point cloud capturing system;virtual avatars;augmented reality system;virtual reality embodiment;display technologies;realistic experience users;interactively simulated behaviour;body weight perception;virtual 3D mirror;augmented reality avatar;body movements;3D representation;Augmented reality;Ownership;agency;presence;mixed reality;optical see-through displays;self-location;H.5.1 [Information Interfaces and Presentation (e.g. HCI)]: Multimedia Information Systems — Artificial, augmented, and virtual realities},
doi={10.1109/ISMAR-Adjunct.2018.00057},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699254,
author={J. {Rambach} and C. {Deng} and A. {Pagani} and D. {Stricker}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Learning 6DoF Object Poses from Synthetic Single Channel Images},
year={2018},
volume={},
number={},
pages={164-169},
abstract={Estimation of 6DoF object poses from single images is a problem of great interest in augmented reality and robotics research since it enables interaction with the object or initialization of pose tracking. Approaches utilizing deep neural networks have shown good performance, however the majority of them rely on training on real images of the objects which can be challenging in terms of ground truth pose acquisition, scalability and full coverage of possible poses. In this paper, we disregard all depth and color information and train a CNN to directly regress 6DoF object poses using only synthetic single channel edge enhanced images. We evaluate our approach against the state-of-the-art using synthetic training images and show a significant improvement on the commonly used LINEMOD benchmark dataset.},
keywords={augmented reality;convolutional neural nets;image colour analysis;image enhancement;learning (artificial intelligence);pose estimation;regression analysis;synthetic training imaging;synthetic single channel edge enhanced imaging;6DoF object pose learning estimation;augmented reality;robotics research;deep neural networks;pose acquisition;CNN;6DoF object pose regression;LINEMOD benchmark dataset;pose tracking;synthetic single channel images;Training;Pose estimation;Solid modeling;Three-dimensional displays;Neural networks;Cameras;Lighting;Computing methodologies—Artificial intelligence—Computer vision—Computer vision tasks;Computing methodologies—Machine learning—Machine learning approaches—Neural networks},
doi={10.1109/ISMAR-Adjunct.2018.00058},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699218,
author={},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Message from the ISMAR 2018 Science and Technology Program Chairs and TVCG Guest Editors},
year={2018},
volume={},
number={},
pages={xix-xx},
abstract={In this special issue of IEEE Transactions on Visualization and Computer Graphics (TVCG), we are pleased to present the TVCG papers from the 17th IEEE International Symposium on Mixed and Augmented Reality (ISMAR 2018), held October 16–20 in Munich, Germany. ISMAR continues the 20-year long tradition of IWAR, ISMR, and ISAR, and is undoubtedly the premier conference for mixed and augmented reality in the world.},
keywords={},
doi={10.1109/ISMAR-Adjunct.2018.00006},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699216,
author={P. {Knierim} and F. {Kiss} and A. {Schmidt}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Look Inside: Understanding Thermal Flux Through Augmented Reality},
year={2018},
volume={},
number={},
pages={170-171},
abstract={The transition from high school to university is an exciting time for students including many new challenges. Particularly in the field of science, technology, engineering, and mathematics, the university dropout rate may reach up to 40%. The studies of physics rely on many abstract concepts and quantities that are not directly visible like energy or heat. We developed a mixed reality application for education, which augments the thermal conduction of metal by overlaying a representation of temperature as false-color visualization directly onto the object. This real-time augmentation avoids attention split and overcomes the perception gap by amplifying the human eye. Augmented and Virtual Reality environments allow students to perform experiments that were impossible to conduct for security or financial reasons. With the application, we try to foster a deeper understanding of the learning material and higher engagement during the studies.},
keywords={augmented reality;computer aided instruction;data visualisation;engineering education;physics computing;thermal flux;high school;university dropout rate;mixed reality application;false-color visualization;real-time augmentation;attention split;perception gap;augmented reality environment;virtual reality environment;science technology engineering mathematics field;learning material;Augmented reality;Data visualization;Physics;Prototypes;Heating systems;Real-time systems;H.5.m [Information Interfaces and Presentation]: Miscellaneous},
doi={10.1109/ISMAR-Adjunct.2018.00059},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699333,
author={A. {Moteki} and N. {Yamaguchi} and A. {Karasudani} and Y. {Kobayashi} and T. {Yoshitake} and J. {Kato} and T. {Aoyagi}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Manufacturing Defects Visualization via Robust Edge-Based Registration},
year={2018},
volume={},
number={},
pages={172-173},
abstract={We propose a visualization method for inspecting manufacturing defects. Industrial products have many straight lines and little texture; therefore, the proposed method uses edges for estimating 6DoF pose of the products (registration). To prevent combinatorial explosion, our method reduces the number of combinations by the condition of edges' geometrical distribution. Moreover, manufacturing defects are detected and visualized by robust registration based on the LMedS. This method realizes on-site product inspection for unskilled workers unfamiliar with AR, and decreases the cost of re-manufacturing. We evaluate our method quantitatively using original CG and real image dataset.},
keywords={edge detection;feature extraction;image registration;inspection;on-site product inspection;manufacturing defects visualization;robust edge-based registration;visualization method;industrial products;straight lines;6DoF pose;combinatorial explosion;edges geometrical distribution;Three-dimensional displays;Manufacturing;Two dimensional displays;Solid modeling;Image edge detection;Visualization;Laboratories;Computing methodologies;Computer graphics;Graphics systems and interfaces;Mixed/augmented reality},
doi={10.1109/ISMAR-Adjunct.2018.00060},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699224,
author={M. {Dani} and G. {Garg} and R. {Perla} and R. {Hebbalaguppe}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Mid-Air Fingertip-Based User Interaction in Mixed Reality},
year={2018},
volume={},
number={},
pages={174-178},
abstract={With data growing at a huge rate, there arises a need for advanced data visualization techniques. Visualizing these data sets in Mixed Reality(MR) mode provides an immersive experience to the user in the context of the real world applications. Most of the existing works can only be used with inordinately priced devices such as Microsoft HoloLens, Meta Glass that use proprietary hardware for data visualization and user interaction through hand gestures. In this paper, we demonstrate a cost-effective solution for data visualization using frugal devices such as Google Cardboard, VR Box etc. in MR mode. However, these devices still employ only primitive modes of interaction such as the magnetic trigger, conductive lever and have a limited user-input capability. To interact with visualizations and facilitate rich user experience, we propose the use of intuitive pointing fingertip gestural interface in the user's Field of View(FoV). The proposed pointing hand gesture recognition framework is driven by cascade of state-of-the-art deep learning model - Faster RCNN for localizing the hand followed by a proposed regression CNN for fingertip localization. We conducted both objective and subjective evaluation to demonstrate the performance of our proposed method. Objective metrics are fingertip recognition accuracy and computational time. The subjective evaluation includes user comfort and effectiveness of fingertip interaction that is proposed.},
keywords={augmented reality;convolutional neural nets;data visualisation;gesture recognition;human computer interaction;learning (artificial intelligence);recurrent neural nets;regression analysis;frugal devices;MR mode;user-input capability;intuitive pointing fingertip gestural interface;pointing hand gesture recognition framework;fingertip localization;user comfort;fingertip interaction;mid-air fingertip-based user interaction;data sets;immersive experience;hand gestures;user experience;deep learning model;data visualization techniques;mixed reality mode;faster RCNN;field of view;regression CNN;Data visualization;Google;Gesture recognition;Cameras;Computer architecture;Virtual reality;User experience;Fingertip gestures;Deep Learning;Mixed Reality;Google Cardboard;Diectic interaction;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities;H.5.2 [Information Interfaces and Presentation]: User Interfaces—Input devices and strategies;I.2.10 [Artificial Intelligence]: Vision and Scene Understanding—Video analysis;I.4.8 [Image Processing and Computer Vision]: Scene Analysis—Time-varying imagery},
doi={10.1109/ISMAR-Adjunct.2018.00061},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699172,
author={I. {García-Pereira} and J. {Gimeno} and M. {Pérez} and C. {Portalés} and S. {Casas}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={MIME: A Mixed-Space Collaborative System with Three Immersion Levels and Multiple Users},
year={2018},
volume={},
number={},
pages={179-183},
abstract={Shared spaces for remote collaboration are nowadays possible by considering a variety of users, devices, immersion systems, interaction capabilities, navigation paradigms, etc. There is a substantial amount of research done in this line, proposing different solutions. However, still a more general solution that considers the heterogeneity of the involved actors/items is lacking. In this paper, we present MIME, a mixed-space tri-collaborative system. Differently from other mixed-space systems, MIME considers three different types of users (in different locations) according to the level of immersion in the system, who can interact simultaneously - what we call a tri-collaboration. For the three types, we provide a solution to navigate, point at objects/locations and make annotations, while users are able to see a virtual representation of the rest of users. Additionally, the total number of users that can simultaneously interact with the system is only restricted by the available hardware, i.e., various users of the same type can be simultaneously connected to the system. We have conducted a preliminary study at the laboratory level, showing that MIME is a promising tool that can be used in many real cases for different purposes.},
keywords={groupware;virtual reality;virtual representation;mixed-space tri-collaborative system;interaction capabilities;immersion systems;remote collaboration;MIME;Augmented reality;Augmented Reality;Virtual Reality;shared spaces;mixed-space;remote collaboration;K.5.1 [Multimedia Information Systems]: Artificial, augmented, and virtual realities},
doi={10.1109/ISMAR-Adjunct.2018.00062},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699267,
author={G. {Ballestin} and F. {Solari} and M. {Chessa}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Perception and Action in Peripersonal Space: A Comparison Between Video and Optical See-Through Augmented Reality Devices},
year={2018},
volume={},
number={},
pages={184-189},
abstract={In this paper, we analyze how we perceive the peripersonal space when involved in a reaching task in an Augmented Reality (AR) environment. In particular, we aim to quantify whether distortions in perception of the spatial layout of the scene occur, by taking into consideration two different AR wearable devices, in particular head-mounted displays (HMD). We performed two tests, and compared the results between the subjects who used an Optical See-Through (OST) HMD (Metavision Meta 2) and those who used a Video See-Through (VST) HMD (a smartphone in conjunction with a headset like the Google Cardboard). The data has been collected from a total of 45 volunteer participants. In the experiment, the subjects had to perform a precision reaching task by overlapping the hand on the perceived target position. Then, we observed how the presence or absence of an internal feedback influenced the homing performance. Our results revealed a better depth estimation, thus a more precise interaction, when using the OST device, which also revealed a lower impact on eye strain and fatigue.},
keywords={augmented reality;helmet mounted displays;human computer interaction;smart phones;wearable computers;Metavision Meta 2;Google Cardboard;precision reaching task;perceived target position;homing performance;OST device;peripersonal space;spatial layout;head-mounted displays;augmented reality environment;AR wearable devices;augmented reality devices;optical see-through HMD;video see-through HMD;smartphone;Resists;Task analysis;Cameras;Augmented reality;Performance evaluation;Headphones;Google;Human-centered computing [Human computer interaction (HCI)]:;—[Human-centered computing]: Human computer interaction (HCI)—Interaction paradigmsMixed / augmented reality;Human-centered computing [Human computer interaction (HCI)]: Empirical studies in HCI—},
doi={10.1109/ISMAR-Adjunct.2018.00063},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8699280,
author={M. {Nishizawa} and K. {Okajima}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, title={Precise Surface Color Estimation Using a Non-Diagonal Reflectance Matrix on an Adaptive Projector-Camera System},
year={2018},
volume={},
number={},
pages={190-195},
abstract={A projector camera system for controlling the appearance of real objects is useful for public viewing. However, projector camera systems need to estimate scenes under the white light from a camera image taken under the color light of the projector and adaptively generate a new projector image. In such a feedback system, the quality of the estimation of the scene color under the white light affects the quality of the projection image directly. In this study, we propose a precise online estimation of the scene color assuming that the reflectance matrix non-diagonal and can be determined with three parameters. This enables the projector-camera system to be applied to design tools or food AR, which require a high-quality grade for material perception simulations.},
keywords={cameras;image colour analysis;matrix algebra;optical projectors;feedback system;scene color;white light;precise surface color estimation;nondiagonal reflectance matrix;adaptive projector-camera system;projector camera system;camera image;color light;public viewing;projector image quality;Reflectivity;Image color analysis;Cameras;Color;Adaptation models;Estimation;Reflection;reflection color;reflectance matrix;non-diagonal;online estimation;adaptive projector camera system;H.5.1 [Information Interface and Presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities;I.4 [Image Processing and Computer Vision]: Digitization and Image Capture—Reflectance;Scene Analysis—Color},
doi={10.1109/ISMAR-Adjunct.2018.00064},
ISSN={},
month={Oct},}