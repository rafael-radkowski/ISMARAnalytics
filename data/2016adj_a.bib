@INPROCEEDINGS{7836422,
author={},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[Title page i]},
year={2016},
volume={},
number={},
pages={i-i},
abstract={The following topics are dealt with: mixed reality; augmented reality; human behavior analysis; visualization; collective visual sensing; and interactive AR tool.},
keywords={augmented reality;behavioural sciences computing;data visualisation;groupware;interactive systems;mixed reality;augmented reality;human behavior analysis;visualization;collective visual sensing;interactive AR tool},
doi={10.1109/ISMAR-Adjunct.2016.0001},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836423,
author={},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[Title page iii]},
year={2016},
volume={},
number={},
pages={iii-iii},
abstract={Presents the title page of the proceedings record.},
keywords={},
doi={10.1109/ISMAR-Adjunct.2016.0002},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836424,
author={},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={[Copyright notice]},
year={2016},
volume={},
number={},
pages={iv-iv},
abstract={Presents the copyright information for the conference. May include reprint permission information.},
keywords={},
doi={10.1109/ISMAR-Adjunct.2016.0003},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836425,
author={},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Table of contents},
year={2016},
volume={},
number={},
pages={v-xiii},
abstract={Presents the table of contents/splash page of the proceedings record.},
keywords={},
doi={10.1109/ISMAR-Adjunct.2016.0004},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836426,
author={},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Message from the ISMAR 2016 General Chair and Deputy General Chairs},
year={2016},
volume={},
number={},
pages={xiv-xiv},
abstract={Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.},
keywords={},
doi={10.1109/ISMAR-Adjunct.2016.0005},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836427,
author={},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Message from the ISMAR 2016 Science and Technology Poster Chairs},
year={2016},
volume={},
number={},
pages={xv-xvi},
abstract={Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.},
keywords={},
doi={10.1109/ISMAR-Adjunct.2016.0006},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836428,
author={},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Message from the Workshop and Tutorial Chairs},
year={2016},
volume={},
number={},
pages={xvii-xvii},
abstract={Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.},
keywords={},
doi={10.1109/ISMAR-Adjunct.2016.0007},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836429,
author={},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Message from the Demonstration Chairs},
year={2016},
volume={},
number={},
pages={xviii-xviii},
abstract={Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.},
keywords={},
doi={10.1109/ISMAR-Adjunct.2016.0008},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836430,
author={},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={ISMAR 2016 Conference Committee Members},
year={2016},
volume={},
number={},
pages={xix-xix},
abstract={Provides a listing of current committee members and society officers.},
keywords={},
doi={10.1109/ISMAR-Adjunct.2016.0009},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836431,
author={},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={ISMAR 2016 Science and Technology Program Committee Members},
year={2016},
volume={},
number={},
pages={xx-xx},
abstract={Provides a listing of current committee members and society officers.},
keywords={},
doi={10.1109/ISMAR-Adjunct.2016.0010},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836432,
author={},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={ISMAR 2016 Steering Committee Members},
year={2016},
volume={},
number={},
pages={xxi-xxi},
abstract={Provides a listing of current committee members and society officers.},
keywords={},
doi={10.1109/ISMAR-Adjunct.2016.0011},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836433,
author={},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Reviewers},
year={2016},
volume={},
number={},
pages={xxii-xxii},
abstract={The conference offers a note of thanks and lists its reviewers.},
keywords={},
doi={10.1109/ISMAR-Adjunct.2016.0012},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836434,
author={},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Keynotes: "The History and Future of Visual SLAM" "Mixing It Up, Mixing It Down"},
year={2016},
volume={},
number={},
pages={xxiii-xxiv},
abstract={Provides an abstract for each of the two keynote presentations and a brief professional biography of each presenter. The complete presentations were not made available for publication as part of the conference proceedings.},
keywords={},
doi={10.1109/ISMAR-Adjunct.2016.0013},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836435,
author={},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Sponsors and supporters},
year={2016},
volume={},
number={},
pages={xxv-xxvii},
abstract={The conference organizers greatly appreciate the support of the various corporate sponsors listed.},
keywords={},
doi={10.1109/ISMAR-Adjunct.2016.0014},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836436,
author={D. {Schmalstieg} and T. {Höllerer}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Augmented Reality – Principles and Practice Tutorial},
year={2016},
volume={},
number={},
pages={xxviii-xxviii},
abstract={Summary form only given. The complete presentation was not made available for publication as part of the conference proceedings.The world is becoming more complex and problem solving often requires teams of experts to work together at the same or from different locations. To support this there is a need for collaborative tools, and a variety of teleconferencing and telepresence technologies have been developed. However, most of them involve some variation of traditional video conferencing, which has limitations, such as not being able to effectively convey spatial cues or share the user's task space. This workshop will focus on how these limitations can be overcome by using Mixed Reality (MR) technology, leading to the development of radically new types of collaborative experiences. The target audience includes everyone with an interest in developing AR applications, academic and industrial professionals and students alike, from beginner level onwards. They will receive a thorough overview of AR technology illustrated with many practical examples. A background in computer graphics and computer vision will be useful, but is not strictly necessary. The tutorial will be conducted with a mixture of slide presentation, video clips and live demonstrations. Audience members will be engaged in an active dialog and will be able to ask questions at any time during the tutorial. Participants will learn a broad foundation of the components and terminology used in developing AR on smartphones, so they can perform their own investigations. They will also gain appreciation for what is currently possible and what isn't, and what to expect from future research.},
keywords={augmented reality;computer graphics;computer vision;groupware;smart phones;teleconferencing;augmented reality;collaborative tools;teleconferencing;telepresence technologies;video conferencing;mixed reality technology;computer graphics;computer vision;smartphones;Tutorials;Augmented reality;Electronic mail;Problem-solving},
doi={10.1109/ISMAR-Adjunct.2016.0015},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836437,
author={},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={DAQRI ARToolKit tutorial},
year={2016},
volume={},
number={},
pages={xxix-xxix},
abstract={No text was not made available for publication as part of the conference proceedings.},
keywords={Tutorials},
doi={10.1109/ISMAR-Adjunct.2016.0016},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836438,
author={},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Microsoft Hololens Tutorial},
year={2016},
volume={},
number={},
pages={xxx-xxx},
abstract={Provides an abstract of the tutorial presentation and a brief professional biography of the presenter. The complete presentation was not made available for publication as part of the conference proceedings.},
keywords={Tutorials},
doi={10.1109/ISMAR-Adjunct.2016.0017},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836439,
author={},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={The Second International Workshop on Diminished Reality as Challenging Issue in Mixed and Augmented Reality (IWDR2016) Summary},
year={2016},
volume={},
number={},
pages={xxxi-xxxi},
abstract={Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.},
keywords={},
doi={10.1109/ISMAR-Adjunct.2016.0018},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836440,
author={G. {Moreau} and {Yue Liu}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Perceptual Issues in AR Workshop Summary},
year={2016},
volume={},
number={},
pages={xxxii-xxxii},
abstract={Augmented reality (AR) is a live direct or indirect view of a real-world environment whose elements are augmented by computer-generated sensory input such as sound, video, graphics or GPS data. With the help of advanced AR technology, the information about the surrounding real world of the user becomes interactive and digitally manipulable and artificial information about the environment and its objects can be overlaid on the real world. Natural, portable and efficient AR system can be implemented by combining human computer interaction (HCI) realized with gesture inputs and wearable computing based on 3D see-through head mounted display, which possesses great application potentials. However, there are currently many human factor problems associated with AR technology. For instance, it involves the perception modeling and display quality forecast of 3D complex scenes; it requires the integration mechanism of the wearable computing devices and the HCI methods as well as the ergonomics analysis; and it suffers from the comfort evaluation and the setup of AR system. A few papers were dedicated to human factors issues those last years in ISMAR as well as in related conferences such as IEEE VR, we strongly believe that with the democratization of AR technologies, the interest will increase. The goal of this workshop is to review the challenges of human factors in AR and the available methods that aim at solving some of those challenges and to give the audience an overview of the variety of the existing and yet to invent evaluation approaches. Beyond the classical presentations and discussions of a workshop, a particular goal of this workshop is the production of a collective document (in a publishable form?) about remaining challenges of human factors in AR. The second output will be a website including challenges, solutions to this challenges and applications pages. This workshop aims at gathering researchers that aim at developing natural, portable and efficient AR system or who want to extend their knowledge about the human factor issues in AR. Practitioners of the field are also welcome to acquire knowledge of what is currently being done and what could be done in a near future. One of the goal of this workshop is also to be interdisciplinary by mixing researchers in computer science, psychophysics and ergonomics.},
keywords={augmented reality;human computer interaction;AR;augmented reality;real-world environment;computer generated sensory;advanced AR technology;artificial information;human computer interaction;wearable computing;gesture inputs;head mounted display;human factor problems;wearable computing devices;3D complex scenes;display quality forecast;ergonomics analysis;HCI methods;human factors;Web site},
doi={10.1109/ISMAR-Adjunct.2016.0019},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836441,
author={J. {Ramirez} and J. {Stadon} and T. {Sanchez}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Media, Arts, Social Sciences, Humanities Design (MASH'D) Workshop Summary},
year={2016},
volume={},
number={},
pages={xxxiii-xxxiii},
abstract={ISMAR 2016 will cover the full range of technologies encompassed by the MR continuum, from interfaces in the real world to fully immersive experiences. This range goes far beyond the traditional definition of AR, which focused on precise 3D tracking, visual display, and real-time performance. MASHD would like to invite contributions from areas fundamental to this mixed and augmented reality, particularly in the fields of digital media, art, social sciences, humanities and design. Now effectively in its seventh year, this separation of the conference was established to allow for a more specific focus on areas relating to creative production in mixed and augmented reality environments, both for entertainment and for more meaningful endeavors. This year sees the integration of MASHD into the main program, however we will provide a small series of separate workshops during the conference. MASHD workshop would be celebrated the 22 of September with possibilities of extending to 23. Participants will be able to attend with the option of day passes, as well as full registrations for the whole ISMAR week from the 19th to 23rd September). The 15th ISMAR brings new horizons to the community as it is the first in the series to be held in Latin America, responding to the recent explosion of commercial and research activities related to AR, MR, and Virtual Reality (VR) by continuing the expansion of its scope.},
keywords={augmented reality;humanities;virtual reality;virtual reality;MR;ISMAR;augmented reality;AR;visual display;3D tracking;MASHD;media-art-social sciences-humanities-&-design workshop},
doi={10.1109/ISMAR-Adjunct.2016.0020},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836442,
author={G. J. {Kim}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Standards for Mixed and Augmented Reality Workshop Summary},
year={2016},
volume={},
number={},
pages={xxxiv-xxxiv},
abstract={Mixed and augmented reality (MAR) is on the brink of large-scale consumer level commercialization. For MAR to succeed and proliferate as an information media and new contents platform, standards will be necessary so that MAR system components can easily be plugged in and contents shared and interoperable. The workshop will be an arena to discuss and lay a foundation to many issues of standardization for MAR, including (but not limited to): proper subareas for standards and abstract levels, physical and environment object representation, content file format, standard calibration process, augmentation and display style standards, standards for non-visual and multimodal augmentation, object feature presentation, benchmarking, etc. Despite its importance, there has been very little discussion of standards for MAR, especially from the academics and research community. It is critical that standards for MAR be developed on a sound basis with input from the academia and research community, for it to be correct, comprehensive, effective and extendible for the future. Standardization for MAR is being pushed through few organizations, such as the ISO (International Standards Organization), OGC (Open Geospatial Consortium) and by privately organizations such as the Perey Associates. Such movements reflect the recent rise in the interest in AR and the large commercialization efforts. However, there are so many areas to cover and the efforts are still limited by the lack of areal experts.},
keywords={augmented reality;mixed-and-augmented reality;MAR;large-scale consumer level commercialization;standard calibration process;information media},
doi={10.1109/ISMAR-Adjunct.2016.0021},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836443,
author={S. {Lukosch} and M. {Billinghurst} and K. {Kiyokawa} and L. {Alem} and S. {Feiner} and M. {Prilla}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Workshop on Collaborative Mixed Reality Environments (CoMiRE) Summary},
year={2016},
volume={},
number={},
pages={xxxv-xxxvi},
abstract={The world is becoming more complex and problem solving often requires teams of experts to work together at the same or from different locations. To support this there is a need for collaborative tools, and a variety of teleconferencing and telepresence technologies have been developed. However, most of them involve some variation of traditional video conferencing, which has limitations, such as not being able to effectively convey spatial cues or share the user's task space. This workshop will focus on how these limitations can be overcome by using Mixed Reality (MR) technology, leading to the development of radically new types of collaborative experiences. Mixed Reality (MR) environments are those that present real world and virtual world objects together within a single display, encompassing Augmented Reality (AR) and Augmented Virtuality (AV), as well as Virtual Reality (VR). With Augmented Reality, virtual data is spatially overlaid on top of a live view of the real world. Augmented Virtuality (AV) refers to the merging of real world objects into a virtual world. Virtual Reality replaces the user's real environments with a computer-generated 3D virtual world and lets the user interact in that world. MR environments can be used to merge the shared perceived realities of different users, as well as enrich each user's own individual experience in a collaborative task. However, despite the potential that MR has for collaborative applications, MR research often focuses on individual usage and more exploration needs to be done on the potential of MR for creating innovative collaborative experiences. This workshop will bring together researchers who are interested in developing collaborative MR systems. We will build a picture of current and prior research on collaboration in MR, as well as set up a common research agenda for work going forward. This, in turn, can be used to grow the research community.},
keywords={augmented reality;groupware;MR research;innovative collaborative experiences;collaborative MR systems;research community;collaborative applications;collaborative task;computer-generated 3D virtual world;virtual reality;virtual data;AV;augmented virtuality;AR;augmented reality;virtual world objects;real world objects;MR environments;MR technology;mixed reality technology;user task space;video conferencing;telepresence technologies;teleconferencing;collaborative tools;experts;problem solving;CoMiRE summary;collaborative mixed reality environments;workshop},
doi={10.1109/ISMAR-Adjunct.2016.0022},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836444,
author={},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Interaction design principles of augmented reality focusing on the ageing population workshop summary},
year={2016},
volume={},
number={},
pages={xxxvii-xxxvii},
abstract={Summary form only given. The complete presentation was not made available for publication as part of the conference proceedings. The workshop aims to bring together experts in the fields of ageing, health and AR design and development. The proposed workshop is timely because it addresses the current crisis in meeting the needs the ageing population. This is a significant real world challenge for technology design and development. The workshop will provide a foundation for developers to identify approaches towards effective user engagement, innovation and design for this vulnerable group.},
keywords={assisted living;augmented reality;virtual information;real-time performance;AR effectiveness;AR usefulness;AR systems;AR applications;AR design principles;interaction design principles;augmented reality;ageing population;Sociology;Statistics;Aging;Augmented reality;Conferences;Space exploration;Human factors},
doi={10.1109/ISMAR-Adjunct.2016.0023},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836445,
author={H. {Saito} and Y. {Itoh} and M. {Sugimoto}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Workshop on Human Behavior Analysis and Visualization for Collective Visual Sensing Summary},
year={2016},
volume={},
number={},
pages={xxxviii-xxxviii},
abstract={This workshop features Collective Visual Sensing (CVS), a developing research area that aims to understand group attention and activities by analyzing information gathered from multiple wearable devices, such as wearable cameras and eye trackers. Understanding user's' gaze information has become a blazing topic in VR/AR community along with the recent growth of wearable display systems. Some existing systems already enable us to shoot objects by “looking” in a VR world. Not only explicit directive information, gazes contains users' implicit attention and behaviors. In the future, such systems would readily allow us to measure “what users want to do” by analyzing their gazes. Furthermore, we foresee that interactions among users in VR/AR environments will drastically change by utilizing collective visual sensing, i.e. analyzing the gaze of multiple users in a shared workspace. To foster this upcoming research topic, this workshop focuses on: (1) developing technologies for extracting measurements through the use of collective visual sensing, (2) developing technologies for understanding group activities and intent, and (3) building assistive systems based on developed technologies for various applications in collaborative workspaces.},
keywords={augmented reality;behavioural sciences computing;gaze tracking;wearable computers;human behavior analysis;collective visual sensing summary visualization;CVS;group attention;multiple wearable devices;user gaze information;VR/AR community;user interactions;measurement extraction;group activities;assistive systems;collaborative workspaces},
doi={10.1109/ISMAR-Adjunct.2016.0024},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836446,
author={A. {Iriarte-Solis} and P. {González-Villegas} and R. {Fuentes-Covarrubias} and G. {Fuentes-Covarrubias}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Mobile Guide to Augmented Reality for Campus of the Autonomous University of Nayarit},
year={2016},
volume={},
number={},
pages={1-4},
abstract={Mobile applications have become very important not only as a mean to disseminate information, but also to serve as guide in unfamiliar contexts. This guide developed with mobile Augmented Reality (AR) aims to give information on the location of points of interest within the UAN campus. The tools used were through markers with Unity3D and Vuforia Software.},
keywords={augmented reality;computer aided instruction;educational technology;mobile computing;mobile guide;augmented reality;Autonomous University of Nayarit;mobile applications;UAN;Unity3D;Vuforia Software;Two dimensional displays;Augmented reality;localization;augmented reality;murals},
doi={10.1109/ISMAR-Adjunct.2016.0025},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836447,
author={A. {Bartoli} and E. {Özgür}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={A Perspective on Non-Isometric Shape-from-Template},
year={2016},
volume={},
number={},
pages={5-10},
abstract={Shape-from-Template (SfT) uses an object's shape template and a deformation law to achieve single-image reconstruction. SfT is a fundamental tool to retexture or augment a deformable object in a monocular video. It has matured for isometric deformations, but the non-isometric case is yet largely open. This is because modeling is generally more complicated and the constraints certainly weaker. Existing algorithms use, for instance, linear elasticity, require one to provide boundary conditions represented by known deformed shape parts and need nonconvex optimization. We use a very simple and generic model to show that non-isometric SfT has a unique solution up to scale under strong perspective imaging and mild deformation curvature. Our model uses a novel type of homography interpretation that we call Perspective-Projection-Affine-Embedding. It may use boundary conditions if available and can be estimated with Linear Least Squares optimization. We provide experimental results on synthetic and real data.},
keywords={concave programming;image reconstruction;least squares approximations;non-isometric shape-from-template;SfT;single-image reconstruction;monocular video;isometric deformation;nonconvex optimization;strong perspective imaging;deformation curvature;perspective-projection-affine-embedding;homography interpretation;linear least squares optimization;Shape;Cameras;Image reconstruction;Boundary conditions;Three-dimensional displays;Geometry},
doi={10.1109/ISMAR-Adjunct.2016.0026},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836448,
author={A. {Rovira} and T. {Taketomi} and R. D. {Constantine} and H. {Kato} and C. {Sandor} and S. {Ikeda}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={EyeAR: Empiric Evaluation of a Refocusable Augmented Reality System},
year={2016},
volume={},
number={},
pages={11-12},
abstract={We present the evaluation of EyeAR, a display with refocusable content based on user's eyes measurements. We carried out a user study to validate the prototype to verify that participants cannot distinguish between real and virtual objects. Participants looked at three pillars (one of which was virtual) placed at different distances from the user. They had to guess which pillar was the virtual one while freely refocusing. The results partially confirmed that our prototype creates virtual objects that are indistinguishable from real objects.},
keywords={augmented reality;human factors;refocusable augmented reality system;EyeAR evaluation;refocusable content;virtual objects;real objects;Prototypes;Visualization;Three-dimensional displays;Augmented reality;Visual systems;Shape;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial;augmented;and virtual realities; I.3.3 [Computing Methodologies]: Picture/Image Generation—Display Methods; H.1.2 [Models and Principles]: User/Machine},
doi={10.1109/ISMAR-Adjunct.2016.0027},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836449,
author={A. {Dolhasz} and I. {Williams} and M. {Frutos-Pascual}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Measuring Observer Response to Object-Scene Disparity in Composites},
year={2016},
volume={},
number={},
pages={13-18},
abstract={Image composites are combinations of image elements from different sources, often combined in a manner to give the appearance of a single, coherent image. This assesses the impact of low-level image feature offsets on observer response with respect to realism of image composites. The response to selected features, namely exposure, contrast and Correlated Colour Temperature (CCT), is evaluated in a series of 3 experiments, each employing 25 human observers. A total of 10890 data points are analysed, 3630 for each experiment, and psychometric functions are fit to this data in order to parametrise and quantify the relationship between human responses and the amount of disparity between object and scene. Average thresholds and their confidence intervals for each of the image features are then presented and discussed, notably indicating a degree of observer variance in realism responses, particularly in the presence of negative contrast disparities. Exposure, as well as CCT offsets are found to be more readily detected, the latter also contributing to some false positives at high offsets, due to illuminationreflectance ambiguity. The resulting thresholds and confidence intervals can be utilised in creating realistic composites, as well as understanding the impact of different features on observers' perception of composite realism.},
keywords={feature extraction;image colour analysis;observers;observer response;object-scene disparity;image composites;image elements;low-level image feature offsets;correlated colour temperature;CCT;psychometric functions;Observers;Mathematical model;Image color analysis;Lighting;Visualization;Computational modeling;Adaptation models;Composite;realism;subjective quality;object;scene;disparity},
doi={10.1109/ISMAR-Adjunct.2016.0028},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836450,
author={A. {Plopski} and T. {Mashita} and A. {Kudo} and T. {Höllerer} and K. {Kiyokawa} and H. {Takemura}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Improving Localization under Varying Illumination},
year={2016},
volume={},
number={},
pages={19-20},
abstract={Localizing the user from a feature database of a scene is a basic and necessary step for presentation of localized augmented reality (AR) content. Commonly such database depicts a single appearance of the scene, due to time and effort required to prepare it. To account for appearance changes under different lighting we propose to generate the feature database from a simulated appearance of the scene model under a number of different lighting conditions. We also propose to extend the feature descriptors used in the localization with a parametric representation of their changes under varying lighting conditions. We compare our method with a standard representation and matching based on L2-norm in a simulation and real world experiments. Our results show that our approach achieves a higher localization rate with fewer feature points and a lower process cost.},
keywords={augmented reality;image matching;image representation;lighting;visual databases;feature descriptors;L2-norm;parametric representation;lighting conditions;appearance changes;localized augmented reality content;feature database;varying illumination;localization improvement;Databases;Lighting;Cameras;Feature extraction;Robustness;Solid modeling;Augmented reality},
doi={10.1109/ISMAR-Adjunct.2016.0029},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836451,
author={A. {Millette} and M. J. {McGuffin}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={DualCAD: Integrating Augmented Reality with a Desktop GUI and Smartphone Interaction},
year={2016},
volume={},
number={},
pages={21-26},
abstract={Head-Mounted Displays (HMDs) combined with 3-or-more Degree-of-Freedom (DoF) input enable rapid manipulation of stereoscopic 3D content. However, such input is typically performed with hands in midair and therefore lacks precision and stability. Also, recent consumer-grade HMDs suffer from limited angular resolution and/or limited field-of-view as compared to a desktop monitor. We present the DualCAD system that implements two solutions to these problems. First, the user may freely switch at runtime between an augmented reality HMD mode, and a traditional desktop mode with precise 2D mouse input and an external desktop monitor. Second, while in the augmented reality HMD mode, the user holds a smartphone in their non-dominant hand that is tracked with 6 DoF, allowing it to be used as a complementary high-resolution display as well as an alternative input device for stylus or multitouch input. Two novel bimanual interaction techniques that leverage the properties of the smartphone are presented. We also report initial user feedback.},
keywords={augmented reality;CAD;graphical user interfaces;helmet mounted displays;mouse controllers (computers);smart phones;augmented reality;desktop GUI;smartphone interaction;head-mounted displays;degree-of-freedom;stereoscopic 3D content manipulation;consumer-grade HMDs;limited angular resolution;limited field-of-view;DualCAD system;precise 2D mouse input;external desktop monitor;6 DoF;complementary high-resolution display;multitouch input;stylus;bimanual interaction techniques;computer-aided design;Three-dimensional displays;Resists;Two dimensional displays;Switches;Mice;Thumb;3D modeling; CAD; Augmented Reality; multimodal; real-world interface prop; HMD; smartphone},
doi={10.1109/ISMAR-Adjunct.2016.0030},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836452,
author={A. {Rukubayihunga} and J. {Didier} and S. {Otmane}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Real Time Noise Reduction to Identify Motion Parameters in AR Maintenance Scenario},
year={2016},
volume={},
number={},
pages={27-30},
abstract={Augmented reality is a field which improves user experience of the real environment by providing some relevant additional data. Understanding what happens in the workspace of the AR system in a maintenance context and for checking compliance of workers actions according to the expected ones are major challenges in this field. Usually, proposed approaches in literature are user centred and consist to gestures classification techniques. We opt to object centred methods. Indeed, when models are well configured, they provide information about motion between parts implied in assembly tasks which could be compared to predefined motion constraints. However, in real conditions, extracted motion curves are very noisy, may be difficult to exploit and may induce some AR systems misinterpretations. In this paper, we propose a method to reduce noise in real time in these curves based on Support Vector Machines confidence scores. The goal is to appropriately weaken false values and correctly straighten rotation axis according to the confidence we could have on pose estimation. Preliminary results are promising but the method still needs some improvements.},
keywords={augmented reality;image denoising;image motion analysis;pose estimation;support vector machines;pose estimation;rotation axis;support vector machines;gesture classification technique;AR system;user experience;augmented reality;AR maintenance scenario;motion parameters;realtime noise reduction;Noise measurement;Support vector machines;Noise reduction;Feature extraction;Pose estimation;Real-time systems;Maintenance engineering;Industrial Augmented Reality;Noise Reduction;Real-Time;Motion Recognition;Compliance},
doi={10.1109/ISMAR-Adjunct.2016.0031},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836453,
author={A. {Irlitti} and R. T. {Smith} and S. {Von Itzstein} and M. {Billinghurst} and B. H. {Thomas}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Challenges for Asynchronous Collaboration in Augmented Reality},
year={2016},
volume={},
number={},
pages={31-35},
abstract={Collaboration is a promising area of investigation for Augmented Reality (AR) applications. While there have been numerous examples of co-located and remote synchronous collaborative AR applications, there has not been the same interest in pursuing asynchronous interfaces. Asynchronous processes differ from their synchronous counterparts due to the collaboration occurring over a period of time, without the requirement of all parties being present simultaneously. For AR applications, asynchronous collaboration is typically considered to be the combination of drawn registered annotations, and their review at a later time. The potential on offer far exceeds this stance. This paper uncovers a unique opportunity for pursuing asynchronous collaboration support in AR, identifies how communications can be enhanced, and discusses the research challenges unique to asynchronous collaboration in AR.},
keywords={augmented reality;groupware;augmented reality;asynchronous collaboration information;AR applications;asynchronous interfaces;registered annotations;Collaboration;Electronic mail;Augmented reality;Context;Stakeholders;Resists;Production;H.5.3 [Information interfaces and presentation (e.g.;HCI)]: Group and Organization Interfaces—Asynchronous interaction—Computer-supported cooperative work; I.3.6 [Computer Graphics]: Methodology and Techniques—Interaction Techniques},
doi={10.1109/ISMAR-Adjunct.2016.0032},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836454,
author={A. {McNamara} and C. {Kabeerdoss}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Mobile Augmented Reality: Placing Labels Based on Gaze Position},
year={2016},
volume={},
number={},
pages={36-37},
abstract={The arrangement of Augmented Reality (AR) labels on mobile displays can cause problems when the labels over populate the screen space. This can result in occlusion, not only of real scene, but also of other labels annotating the scene. We are developing view management systems that arrange and display AR content based on user attention. We present a pilot user study that tests speed and accuracy on a simple information retrieval task on simulated AR scenarios. We varied the presentation in two ways: a naive approach and a gaze-based approach. The results did not show statistical differences between the two methods, yet results from this study provide valuable insight for subsequent studies.},
keywords={augmented reality;information retrieval;mobile computing;gaze-based approach;naive approach;information retrieval;user attention;AR content;view management systems;mobile displays;gaze position;mobile augmented reality labels;Augmented reality;Visualization;Human computer interaction;Mobile communication;Information retrieval;Atmospheric measurements},
doi={10.1109/ISMAR-Adjunct.2016.0033},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836455,
author={A. K. {Hebborn} and N. {Höhner} and S. {Müller}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Augmenting Surface Reconstructions},
year={2016},
volume={},
number={},
pages={38-42},
abstract={The reconstruction of a scene is a fundamental task in many AR applications. Only a dense and complete surface representation enables realistic AR interactions, like occlusions and collisions. The reconstruction of arbitrary scenes is an important issue and many approaches have already been proposed. The results of these methods are mostly dense and accurate, but the reconstructions are still incomplete. This is due to the fact that parts of the scene are invisible during the capturing process (e.g. occluded by another object). We address the problem of filling unknown parts of a surface reconstruction during the capturing process by expanding the well-known KinectFusion pipeline. Our approach diffuses the observed parts to extend the original reconstruction.By showing the results of several scenes, we demonstrate that our algorithm produces visually plausible surfaces. Moreover, we present a method for handling of occlusions and collisions directly on the extended surface.},
keywords={augmented reality;data visualisation;augmenting surface reconstructions;realistic AR interactions;arbitrary scene reconstruction;KinectFusion pipeline;capturing process;augmented reality;Surface reconstruction;Surface treatment;Pipelines;Convolution;Graphics processing units;Kernel;Collision avoidance;Surface Reconstruction;Reconstruction Completion;Surface-Aware Interactions;Surface Diffusion},
doi={10.1109/ISMAR-Adjunct.2016.0034},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836456,
author={A. {Mossel} and M. {Kroeter}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Streaming and Exploration of Dynamically Changing Dense 3D Reconstructions in Immersive Virtual Reality},
year={2016},
volume={},
number={},
pages={43-48},
abstract={We introduce a novel framework that enables large-scale dense 3D scene reconstruction, data streaming over the network and immersive exploration of the reconstructed environment using virtual reality. The system is operated by two remote entities, where one entity - for instance an autonomous aerial vehicle - captures and reconstructs the environment as well as transmits the data to another entity - such as human observer - that can immersivly explore the 3D scene, decoupled from the view of the capturing entity. The performance evaluation revealed the framework's capabilities to perform RGB-D data capturing, dense 3D reconstruction, streaming and dynamic scene updating in real time for indoor environments up to a size of 100m2, using either a state-of-the-art mobile computer or a workstation. Thereby, our work provides a foundation for enabling immersive exploration of remotely captured and incrementally reconstructed dense 3D scenes, which has not shown before and opens up new research aspects in future.},
keywords={autonomous aerial vehicles;image reconstruction;stereo image processing;virtual reality;immersive virtual reality;dense 3D scene reconstruction;data streaming;remote entities;autonomous aerial vehicle;performance evaluation;RGB-D data capturing;dense 3D reconstruction;dynamic scene updating;mobile computer;workstation;Three-dimensional displays;Image reconstruction;Solid modeling;Surface reconstruction;Graphics processing units;Cameras;Streaming media},
doi={10.1109/ISMAR-Adjunct.2016.0035},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836457,
author={A. {Dey} and M. {Billinghurst} and R. W. {Lindeman} and J. E. {Swan II}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={A Systematic Review of Usability Studies in Augmented Reality between 2005 and 2014},
year={2016},
volume={},
number={},
pages={49-50},
abstract={Augmented Reality (AR) interfaces have been studied extensively over the last few decades, with a growing number of user-based experiments. In this paper, we systematically review most AR papers published between 2005 and 2014 that include user studies. A total of 291 papers have been reviewed and classified based on their application areas. The primary contribution of the review is to present the broad landscape of user-based AR research, and to provide a high-level view of how that landscape has changed. We also identify areas where there have been few user studies, and opportunities for future research. This poster describes the methodology of the review and the classifications of AR research that have emerged.},
keywords={augmented reality;pattern classification;augmented reality;user-based AR research classifications;usability studies;Augmented reality;Usability;Collaboration;Haptic interfaces;Entertainment industry;Systematics;Australia},
doi={10.1109/ISMAR-Adjunct.2016.0036},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836458,
author={A. {Oikawa} and I. {Kitahara} and Y. {Kameda} and Y. {Ohta}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Full-Scale Visualization of a Person on a Movable Transparent Screen},
year={2016},
volume={},
number={},
pages={51-52},
abstract={We propose to visualize pre-recorded activity of a person on a movable transparent screen for in-situ reviewing of his/her activity in augmented reality fashion. Activity of the target person was taken as a video by a surveillance camera. Viewers can watch the activity in the scene as if it happened there because segmented image of the target person was projected onto a human-size transparent screen and other static objects around the target person can be visible throughout the transparent screen. Our final goal is to move the transparent screen by mounting it on a small robot and moves the robot to follow the target person in the video. On the way to the final goal, we currently assume that the viewers move the screen manually so as to pose the screen at the same location of the target person in the video. A tracking method of the transparent screen in the scene is devised by utilizing projector-camera calibration and simple infrared marker tracking.},
keywords={augmented reality;data visualisation;image segmentation;robots;video cameras;video surveillance;full-scale visualization;movable transparent screen;augmented reality;video surveillance camera;image segmentation;human-size transparent screen;small robot;target person;tracking method;projector-camera calibration;infrared marker tracking;Cameras;Surveillance;Calibration;Visualization;Augmented reality;Motion segmentation;Spatial augmented reality;projector camera calibration;infrared marker;surveillance camera},
doi={10.1109/ISMAR-Adjunct.2016.0037},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836459,
author={B. {Thompson} and L. {Levy} and A. {Lambeth} and D. {Byrd} and J. {Alcaidinho} and I. {Radu} and M. {Gandy}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Participatory Design of STEM Education AR Experiences for Heterogeneous Student Groups: Exploring Dimensions of Tangibility, Simulation, and Interaction},
year={2016},
volume={},
number={},
pages={53-58},
abstract={In this paper, we present the results of a multi-year participatory design process exploring the space of educational AR experiences for STEM education targeted at students of various ages and abilities. Our participants included teachers, students (ages five to fourteen), educational technology experts, game designers, and HCI researchers. The work was informed by state educational curriculum guidelines. The activities included developing a set of design dimensions which guided our ideation process, iteratively designing, building, and evaluating six prototypes with our stakeholders, and collecting our observations regarding the use of AR STEM applications by target students.},
keywords={augmented reality;computer aided instruction;computer games;human computer interaction;STEM;STEM education AR experiences;heterogeneous student groups;multiyear participatory design process;educational technology experts;game designers;HCI researchers;state educational curriculum guidelines;Games;Education;Ions;Laser beams;Collaboration;Augmented reality;Prototypes;Augmented Reality;Collaborative Design;Education;Handheld AR;K-12},
doi={10.1109/ISMAR-Adjunct.2016.0038},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836460,
author={B. {Sarupuri} and G. A. {Lee} and M. {Billinghurst}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={An Augmented Reality Guide for Assisting Forklift Operation},
year={2016},
volume={},
number={},
pages={59-60},
abstract={Operating forklifts in warehouses is becoming an increasingly difficult task due to higher shelves and narrower aisles. We investigate how Augmented Reality (AR) could aid forklift operators in performing their pallet racking and pick up tasks by superimposing virtual guidelines over a real world camera feed. To test this, we designed and developed a prototype system based on a toy forklift and conducted a user study with it. The results showed that AR cues helped the participants to perform tasks with a higher success rate and provided better usability.},
keywords={augmented reality;warehouse automation;pick up tasks;pallet racking;virtual guidelines;AR;forklift operation;augmented reality guide;Cameras;Guidelines;Augmented reality;Prototypes;Usability;Feeds;Tracking;Augmented Reality;Forklift;Warehouse;Logistics},
doi={10.1109/ISMAR-Adjunct.2016.0039},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836461,
author={B. {Ahmed} and J. H. {Lee} and Y. Y. {Lee} and K. H. {Lee}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Mimicking an Object Using Multiple Projectors},
year={2016},
volume={},
number={},
pages={61-63},
abstract={Recently many researchers have focused on 3D projection mapping systems but reproducing high quality appearances has received relatively less attention. A lot of work has been done in areas of blending multiple projector outputs, 3D projection mapping and large tiled projector mosaics but existing color compensation based frameworks still suffer from contrast compression, color inconsistencies and inappropriate luminance over the randomly shaped projection surface to a certain extent resulting in an inappropriate appearance for realism oriented SAR applications. Until now the problem of having a realistic result with minimal contrast compression and acceptable black levels using projection mapping on 3D objects in order to virtually recreate an original object of similar appearance still remains unsolved. Hereby, we propose a method that enables us to use high quality measured data from original objects and then regenerating the same appearance by projecting optimized images using multiple projectors in order to ensure that projection-rendered results look close to the real object.},
keywords={augmented reality;brightness;data compression;image coding;image colour analysis;optical projectors;rendering (computer graphics);projection-rendering;optimized image projection;minimal contrast compression;realism oriented SAR applications;randomly shaped projection surface;luminance;color inconsistencies;color compensation;projector mosaics;appearance reproduction;3D projection mapping systems;multiple projectors;object mimicking;Decision support systems;Augmented reality;Light transport;projector-pixel optimization;gamuts},
doi={10.1109/ISMAR-Adjunct.2016.0040},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836462,
author={B. {Ridel} and L. {Mignard-Debise} and X. {Granier} and P. {Reuter}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={EgoSAR: Towards a Personalized Spatial Augmented Reality Experience in Multi-user Environments},
year={2016},
volume={},
number={},
pages={64-69},
abstract={Common spatial augmented reality techniques use video projection to superimpose virtual information over a physical scene. As the augmentation happens directly in the real world, multiple users can see the augmented scene, however, the augmentation is the same for all users. We introduce EgoSAR, a new approach that makes it possible to have a personalized, view-dependent augmentation, in multi-user environments. Our key idea is to use retroreflective material for the personalized experience, in conjunction with spatial augmented reality, by combining two different light paths. We implemented our approach in two prototypes combining transparency and either direct or indirect retroreflection. We present two different usage scenarios, show results, and experiment applications that such an approach may provide.},
keywords={augmented reality;indirect retroreflection;direct retroreflection;light paths;personalized experience;retroreflective material;personalized-view-dependent augmentation;augmented scene;virtual information;multiuser environments;personalized spatial augmented reality experience;EgoSAR;Augmented reality;Glass;Prototypes;Mobile communication;Sea surface;Mirrors;Cameras;Spatial Augmented Reality;Retroreflective & Semi-Transparent Display},
doi={10.1109/ISMAR-Adjunct.2016.0041},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836463,
author={B. {Seo} and H. {Wuest}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Robust 3D Object Tracking Using an Elaborate Motion Model},
year={2016},
volume={},
number={},
pages={70-71},
abstract={This paper proposes a new method for robust 3D object tracking from a single RGB image when an object model is available. The proposed method is based on image alignment between consecutive frames over a 3D target object. Different from conventional methods that only rely on image intensity for the alignment, we model intensity variations using the surface normal of the object. From this model, we also define a new constraint for the pose estimation, leading to significant improvement in the tracking robustness. In experiments, we demonstrate the benefits of our method by evaluating it under challenging tracking conditions.},
keywords={image colour analysis;image motion analysis;object tracking;pose estimation;robust 3D object tracking;elaborate motion model;single RGB image;object model;image alignment;image intensity;intensity variations;pose estimation;tracking robustness;Three-dimensional displays;Robustness;Pose estimation;Solid modeling;Computational modeling;Object tracking;Lighting;I.4.8 [Image Processing and Computer Vision]: Scene Analysis—Tracking; H.5.1 [Information Systems]: Information Interface and Representation—Artificial;augmented;and virtual realities},
doi={10.1109/ISMAR-Adjunct.2016.0042},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836464,
author={C. {Barreiros} and E. {Veas} and V. {Pammer-Schindler}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Pre-attentive Features in Natural Augmented Reality Visualizations},
year={2016},
volume={},
number={},
pages={72-73},
abstract={The movement towards cyberphysical systems and Industry 4.0 promises to imbue each and every stage of production with a myriad of sensors. The open question is how people are to comprehend and interact with data originating from industrial machinery. We propose a metaphor that compares machines with natural beings that appeal to people by representing machine states with patterns occurring in nature. Our approach uses augmented reality (AR) to represent machine states as trees of different shapes and colors (BioAR). We performed a study on pre-attentive processing of visual features in AR to determine if our BioAR metaphor conveys fast changes unambiguously and accurately. Our results indicate that the visual features in our BioAR metaphor are processed pre-attentively. In contrast to previous research, for the BioAR metaphor, variations in form induced less errors than variations in hue in the target detection task.},
keywords={augmented reality;cyber-physical systems;data visualisation;Internet;target detection task;BioAR metaphor;visual features;industrial machinery;cyberphysical systems;Industry 4.0;natural augmented reality visualizations;Vegetation;Augmented reality;Data visualization;Biological system modeling;Visualization;Real-time systems;Production facilities;H.1.2 [Models and Principles]: User/Machine Systems—Human information processing; H.5.1 [HCI]: Multimedia Information Systems—Artificial;augmented;and virtual reality},
doi={10.1109/ISMAR-Adjunct.2016.0043},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836465,
author={C. {Lee} and G. L. {Dunn} and I. {Oakley} and J. {Ryu}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Visual Guidance for Encountered Type Haptic Display: A feasibility study},
year={2016},
volume={},
number={},
pages={74-77},
abstract={Virtual/mixed reality leveraging an encountered type haptic display will suffer difficulty if virtual and real objects are spatially discrepant. We propose a new method for resolving this issue, visual guidance. The visual guidance algorithm is defined and described in detail, and contrasted with a previously explored approach. The feasibility of the proposed algorithm is experimentally verified.},
keywords={computer displays;haptic interfaces;virtual reality;visual guidance algorithm;encountered type haptic display;feasibility study;virtual reality;mixed reality;Haptic interfaces;Visualization;Training;Robots;Error analysis;Safety;Presses;Haptic augmented virtuality;spatial discrepancy;encountered type haptic display;visual guidance},
doi={10.1109/ISMAR-Adjunct.2016.0044},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836466,
author={C. {Lim} and C. {Kim} and J. {Park} and H. {Park}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Mobile Augmented Reality Based on Invisible Marker},
year={2016},
volume={},
number={},
pages={78-81},
abstract={This paper proposes an approach for implementing marker-based augmented reality (AR) on smartphone. Specifically, to resolve the obtrusiveness of visual markers, use of infrared (IR) markers that are not visible to the human eye is studied. The main idea is to use an additional external camera with IR functionality to track IR markers that are not detectable in smartphone camera. Additionally is to put a visual fiducial object at the place where the fields of view of the external camera and the smartphone camera are overlapping, which enables the external camera to track the geometric transform between the fiducial object and the IR markers. As a result, since the fiducial object is trackable with the smartphone camera, the smartphone camera pose relative to the IR markers can be computed by using the transform. To validate the feasibility of the proposed approach, a proof-of-concept system is implemented where a visual marker is used as fiducial object for the convenience of implementation. The system accuracy mainly depends on the transform accuracy. Thus, to improve the transform accuracy, two constraints are defined and evaluated: one is that both markers lie on the same plane and the other is that the 3D marker data is available. Through experiments, with the constraints, it is verified that virtual contents can be stably augmented on IR markers in smartphone camera images.},
keywords={augmented reality;cameras;smart phones;mobile augmented reality;invisible marker;marker-based augmented reality;AR;visual markers;infrared markers;IR markers;human eye;IR functionality;visual fiducial object;geometric transform;proof-of-concept system;3D marker data;smartphone camera images;Decision support systems;Augmented reality;mobile augmented reality;invisible marker;fiducial object;external camera with IR functionality},
doi={10.1109/ISMAR-Adjunct.2016.0045},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836467,
author={C. S. C. {Dalim} and A. {Dey} and T. {Piumsomboon} and M. {Billinghurst} and S. {Sunar}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={TeachAR: An Interactive Augmented Reality Tool for Teaching Basic English to Non-Native Children},
year={2016},
volume={},
number={},
pages={82-86},
abstract={Teaching English to children who do not come from an English speaking background is an interesting challenge for educators. In this paper, we present an Augmented reality (AR) tool, TeachAR, for teaching basic English words (colors, shapes, and prepositions) to children for whom English is not a native language. In a pilot study we compare our AR system to a traditional non-AR system. The results indicate a potentially better learning outcome using the TeachAR system than the traditional system. It also showed that children enjoyed using AR-based methods. However, it also showed a few usability issues with the TeachAR interface, which we will improve on in the future.},
keywords={augmented reality;computer aided instruction;graphical user interfaces;human computer interaction;linguistics;teaching;usability issues;TeachAR interface;learning outcome;basic English words teaching;AR tool;nonnative children;interactive augmented reality tool;Shape;Education;Image color analysis;Speech;Color;Augmented reality;Games;Augmented Reality;Teaching and Learning;English Language;Children;Non-Native Speakers},
doi={10.1109/ISMAR-Adjunct.2016.0046},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836468,
author={C. A. {Wiesner} and M. {Ruf} and D. {Sirim} and G. {Klinker}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Visualisation of the Electronic Horizon in Head-Up-Displays},
year={2016},
volume={},
number={},
pages={87-89},
abstract={Dangerous driving situations can arise from poor visibility due to weather conditions or sharp turns. These situations could be prevented, if the driver knew about the future road course, while still being on the lookout for obstacles. The Head-Up-Display (HUD) helps to keep the driver's gaze on the road by projecting virtual information onto the windshield. The electronic horizon provides information about the future road course. The Advanced Driver Assistance Systems Interface Specifications (ADASIS) aims to standardise the way the electronic horizon is distributed amongst the different ADAS applications. Within this work, we combined the HUD with the electronic horizon to create a visualisation of the future road course in the HUD. We implemented this visualisation in a car prototype. The application was developed by using automotive standards, to show feasibility of integrating such a visualisation into a series-production vehicle. In future work we intend to investigate the change of drivers' behaviour while using this application with a user study.},
keywords={data visualisation;driver information systems;head-up displays;electronic horizon visualization;head-up-displays;driving situation;HUD;advanced driver assistance systems interface specifications;ADASIS;future road course visualization;driver behaviour;Visualization;Roads;Automobiles;Prototypes;Automotive components;Protocols;H.1.2 [Models and Principles]: User/Machine Systems—Human factors; H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artifical;augmented;and virtual realities},
doi={10.1109/ISMAR-Adjunct.2016.0047},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836469,
author={C. C. {Martínez} and J. M. {Carranza} and W. {Mayol-Cuevas} and M. O. {Arias Estrada}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Enhancing 3D Mapping via Real-Time Superpixel-based Segmentation},
year={2016},
volume={},
number={},
pages={90-95},
abstract={In this work we address the problem of 3D mapping of environments with large textureless regions, which generates sparse 3D maps that may not represent well the mapped scene. To deal with this problem, we propose to enhance sparse 3D maps by using a superpixel-based segmentation with the aim of generating denser 3D maps of the scene in real time. This can be exploited in virtual reality applications where rapid generation of 3D maps may be required. Superpixels are middle-level features, which represent a homogeneous regions in an image, that can be connected in order to segment untextured areas. Based on this, we propose a GPU architecture for: i) superpixel extraction considering chromatic information, ii) superpixel-based segmentation, generation of connectivity matrix to compute the connected components algorithm and iii) mapping of segmented regions to 3D points. We use the ORBSLAM system [11] to generate a sparse 3D map and to project the untextured segments onto it at 25 fps. We assessed our approach in terms of the segmentation and map quality. Regarding the latter, covered area by the generated map, depth accuracy and computational performance are reported.},
keywords={feature extraction;graphics processing units;image segmentation;image texture;real-time systems;virtual reality;3D mapping;real-time superpixel-based segmentation;large textureless regions;sparse 3D maps;virtual reality;GPU architecture;superpixel extraction;Three-dimensional displays;Image segmentation;Graphics processing units;Simultaneous localization and mapping;Feature extraction;Real-time systems;Cameras;Superpixel;SLIC (Simple Linear Iterative Clustering);Segmentation;SLAM (Simultaneous Localization and Mapping);3D mapping;GPU (Graphics Processor Unit)},
doi={10.1109/ISMAR-Adjunct.2016.0048},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836470,
author={D. {Andersen} and V. {Popescu} and C. {Lin} and M. E. {Cabrera} and A. {Shanghavi} and J. {Wachs}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={A Hand-Held, Self-Contained Simulated Transparent Display},
year={2016},
volume={},
number={},
pages={96-101},
abstract={Hand-held transparent displays are important infrastructure for augmented reality applications. Truly transparent displays are not yet feasible in hand-held form, and a promising alternative is to simulate transparency by displaying the image the user would see if the display were not there. Previous simulated transparent displays have important limitations, such as being tethered to auxiliary workstations, requiring the user to wear obtrusive head-tracking devices, or lacking the depth acquisition support that is needed for an accurate transparency effect for close-range scenes.We describe a general simulated transparent display and three prototype implementations (P1, P2, and P3), which take advantage of emerging mobile devices and accessories. P1 uses an off-theshelf smartphone with built-in head-tracking support; P1 is compact and suitable for outdoor scenes, providing an accurate transparency effect for scene distances greater than 6m. P2 uses a tablet with a built-in depth camera; P2 is compact and suitable for short-distance indoor scenes, but the user has to hold the display in a fixed position. P3 uses a conventional tablet enhanced with on-board depth acquisition and head tracking accessories; P3 compensates for user head motion and provides accurate transparency even for closerange scenes. The prototypes are hand-held and self-contained, without the need of auxiliary workstations for computation.},
keywords={augmented reality;computer displays;smart phones;hand-held self-contained simulated transparent display;augmented reality applications;head-tracking devices;depth acquisition support;smart phone;head-tracking support;Cameras;Head;Prototypes;Geometry;Image color analysis;Workstations;Visualization;Simulated transparent smartphone;simulated transparent tablet;infrastructure for augmented reality applications},
doi={10.1109/ISMAR-Adjunct.2016.0049},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836471,
author={E. {van der Meulen} and M. A. {Cidota} and S. G. {Lukosch} and P. J. M. {Bank} and A. J. C. {van der Helm} and V. T. {Visch}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={A Haptic Serious Augmented Reality Game for Motor Assessment of Parkinson's Disease Patients},
year={2016},
volume={},
number={},
pages={102-104},
abstract={In the clinical community there is a need for assessment tools that allow for objective, quantitative and valid measures of motor dysfunction. In this paper, we report on the design and evaluation of a serious game that engages patients with Parkinson's disease in upper extremity (hand/arm) movements. The game employs augmented reality to show virtual movement targets, i.e. candies falling from a conveyor belt, and a haptic game controller to catch the candies, that is able to acquire quantitative data about the patient's movement. This paper first describes the design process of the game and the system components. Secondly, we present results of our small quantitative evaluation study (N11, age: 26-60, healthy persons) regarding the usability of the system, the task load and user experience of the game. Our findings show that the system has a relatively good usability and the game is engaging, but there is still need for technical improvement with regard to tracking the controller in 3D space.},
keywords={augmented reality;belts;conveyors;diseases;haptic interfaces;human factors;medical computing;serious games (computing);motor assessment;haptic serious augmented reality game;3D space;task load;user experience;conveyor belt;candies;haptic game controller;upper extremity movements;Parkinson's disease patients;motor dysfunction;Games;Belts;Usability;Haptic interfaces;Tracking;Augmented reality;Extremities;Augmented Reality;Optical See-Through Head Mounted Display;Haptic Device;Serious Game Design;Assessment of Upper Extremity Motor Dysfunction;Parkinson's Disease.},
doi={10.1109/ISMAR-Adjunct.2016.0050},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836472,
author={F. {Okura} and Y. {Nishizaki} and T. {Sato} and N. {Kawai} and N. {Yokoya}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Motion Parallax Representation for Indirect Augmented Reality},
year={2016},
volume={},
number={},
pages={105-106},
abstract={Indirect augmented reality (IAR) presents pre-generated augmented images for achieving high-quality geometric and photometric registration between pre-captured images and virtual objects. Meanwhile, IAR causes spatial inconsistency between the real world and presented images when users move from the location where the real scene was captured. This paper describes a novel way to address the spatial inconsistency; namely, enabling viewpoint change in IAR. The key idea of this study is to employ an image-based rendering technique using pre-captured multi-view omnidirectional images to provide free-viewpoint navigation. For a pilot study, we have developed an IAR system representing a motion parallax effect using an optical-flow-based camera motion estimation.},
keywords={augmented reality;cameras;image capture;image motion analysis;rendering (computer graphics);optical-flow-based camera motion estimation;free-viewpoint navigation;pre-captured multiview omnidirectional images;image-based rendering technique;photometric registration;virtual objects;IAR system;motion parallax representation;indirect augmented reality;Cameras;Three-dimensional displays;Augmented reality;Rendering (computer graphics);Optical imaging;Solid modeling;Image reconstruction;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial;augmented;and virtual realities},
doi={10.1109/ISMAR-Adjunct.2016.0051},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836473,
author={G. {Gupta} and N. {Kejriwal} and P. {Pallav} and E. {Hassan} and S. {Kumar} and R. {Hebbalaguppe}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Indoor Localisation and Navigation on Augmented Reality Devices},
year={2016},
volume={},
number={},
pages={107-112},
abstract={We present a novel indoor mapping and localisation approach for Augmented Reality (AR) devices that exploits the fusion of inertial sensors with visual odometry. We have demonstrated the approach using Google Glass (GG) and Google Cardboard (GC) supported with an Android phone. Our work presents an application of Extended Kalman Filter (EKF) for sensor fusion for AR based application where previous work on Bag of Visual Words Pairs (BoVWP) [10] based image matching is used for bundle adjustment on Fused odometry. We present the empirical validation of this approach on three different indoor spaces in an office environment. We concluded that vision complimented with inertial data effectively compensate the ego-motion of the user, improving the accuracy of map generation and localisation.},
keywords={augmented reality;Kalman filters;nonlinear filters;sensor fusion;indoor localisation;indoor navigation;augmented reality devices;visual odometry;Google Glass;Google Cardboard;GG;GC;Android phone;extended Kalman filter;EKF;sensor fusion;bag of visual words pairs;BoVWP;image matching;bundle adjustment;Fused odometry;ego-motion;map generation;map localisation;Servers;Cameras;Sensor fusion;Visualization;Simultaneous localization and mapping;Optical sensors;Human-centered computing [Interaction paradigms]: Mixed/augmented reality—; Mathematics of computing [Probabilistic reasoning algorithms]: Kalman filters—},
doi={10.1109/ISMAR-Adjunct.2016.0052},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836474,
author={G. {Luzhnica} and C. {Öjeling} and E. {Veas} and V. {Pammer}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Technical Concept and Technology Choices for Implementing a Tangible Version of the Sokoban Game},
year={2016},
volume={},
number={},
pages={113-114},
abstract={This paper presents and discusses the technical concept of a virtual reality version of the Sokoban game with a tangible interface. The underlying rationale is to provide spinal-cord injury patients who are learning to use a neuroprosthesis to restore their capability of grasping with a game environment for training. We describe as relevant elements to be considered in such a gaming concept: input, output, virtual objects, physical objects, activity tracking and personalised level recommender. Finally, we also describe our experiences with instantiating the overall concept with hand-held mobile phones, smart glasses and a head mounted cardboard setup.},
keywords={helmet mounted displays;interactive devices;medical computing;mobile computing;patient rehabilitation;serious games (computing);virtual reality;head mounted cardboard setup;smart glasses;hand-held mobile phones;personalised level recommender;activity tracking;physical objects;virtual objects;gaming concept;game environment;neuroprosthesis;spinal-cord injury patients;tangible interface;virtual reality version;tangible version;Sokoban game;technology choices;Games;Training;Grasping;Virtual reality;Mobile handsets;Glass;Medical treatment;H.5.2 [HCI]: User Interfaces—Input devices and strategies; H.5.1 [HCI]: Multimedia Information Systems—Artificial;augmented;and virtual realities},
doi={10.1109/ISMAR-Adjunct.2016.0053},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836475,
author={G. A. {Lee} and J. {Chen} and M. {Billinghurst} and R. {Lindeman}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Enhancing Immersive Cinematic Experience with Augmented Virtuality},
year={2016},
volume={},
number={},
pages={115-116},
abstract={Watching spherical panorama movies is one of the applications of head mounted displays, especially the growing number of lowcost consumer devices. In this poster, we show how to enhance a personal immersive cinematic experience by embedding the user's body in the movie scene using Augmented Virtuality technology. User embodiment, transitioning between real and virtual spaces, and adding interactivity are the main benefits of our approach. We present a proof of concept prototype, and summarize the findings from a focus group held to collect feedback from potential users. These insights will help developers who are creating immersive cinematic experiences.},
keywords={augmented reality;cinematography;helmet mounted displays;interactivity;virtual spaces;real spaces;movie scene immersion;head mounted displays;spherical panorama movies;augmented virtuality technology;immersive cinematic experience enhancement;Motion pictures;Prototypes;Augmented virtuality;Resists;Head;Virtual environments;Augmented Virtuality;Mixed Reality;Panorama Movie;Transitional Interface},
doi={10.1109/ISMAR-Adjunct.2016.0054},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836476,
author={H. {Sasanuma} and Y. {Manabe} and N. {Yata}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Diminishing Real Objects and Adding Virtual Objects Using a RGB-D Camera},
year={2016},
volume={},
number={},
pages={117-120},
abstract={AR technology is often used in applications that simulate an arrangement of furniture. These applications superimpose CG furnitures on any place. These applications cannot replace the CG furniture with a real furniture if there is a real furniture. To solve this problem, this paper proposes a method that can erase real object from the real environment (Diminished Reality) and add virtual object to the real environment (Augmented Reality) including the region of erased object using RGB-D camera.},
keywords={augmented reality;furniture;image colour analysis;object detection;real objects;virtual objects;RGB-D camera;AR technology;CG furnitures;diminished reality;augmented reality;Surface treatment;Cameras;Augmented reality;Image segmentation;Three-dimensional displays;Surface texture;Real-time systems;Augmented Reality;Diminished Reality;RGB-D camera},
doi={10.1109/ISMAR-Adjunct.2016.0055},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836477,
author={H. {Naik} and M. {Bahaa} and F. {Tombari} and P. {Keitler} and N. {Navab}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Frustration Free Pose Computation For Spatial AR Devices in Industrial Scenario},
year={2016},
volume={},
number={},
pages={121-122},
abstract={The quest for finding the killer application for Industrial Augmented Reality(IAR) is still active. The existing solutions are ingenious but most can not be directly integrated with the existing industrial workflows. Generally, IAR applications require modifications in the industrial workflows depending on the tracking methodology. These modifications end up being an overhead for the users and deter them from using AR solutions. In this poster we propose a resourceful solution to achieve end-to-end workflow integration with minimum effort from the user end. The solution is suited for laser guided Spatial Augmented Reality(SAR) systems mainly preferred for industrial manufacturing applications. We also introduce a new concept for pose computation, which is inspired from existing mechanical concept of part alignment. The accuracy of our method is comparable to the classical marker based methods. The complete process of pose computation, from initialisation to refinement, is designed to be plug and play.},
keywords={augmented reality;manufacturing processes;production engineering computing;classical marker based methods;industrial manufacturing applications;SAR systems;laser guided spatial augmented reality system;end-to-end workflow integration;tracking methodology;IAR;industrial workflows;industrial augmented reality;spatial AR devices;frustration free pose computation;Computational modeling;Solid modeling;Laser applications;Three-dimensional displays;Iterative closest point algorithm;Cameras;Alignment;Industrial AR;Stereo;Laser;Production;Manufacturing},
doi={10.1109/ISMAR-Adjunct.2016.0056},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836478,
author={I. {Ihm} and Y. {Kim} and J. {Lee} and J. {Jeong} and I. {Park}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Low-Cost Depth Camera Pose Tracking for Mobile Platforms},
year={2016},
volume={},
number={},
pages={123-126},
abstract={The KinectFusion algorithm is now used routinely to reconstruct dense 3D surfaces at real-time frame rates using a commodity depth camera. To achieve robust pose estimation, the method conducts the frame-to-model tracking during camera tracking that must inevitably accompany the memory-bound, GPU-assisted volumetric computations for the model manipulation, to which mobile processors are often more vulnerable than PC-based processors. In this paper, we present an effective camera-tracking method that is based on the computationally lighter frame-to-frame tracking method. This method's tendency toward rapid accumulation of pose estimation errors is suppressed effectively via a predictor-corrector technique. By removing the costly volumetric computations from the pose estimation process, our camera tracking system becomes more efficient in terms of both time and space complexity, offering a compact implementation of depth sensor-based camera tracking on low-end platforms such as mobile devices in addition to high-end PCs.},
keywords={cameras;computer graphics;error analysis;graphics processing units;mobile computing;pose estimation;program processors;low-cost depth camera pose tracking;KinectFusion algorithm;dense 3D surfaces reconstruction;robust pose estimation;frame-to-model tracking;GPU-assisted volumetric computations;PC-based processors;mobile processors;pose estimation errors;predictor-corrector technique;low-end platforms;depth sensor-based camera tracking;Cameras;Three-dimensional displays;Computational modeling;Iterative closest point algorithm;Solid modeling;Pose estimation;Mobile communication;I.3.3 [Computer Graphics]: Picture/Image Generation—Digitizing and Scanning; I.4.8 [Image Processing and Computer Vision]: Scene Analysis—Tracking H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial},
doi={10.1109/ISMAR-Adjunct.2016.0057},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836479,
author={I. A. {Kakadiaris} and M. M. {Islam} and T. {Xie} and C. {Nikou} and A. B. {Lumsden}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={iRay: Mobile AR Using Structure Sensor},
year={2016},
volume={},
number={},
pages={127-128},
abstract={Using depth information has become more popular in recent years, as it adds a new dimension to 2D camera views. We have developed a novel mobile application called iRay, which uses depth information to achieve highly accurate markerless registration on a mobile device for medical use. We use a Structure Sensor to capture depth information that is portable to iPad. Its SDK also provides SLAM data to track pose. ICP is applied to achieve highly accurate registration between the 3D surface of a human torso and a pre-scanned torso model. The experiments demonstrate our results under motion blur, partial occlusion, and small movements. This application has the potential to be used for medical education and intervention.},
keywords={medical computing;mobile computing;iRay;mobile AR;structure sensor;medical education;partial occlusion;motion blur;prescanned torso model;human torso;3D surface;ICP;SLAM data;SDK;iPad;medical use;mobile device;mobile application;2D camera views;depth information;Medical diagnostic imaging;Augmented reality;Torso;Three-dimensional displays;Mobile communication;Cameras;Medical AR;Markerless Registration;Mobile AR;ICP;Structure Sensor},
doi={10.1109/ISMAR-Adjunct.2016.0058},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836480,
author={I. {Seidinger} and J. {Grubert}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={3D Character Customization for Non-Professional Users in Handheld Augmented Reality},
year={2016},
volume={},
number={},
pages={129-134},
abstract={In gaming, customizing individual characters, can create personal bonds between players and their characters. Hence, character customization is a standard component in many games. While mobile Augmented Reality (AR) games become popular, to date, no 3D character editor for AR games exists. We investigate the feasibility of 3D character customization for smartphone-based AR in an iterative design process.Specifically, we present findings from creating AR prototypes in a handheld AR setting. In a first user study, we found that a tangible AR prototype resulted in higher hedonistic measures than a camerabased approach. In a follow up study, we compared the tangible AR prototype with a non-AR touchscreen version for selection, scaling, translation and rotation tasks in a 3D character customization setting. The tangible AR version resulted in significantly better results for stimulation and novelty measures than the non-AR version. At the same time, it maintained a proficient level in pragmatic measures such as accuracy and efficiency.},
keywords={augmented reality;computer games;mobile computing;smart phones;3D character customization;handheld augmented reality;mobile augmented reality games;smartphone-based AR;iterative design process;tangible AR prototype;nonAR touchscreen;Prototypes;Games;Three-dimensional displays;Mobile communication;Cameras;Augmented reality;Pragmatics;K.6.1 [Management of Computing and Information Systems]: Project and People Management—Life Cycle; K.7.m [The Computing Profession]: Miscellaneous—Ethics},
doi={10.1109/ISMAR-Adjunct.2016.0059},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836481,
author={J. A. {de Jesús Osuna-Coutiño} and C. {Cruz-Martínez} and J. {Martinez-Carranza} and M. {Arias-Estrada} and W. {Mayol-Cuevas}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={I Want to Change My Floor: Dominant Plane Recognition from a Single Image to Augment the Scene},
year={2016},
volume={},
number={},
pages={135-140},
abstract={Augmented reality combines real footage taken of a scene with virtual elements. However, most current methods rely on camera localisation and 3D reconstruction or point cloud generation in order to integrate augmented reality to the footage. In contrast, in this work we present a novel method to augment virtual reality to the scene based on the recognition of dominant planes in interior scenes. Our method uses a rule system to select predefined decisions on a set of variables in order to infer dominant planes in the scene. For this, we propose to combine information from texture features, a measure of blurring in the dominant planes, and a segmentation of regions in a scene based on superpixels. The rule system infers the regions corresponding to the dominant planes, whilst light intensity in the scene is inferred from segmented regions. We also propose an approach to remove regions misclassified as dominant planes. Finally, the floor in the scene is recognised as the most dominant plane and then replaced with an augmented texture. We demonstrate our approach in a video sequence where our method is applied in a frame-to-frame basis, thus, for each single image in the video sequence, the floor is automatically recognised as the most dominant plane and then replaced with a virtual texture and furthermore, whose appearance is modified according to our coarse light model inferred from our approach.},
keywords={augmented reality;feature extraction;image restoration;image sequences;image texture;object recognition;dominant plane recognition;scene augmentation;augmented reality;point cloud generation;rule system;texture features;blurring measure;superpixels;light intensity;video sequence;coarse light model;Augmented reality;Three-dimensional displays;Training;Logistics;Image recognition;Cameras;Computational modeling;Augmented Reality;Plane Recognition;Dominant Planes;Single Image},
doi={10.1109/ISMAR-Adjunct.2016.0060},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836482,
author={J. {Lanier} and V. {Mateevitsi} and K. {Rathinavel} and L. {Shapira} and J. {Menke} and P. {Therien} and J. {Hudman} and G. {Speiginer} and A. S. {Won} and A. {Banburski} and X. {Benavides} and J. {Amores} and J. P. {Lurashi} and W. {Chang}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={The RealityMashers: Augmented Reality Wide Field-of-View Optical See-Through Head Mounted Displays},
year={2016},
volume={},
number={},
pages={141-146},
abstract={Optical see-through (OST) displays can overlay computer generated graphics on top of the physical world, effectually fusing the two worlds together. However, current OST displays have a limited (compared to the human) field-of-view (FOV) and are powered by laptops which hinders their mobility. Furthermore the systems are designed for single-user experiences and therefore cannot be used for collocated multi-user applications. In this paper we contribute the design of the RealityMashers, two wide FOV OST displays that can be manufactured using rapid-prototyping techniques. We also contribute preliminary user feedback providing insights into enhancing future RealityMasher experiences. By providing the RealityMasher's schematics we hope to make Augmented Reality more accessible and as a result accelerate the research in the field.},
keywords={augmented reality;RealityMashers;augmented reality wide field-of-view optical see-through head mounted displays;computer generated graphics;physical world;wide-FOV-OST displays;rapid-prototyping techniques;Augmented reality;Resists;Software;Tracking;Hardware;Portable computers;Optical imaging;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial;augmented and virtual realities; H.5.3 [Information Interfaces and Presentation]: Group and Organization Interfaces—Collaborative computing},
doi={10.1109/ISMAR-Adjunct.2016.0061},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836483,
author={J. {Tsukamoto} and D. {Iwai} and K. {Kashima}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Distributed Optimization for Shadow Removal in Spatial Augmented Reality},
year={2016},
volume={},
number={},
pages={147-148},
abstract={This paper proposes a novel shadow removal technique for cooperative projection system based-on spatio-temporal prediction. In our previous work, we proposed a distributed feedback algorithm, which is implementable in cooperative projection environments subject to data transfer constraints between components. A weakness of this scheme is that the compensation is conducted in each pixel independently. As a result, spatio-temporal information of the environmental change cannot be utilized even if it is available. In view of this, we specifically investigate the situation where some of projectors are occluded by a moving object whose oneframe-ahead behavior is predictable. In order to remove the resulting shadow, we propose a novel error propagating scheme that is still implementable in a distributed manner, and enables us to incorporate the prediction information of the obstacle. It is demonstrated experimentally that the proposed method significantly improves the shadow removal performance comparison to the previous work.},
keywords={augmented reality;image motion analysis;optimisation;spatiotemporal phenomena;shadow removal performance improvement;obstacle prediction information;error propagating scheme;one-frame-ahead behavior;environmental change;spatiotemporal information;spatiotemporal prediction;cooperative projection system;spatial augmented reality;distributed optimization;Robustness;Radiometry;Cameras;Augmented reality;Optimization;Electronic mail;Heuristic algorithms;Computing methodologies [Computer graphics]: Graphics systems and interfaces—Mixed/augmented reality},
doi={10.1109/ISMAR-Adjunct.2016.0062},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836484,
author={J. {Lee} and K. {Lee} and B. {Nam} and Y. {Wu}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={IoT Platform-based iAR: a Prototype for Plant O M Applications},
year={2016},
volume={},
number={},
pages={149-150},
abstract={In this paper, we present an iAR prototype designed for offshore/onshore plant's piping O&M. We have defined terms of iAR as “integrated-AR” and “intelligent-AR” in previous paper. That is, AR technology or application should be integrated with CAD system (or CAD oriented system) and be able to support engineering knowledge to be the successful industry application. The system uses “Intergraph Smart™ 3D for Plant” to export piping model data and “PTC ThingWorx” as IoT platform. The architecture of our prototype involves two main modules: converting pipe drawing data to 3D model and connecting sensor data to 3D model. The first module includes parametric piping symbol data generator, which can directly generate 3D geometry model from isometric piping drawing files. In the second module, we developed fast sensor data connecting methods for connecting sensor device with 3D model. By combining these processing modules, the proposed system is able to successfully apply to plant's O&M (operation and maintenance).},
keywords={augmented reality;CAD;Internet of Things;mechanical engineering computing;connecting sensor device;IoT platform-based iAR prototype;offshore/onshore plant's;intelligent AR technology;CAD oriented system;Intergraph Smart 3D;piping model data;PTC ThingWorx;pipe drawing data;parametric piping symbol data generator;3D geometry model;isometric piping drawing files;fast sensor data connecting methods;Augmented reality;Internet of Things;O&M;S3D;Isometric drawing;ThingWorx;IoT Platform},
doi={10.1109/ISMAR-Adjunct.2016.0063},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836485,
author={K. {Lee} and J. {Kim} and J. {Ryu} and J. {Kim}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Chromaticity Based Local Linear Regression for Color Distortion Estimation of Optical See-Through Displays},
year={2016},
volume={},
number={},
pages={151-153},
abstract={Optical See-Through displays (OST Displays) require the appearance of the virtual content to closely resemble that of the real world. However, current OST displays typically show color distortion due to hardware limitations. To overcome this problem, we propose a chromaticity based local linear regression model for color distortion estimation of OST displays. From our initial experiments, it was observed that local samples with similar chromaticity can be modeled more linearly rather than simple nearest neighbors in a color space. We argue that the accuracy of local linear regression can be further improved by placing emphasis on similarity of chromaticity. Experimental results showed that the proposed chromaticity based color correction achieves higher accuracy compared to the nearest neighbor method, for any training set size or neighborhood size as well as global regression. It confirms that chromaticity based nearest neighbors can be modeled more linearly together than simple nearest neighbors in a color space.},
keywords={colour displays;estimation theory;regression analysis;virtual reality;chromaticity based local linear regression;color distortion estimation;optical see-through displays;OST displays;virtual content;chromaticity based color correction;chromaticity based nearest neighbors;Augmented reality;OST Display;color distortion model;color correction;local linear regression;augmented reality},
doi={10.1109/ISMAR-Adjunct.2016.0064},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836486,
author={L. {Qian} and A. {Winkler} and B. {Fuerst} and P. {Kazanzides} and N. {Navab}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Modeling Physical Structure as Additional Constraints for Stereoscopic Optical See-Through Head-Mounted Display Calibration},
year={2016},
volume={},
number={},
pages={154-155},
abstract={For stereoscopic optical see-through head-mounted display calibration, existing methods that calibrate both eyes at the same time highly depend on the HMD user's unreliable depth perception. On the other hand, treating both eyes separately requires the user to perform twice the number of alignment tasks, and does not satisfy the physical structure of the system. This paper introduces a novel method that models physical structure as additional constraints and explicitly solves for intrinsic and extrinsic parameters of the stereoscopic system by optimizing a unified cost function. The calibration does not involve the unreliable depth alignment of the user, and lessens the burden for user interaction.},
keywords={augmented reality;calibration;helmet mounted displays;optimisation;physical structure;stereoscopic optical see-through head-mounted display calibration;HMD user depth perception;cost function optimization;augmented reality;Calibration;Stereo image processing;Augmented reality;Adaptive optics;Optimization methods;Data acquisition;Augmented Reality;SPAAM;OST-HMD Calibration and Human Factors},
doi={10.1109/ISMAR-Adjunct.2016.0065},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836487,
author={L. {Qian} and A. {Winkler} and B. {Fuerst} and P. {Kazanzides} and N. {Navab}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Reduction of Interaction Space in Single Point Active Alignment Method for Optical See-Through Head-Mounted Display Calibration},
year={2016},
volume={},
number={},
pages={156-157},
abstract={With users always involved in the calibration of optical see-through head-mounted displays, the accuracy of calibration is subject to human-related errors, for example, postural sway, an unstable input medium, and fatigue. In this paper we propose a new calibration approach: Fixed-head 2 degree-of-freedom (DOF) interaction for Single Point Active Alignment Method (SPAAM) reduces the interaction space from a typical 6 DOF head motion to a 2 DOF cursor position on the semi-transparent screen. It uses a mouse as input medium, which is more intuitive and stable, and reduces user fatigue by simplifying and speeding up the calibration procedure. A multi-user study confirmed the significant reduction of human-related error by comparing our novel fixed-head 2 DOF interaction to the traditional interaction methods for SPAAM.},
keywords={augmented reality;calibration;helmet mounted displays;human factors;mouse controllers (computers);interaction space reduction;single point active alignment method;optical see-through head-mounted display calibration;human-related errors;fixed-head 2 degree-of-freedom interaction;DOF interaction;SPAAM;6 DOF head motion;2 DOF cursor position;semitransparent screen;mouse;user fatigue reduction;augmented reality;Calibration;Augmented reality;Head;Adaptive optics;Three-dimensional displays;Optical imaging;Mice;Augmented Reality;SPAAM;OST-HMD Calibration and Human Factors},
doi={10.1109/ISMAR-Adjunct.2016.0066},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836488,
author={M. {Mori} and J. {Orlosky} and K. {Kiyokawa} and H. {Takemura}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={A Transitional AR Furniture Arrangement System with Automatic View Recommendation},
year={2016},
volume={},
number={},
pages={158-159},
abstract={Augmented Reality furniture arrangement systems are useful for viewing room or building layouts without having to buy or move real furniture. However, such systems often require users to physically and frequently change their viewpoint of the physical space, which requires manual manipulation of the scene, and are often limited to 2D tablet or phone interfaces. To help address this problem, we have developed a system that automatically calculates the most suitable viewpoint to improve understanding of the room layout as a whole, and allows the user to easily transition to that viewpoint in a head mounted display (HMD) with minimal interaction. Experimental results show that users can grasp the layout more easily with our system compared to a conventional AR interface.},
keywords={augmented reality;furniture;helmet mounted displays;recommender systems;user interfaces;transitional AR furniture arrangement system;automatic view recommendation;augmented reality furniture arrangement systems;room layouts;building layouts;2D tablet;phone interfaces;head mounted display;HMD;Layout;Augmented reality;Solid modeling;Cameras;Electronic mail;Grasping;Headphones;H.5.1 [Multimedia Information Systems]: Artificial;augmented;and virtual realities},
doi={10.1109/ISMAR-Adjunct.2016.0067},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836489,
author={M. {Cavallo} and G. A. {Rhodes} and A. G. {Forbes}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Riverwalk: Incorporating Historical Photographs in Public Outdoor Augmented Reality Experiences},
year={2016},
volume={},
number={},
pages={160-165},
abstract={This paper introduces a user-centered Augmented Reality (AR) approach for publishing 2D media archives as interactive content. We discuss the relevant technical considerations for developing an effective application for public outdoor AR experiences that leverage context-specific elements in a challenging, real-world environment. Specifically, we show how a classical marker-less approach can be combined with mobile sensors and geospatial information in order apply our knowledge of the surroundings to the experience itself. Our contributions provide the enabling technology for Chicago 0,0 Riverwalk, a novel app-based AR experience that superimposes historical imagery onto matching views in downtown Chicago, Illinois along an open, pedestrian waterfront located on the bank of the Chicago River. Historical photographs of sites along the river are superimposed onto buildings, bridges, and other architectural features through image-based AR tracking, providing a striking experience of the city's history as rooted in extant locations along the river.},
keywords={augmented reality;history;image processing;multimedia computing;user interfaces;historical photographs;public outdoor augmented reality experiences;user-centered augmented reality;2D media archive;interactive content;context-specific elements;mobile sensors;geospatial information;Chicago Riverwalk;historical imagery;Chicago Illinois;pedestrian waterfront;Chicago river;image-based AR tracking;multimedia information systems;Cameras;Sensors;Two dimensional displays;Rivers;Three-dimensional displays;Augmented reality;Mobile communication;H.5.1 [Information interfaces and presentation (e.g. HCI)]: Multimedia Information Systems—Artificial;augmented and virtual realities},
doi={10.1109/ISMAR-Adjunct.2016.0068},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836490,
author={M. {Gandy} and L. {Levy} and S. {Robertson} and J. {Johnson} and J. {Wilson} and T. {Lemieux} and S. {Tamasi} and D. {Mashman} and M. {Sumler} and L. L. {Hill}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Designing AR Systems to Explore Point-of-View, Bias, and Trans-cultural Conflict},
year={2016},
volume={},
number={},
pages={166-171},
abstract={Over ten years ago, we created a novel dramatic augmented reality (AR) experience exploring bias and point-of-view (PoV) based upon the classic film “Twelve Angry Men,” which allowed a user to experience a dramatic jury room deliberation from the PoV of each of four different characters. Recently, informed by this previous work, we have created a new AR platform for engaging users in different PoVs, exposing forms of biases, and studying cultural conflicts. We are currently using this system for training and assessment in two domains: healthcare and psychological studies of terrorism. In this paper we present the requirements we have identified for this type of user experience, the co-design of both AR environments with domain experts, and the results of an initial user study of technology acceptance that yielded positive feedback from participants.},
keywords={augmented reality;human factors;AR systems;point-of-view;transcultural conflict;augmented reality experience;health care;terrorism psychological studies;user experience;Medical services;Training;Psychology;Prototypes;Augmented reality;Teamwork;Films;Augmented Reality;Training;User Studies;Communication},
doi={10.1109/ISMAR-Adjunct.2016.0069},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836491,
author={M. A. {Cidota} and R. M. S. {Clifford} and S. G. {Lukosch} and M. {Billinghurst}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Using Visual Effects to Facilitate Depth Perception for Spatial Tasks in Virtual and Augmented Reality},
year={2016},
volume={},
number={},
pages={172-177},
abstract={When developing serious games relying on spatial placement tasks in Augmented/Virtual Reality (AR/VR), having an accurate depth perception of virtual objects is highly important. It even becomes essential when such games are used for assessment of motion disorders. Patients should be able to naturally interact with the virtual and augmented environment and to perceive the correct 3D position of objects around them. We report on the results of a user study with 16 participants exploring whether visual effects applied to 3D content in AR and VR provide a better depth perception, level of presence or engagement to users. The findings show that visual effects both in AR and VR have a negative impact on perceived depth. The measured performance in VR though is best using visual effects while in AR without using visual effects. Both in AR and VR, visual effects influenced the user's level of presence negatively.},
keywords={augmented reality;medical computing;patient care;serious games (computing);task analysis;visual effects;depth perception;spatial tasks;virtual reality;augmented reality;serious games;AR/VR;virtual objects;motion disorders;3D position;patient care;Visual effects;Games;Virtual environments;Cameras;Visualization;Green products;Resists;AR;VR;HMD;Natural Hand Interaction;Stereo Vision;Visual Effects;Depth Perception;Presence;Immersion},
doi={10.1109/ISMAR-Adjunct.2016.0070},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836492,
author={M. {Tenemaza} and J. {Ramírez} and A. {de Antonio}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Acceptability of an A2R Application: Analysis of Correlations between Factors in a TAM},
year={2016},
volume={},
number={},
pages={178-183},
abstract={Adaptive Augmented Reality (A2R) presents the environment and useful information for the user decision making, this is in real time. Nowadays, there is a gap in A2R acceptance research, although A2R is an emergent technology, specific studies of A2R acceptance has not been conducted yet. In contrast, Augmented Reality (AR) acceptance and recommendation acceptance has actually been addressed in several works. Thus, to analyze A2R acceptance, an A2R prototype was developed relying on hybrid and mobile technology. This application guides visitants to locate remarkable places at the historic center of Quito. The guide proposes places according to the user's preferences or interests gathered from a social network. Based on this prototype, an experiment with a sample of young people between 18 and 25 years old was carried out in order to determine the acceptance of A2R technology. An adaptation of Davis, Leues and Vecandesh's Technology acceptance models was applied. This study analyzed the correlations among different variables such as perceived adaptability, perceived enjoyment, perceived easy of use, perceived usefulness and attitude towards using. The results show that the perceived adaptability, perceived ease of use, perceived usefulness and perceived enjoyment are crucial in the success of A2R application.},
keywords={augmented reality;decision making;history;human computer interaction;human factors;information filtering;mobile computing;recommender systems;social networking (online);technology management;user attitude;perceived usefulness;perceived easy of use;perceived enjoyment;perceived adaptability;technology acceptance models;social network;user preferences;Quito;hybrid technology;mobile technology;recommendation acceptance;A2R acceptance;decision making;adaptive augmented reality;correlation analysis;TAM;Augmented reality;Prototypes;Correlation;Adaptation models;Facebook;Analytical models;Decision making;H [Human-centered computing H.1 HCI design and evaluation methods H.2 Interaction paradigmsUser studies Computing methodologies Mixed/augmented reality]: —},
doi={10.1109/ISMAR-Adjunct.2016.0071},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836493,
author={M. {Billeter} and G. {Röthlin} and J. {Wezel} and D. {Iwai} and A. {Grundhöfer}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={A LED-Based IR/RGB End-to-End Latency Measurement Device},
year={2016},
volume={},
number={},
pages={184-188},
abstract={Achieving a minimal latency within augmented reality (AR) systems is one of the most important factors to achieve a convincing visual impression. It is even more crucial for non-video augmentations such as dynamic projection mappings because in that case the superimposed imagery has to exactly match the dynamic real surface, which obviously cannot be directly influenced or delayed in its movement. In those cases, the inevitable latency is usually compensated for using prediction and extrapolation operations, which require accurate information about the occurring overall latency to exactly predict to the right time frame for the augmentation. Different strategies have been applied to accurately compute this latency. Since some of these AR systems operate within different spectral bands for input and output, it is not possible to apply latency measurement methods encoding time stamps directly into the presented output images as these might not be sensed by used input device.We present a generic latency measurement device which can be used to accurately measure the overall end-to-end latency of camera-based AR systems with an accuracy below one millisecond. It comprises a LED-based time stamp generator displaying the time as a gray code on spatially and spectrally multiple locations. It is controlled by a micro-controller and sensed by an external camera device observing the output display as well as the LED device at the same time.},
keywords={augmented reality;cameras;infrared detectors;light emitting diodes;LED-based IR-RGB end-to-end latency measurement device;augmented reality systems;visual impression;nonvideo augmentations;dynamic projection mappings;superimposed imagery;extrapolation operations;camera-based AR systems;end-to-end latency;LED-based time stamp generator;gray code;microcontroller;external camera device;Cameras;Light emitting diodes;Hardware;Reflective binary codes;Clocks;Standards;H.5.2 [HCI]: User Interfaces—Benchmarking},
doi={10.1109/ISMAR-Adjunct.2016.0072},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836494,
author={M. {Garon} and P. {Boulet} and J. {Doiron} and L. {Beaulieu} and J. {Lalonde}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Real-Time High Resolution 3D Data on the HoloLens},
year={2016},
volume={},
number={},
pages={189-191},
abstract={The recent appearance of augmented reality headsets, such as the Microsoft HoloLens, is a marked move from traditional 2D screen to 3D hologram-like interfaces. Striving to be completely portable, these devices unfortunately suffer multiple limitations, such as the lack of real-time, high quality depth data, which severely restricts their use as research tools. To mitigate this restriction, we provide a simple method to augment a HoloLens headset with much higher resolution depth data. To do so, we calibrate an external depth sensor connected to a computer stick that communicates with the HoloLens headset in real-time. To show how this system could be useful to the research community, we present an implementation of small object detection on HoloLens device.},
keywords={augmented reality;helmet mounted displays;image resolution;object detection;HoloLens device;external depth sensor;real-time high quality depth data;3D hologram-like interfaces;2D screen;Microsoft HoloLens headset;augmented reality headsets;real-time high resolution 3D data;Three-dimensional displays;Real-time systems;Object detection;Webcams;Calibration;Headphones},
doi={10.1109/ISMAR-Adjunct.2016.0073},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836495,
author={M. {Krichenbauer} and G. {Yamamoto} and T. {Taketomi} and C. {Sandor} and H. {Kato} and S. {Feiner}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Evaluating Positional Head-Tracking in Immersive VR for 3D Designers},
year={2016},
volume={},
number={},
pages={192-193},
abstract={With the ongoing introduction of wide-FOV VR head-worn displays into the consumer market, the application of VR 3D UIs to professional work environments is attracting increasing attention. One of the most conspicuous concepts is immersive 3D modeling and content creation. In spite of the long research history, there have been very few analyses of the effect of 3D UIs on productivity in 3D design. In this work, we explore the effect of positional head-tracking on task performance in 3D design. Previous studies have come to different conclusions on the importance of headtracking and did not investigate professional 3D modeling tools. In contrast, we performed a user study with design students using professional software on a task that closely emulates their work. Surprisingly, we did not find a significant effect of head-tracking on task-completion time, neither when using a traditional 2D mouse nor when using a pinch glove as a 3D input device.},
keywords={computer science education;graphical user interfaces;helmet mounted displays;human computer interaction;professional aspects;solid modelling;virtual reality;positional head-tracking;immersive VR;3D designers;wide-FOV VR head-worn displays;VR 3D UI;professional work environments;immersive 3D modeling;immersive 3D content creation;task performance;professional 3D modeling tools;design students;professional software;task-completion time;Three-dimensional displays;Mice;Two dimensional displays;Cameras;Performance evaluation;Navigation;Augmented reality;Virtual Reality;Immersive Authoring},
doi={10.1109/ISMAR-Adjunct.2016.0074},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836496,
author={N. {Cooper} and F. {Milella} and I. {Cant} and C. {Pinto} and M. {White} and G. {Meyer}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Augmented Cues Facilitate Learning Transfer from Virtual to Real Environments},
year={2016},
volume={},
number={},
pages={194-198},
abstract={The aim of this study was to investigate whether augmented cues that have previously been shown to enhance performance and user satisfaction in VR training translate into performance improvements in real environments. Subjects were randomly allocated into 3 groups. Group 1 were trained to perform a real tyre change, group 2 were trained in a conventional VR setting, while group 3 were trained in VR with augmented cues. After training participants were tested on a real tyre change task. Overall time to completion was recorded as objective measure; subjective ratings of presence, perceived workload and discomfort were recorded using questionnaires. The performances of the three groups were compared. Overall, participants who received VR training performed significantly faster on the real task than participants who completed the real tyre change only. The difference between the virtual reality training groups was found to be not significant. However, participants who were trained with augmented cues performed the real tyre change with fewer errors than participants in the minimal cues training group. Systematic differences in subjective ratings that reflected objective performance were also observed.},
keywords={augmented reality;computer based training;augmented cues;learning transfer;virtual environments;real environments;VR training;Training;Solid modeling;Virtual reality;Atmospheric measurements;Particle measurements;Computational modeling;Wheels;virtual reality;training transfer;multisensory feedback;augmented cues;performance;presence;workload;simulation sickness},
doi={10.1109/ISMAR-Adjunct.2016.0075},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836497,
author={N. {Haouchine} and M. {Berger} and S. {Cotin}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Simultaneous Pose Estimation and Augmentation of Elastic Surfaces from a Moving Monocular Camera},
year={2016},
volume={},
number={},
pages={199-202},
abstract={We present in this paper an original method to estimate the pose of a monocular camera while simultaneously modeling and capturing the elastic deformation of the object to be augmented. Our method tackles a challenging problem where ambiguities between rigid motion and non-rigid deformation are present. This issue represents a major lock for the establishment of an efficient surgical augmented reality where endoscopic camera moves and organs deform. Using an underlying physical model to estimate the low stressed regions our algorithm separates the rigid body motion from the elastic deformations using polar decomposition of the strain tensor. Following this decomposition, a constrained minimization, that encodes both the optical and the physical constraints, is resolved at each frame. Results on real and simulated data are exposed to show the effectiveness of our approach.},
keywords={augmented reality;cameras;elastic deformation;image motion analysis;pose estimation;shear modulus;elastic surfaces;moving monocular camera;pose estimation;physical constraints;optical constraints;constrained minimization;strain tensor;polar decomposition;rigid body motion;low stressed regions estimate;physical model;endoscopic camera;surgical augmented reality;nonrigid deformation;elastic deformation;Cameras;Shape;Three-dimensional displays;Strain;Computational modeling;Pose estimation;Stress;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial;augmented;and virtual realities; I.3.5 [Computer Graphics]: Computational Geometry and Object Modeling—Physically based modeling},
doi={10.1109/ISMAR-Adjunct.2016.0076},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836498,
author={N. A. M. {ElSayed} and R. T. {Smith} and B. H. {Thomas}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={HORUS EYE: See the Invisible Bird and Snake Vision for Augmented Reality Information Visualization},
year={2016},
volume={},
number={},
pages={203-208},
abstract={This paper presents a novel technique, called Horus Eye, for augmented reality information visualization. “Horus”, the famous deity in ancient Egyptian mythology, inspires this visualization technique, which is designed to simulate bird and snake vision to highlight data of interest. The contribution of this approach is the merging of information with the real scene, leveraging the real world context to interpret the data. Our technique is a context-based interactive visualization, controlled by users' queries. This paper presents a work in progress with use cases and two adaptations of Horus Eye.},
keywords={augmented reality;data visualisation;Horus eye;invisible bird vision;snake vision;augmented reality information visualization;ancient Egyptian mythology;context-based interactive visualization;Data visualization;Birds;Image color analysis;Visualization;Sensitivity;Brightness;Sugar;Horus Eye;Augmented Reality;Visualization;Information Visualization;Blended Information;Scene Manipulation;Diminished Reality},
doi={10.1109/ISMAR-Adjunct.2016.0077},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836499,
author={P. {Gobira} and A. {Mozelli}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={An Artistic and Curatorial Installation in Virtual Reality: The Development of an Artistic Low-Cost Interface at University},
year={2016},
volume={},
number={},
pages={209-214},
abstract={The objective of this study is to report the learning experience and development of a mobile art installation of low-cost immersive virtual reality using technologies for the development of digital games. Through virtual simulation of the use of a common space in the university, the art gallery of the Guignard School of art at UEMG in Brazil. The article describes how the installation was developed as well as how the users interactions challenges have been solved in the virtual environment, since to make the movement of users within the virtual environment possible there were not external interfaces such as joysticks or motion cameras to interact. The development of the installation, which provided greater visibility to the extension project called “Living Room”, is the main reason for the creation of the installation and for elaborating questions on the relationship between virtual exhibition design and interactivity.},
keywords={art;computer aided instruction;computer games;digital simulation;human computer interaction;mobile computing;museums;virtual reality;artistic installation;curatorial installation;artistic low-cost interface;learning experience;mobile art installation;low-cost immersive virtual reality;digital game development;virtual simulation;Guignard School of art;art gallery;UEMG;Brazil;user interaction;user movement;Living Room;virtual exhibition design;Art;Solid modeling;Virtual environments;Three-dimensional displays;Games;Cameras;Virtual reality;Immersion;Exhibition design;Virtual museum},
doi={10.1109/ISMAR-Adjunct.2016.0078},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836500,
author={P. {Issartel} and L. {Besançon} and T. {Isenberg} and M. {Ammi}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={A Tangible Volume for Portable 3D Interaction},
year={2016},
volume={},
number={},
pages={215-220},
abstract={We present a new mixed reality approach to achieve tangible object manipulation with a single, fully portable, and self-contained device. Our solution is based on the concept of a “tangible volume.” We turn a tangible object into a handheld fish-tank display. Our approach, however, goes beyond traditional fish-tank VR in that it can be viewed from all sides, and that the tangible volume represents a volume of space that can be freely manipulated within a virtual scene. This volume can be positioned onto virtual objects to directly grasp them and to manipulate them in 3D space. We investigate this concept with a user study to evaluate the intuitiveness of using a tangible volume for grasping and manipulating virtual objects. The results show that a majority of participants spontaneously understood the idea of grasping a virtual object “through” the tangible volume.},
keywords={graphical user interfaces;notebook computers;virtual reality;tangible user interface;virtual object grasping;virtual object manipulation;intuitiveness evaluation;virtual scene;handheld fish-tank display;self-contained device;portable device;tangible object manipulation;mixed reality;tangible volume;portable 3D interaction;Three-dimensional displays;Mobile handsets;Visualization;Rendering (computer graphics);Shape;User interfaces;Virtual reality;Tangible user interface;3D manipulation;mixed reality;fish-tank VR},
doi={10.1109/ISMAR-Adjunct.2016.0079},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836501,
author={P. {Ramakrishna} and E. {Hassan} and R. {Hebbalaguppe} and M. {Sharma} and G. {Gupta} and L. {Vig} and G. {Sharma} and G. {Shroff}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={An AR Inspection Framework: Feasibility Study with Multiple AR Devices},
year={2016},
volume={},
number={},
pages={221-226},
abstract={We present an Augmented Reality (AR) based re-configurable framework for inspection that can be utilized in cross-domain applications such as maintenance and repair assistance in industrial inspection, health sector to record vitals, and automotive/avionics domain inspection, amongst others. The novelty of the inspection framework as compared to the existing counterparts are three fold. Firstly, the inspection check-list can be prioritized by detecting the parts viewed in inspector's field using deep learning principles. Second, the backend of the framework is easily configurable for different applications where instructions and assistance manuals can be directly imported and visually integrated with inspection type. Third, we conduct a feasibility study on inspection modes such as Google Glass, Google Cardboard, Paper based and Tablet for inspection turnaround time, ease, and usefulness by taking a 3D printer inspection use-case.},
keywords={augmented reality;inspection;learning (artificial intelligence);AR inspection framework;multiple AR devices;augmented reality based re-configurable framework;inspection check-list;deep learning principles;inspection turnaround time;3D printer inspection;Inspection;Google;Object detection;Servers;Glass;Printers;Proposals;H.5.1 [Information Interfaces and Presentation]: Artificial;Augmented;and Virtual Realities—; [Human-centered computing]: Ubiquitous and mobile computing—;Ambient Intelligence H.5.2 [Information Interfaces and Presentation]: User Interfaces&m},
doi={10.1109/ISMAR-Adjunct.2016.0080},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836502,
author={P. {Petitprez} and E. {Kerrien} and P. {Villard}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={PoLAR: A Portable Library for Augmented Reality},
year={2016},
volume={},
number={},
pages={227-230},
abstract={We present here a novel cross-platform library to facilitate research and development applications dealing with augmented reality (AR). Features include 2D and 3D objects visualization and interaction, camera flow and image manipulation, and soft-body deformation. Our aim is to provide computer vision specialists' with tools to facilitate AR application development by providing easy and state of the art access to GUI creation, visualization and hardware management.We demonstrate both the simplicity and the efficiency of coding AR applications through three detailed examples. PoLAR can be downloaded at http://polar.inria.fr and is distributed under the GPL licence.},
keywords={augmented reality;data visualisation;digital libraries;graphical user interfaces;image processing;research and development;software tools;solid modelling;3D models;hardware management;GUI creation;AR application development tools;computer vision specialists;soft-body deformation;image manipulation;camera flow;objects interaction;3D objects visualization;2D objects visualization;research and development applications;cross-platform library;augmented reality;portable library;PoLAR;Three-dimensional displays;Libraries;Cameras;Two dimensional displays;Engines;Augmented reality;Graphical user interfaces;1.3.4 [Computer Graphics]: Graphics Utilities—Application packages; I.4.9 [Image Processing and Computer Vision]: Applications—[I.6.8]: Simulation and Modeling—Types of Simulation Animation},
doi={10.1109/ISMAR-Adjunct.2016.0081},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836503,
author={R. {Roberto} and J. P. {Lima} and T. {Araújo} and V. {Teichrieb}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Evaluation of Motion Tracking and Depth Sensing Accuracy of the Tango Tablet},
year={2016},
volume={},
number={},
pages={231-234},
abstract={This paper presents an evaluation of the Tango tablet regarding its motion tracking and depth perception capabilities. A methodology for performing such kind of evaluation is proposed. Motion tracking error is assessed in both small workspace and large environments. In the small workspace scenario, the distances reported by the motion tracking system are compared with values measured with the aid of a graph paper. In the large environment condition, the tracking error consists in the difference between initial and final positions given by the system when the device moves around the environment and returns to the same location. Depth sensing precision is evaluated by comparing the 3D coordinates reported by the system of the inner corners of a chessboard pattern with ground truth values obtained from color camera image processing. The results show that Tango tablet sometimes presents large motion tracking errors, which may harm AR experience. In addition, Tango tablet depth sensing presents average error values similar to desktop depth cameras, but it is more sensitive to infrared reflection properties of the objects to be mapped.},
keywords={image colour analysis;image motion analysis;mobile computing;object tracking;motion tracking evaluation;depth sensing accuracy;Tango tablet;depth perception;small workspace scenario;graph paper;3D coordinates;color camera image processing;infrared reflection properties;Tracking;Sensors;Cameras;Three-dimensional displays;Google;Image color analysis;Performance evaluation;I.4.8 [Image Processing and Computer Vision]: Scene Analysis—Depth Cues;Range Data;Tracking; H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial;Augmented;and Virtual Realities},
doi={10.1109/ISMAR-Adjunct.2016.0082},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836504,
author={R. {Dauenhauer} and T. {Müller}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={An Evaluation of Information Connection in Augmented Reality for 3D Scenes with Occlusion},
year={2016},
volume={},
number={},
pages={235-237},
abstract={Most augmented reality applications connect virtual information to anchors, i.e. physical places or objects, by using spatial overlays or proximity. However, for industrial use cases this is not always feasible because specific parts must remain fully visible in order to meet work or security requirements. In these situations virtual information must be displayed at alternative positions while connections to anchors must still be clearly recognizable. In our previous research we were the first to show that for simple scenes connection lines are most suitable for this. To extend these results to more complex environments, we conducted an experiment on the effects of visual interruptions in connection lines and incorrect occlusion. Completion time and subjective mental effort for search tasks were used as measures. Our findings confirm that also in 3D scenes with partial occlusion connection lines are preferable to connect virtual information with anchors if an assignment via overlay or close proximity is not feasible. The results further imply that neither incorrectly used depth cues nor missing parts of connection lines make a significant difference concerning completion time or subjective mental effort. For designers of industrial augmented reality applications this means that they can choose either visualization based on their needs.},
keywords={augmented reality;data visualisation;engines;information systems;visualization;industrial augmented reality applications;partial occlusion connection lines;visual interruptions;virtual information display;security requirements;proximity;spatial overlays;anchors;3D scenes;information connection evaluation;Color;Visualization;Augmented reality;Three-dimensional displays;Encoding;Context;Maintenance engineering;Augmented reality;perception;occlusion},
doi={10.1109/ISMAR-Adjunct.2016.0083},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836505,
author={R. {de Lima} and J. {Martinez-Carranza} and A. {Morales-Reyes} and W. {Mayol-Cuevas}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={On Combining a Semi-Calibrated Stereo Camera and Massive Parallelism for Fast Plane Extraction},
year={2016},
volume={},
number={},
pages={238-243},
abstract={We present a novel methodology that combines stereo vision and parallel processing, based on GPU and the use of binary descriptors, for fast plane extraction. Typical stereo algorithms require an image rectification stage that has to run in a frame-to-frame basis, increasing the computational burden and with the possibility of compromising high frame rate operation. Hence, we propose to use a semi-calibrated stereo approach, meaning that only calibration of extrinsic parameters of the stereo rig is carried out, thus avoiding a rectification process of the frames captured by the stereo camera. For the latter, we rely on feature matching of salient points detected on the stereo images, from which image correspondences are obtained. These correspondences are triangulated in order to generate a point cloud that is passed to a plane fitting module. As feature matching is a heavy task, we present a novel GPU architecture to accelerate such process, thus achieving real-time performance of up to 50 fps for the whole process. To demonstrate our approach, we also present an augmented reality application that exploits the planes extracted with our proposed methodology.},
keywords={feature extraction;graphics processing units;image matching;image sensors;parallel processing;stereo image processing;semicalibrated stereo camera;massive parallelism;fast plane extraction;stereo vision;parallel processing;binary descriptors;stereo algorithms;image rectification stage;frame-to-frame basis;extrinsic parameters;rectification process;feature matching;plane fitting module;GPU architecture;Feature extraction;Cameras;Three-dimensional displays;Augmented reality;Real-time systems;Kernel;Calibration;Plane Extraction;High Level Structures;Stereo Vision},
doi={10.1109/ISMAR-Adjunct.2016.0084},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836506,
author={S. {Jiddi} and P. {Robert} and E. {Marchand}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Reflectance and Illumination Estimation for Realistic Augmentations of Real Scenes},
year={2016},
volume={},
number={},
pages={244-249},
abstract={The acquisition of surface material properties and lighting conditions is a fundamental step for photo-realistic Augmented Reality (AR). In this paper, we present a new method for the estimation of diffuse and specular reflectance properties of indoor real static scenes. Using an RGB-D sensor, we further estimate the 3D position of light sources responsible for specular phenomena and propose a novel photometry-based classification for all the 3D points. Our algorithm allows convincing AR results such as realistic virtual shadows as well as proper illumination and specularity occlusions.},
keywords={augmented reality;estimation theory;image classification;image colour analysis;photometry;reflectance estimation;illumination estimation;realistic augmentations;real scenes;surface material properties acquisition;photo-realistic augmented reality;RGB-D sensor;photometry-based classification;Three-dimensional displays;Lighting;Light sources;Image color analysis;Estimation;Image reconstruction;Color;I.4.1 [Image Processing and Computer Vision]: Digitization and Image Capture—Reflectance; I.4.8 [Image Processing and Computer Vision]: Scene Analysis—Photometry},
doi={10.1109/ISMAR-Adjunct.2016.0085},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836507,
author={S. {Ghasemi} and M. {Otsuki} and P. {Milgram}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Use of Random Dot Pattern for Achieving X-Ray Vision with Stereoscopic Augmented Reality Displays},
year={2016},
volume={},
number={},
pages={250-251},
abstract={This paper presents a possible solution to some of the challenges involved with creating a stereoscopic augmented reality `X-ray vision` display, which enables presentation of computer generated (virtual) objects as if they lie behind a real object surface, while maintaining the ability to effectively perceive any information that might be present on that surface. The method involves overlaying random dot patterns onto a real object surface prior to rendering a virtual object. Results from preliminary experiments have shown that the use of random dot patterns can be effective in contributing to the percept of transparency for the case of flat real surfaces with subtle textures. This suggests that the addition of such patterns may also help in perceiving the correct depth order of virtual objects in such images. Moreover, experimental results indicate that, by controlling the relative dot size and dot density of the patterns, it should be possible also to retain sufficient information about the real surface. Future research should be aimed towards the feasibility and effectiveness of applying this method to more realistic AR conditions.},
keywords={augmented reality;computer displays;rendering (computer graphics);X-rays;random dot pattern;stereoscopic augmented reality X-ray vision display;computer generated virtual objects;rendering;Augmented reality;Surface texture;Stereo image processing;Three-dimensional displays;Surface treatment;X-ray imaging;Observers;stereoscopic augmented reality;x-ray vision},
doi={10.1109/ISMAR-Adjunct.2016.0086},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836508,
author={S. {Delabrida} and A. A. F. {Loureiro} and T. {D'Angelo} and R. A. R. {Oliveira} and B. {Thomas} and E. {Carvalho} and M. {Billinghurst}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={A Low Cost Optical See-Through HMD - Do-It-Yourself},
year={2016},
volume={},
number={},
pages={252-257},
abstract={HMD that provides Virtual Reality and Augmented Reality capabilities are emerging in the last years. Manufacturers are seeking to design hardware that maximizes the field of view (FoV) and depth of field (DoF). On the other hand, we did not found a device that allows the variation of these parameters. This paper presents a model to build HMD using mirrors reflection. Our model allows the adjustment of the DoF. Consequently, the FoV also changes. Our prototype shows that the proposed feature is a feasible alternative. Furthermore, we present a simple and low-cost alternative for HMD development.},
keywords={augmented reality;helmet mounted displays;mirrors;low cost optical see-through HMD;virtual reality capabilities;augmented reality capabilities;field of view;depth of field;mirror reflection;Resists;Optical distortion;Optical imaging;Optical design;Biomedical optical imaging;Image resolution;Augmented reality},
doi={10.1109/ISMAR-Adjunct.2016.0087},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836509,
author={S. {Ikeda} and A. N. {Trung} and T. {Komae} and F. {Shibata} and A. {Kimura}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Randomly Distributed Small Chip Makers},
year={2016},
volume={},
number={},
pages={258-259},
abstract={In this paper, we propose a novel marker design and its tracking algorithm for room-sized MR/AR environments. The markers and the algorithm are designed to solve the following practical problems: i) the difficulties in creating and arranging markers and ii) the trade-off between inconspicuousness and robustness of markers. The proposed markers are small chips that are cut off a large paper sheet, and are arranged at random positions in an environment or on objects. This paper shows the design concept and feasibility of the proposed markers.},
keywords={augmented reality;object tracking;randomly distributed small chip makers;tracking algorithm;room-sized MR-AR environments;mixed reality;augmented reality;Visualization;Augmented reality;Three-dimensional displays;Robustness;Cameras;Simultaneous localization and mapping;Shape;H.5.1 [Information Intefaces and Presentation]: Multimedia Information Systems—Artificial;augmented;and virtual realities; I.4.8 [Image processing and Computer Vision]: Scene Analysis—Tracking},
doi={10.1109/ISMAR-Adjunct.2016.0088},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836510,
author={S. {Vassigh} and A. {Elias} and F. R. {Ortega} and D. {Davis} and G. {Gallardo} and H. {Alhaffar} and L. {Borges} and J. {Bernal} and N. D. {Rishe}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Integrating Building Information Modeling with Augmented Reality for Interdisciplinary Learning},
year={2016},
volume={},
number={},
pages={260-261},
abstract={Augmented Reality provides a way to enhance the classroom experience. In particular, student learning about building systems in the fields of Architecture, Civil, and Mechanical Engineering may improve, if visualization outside the classroom is provided. We propose that AR-SKOPE, an application that integrates Building Information Modelling and Augmented Reality may improve learning. This application allows students to visit specific buildings and investigate their various systems with supplementary information using a phone or tablet. We are currently testing our early prototype to conduct a semester-long study.},
keywords={architecture;augmented reality;buildings (structures);computer aided instruction;education;AR-SKOPE;mechanical engineering;civil engineering;architecture;building systems;student learning;classroom experience;interdisciplinary learning;augmented reality;building information modeling integration;Buildings;Augmented reality;Solid modeling;Education;Magnetic resonance imaging;Architecture;Visualization;Augmented Reality;Teaching Tools;Building Information Modelling;Architecture and Engineering Education},
doi={10.1109/ISMAR-Adjunct.2016.0089},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836511,
author={S. {Hegde} and R. {Perla} and R. {Hebbalaguppe} and E. {Hassan}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={GestAR: Real Time Gesture Interaction for AR with Egocentric View},
year={2016},
volume={},
number={},
pages={262-267},
abstract={The existing, sophisticated AR gadgets1 in the market today are mostly exorbitantly priced. This limits their usage for the upcoming academic research institutes and also their reach to the mass market in general. Among the most popular and frugal head mounts, Google Cardboard (GC) and Wearality2 are video-see-through devices that can provide immersible AR and VR experiences with a smartphone. Stereo-rendering of camera feed and overlaid information on smartphone helps us experience AR with GC. These frugal devices have limited user-input capability, allowing user interactions with GC such as head tilting, magnetic trigger and conductive lever. Our paper proposes a reliable and intuitive gesture based interaction technique for these frugal devices. The hand gesture recognition employs the Gaussian Mixture Models (GMM) based on human skin pixels and tracks segmented foreground using optical flow to detect hand swipe direction for triggering a relevant event. Realtime performance is achieved by implementing the hand gesture recognition module on a smartphone and thus reducing the latency. We augment real-time hand gestures as new GC's interface with its evaluation done in terms of subjective metrics and with the available user interactions in GC.},
keywords={augmented reality;Gaussian processes;gesture recognition;image segmentation;image sequences;mixture models;rendering (computer graphics);stereo image processing;tracking;subjective metrics;GC interface;hand gesture recognition module;hand swipe direction detection;optical flow;segmented foreground tracking;human skin pixels;GMM;Gaussian mixture models;intuitive gesture based interaction;user interactions;smartphone helps;stereo-rendering;VR experiences;video-see-through devices;Wearality2;Google Cardboard;head mounts;AR gadgets;egocentric view;augmented reality;real time gesture interaction;GestAR;Gesture recognition;Feature extraction;Cameras;Magnetic resonance imaging;Inspection;Google;Adaptive optics;H.5.1 [Information Interfaces and Presentation]: Artificial;Augmented;and Virtual Realities—; H.5.2 [Information Interfaces and Presentation]: User Interfaces—Interaction Styles I.4.8 [Computing Methodologies]: Image Processing and Computer V},
doi={10.1109/ISMAR-Adjunct.2016.0090},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836512,
author={T. {Lin} and C. {Chen} and J. {Wang} and M. {Shieh}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Motion-Aware Iterative Closest Point Estimation for Fast Visual Odometry},
year={2016},
volume={},
number={},
pages={268-269},
abstract={Iterative closest point (ICP) algorithm is a common localization method used to estimate camera poses by aligning two depth frames. Since the input depth map is easily distorted when the camera is in large motion, it might result in incorrect pose estimation and produce apparent drift for ICP-based applications. To alleviate this problem, instead of using the time-consuming graph-based optimization approach for post processing, this work aims at refining poses when detecting noisy depth maps and presents a hybrid decision mechanism to detect noisy depth maps based on the characteristic of ICP. The camera pose of the next frame is decided by referring to the last frame instead of the current frame when a noisy depth map is detected, by doing so, we can prevent the errors produced in the current frame from propagating to the next frame, thus reducing drift. Experimental results show that the relative pose error reduce to 58% in average at the time when large motion happens.},
keywords={estimation theory;graph theory;iterative methods;optimisation;pose estimation;motion-aware iterative closest point estimation;fast visual odometry;ICP algorithm;camera pose estimation;graph-based optimization;noisy depth maps detection;Iterative closest point algorithm;Cameras;Noise measurement;Three-dimensional displays;Visualization;Optimization;Visual odometry;Iterative closest point (ICP) algorithm},
doi={10.1109/ISMAR-Adjunct.2016.0091},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836513,
author={W. {Chen} and C. {Chen} and J. {Wang} and M. {Shieh}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Effective Registration for Multiple Users AR System},
year={2016},
volume={},
number={},
pages={270-271},
abstract={Registration is an important task in augmented reality (AR) systems. For markerless AR, feature descriptors are generally used as a basis of registration process, which is expected to be robust for various application scenarios. This work aims at exploring effective schemes to improve the registration results, especially for applications with large viewpoint angles. Using the proposed scheme, the registration error can be reduced by only evaluating feature points near the virtual object and within the region of interest. Experimental results reveal that about 30% to 50% registration error and 10 times data size of features can be reduced by applying the proposed schemes. Thus, the bandwidth requirement for transmitting features among different users is also decreased accordingly.},
keywords={augmented reality;computer vision;feature extraction;viewpoint angle;registration process;feature descriptors;markerless AR;augmented reality system;multiple users AR system;Augmented reality;Robustness;Computer vision;Feature extraction;Transmission line matrix methods;Signal processing algorithms;Bandwidth;Multi-users augmented reality (AR) system;virtual object registration;computer vision},
doi={10.1109/ISMAR-Adjunct.2016.0092},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836514,
author={W. {Hoff} and H. {Zhang}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Learning Object and State Models for AR Task Guidance},
year={2016},
volume={},
number={},
pages={272-273},
abstract={We present a method for automatically learning object and state models, which can be used for recognition in an augmented reality task guidance system. We assume that the task involves objects whose appearance is fairly consistent, but the background may vary. The novelty of our approach is that the system can be automatically constructed from examples of experts performing the task. As a result, the system can be easily adapted to new tasks. The approach makes use of the fact that the key features of the object are consistently present in multiple viewing instances; whereas features from the background or irrelevant objects are not consistently present. Using information theory, we automatically identify the features that can best discriminate between object states. In evaluations, our prototype successfully recognized object states in all trials.},
keywords={augmented reality;information theory;learning (artificial intelligence);object recognition;object models;state models;AR task guidance;augmented reality task guidance system;multiple viewing instances;information theory;object state recognition;learning;Maintenance engineering;Databases;Feature extraction;Training;Augmented reality;Printers;Entropy;State recognition;egocentric vision;augmented reality;task guidance},
doi={10.1109/ISMAR-Adjunct.2016.0093},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836515,
author={Y. X. {Zhang} and Z. {Zhu} and Z. {Yun}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Empower VR Art and AR Book with Spatial Interaction},
year={2016},
volume={},
number={},
pages={274-279},
abstract={In some circumstances it is necessary to allow the public to interact with AR or VR contents in physical space in an easy to use and low cost way, such as mark based AR book in classroom and interactive VR art or AR art in exhibition or museum in open space. The authors developed tangible user interface elements based on marker recognition. The user interface elements include virtual buttons, virtual rotate, and virtual hotspot. The user elements were integrated into various kinds of digital presentation systems by optimizing the logistic structure and interaction design of the user interface system to realize convenient spatial interactions. Thus, providing friendly user interaction and effective communication of information. Especially when interacting with art works, this interface could hide technology aspects and reduce the technology noise over art, and bring magical and amazing experience to users.},
keywords={art;augmented reality;human computer interaction;user interface management systems;AR book;spatial interaction;AR contents;VR contents;interactive VR art;interactive AR art;exhibition;museum;tangible user interface;marker recognition;virtual buttons;virtual rotate;virtual hotspot;digital presentation systems;logistic structure;interaction design;user interface system;user friendly interaction;technology noise reduction;augmented reality;virtual reality;Augmented reality;Art;Cameras;Mice;Sensors;Flowcharts;Augmented Reality;Spatial Interaction;Virtual Controller;Interface},
doi={10.1109/ISMAR-Adjunct.2016.0094},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836516,
author={Y. {Zhang} and Z. {Zhu} and F. A. {Sourou}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={On Stage Interactive Spatial AR for Drama Performance},
year={2016},
volume={},
number={},
pages={280-283},
abstract={The authors fuse the virtual objects and visual effects (VFX) and the performance of actors and actresses on real drama stage by developing an interactive spatial AR system, in which actors and actresses were interacting with virtual objects and VFX by their motion and gesture in real-time performance, and the images of virtual objects and VFX that projected on a transparent projection screen were aligned and matched to calibrate with their body part's position. The audience will see virtual objects and VFX are seamlessly matched on the real performers on stage space as if they are real things that are just under the control of the real performers. The audience will immerse into the drama scenes more deeply, hence offering the audience higher aesthetics feelings also bringing new possibilities to drama art creation.},
keywords={interactive systems;virtual reality;stage interactive spatial AR;drama performance;virtual objects;visual effects;interactive spatial AR system;real-time performance;transparent projection screen;higher aesthetics feelings;Skeleton;Art;Three-dimensional displays;Visual effects;Virtual environments;Augmented reality;Fuses;spatial AR;virtual objects;VFX;drama;transparent projection screen;stage},
doi={10.1109/ISMAR-Adjunct.2016.0095},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836517,
author={Y. Y. {Lee} and B. {Ahmed} and J. H. {Lee} and H. {An} and K. H. {Lee}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Augmenting Three-dimensional Effects in Digital Exhibition of a Cultural Artifact using 3D Pseudo Hologram},
year={2016},
volume={},
number={},
pages={284-287},
abstract={With the surge of realistic 3D contents, digital exhibition of a cultural artifact gets much attention as one of the major applications. However, in the case of conventional digital exhibition the sense of artificiality is still a challenge because virtual contents are represented through a flat display device that often neglects the differences in depth perception. To represent the cultural artifact in a realistic manner, it is necessary to provide a natural visual depth as well as realistic reproduction of the colors of the appearance material.In this paper, we present a new 3D pseudo hologram system that provides natural three-dimensional effect to virtual cultural artifacts to multiple users simultaneously without wearing special glasses. The proposed system consists of multiple projector-camera pairs, a 3D projection surface by the physical mock-up of an original artifact, and a half-silvered mirror. We project the digital image of a cultural artifact onto the physical mock-up with geometric and radiometric calibrations using the multiple projector-camera system. The experimental result shows an appealing photorealistic threedimensional pseudo hologram image from a wide view angle. We are expecting to see various immersive experience to the user by the proposed system.},
keywords={augmented reality;exhibitions;history;three-dimensional effect augmentation;digital exhibition;realistic 3D contents;natural visual depth;realistic color reproduction;appearance material;natural three-dimensional effect;virtual cultural artifacts;3D projection surface;physical mock-up;half-silvered mirror;digital image projection;geometric calibration;radiometric calibration;multiple projector-camera system;photorealistic three-dimensional pseudohologram image;immersive user experience;Three-dimensional displays;Cameras;Cultural differences;Image color analysis;Lighting;Radiometry;Distortion;H.5.1 [INFORMATION INTERFACES AND PRESENTATION]: Multimedia Information Systems—Artificial;augmented;and virtual realities},
doi={10.1109/ISMAR-Adjunct.2016.0096},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836518,
author={Y. {Kemmoku} and T. {Komuro}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={AR Tabletop Interface using a Head-Mounted Projector},
year={2016},
volume={},
number={},
pages={288-291},
abstract={In this paper, we propose a tabletop interface in which a user wears a projector with a depth camera on his or her head and can perform touch operations on an image projected on a flat surface. By using the head-mounted projector, images are always projected in front of the user in the direction of the user's gaze. By changing the image to be projected based on the user's head movement, this interface realizes a large effective screen size. The system superimposes an image on the flat surface by performing plane detection, placing the image on the detected plane, performing perspective projection to obtain a 2D image, and projecting the 2D image using the projector. Registration between the real world and the image is performed by estimating the user's head pose using the detected plane information. Furthermore, touch input is recognized by detecting the user's finger on the plane using the depth camera. We implemented some application examples into the system to demonstrate the usefulness of the proposed interface.},
keywords={augmented reality;cameras;gaze tracking;human computer interaction;microcomputers;optical projectors;depth camera;user finger detection;user head pose estimation;image registration;2D image;plane detection;user head movement;user gaze;touch operations;depth camera;projector;tabletop interface;head-mounted projector;AR tabletop interface;Head;Cameras;Three-dimensional displays;Two dimensional displays;Resists;Image edge detection;Augmented reality;Augmented reality;peephole interaction;projectorcamera system;depth camera},
doi={10.1109/ISMAR-Adjunct.2016.0097},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836519,
author={S. {Mori} and M. {Maezawa} and N. {Ienaga} and H. {Saito}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Detour Light Field Rendering for Diminished Reality Using Unstructured Multiple Views},
year={2016},
volume={},
number={},
pages={292-293},
abstract={Instructor's perspective videos are useful for presenting intuitive visual instructions for trainees in medical and industrial settings. In such videos, the instructor's arms often obstruct the trainee's view of the work area. In this article, we present a diminished reality method for visualizing the work area hidden by an instructor's arms by capturing the work area with multiple cameras. To achieve such diminished reality, we propose detour light field rendering (DLFR), in which light rays avoid passing through penalty points set in the unstructured light fields reconstructed from multiple viewpoint images. In DLFR, the camera blending field used in an existing freeviewpoint image generation method known as unstructured lumigraph is re-designed based on our use cases. In this re-design, lesser weights are given to light rays as they pass close to given penalty points. Experimental results demonstrate that using DLFR, the appearance of an undesirable object can be removed from an image in real time.},
keywords={augmented reality;rendering (computer graphics);video signal processing;freeviewpoint image generation;DLFR;videos;industrial settings;medical settings;unstructured multiple views;diminished reality;detour light field rendering;Cameras;Rendering (computer graphics);Videos;Real-time systems;Three-dimensional displays;Visualization;Image reconstruction;Diminished Relity;Light Field Rendering;Computational Photography;Multiple Views},
doi={10.1109/ISMAR-Adjunct.2016.0098},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836520,
author={N. {Ienaga} and F. {Bork} and S. {Meerits} and S. {Mori} and P. {Fallavollita} and N. {Navab} and H. {Saito}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={First Deployment of Diminished Reality for Anatomy Education},
year={2016},
volume={},
number={},
pages={294-296},
abstract={Understanding the anatomy of the human body is vital for everyone working in the medical domain. Augmented reality (AR) systems for anatomy teaching, which display virtual information directly on top of a users' body, have proven to facilitate mental mapping compared to traditional teaching paradigms. In this paper, we explore the potential of diminished reality (DR) in the context of anatomy education. As a first necessary step to achieving a DR anatomy education system, parts of the human body have to be extracted and diminished from the video stream. Our system diminishes either the arm or head of the user by projecting a background image recovered using RGB-D cameras. Such a system, if combined with an accurate overlay of virtual counterparts, could potentially improve the learning effect by attracting the users' attention to the virtual information and improve visual perception by avoiding the well-known floating effect of AR.},
keywords={augmented reality;biomedical education;cameras;computer aided instruction;image colour analysis;medical computing;visual perception;diminished reality;human body anatomy education;medical domain;augmented reality systems;AR systems;virtual information;user body;mental mapping;DR anatomy education;video stream;user head;user arm;background image projection;RGB-D cameras;learning effect improvement;visual perception improvement;Cameras;Augmented reality;Education;Head;Streaming media;Three-dimensional displays;Image reconstruction;Mixed Reality;Diminished Reality;Anatomy Education},
doi={10.1109/ISMAR-Adjunct.2016.0099},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7836521,
author={T. {Sawabe} and M. {Kanbara} and N. {Hagita}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, title={Diminished Reality for Acceleration — Motion Sickness Reduction with Vection for Autonomous Driving},
year={2016},
volume={},
number={},
pages={297-299},
abstract={This paper proposes the Diminished Reality (DR) method for an acceleration stimulus to reduce motion sickness for an autonomous vehicle by presenting vection for user before the real acceleration occurs. The technology of an autonomous vehicle has been rapidly developed in all over the world. Instead of controlling vehicle by passenger themselves, the autonomous system helps them acceleration and deceleration controls and safety controls as well. However, it is predictable that the number of passengers who get motion sickness increases because they receive an unexpected acceleration stimulus for the autonomous driving.In the field of the Virtual Reality, the technology to create virtual acceleration stimulus to the passenger in the driving simulator or flight simulator have been developed. However, our approach for using pseudo to reduce the acceleration stimulus which occurs in autonomous driving to prevent the motion sickness is the opposite problem of these conventional VR researches. The real acceleration stimulus from the vehicle is reduced by presenting vection before the real acceleration occurs.In this research, we demonstrate the idea of technology that use the vection with an augmented reality system to reduce an effect of the real acceleration in the vehicle which mainly the factor for the motion sickness.},
keywords={aerospace simulation;augmented reality;autonomous aerial vehicles;augmented reality;flight simulator;driving simulator;virtual reality;autonomous vehicle;acceleration stimulus;autonomous driving;vection;motion sickness reduction;diminished reality;Acceleration;Gravity;Autonomous vehicles;Safety;Resists;Motion measurement},
doi={10.1109/ISMAR-Adjunct.2016.0100},
ISSN={},
month={Sep.},}