@INPROCEEDINGS{8115383,
author={},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Title page i]},
year={2017},
volume={},
number={},
pages={i-i},
abstract={Presents the title page of the proceedings record.},
keywords={},
doi={10.1109/ISMAR.2017.1},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8115384,
author={},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Title page iii]},
year={2017},
volume={},
number={},
pages={iii-iii},
abstract={Presents the title page of the proceedings record.},
keywords={},
doi={10.1109/ISMAR.2017.2},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8115385,
author={},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Copyright notice]},
year={2017},
volume={},
number={},
pages={iv-iv},
abstract={Presents the copyright information for the conference. May include reprint permission information.},
keywords={},
doi={10.1109/ISMAR.2017.3},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8115386,
author={},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Table of contents},
year={2017},
volume={},
number={},
pages={v-viii},
abstract={The following topics are dealt with: mixed reality; augmented reality; tracking and localization; rendering; acquisition and learning; projector camera systems; viewing and occlusion; illumination and consistency; perception; and usability and acceptance.},
keywords={computer graphics;interactive systems;lighting;optical projectors;tracking;mixed reality;augmented reality;tracking;localization;rendering;acquisition;learning;projector camera systems;occlusion;illumination;perception;usability},
doi={10.1109/ISMAR.2017.4},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8115387,
author={},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Message from the ISMAR 2017 General Chair and Deputy General Chairs},
year={2017},
volume={},
number={},
pages={ix-ix},
abstract={Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.},
keywords={},
doi={10.1109/ISMAR.2017.5},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8115388,
author={},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Message from the ISMAR 2017 Science and Technology Program Chairs and TVCG Guest Editors},
year={2017},
volume={},
number={},
pages={x-xii},
abstract={Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.},
keywords={},
doi={10.1109/ISMAR.2017.6},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8115389,
author={},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Message from the ISMAR 2017 Science and Technology Program Chairs},
year={2017},
volume={},
number={},
pages={xiii-xv},
abstract={Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.},
keywords={},
doi={10.1109/ISMAR.2017.7},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8115390,
author={},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Message from the ISMAR 2017 Science and Technology Poster Chairs},
year={2017},
volume={},
number={},
pages={xvi-xvii},
abstract={Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.},
keywords={},
doi={10.1109/ISMAR.2017.8},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8115391,
author={},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Message from the Workshop and Tutorial Chairs},
year={2017},
volume={},
number={},
pages={xviii-xviii},
abstract={Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.},
keywords={},
doi={10.1109/ISMAR.2017.9},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8115392,
author={},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Message from the Demonstration Chairs},
year={2017},
volume={},
number={},
pages={xix-xix},
abstract={Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.},
keywords={},
doi={10.1109/ISMAR.2017.10},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8115393,
author={},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={ISMAR 2017 Conference Committee Members},
year={2017},
volume={},
number={},
pages={xx-xx},
abstract={Provides a listing of current committee members and society officers.},
keywords={},
doi={10.1109/ISMAR.2017.11},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8115394,
author={},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={ISMAR 2017 Science and Technology Program Committee Members},
year={2017},
volume={},
number={},
pages={xxi-xxi},
abstract={Provides a listing of current committee members and society officers.},
keywords={},
doi={10.1109/ISMAR.2017.12},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8115395,
author={},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={ISMAR 2017 Steering Committee Members},
year={2017},
volume={},
number={},
pages={xxii-xxii},
abstract={Provides a listing of current committee members and society officers.},
keywords={},
doi={10.1109/ISMAR.2017.13},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8115396,
author={},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Reviewers},
year={2017},
volume={},
number={},
pages={xxiii-xxiii},
abstract={The conference offers a note of thanks and lists its reviewers.},
keywords={},
doi={10.1109/ISMAR.2017.14},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8115397,
author={},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Keynotes},
year={2017},
volume={},
number={},
pages={xxiv-xxv},
abstract={Provides an abstract for each of the keynote presentations and may include a brief professional biography of each},
keywords={},
doi={10.1109/ISMAR.2017.15},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8115398,
author={},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Sponsors and Supporters},
year={2017},
volume={},
number={},
pages={xxvi-xxviii},
abstract={The conference organizers greatly appreciate the support of the various corporate sponsors listed.},
keywords={},
doi={10.1109/ISMAR.2017.16},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8115399,
author={N. {Zioulis} and A. {Papachristou} and D. {Zarpalas} and P. {Daras}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Improving Camera Pose Estimation via Temporal EWA Surfel Splatting},
year={2017},
volume={},
number={},
pages={1-10},
abstract={Camera pose estimation is a fundamental problem of Augmented Reality and 3D reconstruction systems. Recently, despite the new better performing direct methods being developed, state-of-the-art methods are still estimating erroneous poses due to sensor noise, environmental conditions and challenging trajectories. Adding a back-end mapping process, SLAM systems achieve better performance and are more robust, but require higher computational resources, limiting their applicability. Therefore, lighter solutions to improve the accuracy of pose estimates are required. In this work we demonstrate the effectiveness of lighter data structures, namely surface elements, and exploit the temporality of sensor data streams to accumulate moving camera frames and improve tracking. This representation allows us to splat a photometric and geometric model simultaneously and use it to improve the performance of dense RGB-D camera pose estimation methods. Exploiting Elliptical Weighted Average splatting to produce high quality photometric results also allows us to detect erroneous poses through a novel visual quality analysis process. We show evidence of the EWA temporal model's effectiveness in publicly available datasets and argue that point-based representations are a good candidate for building lighter systems that should be further explored.},
keywords={augmented reality;cameras;image colour analysis;image reconstruction;pose estimation;robot vision;SLAM (robots);temporal EWA surfel splatting;fundamental problem;Augmented Reality;3D reconstruction systems;direct methods;environmental conditions;back-end mapping process;SLAM systems;higher computational resources;sensor data streams;moving camera frames;photometric model;geometric model;high quality photometric results;EWA temporal model;RGB-D camera pose estimation methods;visual quality analysis process;Cameras;Pose estimation;Image color analysis;Simultaneous localization and mapping;Three-dimensional displays;Colored noise;Solid modeling;Camera pose estimation;Surface elements (surfels);Elliptical Weighted Average (EWA);Point-based rendering (PBR);Splatting;Tracking;SLAM;AR;3D reconstruction;Visual quality analysis (VQA)},
doi={10.1109/ISMAR.2017.17},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8115400,
author={P. {Li} and T. {Qin} and B. {Hu} and F. {Zhu} and S. {Shen}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Monocular Visual-Inertial State Estimation for Mobile Augmented Reality},
year={2017},
volume={},
number={},
pages={11-21},
abstract={Mobile phones equipped with a monocular camera and an inertial measurement unit (IMU) are ideal platforms for augmented reality (AR) applications, but the lack of direct metric distance measurement and the existence of aggressive motions pose significant challenges on the localization of the AR device. In this work, we propose a tightly-coupled, optimization-based, monocular visual-inertial state estimation for robust camera localization in complex indoor and outdoor environments. Our approach does not require any artificial markers, and is able to recover the metric scale using the monocular camera setup. The whole system is capable of online initialization without relying on any assumptions about the environment. Our tightly-coupled formulation makes it naturally robust to aggressive motions. We develop a lightweight loop closure module that is tightly integrated with the state estimator to eliminate drift. The performance of our proposed method is demonstrated via comparison against state-of-the-art visual-inertial state estimators on public datasets and real-time AR applications on mobile devices. We release our implementation on mobile devices as open source software1.},
keywords={augmented reality;cameras;distance measurement;inertial navigation;mobile computing;SLAM (robots);robust camera localization;complex indoor environments;aggressive motions;state estimator;mobile devices;mobile augmented reality;mobile phones;inertial measurement unit;direct metric distance measurement;monocular camera;tightly-coupled optimization-based monocular visual-inertial state estimation;complex outdoor environments;public datasets;real-time AR applications;open source software;Cameras;Feature extraction;Mobile handsets;Measurement;State estimation;Visualization;Robustness},
doi={10.1109/ISMAR.2017.18},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8115401,
author={J. {Orlosky} and P. {Kim} and K. {Kiyokawa} and T. {Mashita} and P. {Ratsamee} and Y. {Uranishi} and H. {Takemura}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={VisMerge: Light Adaptive Vision Augmentation via Spectral and Temporal Fusion of Non-visible Light},
year={2017},
volume={},
number={},
pages={22-31},
abstract={Low light situations pose a significant challenge to individuals working in a variety of different fields such as firefighting, rescue, maintenance and medicine. Tools like flashlights and infrared (IR) cameras have been used to augment light in the past, but they must often be operated manually, provide a field of view that is decoupled from the operator's own view, and utilize color schemes that can occlude content from the original scene. To help address these issues, we present VisMerge, a framework that combines a thermal imaging head mounted display (HMD) and algorithms that temporally and spectrally merge video streams of different light bands into the same field of view. For temporal synchronization, we first develop a variant of the time warping algorithm used in virtual reality (VR), but redesign it to merge video see-through (VST) cameras with different latencies. Next, using computer vision and image compositing we develop five new algorithms designed to merge non-uniform video streams from a standard RGB camera and small form-factor infrared (IR) camera. We then implement six other existing fusion methods, and conduct a series of comparative experiments, including a system level analysis of the augmented reality (AR) time warping algorithm, a pilot experiment to test perceptual consistency across all eleven merging algorithms, and an in-depth experiment on performance testing the top algorithms in a VR (simulated AR) search task. Results showed that we can reduce temporal registration error due to inter-camera latency by an average of 87.04%, that the wavelet and inverse stipple algorithms were perceptually rated the highest, that noise modulation performed best, and that freedom of user movement is significantly increased with visualizations engaged.},
keywords={augmented reality;cameras;computer vision;feature extraction;helmet mounted displays;image colour analysis;image fusion;image registration;image segmentation;image sensors;infrared imaging;medical image processing;video signal processing;video streaming;virtual reality;VisMerge;light adaptive vision augmentation;spectral fusion;temporal fusion;nonvisible light;color schemes;temporal synchronization;time warping algorithm;virtual reality;computer vision;image compositing;nonuniform video streams;system level analysis;augmented reality time;temporal registration error;light bands;low-light situations;thermal imaging head mounted display;HMD;temporally merge video streams;spectrally merge video streams;small form-factor infrared camera;small form-factor IR camera;perceptual consistency;performance testing;VR search task;simulated AR search task;temporal registration error reduction;wavelet algorithm;inverse stipple algorithm;noise modulation;VST cameras;Cameras;Streaming media;Calibration;Resists;Algorithm design and analysis;Image color analysis;Head;Vision augmentation;augmented reality;infrared;image fusion;timewarping},
doi={10.1109/ISMAR.2017.19},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8115402,
author={A. {Fond} and M. {Berger} and G. {Simon}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Facade Proposals for Urban Augmented Reality},
year={2017},
volume={},
number={},
pages={32-41},
abstract={We introduce a novel object proposals method specific to building facades. We define new image cues that measure typical facade characteristics such as semantic, symmetry and repetitions. They are combined to generate a few facade candidates in urban environments fast. We show that our method outperforms state-of-the-art object proposals techniques for this task on the 1000 images of the Zurich Building Database. We demonstrate the interest of this procedure for augmented reality through facade recognition and camera pose initialization. In a very time-efficient pipeline we classify the candidates and match them to a facade references database using CNN-based descriptors. We prove that this approach is more robust to severe changes of viewpoint and occlusions than standard object recognition methods.},
keywords={augmented reality;buildings (structures);feature extraction;image classification;image segmentation;object detection;object recognition;pose estimation;structural engineering computing;urban environments;facade recognition;time-efficient pipeline;facade references database;standard object recognition methods;facade proposals;urban augmented reality;building facades;facade candidates;object proposals techniques;Zurich building database;object proposals method;image cues;facade characteristics;CNN-based descriptors;occlusions;camera pose initialization;Proposals;Semantics;Image edge detection;Three-dimensional displays;Cameras;Windows},
doi={10.1109/ISMAR.2017.20},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8115403,
author={S. {Willi} and A. {Grundhöfer}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Robust Geometric Self-Calibration of Generic Multi-Projector Camera Systems},
year={2017},
volume={},
number={},
pages={42-51},
abstract={Calibration of multi-projector-camera systems (MPCS) is a cumbersome and time-consuming process. It is of great importance to have robust, fast and accurate calibration procedures at hand for a wide variety of practical applications. We propose a fully automated self-calibration method for arbitrarily complex MPCS. It enables reliable and accurate intrinsic and extrinsic calibration without any human parameter tuning. We evaluated the proposed methods using more than ten multi-projection datasets ranging from a toy castle set up consisting of three cameras and one projector up to a half dome display system with more than 30 devices. Comparisons to reference calibrations, which were generated using the standard checkerboard calibration approach [44], show the reliability of our proposed pipeline, while a ground truth evaluation also shows that the resulting reconstructed point cloud accurately matches the shape of the reference geometry. Besides being fully automatic without the necessity of parameter fine tuning, the proposed method also significantly reduces the installation time of MPCS compared to checkerboard-based methods and makes it more suitable for real-world applications.},
keywords={calibration;cameras;image reconstruction;image sensors;optical projectors;robust geometric self-calibration;generic multiprojector camera systems;cumbersome time-consuming process;robust calibration procedures;fast calibration procedures;accurate calibration procedures;fully automated self-calibration method;arbitrarily complex MPCS;intrinsic calibration;extrinsic calibration;human parameter tuning;multiprojection datasets;half dome display system;reference calibrations;ground truth evaluation;parameter fine tuning;standard checkerboard calibration approach;Cameras;Calibration;Robustness;Surface treatment;Three-dimensional displays;Geometry;Projector-camera systems;Calibration and registration of sensing systems;Display hardware;including 3D;stereoscopic and multi-user Entertainment;broadcast},
doi={10.1109/ISMAR.2017.21},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8115404,
author={Y. {Watanabe} and T. {Kato} and M. {ishikawa}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Extended Dot Cluster Marker for High-speed 3D Tracking in Dynamic Projection Mapping},
year={2017},
volume={},
number={},
pages={52-61},
abstract={The technique of Projection Mapping, which is useful for merging real-world geometry with an augmented appearance, is a promising core technology for augmented reality (AR). In recent years, dynamically changing environments, mainly a consequence of the growing demand for interactive user experiences, have contributed to a new style of AR applications. However, performance levels of current systems for realizing 3D effects, in terms of the tracking speed and projection ability, are insufficient to meet these demands. In this paper, we present a high-speed, occlusion-robust marker-based 3D tracking method achieved by only using a monocular monochrome image. The objective of our research is to develop an automatic marker design method for any 3D shape and an effective framework for stabilizing tracking at high throughput by extending the latest promising work based on a deformable dot cluster marker [46]. Furthermore, this tracking method was used in combination with a high-speed projector, both of which can achieve high throughput and low latency, on the order of milliseconds. This enabled us to realize a high-quality computational display capable of representing the material appearance of a dynamically moving target. The demonstration showed that the effect of a dynamically changing appearance with nearly imperceptible latency drastically enriches the sense of immersion in the recognition of augmented materials with the naked eye.},
keywords={augmented reality;image segmentation;mobile robots;object detection;object tracking;optical projectors;robot vision;tracking;automatic marker design method;deformable dot cluster marker;tracking method;high-speed projector;high-quality computational display;material appearance;dynamically moving target;dynamically changing appearance;augmented materials;extended dot cluster marker;high-speed 3D;real-world geometry;augmented appearance;augmented reality;interactive user experiences;tracking speed;projection ability;occlusion-robust marker;monocular monochrome image;dynamic projection;projection mapping;Three-dimensional displays;Target tracking;Solid modeling;Robustness;Shape;Heuristic algorithms;Augmented reality},
doi={10.1109/ISMAR.2017.22},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8115405,
author={A. K. {Hebborn} and N. {Höhner} and S. {Müller}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Occlusion Matting: Realistic Occlusion Handling for Augmented Reality Applications},
year={2017},
volume={},
number={},
pages={62-71},
abstract={Nowadays, visualizations in Augmented Reality have to be as realistic as possible with lowest possible computational cost. In this paper, we present a real-time solution to realize dynamic occlusions. Sometimes virtual objects are softly, partially or totally occluded by real objects. Incorrect and inaccurate occlusion handling breaks the illusion of co-existence between the real and virtual world on the one hand and can result in wrong depth perception on the other hand. Our approach formulates the occlusion problem as alpha matting problem. Instead of calculating the visibility for each pixel of the virtual objects we estimate a blending coefficient. This enables a seamless integration of virtual objects in the real world, even for fuzzy foreground objects (like hair). Our approach takes raw depth information of the real scene (e.g. obtained by a low cost depth sensor) to realize rough foreground background segmentation. The blending coefficient between transitions where depth values are typically noisy is estimated based on the color image. Experimental evaluations of several scenes demonstrate that our algorithm produces consistent and visually appealing occlusions between the real and virtual scene with low computational cost. Furthermore, we compare the results with related depth-based approaches and show that our algorithm overcomes previous limitations.},
keywords={augmented reality;fuzzy set theory;image segmentation;virtual world;wrong depth perception;alpha matting problem;virtual objects;blending coefficient;fuzzy foreground objects;rough foreground background segmentation;virtual scene;occlusion matting;realistic occlusion handling;Augmented Reality applications;Image color analysis;Image edge detection;Solid modeling;Augmented reality;Real-time systems;Color;Noise measurement;Realistic Occlusion;Dynamic Occlusion Handling;Augmented Reality;Alpha Matting;Natural Image Matting},
doi={10.1109/ISMAR.2017.23},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8115406,
author={D. R. {Walton} and D. {Thomas} and A. {Steed} and A. {Sugimoto}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Synthesis of Environment Maps for Mixed Reality},
year={2017},
volume={},
number={},
pages={72-81},
abstract={When rendering virtual objects in a mixed reality application, it is helpful to have access to an environment map that captures the appearance of the scene from the perspective of the virtual object. It is straightforward to render virtual objects into such maps, but capturing and correctly rendering the real components of the scene into the map is much more challenging. This information is often recovered from physical light probes, such as reflective spheres or fisheye cameras, placed at the location of the virtual object in the scene. For many application areas, however, real light probes would be intrusive or impractical. Ideally, all of the information necessary to produce detailed environment maps could be captured using a single device. We introduce a method using an RGBD camera and a small fisheye camera, contained in a single unit, to create environment maps at any location in an indoor scene. The method combines the output from both cameras to correct for their limited field of view and the displacement from the virtual object, producing complete environment maps suitable for rendering the virtual content in real time. Our method improves on previous probeless approaches by its ability to recover high-frequency environment maps. We demonstrate how this can be used to render virtual objects which shadow, reflect and refract their environment convincingly.},
keywords={augmented reality;cameras;lighting;rendering (computer graphics);mixed reality application;virtual object;reflective spheres;fisheye cameras;virtual content;high-frequency environment maps;environment map synthesis;virtual object rendering;scene appearance;physical light probes;RGBD camera;indoor scene;camera field-of-view;Cameras;Probes;Lighting;Rendering (computer graphics);Real-time systems;Light sources;Virtual reality},
doi={10.1109/ISMAR.2017.24},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8115407,
author={D. {Mandl} and K. M. {Yi} and P. {Mohr} and P. M. {Roth} and P. {Fua} and V. {Lepetit} and D. {Schmalstieg} and D. {Kalkofen}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Learning Lightprobes for Mixed Reality Illumination},
year={2017},
volume={},
number={},
pages={82-89},
abstract={This paper presents the first photometric registration pipeline for Mixed Reality based on high quality illumination estimation using convolutional neural networks (CNNs). For easy adaptation and deployment of the system, we train the CNNs using purely synthetic images and apply them to real image data. To keep the pipeline accurate and efficient, we propose to fuse the light estimation results from multiple CNN instances and show an approach for caching estimates over time. For optimal performance, we furthermore explore multiple strategies for the CNN training. Experimental results show that the proposed method yields highly accurate estimates for photo-realistic augmentations.},
keywords={augmented reality;feedforward neural nets;image registration;learning (artificial intelligence);lighting;pipeline processing;lightprobes;mixed reality illumination;photometric registration pipeline;high quality illumination estimation;convolutional neural networks;purely synthetic images;image data;light estimation results;multiple CNN instances;caching estimates;optimal performance;CNN training;Lighting;Cameras;Training;Three-dimensional displays;Image reconstruction;Virtual reality;Rendering (computer graphics)},
doi={10.1109/ISMAR.2017.25},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8115408,
author={H. {Regenbrecht} and K. {Meng} and A. {Reepen} and S. {Beck} and T. {Langlotz}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Mixed Voxel Reality: Presence and Embodiment in Low Fidelity, Visually Coherent, Mixed Reality Environments},
year={2017},
volume={},
number={},
pages={90-99},
abstract={Mixed Reality aims at combining virtual reality with the user's surrounding real environment in a way that they form one, coherent reality. A coherent visual quality is of utmost importance, expressed in measures of e.g. resolution, framerate, and latency for both the real and the virtual domains. For years, researchers have focused on maximizing the quality of the virtual visualization mimicking the real world to get closer to visual coherence. This however, makes Mixed Reality systems overly complex and requires high computational power. In this paper, we propose a different approach by decreasing the realism of one or both visual realms, real and virtual, to achieve visual coherence. Our system coarsely voxelizes the real and virtual environments, objects, and people to provide a believable, coherent mixed voxel reality. In this paper we present the general idea, the current implementation and demonstrate the effectiveness of our approach by technical and empirical evaluations. Our mixed voxel reality system serves as a platform for low-cost presence research and studies on human perception and cognition, a host of diagnostic and therapeutic applications, and for a variety of Mixed Reality applications where users' embodiment is important. Our findings challenge some commonplace assumptions on more is better approaches in mixed reality research and practice-sometimes less can be more.},
keywords={augmented reality;data visualisation;mixed reality research;Mixed Reality applications;low-cost presence research;mixed voxel reality system;coherent mixed voxel reality;believable voxel reality;virtual environments;real environments;visual realms;high computational power;Mixed Reality systems;visual coherence;virtual visualization;virtual domains;coherent visual quality;virtual reality;mixed reality environments;coherent reality environments;Virtual reality;Visualization;Cameras;Rendering (computer graphics);Hardware;Coherence;mixed reality;augmented reality;believability;presence;voxel grid},
doi={10.1109/ISMAR.2017.26},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8115409,
author={E. {Barba} and R. Z. {Marroquin}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={A Primer on Spatial Scale and Its Application to Mixed Reality},
year={2017},
volume={},
number={},
pages={100-110},
abstract={As mixed reality grows in popularity, the concepts and language we use to describe it will need to evolve as well. Having such concepts will allow for better interdisciplinary collaboration in both the arts and sciences, help to inform the creation of new software tools that enable the further evolution of the field, and will enable mixed reality research to advance scientific understanding in other disciplines. We provide an explication of the concept of spatial scale, including its relevant history in the fields of psychology and geography, and demonstrate its relevance to mixed reality. Through two case studies we show that spatial scale can operate effectively as a system of classification and analysis for mixed reality, and identify two concepts-scale transitions and the scale/complexity tradeoff-as critical to using this concept in future discussions of mixed reality.},
keywords={geography;human computer interaction;psychology;software tools;virtual reality;spatial scale;mixed reality research;interdisciplinary collaboration;software tools;psychology;geography;classification;human-centered computing;Virtual reality;Psychology;Cognition;Geography;Collaboration;Media;mixed reality;augmented reality;virtual reality;spatial scale;multiscale analysis;scale transitions},
doi={10.1109/ISMAR.2017.27},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8115410,
author={C. {Diaz} and M. {Walker} and D. A. {Szafir} and D. {Szafir}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Designing for Depth Perceptions in Augmented Reality},
year={2017},
volume={},
number={},
pages={111-122},
abstract={Augmented reality technologies allow people to view and interact with virtual objects that appear alongside physical objects in the real world. For augmented reality applications to be effective, users must be able to accurately perceive the intended real world location of virtual objects. However, when creating augmented reality applications, developers are faced with a variety of design decisions that may affect user perceptions regarding the real world depth of virtual objects. In this paper, we conducted two experiments using a perceptual matching task to understand how shading, cast shadows, aerial perspective, texture, dimensionality (i.e., 2D vs. 3D shapes) and billboarding affected participant perceptions of virtual object depth relative to real world targets. The results of these studies quantify trade-offs across virtual object designs to inform the development of applications that take advantage of users' visual abilities to better blend the physical and virtual world.},
keywords={augmented reality;image matching;image texture;depth perceptions;augmented reality technologies;virtual objects;physical objects;augmented reality applications;design decisions;user perceptions;virtual object depth relative;virtual object designs;perceptual matching task;Augmented reality;Visualization;Two dimensional displays;Virtual environments;Legged locomotion},
doi={10.1109/ISMAR.2017.28},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8115411,
author={L. {Chen} and T. W. {Day} and W. {Tang} and N. W. {John}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Recent Developments and Future Challenges in Medical Mixed Reality},
year={2017},
volume={},
number={},
pages={123-135},
abstract={Mixed Reality (MR) is of increasing interest within technology-driven modern medicine but is not yet used in everyday practice. This situation is changing rapidly, however, and this paper explores the emergence of MR technology and the importance of its utility within medical applications. A classification of medical MR has been obtained by applying an unbiased text mining method to a database of 1,403 relevant research papers published over the last two decades. The classification results reveal a taxonomy for the development of medical MR research during this period as well as suggesting future trends. We then use the classification to analyse the technology and applications developed in the last five years. Our objective is to aid researchers to focus on the areas where technology advancements in medical MR are most needed, as well as providing medical practitioners with a useful source of reference.},
keywords={data mining;medical computing;text analysis;virtual reality;MR technology;medical applications;unbiased text mining method;classification results;medical MR research;medical mixed reality;Virtual reality;Market research;Surgery;Mobile communication;Training;Databases},
doi={10.1109/ISMAR.2017.29},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8115412,
author={C. A. {Wiesner} and M. {Ruf} and D. {Sirim} and G. {Klinker}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={3D-FRC: Depiction of the future road course in the Head-Up-Display},
year={2017},
volume={},
number={},
pages={136-143},
abstract={The introduction of Head-Up-Displays (HUDs) have opened up avenues for a whole range of novel AR applications. However, until these applications become available for the mass market a number of problems need to be tackled. For example, the field of view (FoV) of current HUDs is extremely limited, and real world tracking and 3D reconstruction are still not precise enough to show driving information embedded into wide areas of complex traffic environment. It is not possible to show true AR-visualizations in the display areas provided by the current FoVs. In this paper, we investigate how an AR-like visualization approach in current HUDs (with a limited FoV) can support drivers in foreseeing the future road course. This visualisation uses the already established concept of an electronic horizon. By complying with automotive standards, our application can be easily adapted for series production. With this visualisation we performed a user study, investigating the effect on drivers' gaze behaviour. For this reason the test subjects were equipped with an eye tracking system. The results showed a decrease in both, the number of gazes as well as total glance time on the head unit and the instrument cluster. We also investigated the test subjects' braking behaviour around sharp bends of the road which showed an overall improvement when the visualisation was enabled. Furthermore it showed an increase of the mean glance duration in the area of the HUD. Note that the eye tracking system is not capable of distinguishing between glances at the visualisation in the HUD and the users' glance at objects behind the visualisation - overlapping with the HUD. This would require tracking the test persons' depth of focus. The study showed that developers need to be concerned about not displaying excessively in the HUD, so as not to distract drivers. It furthermore showed that AR-like visualizations have the potential to decrease the time the driver is not looking at the road creating a safer driving experience.},
keywords={automotive engineering;data visualisation;head-up displays;road safety;road vehicles;traffic engineering computing;future road course;head-up-display;HUD;FoV;world tracking;wide areas;complex traffic environment;visualization approach;eye tracking system;total glance time;mean glance duration;head-up-displays;Visualization;Roads;Automobiles;Instruments;Navigation;Gaze tracking},
doi={10.1109/ISMAR.2017.30},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8115413,
author={M. A. {Cidota} and P. J. M. {Bank} and P. W. {Ouwehand} and S. G. {Lukosch}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Assessing Upper Extremity Motor Dysfunction Using an Augmented Reality Game},
year={2017},
volume={},
number={},
pages={144-154},
abstract={Advances in technology offer new opportunities for a better understanding of how different disorders affect motor function. In this paper, we explore the potential of an augmented reality (AR) game implemented using free hand and body tracking to develop a uniform, cost-effective and objective methods for evaluation of upper extremity motor dysfunction in different patient groups. We conducted a study with 20 patients (10 Parkinson's Disease patients and 10 stroke patients) who performed hand/arm movement tasks in four different conditions in AR and one condition in real world. Despite usability issues mainly due to non-robust hand tracking, the patients were moderately engaged while playing the AR game. Our findings show that moving virtual objects was less targeted, took more time and was associated with larger trunk displacement and a lower variability of elbow angle and upper arm angle than moving real objects. No significant correlations were observed between characteristics of movements in AR and movements in the real world. Still, our findings suggest that the AR game may be suitable for assessing the hand and arm function of mildly affected patients if usability can be further improved.},
keywords={augmented reality;biomechanics;computer games;diseases;human computer interaction;medical computing;medical disorders;neurophysiology;patient rehabilitation;upper extremity motor dysfunction;augmented reality game;body tracking;nonrobust hand tracking;virtual objects;upper arm angle;arm function;mildly affected patients;Parkinson's disease;stroke patient;hand-arm movement tasks;trunk displacement;Games;Diseases;Visualization;Tracking;Thumb;Augmented Reality Games;Engagement;Upper Extremity Motor Dysfunction;Assessment;Parkinson's Disease;Stroke patients},
doi={10.1109/ISMAR.2017.31},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8115414,
author={D. {Harborth} and S. {Pape}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Exploring the Hype: Investigating Technology Acceptance Factors of Pokémon Go},
year={2017},
volume={},
number={},
pages={155-168},
abstract={We investigate the technology acceptance factors of the AR smart-phone game Pokémon Go with a PLS-SEM approach based on the UTAUT2 model by Venkatesh et al. [1]. Therefore, we conducted an online study in Germany with 683 users of the game. Many other studies rely on the users' imagination of the application's functionality or laboratory environments. In contrast, we asked a relatively large user base already interacting in the natural environment with the application. Not surprisingly, the strongest predictor of behavioral intention to play Pokémon Go is hedonic motivation, i.e. fun and pleasure due to playing the game. Additionally, we find medium-sized effects of effort expectancy on behavioral intention, and of habit on behavioral intention and use behavior. These results imply that AR applications - besides needing to be easily integrable in the users' daily life - should be designed in an intuitive and easily understandable way. We contribute to the understanding of the phenomenon of Pokémon Go by investigating established acceptance factors that potentially fostered the massive adoption of the game.},
keywords={augmented reality;computer games;mobile computing;smart phones;statistical analysis;Pokémon Go;PLS-SEM approach;UTAUT2 model;behavioral intention;technology acceptance factors;AR smart-phone game;Games;Mobile communication;Augmented reality;Information systems;Laboratories;Solid modeling;Mathematical model},
doi={10.1109/ISMAR.2017.32},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8115415,
author={F. {Bork} and R. {Barmaki} and U. {Eck} and K. {Yu} and C. {Sandor} and N. {Navab}},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Empirical Study of Non-Reversing Magic Mirrors for Augmented Reality Anatomy Learning},
year={2017},
volume={},
number={},
pages={169-176},
abstract={Left-right confusion occurs across the entire population and refers to an impeded ability to distinguish between left and right. In medicine this phenomenon is particularly relevant as left and right are always defined with respect to the patient's point of view, i.e. the doctor's right is the patient's left. Traditional anatomy learning resources such as illustrations in textbooks naturally consider this by consistently depicting the anatomy of a patient as seen by an observer standing in front. Augmented Reality Magic Mirrors (MM) are one example of novel anatomy teaching resources and show a user's digital mirror image augmented with virtual anatomy on a large display. As left and right appear to be reversed in such MM setups, similar to real-world physical mirrors, intriguing perceptual questions arise: is a non-reversing MM (NRMM) the more natural choice for the task of anatomy learning and do users even learn anatomy the wrong way with a traditional, reversing MM (RMM)? In this paper, we explore the perceptual differences between an NRMM and RMM design and present the first empirical study comparing these two concepts for the purpose of anatomy learning. Experimental results demonstrate that medical students perform significantly better at identifying anatomically correct placement of virtual organs in an NRMM. However, interaction was significantly more difficult compared to an RMM. We explore the underlying psychological effects and discuss the implications of using an NRMM on user perception, knowledge transfer, and interaction. This study is relevant for the design of future MM systems in the medical domain and lessons-learned can be transferred to other application domains.},
keywords={augmented reality;biomedical education;computer aided instruction;data visualisation;medical computing;psychology;teaching;NRMM;RMM;future MM systems;augmented reality anatomy learning;impeded ability;patient;traditional anatomy learning resources;Augmented Reality Magic Mirrors;anatomy teaching resources;virtual anatomy;MM setups;real-world physical mirrors;intriguing perceptual questions;medicine;virtual organs;knowledge transfer;nonreversing MM;Mirrors;Education;Augmented reality;Medical diagnostic imaging;Games},
doi={10.1109/ISMAR.2017.33},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8115416,
author={},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={Author index},
year={2017},
volume={},
number={},
pages={177-177},
abstract={Presents an index of the authors whose articles are published in the conference proceedings record.},
keywords={},
doi={10.1109/ISMAR.2017.34},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8115417,
author={},
booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, title={[Publisher's information]},
year={2017},
volume={},
number={},
pages={178-178},
abstract={Provides a listing of current committee members and society officers.},
keywords={},
doi={10.1109/ISMAR.2017.35},
ISSN={},
month={Oct},}