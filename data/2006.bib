@INPROCEEDINGS{4079240,
author={},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={ISMAR 2006 - The Fifth IEEE and ACM International Symposium on Mixed and Augmented Reality},
year={2006},
volume={},
number={},
pages={c1-c1},
abstract={The following topics are dealt with: applications of augmented reality and mixed reality systems; tracking; sensors; user interfaces; system architecture; user interaction; information presentation; human factors.},
keywords={augmented reality;human factors;sensors;tracking;user interfaces;augmented reality;mixed reality;tracking;sensors;user interfaces;system architecture;user interaction;information presentation;human factors},
doi={10.1109/ISMAR.2006.297778},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079241,
author={},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Proceedings - ISMAR 2006 - International Symposium on Mixed and Augmented Reality},
year={2006},
volume={},
number={},
pages={},
abstract={},
keywords={},
doi={10.1109/ISMAR.2006.297779},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079242,
author={},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Table of contents},
year={2006},
volume={},
number={},
pages={v-viii},
abstract={Presents the table of contents of the proceedings.},
keywords={},
doi={10.1109/ISMAR.2006.297780},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079243,
author={},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Message from the General Chairs},
year={2006},
volume={},
number={},
pages={ix-x},
abstract={Presents the welcome message from the conference proceedings.},
keywords={},
doi={10.1109/ISMAR.2006.297781},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079244,
author={},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Message from the Program Chairs},
year={2006},
volume={},
number={},
pages={xi-xii},
abstract={Presents the welcome message from the conference proceedings.},
keywords={},
doi={10.1109/ISMAR.2006.297782},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079245,
author={},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Committees, Chairs Additional Reviewers},
year={2006},
volume={},
number={},
pages={xiii-xviii},
abstract={Provides a listing of current committee members.},
keywords={},
doi={10.1109/ISMAR.2006.297783},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079247,
author={G. {Klinker} and S. {Noelle} and T. {Ohshima} and M. {Toennis}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Workshop - Industrial augmented reality},
year={2006},
volume={},
number={},
pages={xx-xx},
abstract={},
keywords={},
doi={10.1109/ISMAR.2006.297785},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079248,
author={J. {Frahm} and J. {Rolland} and A. {State} and O. {Cakmakci}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Tutorials - MAR Tutorial 1 (half day) ISMAR Tutorial 2 (half day)},
year={2006},
volume={},
number={},
pages={xxi-xxi},
abstract={Provides an abstract for each of the presentations and a brief professional biography of each presenter. The complete presentations were not made available for publication as part of the conference proceedings.},
keywords={},
doi={10.1109/ISMAR.2006.297786},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079249,
author={R. {Raskar} and T. A. {Furness}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Keynote speeches - The poor man's palace: special effects in the real world / Do we have six brains?},
year={2006},
volume={},
number={},
pages={xxii-xxiii},
abstract={Provides an abstract for each of the keynote presentations and may include a brief professional biography of each},
keywords={},
doi={10.1109/ISMAR.2006.297787},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079250,
author={M. A. {Livingston}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Quantification of visual capabilities using augmented reality displays},
year={2006},
volume={},
number={},
pages={3-12},
abstract={In order to be able to perceive and recognize objects or surface properties of objects, one must be able to resolve the features. These perceptual tasks can be difficult for both graphical representations and real objects in augmented reality (AR) displays. This paper presents the results of objective measurements and two user studies. The first evaluation explores visual acuity and contrast sensitivity; the second explores color perception. Both experiments test users' capabilities with their natural vision against their capabilities using commercially-available AR displays. The limited graphical resolution, reduced brightness, and uncontrollable visual context of the merged environment demonstrably reduce users' visual capabilities. The paper concludes by discussing the implications for display design and AR applications, as well as outlining possible extensions to the current studies.},
keywords={augmented reality;computer vision;face recognition;image colour analysis;image resolution;visual capabilities quantification;augmented reality displays;graphical representations;color perception;commercially-available AR displays;graphical resolution;Augmented reality;Testing;Space exploration;Computer displays;Computer graphics;Humans;Optical sensors;Retina;Three dimensional displays;Brightness},
doi={10.1109/ISMAR.2006.297788},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079251,
author={M. {Tonnis} and G. {Klinker}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Effective control of a car driver's attention for visual and acoustic guidance towards the direction of imminent dangers},
year={2006},
volume={},
number={},
pages={13-22},
abstract={In cars, augmented reality is becoming an interesting means to enhance active safety in the driving task. Guiding a driver's attention to an imminent danger somewhere around the car is a potential application. In a research project with the automotive industry, we are exploring different approaches towards alerting drivers to such dangers. First results were presented last year. We have extended two of these approaches. One uses AR to visualize the source of danger in the driver's frame of reference while the other one presents information in a bird's eye schematic map. Our extensions were the incorporation of a real head-up display, improved visual perception and acoustic support. Both schemes were evaluated both with and without 3D encoded sound. This paper reports on a user test in which 24 participants provided objective and subjective measurements. The results indicate that the AR-based three-dimensional presentation scheme with and without sound support systematically outperforms the bird's eye schematic map.},
keywords={augmented reality;driver information systems;augmented reality;acoustic guidance;visual guidance;head-up display;visual perception;acoustic support;Visualization;Augmented reality;Displays;Automotive components;Animation;Vehicle safety;Automotive engineering;Visual perception;Acoustic testing;Acoustic measurements},
doi={10.1109/ISMAR.2006.297789},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079252,
author={J. {Oh} and H. {Hua}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={User evaluations on form factors of tangible magic lenses},
year={2006},
volume={},
number={},
pages={23-32},
abstract={Magic Lens is a small inset window embedded in a large context display, which provides an alternative view to the region of interest selected from the context view. This metaphor is used for 3D visualization in our Augmented Virtual Environment infrastructure, SCAPE (Stereoscopic Collaboration in Augmented and Projective Environments), which is composed of an immersive room display for a high level of detail, life-size virtual world and a workbench display for simplified god-like view to the world. A tangible Magic Lens is used on the workbench display to allow direct and intuitive selection of continuous levels of detail, bridging the gap between the two extreme levels of detail in SCAPE. This paper presents our first step to the user evaluations of tangible Magic Lens. We conducted two sets of user evaluations, one mainly testing the lens aspect ratio, and another for the lens size. For both of the tests, two types of tasks are conducted: information gathering and relating the detailed information with the context. We found that the aspect ratio of a lens plays more important role in user preference for smaller lenses than for larger ones. Meanwhile, the size of a lens is the most important factor that affects the user performance in the two types of tasks.},
keywords={augmented reality;data visualisation;graphical user interfaces;three-dimensional displays;user evaluation;tangible magic lens;small inset window;large context display;3D visualization;augmented virtual environment infrastructure;stereoscopic collaboration;immersive room display;form factor;Lenses;Navigation;Virtual environment;Visualization;Graphical user interfaces;Collaborative work;Three dimensional displays;Augmented reality;Optical design;Testing},
doi={10.1109/ISMAR.2006.297790},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079253,
author={B. H. {Thomas}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Evaluation of three input techniques for selection and annotation of physical objects through an augmented reality view},
year={2006},
volume={},
number={},
pages={33-36},
abstract={This paper presents results from a study into the usability issues of two tasks (selection and annotation of a physical object) for users operating mobile augmented reality systems. The study compared the following three different modes of cursor manipulation: a handheld mouse, a head cursor, and an image-plane vision-tracked device. The selection task was evaluated based on number of mouse button clicks, completion time, and a subjective survey. The annotation task was evaluated based on accuracy of the annotation, completion time, and a subjective survey.},
keywords={augmented reality;human factors;interactive devices;mobile computing;user interfaces;wearable computers;mobile augmented reality system;usability issue;physical object selection;physical object annotation;cursor manipulation;handheld mouse;head cursor;image-plane vision-tracked device;wearable computer;input device;human factor;user interface technology;Augmented reality;Wearable computers;Head;Mice;Collaborative work;Virtual reality;Usability;User interfaces;Computer graphics;Cameras},
doi={10.1109/ISMAR.2006.297791},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079254,
author={G. {Simon}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Automatic online walls detection for immediate use in AR tasks},
year={2006},
volume={},
number={},
pages={39-42},
abstract={This paper proposes a method to automatically detect and reconstruct planar surfaces for immediate use in AR tasks. Traditional methods for plane detection are typically based on the comparison of transfer errors of a homography, which make them sensitive to the choice of a discrimination threshold. We propose a very different approach: the image is divided into a grid and rectangles that belong to the same planar surface are clustered around the local maxima of a Hough transform. As a result, we simultaneously get clusters of coplanar rectangles and the image of their intersection line with a reference plane, which easily leads to their 3D position and orientation. Results are shown on both synthetic and real data.},
keywords={augmented reality;Hough transforms;image reconstruction;object detection;pattern clustering;automatic online wall detection;augmented reality;planar surface reconstruction;plane detection;discrimination threshold;Hough transform;coplanar rectangle;pattern clustering;Layout;Image reconstruction;Surface reconstruction;Buildings;Cameras;Augmented reality;Rough surfaces;Surface roughness;Detection algorithms;Object detection},
doi={10.1109/ISMAR.2006.297792},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079255,
author={M. {Bauer} and M. {Schlegel} and D. {Pustka} and N. {Navab} and G. {Klinker}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Predicting and estimating the accuracy of n-occular optical tracking systems},
year={2006},
volume={},
number={},
pages={43-51},
abstract={Marker-based optical tracking systems are widely used in augmented reality, medical navigation and industrial applications. We propose a model for the prediction of the target registration error (TRE) in these kinds of tracking systems by estimating the fiducial location error (FLE) from two-dimensional errors on the image plane and propagating that error to a given point of interest. We have designed a set of experiments in order to estimate the actual parameters of the model for any given tracking system. We present the results of a study which we used to demonstrate the effect of different sources of error. The method is applied to real applications to show the usefulness for any kind of augmented reality system. We also present a set of tools that can be used to visualize the accuracy at design time.},
keywords={augmented reality;image registration;optical tracking;pose estimation;target tracking;n-occular marker-based optical tracking system;target registration error prediction model;fiducial location error estimation;augmented reality system;object pose estimation;image plane error;Target tracking;Cameras;Biomedical optical imaging;Augmented reality;Optical propagation;Estimation error;Visualization;Computer vision;Covariance matrix;Biomedical imaging},
doi={10.1109/ISMAR.2006.297793},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079256,
author={M. {Pressigout} and E. {Marchand}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Hybrid tracking algorithms for planar and non-planar structures subject to illumination changes},
year={2006},
volume={},
number={},
pages={52-55},
abstract={Augmented reality (AR) aims to fuse a virtual world and a real one in an image stream. When considering only a vision sensor, it relies on registration techniques that have to be accurate and fast enough for on-line augmentation. This paper proposes a real-time, robust and efficient 3D model-based tracking algorithm monocular vision system. A virtual visual servoing approach is used to estimate the pose between the camera and the object. The integration of texture information in the classical non-linear edge-based pose computation provides a more reliable tracker. Several illumination models have been considered and compared to better deal with the illumination change in the scene. The method presented in this paper has been validated on several video sequences for augmented reality applications.},
keywords={augmented reality;computer vision;image registration;tracking;hybrid tracking algorithms;nonplanar structures;illumination changes;augmented reality;virtual world;registration techniques;online augmentation;3D model-based tracking algorithm monocular vision system;virtual visual servoing approach;texture information integration;Lighting;Augmented reality;Fuses;Streaming media;Real time systems;Robustness;Machine vision;Visual servoing;Cameras;Layout},
doi={10.1109/ISMAR.2006.297794},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079257,
author={G. {Bleser} and H. {Wuest} and D. {Stricker}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Online camera pose estimation in partially known and dynamic scenes},
year={2006},
volume={},
number={},
pages={56-65},
abstract={One of the key requirements of augmented reality systems is a robust real-time camera pose estimation. In this paper we present a robust approach, which does neither depend on offline pre-processing steps nor on pre-knowledge of the entire target scene. The connection between the real and the virtual world is made by a given CAD model of one object in the scene. However, the model is only needed for initialization. A line model is created out of the object rendered from a given camera pose and registrated onto the image gradient for finding the initial pose. In the tracking phase, the camera is not restricted to the modeled part of the scene anymore. The scene structure is recovered automatically during tracking. Point features are detected in the images and tracked from frame to frame using a brightness invariant template matching algorithm. Several template patches are extracted from different levels of an image pyramid and are used to make the 2D feature tracking capable for large changes in scale. Occlusion is detected already on the 2D feature tracking level. The features' 3D locations are roughly initialized by linear triangulation and then refined recursively over time using techniques of the Extended Kalman Filter framework. A quality manager handles the influence of a feature on the estimation of the camera pose. As structure and pose recovery are always performed under uncertainty, statistical methods for estimating and propagating uncertainty have been incorporated consequently into both processes. Finally, validation results on synthetic as well as on real video sequences are presented.},
keywords={augmented reality;CAD;cameras;feature extraction;gradient methods;hidden feature removal;image matching;image registration;Kalman filters;nonlinear filters;object detection;pose estimation;real-time systems;rendering (computer graphics);statistical analysis;tracking;online robust real-time camera pose estimation;partially known scene;dynamic scene;augmented reality system;virtual world;CAD model;image gradient registration;object rendering;brightness invariant template matching algorithm;image template patch extraction;2D feature tracking;image pyramid;occlusion detection;linear triangulation;extended Kalman filter;statistical method;point feature detection;Cameras;Layout;Robustness;Uncertainty;Augmented reality;Real time systems;Rendering (computer graphics);Computer vision;Brightness;Quality management},
doi={10.1109/ISMAR.2006.297795},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079258,
author={J. {Pilet} and A. {Geiger} and P. {Lagger} and V. {Lepetit} and P. {Fua}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={An all-in-one solution to geometric and photometric calibration},
year={2006},
volume={},
number={},
pages={69-78},
abstract={We propose a fully automated approach to calibrating multiple cameras whose fields of view may not all overlap. Our technique only requires waving an arbitrary textured planar pattern in front of the cameras, which is the only manual intervention that is required. The pattern is then automatically detected in the frames where it is visible and used to simultaneously recover geometric and photometric camera calibration parameters. In other words, even a novice user can use our system to extract all the information required to add virtual 3D objects into the scene and light them convincingly. This makes it ideal for Augmented Reality applications and we distribute the code under a GPL license.},
keywords={augmented reality;cameras;image registration;geometric-photometric calibration;multiple cameras;textured planar pattern;photometric camera calibration parameters;virtual 3D objects;augmented reality applications;Photometry;Calibration;Cameras;Augmented reality;Licenses;Robustness;Manuals;Data mining;Layout;Geometry},
doi={10.1109/ISMAR.2006.297796},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079259,
author={K. {Satoh} and K. {Takemoto} and S. {Uchiyama} and H. {Yamamoto}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={A registration evaluation system using an industrial robot},
year={2006},
volume={},
number={},
pages={79-87},
abstract={This paper describes an evaluation system using an industrial robot, constructed for the purpose of evaluating registration technology for Mixed Reality. In this evaluation system, the tip of the robot arm plays the role of the user's head, where a head- mounted display is mounted. By using an industrial robot, we can obtain the ground truth of the camera pose with a high level of accuracy and robustness. Additionally, we have the ability to play back the same specified operations repeatedly under identical conditions. In addition to the system implementation, we propose evaluation methods for motion robustness, relative orientation robustness, relative distance robustness, jitter, and an overall evaluation. We verify the validity of this system through some experiments.},
keywords={augmented reality;computer vision;image registration;industrial robots;registration evaluation system;industrial robot;robot arm;relative orientation robustness;relative distance robustness;motion robustness;mixed reality;augmented reality;head tracking;Service robots;Cameras;Virtual reality;Robot vision systems;Robustness;Head;Displays;Jitter;Fixtures;Chromium},
doi={10.1109/ISMAR.2006.297797},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079260,
author={R. {Azuma} and H. {Neely} and M. {Daily} and J. {Leonard}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Performance analysis of an outdoor augmented reality tracking system that relies upon a few mobile beacons},
year={2006},
volume={},
number={},
pages={101-104},
abstract={We describe and evaluate a new tracking concept for outdoor Augmented Reality. A few mobile beacons added to the environment correct errors in head-worn inertial and GPS sensors. We evaluate the accuracy through detailed simulation of many error sources. The most important parameters are the errors in measuring the beacon and user's head positions, and the geometric configuration of the beacons around the point to augment. Using Monte Carlo simulations, we identify combinations of beacon configurations and error parameters that meet a specified goal of 1 m net error at 100 m range.},
keywords={augmented reality;Global Positioning System;military computing;Monte Carlo methods;target tracking;outdoor augmented reality tracking system;mobile beacons;tracking concept;head-worn inertial sensors;GPS sensors;head positions;geometric configuration;Monte Carlo simulations;Performance analysis;Augmented reality;Infrared sensors;Global Positioning System;Computer errors;Computer displays;Military computing;Unmanned aerial vehicles;Sensor systems;Laboratories},
doi={10.1109/ISMAR.2006.297798},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079261,
author={D. {Pustka} and M. {Huber} and M. {Bauer} and G. {Klinker}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Spatial relationship patterns: elements of reusable tracking and calibration systems},
year={2006},
volume={},
number={},
pages={88-97},
abstract={With tracking setups becoming increasingly complex, it gets more difficult to find suitable algorithms for tracking, calibration and sensor fusion. A large number of solutions exists in the literature for various combinations of sensors, however, no development methodology is available for systematic analysis of tracking setups. When modeling a system as a spatial relationship graph (SRG), which describes coordinate systems and known transformations, all algorithms used for tracking and calibration correspond to certain patterns in the graph. This paper introduces a formal model for representing such spatial relationship patterns and presents a small catalog of patterns frequently used in augmented reality systems. We also describe an algorithm to identify patterns in SRGs at runtime for automatic construction of data flows networks for tracking and calibration.},
keywords={augmented reality;calibration;data flow graphs;image fusion;pose estimation;tracking;spatial relationship graph pattern;reusable tracking;calibration system;sensor fusion;augmented reality system;pattern identification algorithm;data flow network;pose estimation;Calibration;Sensor systems;Augmented reality;Runtime;Cameras;Data flow computing;Buildings;Software algorithms;Sensor fusion;Instruments},
doi={10.1109/ISMAR.2006.297799},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079262,
author={J. {Platonov} and H. {Heibel} and P. {Meier} and B. {Grollmann}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={A mobile markerless AR system for maintenance and repair},
year={2006},
volume={},
number={},
pages={105-108},
abstract={We present a solution for AR based repair guidance. This solution covers software as well as hardware related issues. In particular we developed a markerless CAD based tracking system which can deal with different illumination conditions during the tracking stage, partial occlusions and rapid motion. The system is also able to automatically recover from occasional tracking failures. On the hardware side the system is based on an off the shelf notebook, a wireless mobile setup consisting of a wide-angle video camera and an analog video transmission system. This setup has been tested with a monocular full-color video-see-through HMD and additionally with a monochrome optical-see-through HMD. Our system underwent several extensive test series under real industrial conditions and proved to be useful for different maintenance and repair scenarios.},
keywords={augmented reality;CAD;computerised instrumentation;maintenance engineering;mobile markerless AR system;maintenance;repair guidance;markerless CAD based tracking system;partial occlusions;rapid motion;monocular full-color video-see-through HMD;monochrome optical-see-through HMD;augmented reality;Computer vision;Cameras;Computer aided manufacturing;Hardware;Augmented reality;Tracking;Computer applications;Lighting;System testing;Manufacturing industries},
doi={10.1109/ISMAR.2006.297800},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079263,
author={G. {Reitmayr} and T. W. {Drummond}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Going out: robust model-based tracking for outdoor augmented reality},
year={2006},
volume={},
number={},
pages={109-118},
abstract={This paper presents a model-based hybrid tracking system for outdoor augmented reality in urban environments enabling accurate, realtime overlays for a handheld device. The system combines several well-known approaches to provide a robust experience that surpasses each of the individual components alone: an edge-based tracker for accurate localisation, gyroscope measurements to deal with fast motions, measurements of gravity and magnetic field to avoid drift, and a back store of reference frames with online frame selection to re-initialize automatically after dynamic occlusions or failures. A novel edge-based tracker dispenses with the conventional edge model, and uses instead a coarse, but textured, 3D model. This yields several advantages: scale-based detail culling is automatic, appearance-based edge signatures can be used to improve matching and the models needed are more commonly available. The accuracy and robustness of the resulting system is demonstrated with comparisons to map-based ground truth data.},
keywords={augmented reality;edge detection;hidden feature removal;image matching;image texture;solid modelling;tracking;robust model-based hybrid tracking system;outdoor augmented reality;urban environment;handheld device;edge-based tracker;gyroscope measurement;online frame selection;dynamic occlusion;3D texture model;appearance-based edge signature;point-based image matching;scale-based detail culling;appearance based-line detection;accurate localisation;gravity field measurement;magnetic field measurement;Robustness;Augmented reality;Magnetic sensors;Image edge detection;Tracking;Magnetic field measurement;Sensor fusion;Global Positioning System;Detectors;Information systems},
doi={10.1109/ISMAR.2006.297801},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079264,
author={A. {Olwal}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={LightSense: enabling spatially aware handheld interaction devices},
year={2006},
volume={},
number={},
pages={119-122},
abstract={The vision of spatially aware handheld interaction devices has been hard to realize. The difficulties in solving the general tracking problem for small devices have been addressed by several research groups and examples of issues are performance, hardware availability and platform independency. We present LightSense, an approach that employs commercially available components to achieve robust tracking of cell phone LEDs, without any modifications to the device. Cell phones can thus be promoted to interaction and display devices in ubiquitous installations of systems such as the ones we present here. This could enable a new generation of spatially aware handheld interaction devices that would unobtrusively empower and assist us in our everyday tasks.},
keywords={cellular radio;human computer interaction;interactive devices;LED displays;tracking;spatially aware handheld interaction device;tracking problem;robust cell phone LED tracking;LightSense;Displays;Cellular phones;Pervasive computing;Light emitting diodes;Handheld computers;Cameras;Virtual reality;Augmented reality;Ubiquitous computing;Computer interfaces},
doi={10.1109/ISMAR.2006.297802},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079265,
author={M. F. {Zaeh} and W. {Vogl}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Interactive laser-projection for programming industrial robots},
year={2006},
volume={},
number={},
pages={125-128},
abstract={A method for intuitive and efficient programming of industrial robots based on Augmented Reality (AR) is presented, in which tool trajectories and target coordinates are interactively visualized and manipulated in the robot's environment by means of laser projection. The virtual information relevant for programming, such as trajectories and target coordinates, is projected into the robot's environment and can be manipulated interactively. For an intuitive and efficient user input to the system, spatial interaction techniques have been developed, which enable the user to virtually draw the desired motion paths for processing a work piece surface, directly onto the respective object. The discussed method has been implemented in an integrated AR-user interface and has been initially evaluated in an experimental programming scenario. The obtained results indicate that it enables significantly faster and easier programming of processing tasks compared to currently available shop-floor programming methods.},
keywords={augmented reality;data visualisation;industrial robots;robot programming;user interfaces;interactive laser-projection;industrial robot programming;augmented reality;interactive visualization;tool trajectory;target coordinate;virtual information;motion path;AR-based user interface;Robot programming;Service robots;Robot kinematics;Displays;Robotics and automation;Optical mixing;Data visualization;Augmented reality;Trajectory;User interfaces},
doi={10.1109/ISMAR.2006.297803},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079266,
author={M. {Takemura} and I. {Kitahara} and Y. {Ohta}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Photometric inconsistency on a mixed-reality face},
year={2006},
volume={},
number={},
pages={129-138},
abstract={A mixed-reality face (MR face) is a mosaic face with real and virtual facial parts, presented by overlaying a virtual facial part on a real face using mixed-reality techniques. An MR face is an effective means to improve communication in mixed-reality space by restoring the eye expressions lost when wearing HMDs. Photometric registration between the real and virtual parts is important because our eyes are very sensitive, even to small changes in human faces. However, efforts to achieve perfect 'physical' photometric registration on an MR face are not feasible in an ordinary MR space. Therefore, it is essential to clarify the sensitivity of our eyes to the photometric inconsistencies on an MR face, and to concentrate on resolving them. In this paper, we first present the results of a systematic experiment that evaluated our sensitivity to the photometric inconsistencies on an MR face. Then, a technique to resolve the inconsistency and an experimental system to demonstrate the effectiveness of an MR face are described.},
keywords={face recognition;image registration;image segmentation;virtual reality;photometric inconsistency;mixed-reality face;mosaic face;virtual facial part;photometric registration;Photometry;Virtual reality;Eyes;Humans;Space technology;Displays;Face recognition;Extraterrestrial measurements;Reflectivity;Color},
doi={10.1109/ISMAR.2006.297804},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079267,
author={J. {Ehara} and H. {Saito}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Texture overlay for virtual clothing based on PCA of silhouettes},
year={2006},
volume={},
number={},
pages={139-142},
abstract={In this paper, we propose a method for overlaying an arbitrary texture image onto a surface of a plain T-shirt worn by a user. For overlaying arbitrary textures onto the surface of the T-shirt, we need to know the deformation of the surface. For estimating the deformation of the surface from the input images, we use a two-phase process: learning and searching. In the learning phase, the system learns the relationship between the deformation of the surface and the silhouette of the T-shirt region in the image. A database of a number of training images in which a person wearing a T-shirt with markers moves through a variety of positions is used for this learning. Using the database, the system can learn the relationship between the shape of the silhouette and the surface deformation that is provided by the 2D positions of the markers on the surface of the T-shirt. In the searching phase, the silhouette of the user's T-shirt is extracted from the input image, and then, a search for a similar silhouette in the database is conducted in the subspace of the silhouette, which is computed using a PCA of the database. By using the proposed method for estimating the deformation of the surface of the T-shirt, we perform experiments for overlaying virtual clothing.},
keywords={augmented reality;clothing;estimation theory;feature extraction;image texture;principal component analysis;visual databases;texture overlay;virtual clothing;PCA;principal component analysis;arbitrary texture image;surface deformation estimation;learning phase;2D positions;Clothing;Principal component analysis;Surface texture;Image databases;Shape;Biological system modeling;Motion estimation;Humans;Surface fitting;Augmented reality},
doi={10.1109/ISMAR.2006.297805},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079268,
author={I. {Barakonyi} and D. {Schmalstieg}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Ubiquitous animated agents for augmented reality},
year={2006},
volume={},
number={},
pages={145-154},
abstract={Most of today's Augmented Reality (AR) systems operate as passive information browsers relying on a finite and deterministic world model and a predefined hardware and software infrastructure. We propose an AR framework that dynamically and proactively exploits hitherto unknown applications and hardware devices, and adapts the appearance of the user interface to persistently stored and accumulated user preferences. Our framework explores proactive computing, multi-user interface adaptation, and user interface migration. We employ mobile and autonomous agents embodied by real and virtual objects as an interface and interaction metaphor, where agent bodies are able to opportunistically migrate between multiple AR applications and computing platforms to best match the needs of the current application context. We present two pilot applications to illustrate design concepts.},
keywords={augmented reality;computer animation;mobile agents;mobile computing;user interfaces;ubiquitous animated agent;augmented reality;passive information browser;user preference;proactive computing;multiuser interface adaptation;user interface migration;mobile agent;autonomous agent;interaction metaphor;Animation;Augmented reality;Application software;Pervasive computing;Hardware;Autonomous agents;Computer graphics;User interfaces;Computer interfaces;Virtual reality},
doi={10.1109/ISMAR.2006.297806},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079269,
author={S. {Guven} and S. {Feiner}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Visualizing and navigating complex situated hypermedia in augmented and virtual reality},
year={2006},
volume={},
number={},
pages={155-158},
abstract={We present a set of techniques that enable mobile users to visualize and navigate complex hypermedia structures embedded in the real world, through augmented reality or virtual reality. Situating hypermedia in the 3D physical environment makes it possible to represent information about users' surroundings in context. However, it requires addressing a new set of problems beyond those of visualizing hypermedia on a 2D display: Nodes and links can potentially be distributed across large distances, and may be occluded by other objects, both real and virtual. Our techniques address these issues by enabling mobile users to select and manipulate portions of the hypermedia structure by tilting, lifting and shifting them, to view more clearly links and nodes that would otherwise be occluded or ambiguously connected.},
keywords={augmented reality;virtual reality;complex situated hypermedia;augmented reality;virtual reality;mobile users;complex hypermedia structures;Visualization;Navigation;Virtual reality;Augmented reality;User interfaces;Buildings;Computer science;Computer displays;Two dimensional displays;Chromium},
doi={10.1109/ISMAR.2006.297807},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079270,
author={J. {Wither} and S. {Diverdi} and T. {Hollerer}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Using aerial photographs for improved mobile AR annotation},
year={2006},
volume={},
number={},
pages={159-162},
abstract={We present a mobile augmented reality system for outdoor annotation of the real world. To reduce user burden, we use aerial photographs in addition to the wearable system's usual data sources (position, orientation, camera and user input). This allows the user to accurately annotate 3D features with only a few simple interactions from a single position by aligning features in both their first-person viewpoint and in the aerial view. We examine three types of aerial photograph features - corners, edges, and regions - that are suitable for a wide variety of useful mobile augmented reality applications, and are easily visible on aerial photographs. By using aerial photographs in combination with wearable augmented reality, we are able to achieve much higher accuracy 3D annotation positions than was previously possible from a single user location.},
keywords={augmented reality;human factors;mobile computing;photography;wearable computers;aerial photograph features;mobile AR annotation;augmented reality system;outdoor annotation;wearable system;3D feature annotation positions;user experience;Augmented reality;Layout;Cameras;Computer graphics;Eyes;Laboratories;Application software;Chromium;Feeds;Avatars},
doi={10.1109/ISMAR.2006.297808},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079271,
author={A. {Stafford} and W. {Piekarski} and B. H. {Thomas}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Implementation of god-like interaction techniques for supporting collaboration between outdoor AR and indoor tabletop users},
year={2006},
volume={},
number={},
pages={165-172},
abstract={This paper presents a new interaction metaphor we have termed "god-like interaction". This is a metaphor for improved communication of situational and navigational information between outdoor users, equipped with mobile augmented reality systems, and indoor users, equipped with tabletop projector display systems. Physical objects are captured by a series of cameras viewing a table surface indoors, the data is sent over a wireless network, and is then reconstructed at a real-world location for outdoor augmented reality users. Our novel god-like interaction metaphor allows users to communicate information using physical props as well as natural gestures. We have constructed a system that implements our god-like interaction metaphor as well as a series of novel applications to facilitate collaboration between indoor and outdoor users. We have extended a well-known video based rendering algorithm to make it suitable for use on outdoor wireless networks of limited bandwidth. This paper also describes the limitations and lessons learned during the design and construction of the hardware that supports this research.},
keywords={augmented reality;mobile computing;user interfaces;god-like interaction techniques;indoor tabletop users;navigational information;augmented reality systems;tabletop projector display systems;cameras;wireless network;video based rendering algorithm;Collaboration;Augmented reality;Wireless networks;Mobile communication;Navigation;Displays;Cameras;Surface reconstruction;Bandwidth;Hardware},
doi={10.1109/ISMAR.2006.297809},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079272,
author={K. {Saitoh} and T. {Machida} and K. {Kiyokawa} and H. {Takemura}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={A 2D-3D integrated interface for mobile robot control using omnidirectional images and 3D geometric models},
year={2006},
volume={},
number={},
pages={173-176},
abstract={This paper proposes a novel visualization and interaction technique for remote surveillance using both 2D and 3D scene data acquired by a mobile robot equipped with an omnidirectional camera and an omnidirectional laser range sensor. In a normal situation, telepresence with an egocentric-view is provided using high resolution omnidirectional live video on a hemispherical screen. As depth information of the remote environment is acquired, additional 3D information can be overlaid onto the 2D video image such as passable area and roughness of the terrain in a manner of video see-through augmented reality. A few functions to interact with the 3D environment through the 2D live video are provided, such as path-drawing and path-preview. Path-drawing function allows to plan a robot's path by simply specifying 3D points on the path on screen. Path- preview function provides a realistic image sequence seen from the planned path using a texture-mapped 3D geometric model in a manner of virtualized reality. In addition, a miniaturized 3D model is overlaid on the screen providing an exocentric view, which is a common technique in virtual reality. In this way, our technique allows an operator to recognize the remote place and navigate the robot intuitively by seamlessly using a variety of mixed reality techniques on a spectrum of Milgram's real-virtual continuum.},
keywords={data visualisation;image sensors;image sequences;mobile robots;robot vision;2D-3D integrated interface;mobile robot control;omnidirectional images;3D geometric;remote surveillance;data visualization;interaction technique;egocentric-view;hemispherical screen;augmented reality;path-drawing function;realistic image sequence;virtualized reality;real-virtual continuum;Mobile robots;Robot control;Solid modeling;Virtual reality;Data visualization;Surveillance;Layout;Robot vision systems;Cameras;Laser modes},
doi={10.1109/ISMAR.2006.297810},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079273,
author={S. {Gupta} and C. {Jaynes}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={The universal media book: tracking and augmenting moving surfaces with projected information},
year={2006},
volume={},
number={},
pages={177-180},
abstract={We explore the integration of projected imagery with a physical book that acts as a tangible interface to multimedia data. Using a camera and projector pair, a tracking framework is presented wherein the 3D position of planar pages are monitored as they are turned back and forth by a user, and data is correctly warped and projected onto each page at interactive rates. The book pages are blank, so traditional approaches to tracking physical features on the display surface do not apply. Instead, in each frame, feature points are independently extracted from the camera and projector images, and matched in order to recover the geometry of the pages in motion. The book can be loaded with multimedia content, including images, videos, and volumetric datasets (in which case a page can be removed from the book and used to navigate through a virtual 3D volume).},
keywords={augmented reality;data visualisation;feature extraction;image matching;image motion analysis;multimedia computing;tracking;user interfaces;universal media book;moving surface augmentation;moving surface tracking;projected imagery;tangible interface;multimedia data;feature extraction;image matching;virtual 3D volume;augmented book;volumetric visualization;mixed-reality interface;Books;Videos;Cameras;Computer displays;Computer vision;Liquid crystal displays;Data mining;Geometry;Computer interfaces;Layout},
doi={10.1109/ISMAR.2006.297811},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079274,
author={S. {Irawati} and S. {Green} and M. {Billinghurst} and A. {Duenser} and H. {Ko}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={"Move the couch where?" : developing an augmented reality multimodal interface},
year={2006},
volume={},
number={},
pages={183-186},
abstract={This paper describes an augmented reality (AR) multimodal interface that uses speech and paddle gestures for interaction. The application allows users to intuitively arrange virtual furniture in a virtual room using a combination of speech and gestures from a real paddle. Unlike other multimodal AR applications, the multimodal fusion is based on the combination of time-based and semantic techniques to disambiguate a users speech and gesture input. We describe our AR multimodal interface architecture and discuss how the multimodal inputs are semantically integrated into a single interpretation by considering the input time stamps, the object properties, and the user context.},
keywords={augmented reality;gesture recognition;speech recognition;speech-based user interfaces;augmented reality multimodal interface;paddle gesture;speech gesture;multimodal fusion;semantic technique;time-based technique;Augmented reality;Speech;Application software;Virtual reality;Computer interfaces;Virtual environment;Mice;User interfaces;Graphical user interfaces;Space technology},
doi={10.1109/ISMAR.2006.297812},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079275,
author={G. {Bianchi} and C. {Jung} and B. {Knoerlein} and G. {Szekely} and M. {Harders}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={High-fidelity visuo-haptic interaction with virtual objects in multi-modal AR systems},
year={2006},
volume={},
number={},
pages={187-196},
abstract={The driving force of our research is the precise combination of real and - possibly indistinguishable - virtual objects in an interactive augmented reality environment. This requires real-time, multimodal simulation, as well as stable and accurate overlay of the computer-generated objects. This paper describes several methods to improve accuracy and stability of our hybrid augmented reality system. In a comparison of two approaches to hybrid head pose refinement, we show that Quasi-Newton method enables high performance optimization for image space error minimization. Moreover, a 3D landmark refinement step is proposed, which significantly improves quality and robustness of the overlay process. The enhanced system is demonstrated in an interactive AR environment, which provides accurate haptic feedback from real and virtual deformable objects. Finally, the effect of landmark occlusion on tracking stability during user interaction is also analyzed.},
keywords={augmented reality;haptic interfaces;image processing;Newton method;high-fidelity visuo-haptic interaction;virtual objects;multimodal AR systems;interactive augmented reality;multimodal simulation;computer-generated objects;Quasi-Newton method;image space error minimization;3D landmark refinement step;haptic feedback;virtual deformable objects;landmark occlusion;tracking stability;user interaction;Augmented reality;Computational modeling;Computer simulation;Stability;Head;Optimization;Computer errors;Minimization methods;Robustness;Haptic interfaces},
doi={10.1109/ISMAR.2006.297813},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079276,
author={T. {Edmunds} and D. K. {Pai}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={An event architecture for distributed interactive multisensory rendering},
year={2006},
volume={},
number={},
pages={197-202},
abstract={We describe an architecture for coping with latency and asynchrony of multisensory events in interactive virtual environments. We propose to decompose multisensory interactions into a series of discrete, perceptually significant events, and structure the application architecture within this event-based context. We analyze the sources of latency, and develop a framework for event prediction and scheduling. Our framework decouples synchronization from latency, and uses prediction to reduce latency when possible. We evaluate the performance of the architecture using vision-based motion sensing and multisensory rendering using haptics, sounds, and graphics. The architecture makes it easy to achieve good performance using commodity off-the-shelf hardware.},
keywords={augmented reality;rendering (computer graphics);software architecture;event architecture;distributed interactive multisensory rendering;interactive virtual environments;event-based context;vision-based motion sensing;commodity off-the-shelf hardware;Delay;Rendering (computer graphics);Frequency synchronization;Virtual environment;Trajectory;Haptic interfaces;Graphics;Augmented reality;Computer displays;Computer networks},
doi={10.1109/ISMAR.2006.297814},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079277,
author={J. {Fischer} and D. {Bartz} and W. {Strasser}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Enhanced visual realism by incorporating camera image effects},
year={2006},
volume={},
number={},
pages={205-208},
abstract={In video see-through augmented reality (AR), virtual objects are overlaid over digital video images. One particular problem of this image mixing process is that the visual appearance of the computer graphics differs strongly from the real background image. The reason for this is that typical AR systems use fast but simple real-time rendering techniques for displaying virtual objects. In this paper, methods for reducing the impact of three effects which make virtual and real objects easily distinguishable are presented. The first effect is camera image noise, which is contained in the data delivered by the image sensor used for capturing the real scene. The second effect considered is edge aliasing, which makes distinguishing virtual objects from real objects simple. Finally, we consider motion blur, which is caused by the temporal integration of color intensities in the image sensor during fast movements of the camera or observed objects. In this paper, we present a system for generating a realistic simulation of image noise based on a new camera calibration step. Additionally, a rendering algorithm is introduced, which performs a smooth blending between the camera image and virtual objects at their boundary in order to reduce aliasing. Lastly, a rendering method is presented, which produces motion blur according to the current camera movement. The implementation of the new rendering techniques utilizes the programmability of modern graphics processing units (GPUs) and delivers real-time frame rates.},
keywords={augmented reality;image enhancement;image registration;enhanced visual realism;camera image effects;augmented reality;virtual objects;digital video images;image noise;graphics processing units;Cameras;Rendering (computer graphics);Image sensors;Augmented reality;Computer graphics;Real time systems;Layout;Colored noise;Noise generators;Calibration},
doi={10.1109/ISMAR.2006.297815},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079278,
author={E. {Mendez} and D. {Kalkofen} and D. {Schmalstieg}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Interactive context-driven visualization tools for augmented reality},
year={2006},
volume={},
number={},
pages={209-218},
abstract={In this article we present an interaction tool, based on the Magic Lenses technique, that allows a 3D scene to be affected dynamically given contextual information, for example, to support information filtering. We show how elements of a scene graph are grouped by context in addition to hierarchically, and, how this enables us to locally modify their rendering styles. This research has two major contributions, the use of context sensitivity with 3D Magic Lenses in a scene graph and the implementation of multiple volumetric 3D Magic Lenses for Augmented Reality setups. We have developed our tool for the Studierstube framework which allows us doing rapid prototyping of Virtual and Augmented Reality applications. Some application directions are shown throughout the paper. We compare our work with other methods, highlight strengths and weaknesses and finally discuss research directions for our work.},
keywords={augmented reality;data visualisation;information filtering;virtual prototyping;interactive context-driven visualization tools;augmented reality;magic lenses technique;3D scene;information filtering;rendering styles;Studierstube framework;virtual rapid prototyping;Visualization;Augmented reality;Lenses;Layout;Information filtering;Computer graphics;Rendering (computer graphics);Liver;Virtual reality;User interfaces},
doi={10.1109/ISMAR.2006.297816},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079279,
author={B. {Okumura} and M. {Kanbara} and N. {Yokoya}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Augmented reality based on estimation of defocusing and motion blurring from captured images},
year={2006},
volume={},
number={},
pages={219-225},
abstract={Photometric registration is as important as geometric registration to generate a seamless augmented reality scene. Especially the difference in image quality between a real image and virtual objects caused by defocusing and motion blurring in capturing a real scene image easily exhibits the seam between real and virtual worlds. To avoid this problem in video see-through augmented reality, it is necessary to simulate the optical system of camera when virtual objects are rendered. This paper proposes an image composition method for video see-through augmented reality, which is based on defocusing and motion blurring estimation from the captured real image and rendering of virtual objects with blur effects. In experiments, the effectiveness of the proposed method is confirmed by comparing a real image with virtual objects rendered by the proposed method.},
keywords={augmented reality;image registration;motion estimation;photometry;video cameras;video see-through augmented reality;motion blurring estimation;photometric registration;geometric registration;image quality;virtual object rendering;real scene;optical camera system;image composition;defocusing estimation;Augmented reality;Motion estimation;Cameras;Rendering (computer graphics);Image quality;Photometry;Layout;Lenses;Degradation;Information science},
doi={10.1109/ISMAR.2006.297817},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079280,
author={W. A. {Moussa} and J. {Jessel} and E. {Dubois}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Notational-based prototyping of mixed interactions},
year={2006},
volume={},
number={},
pages={229-230},
abstract={Development of mixed reality systems is almost always following an ad-hoc process. The development cycle often turns out to be highly expensive and time consuming. This paper presents a new prototyping approach: a combination of model-based design and simulation based prototyping.},
keywords={augmented reality;software prototyping;user interfaces;notational-based prototyping;mixed interactions;mixed reality systems;ad-hoc process;model-based design;simulation based prototyping;Prototypes;Virtual reality;Virtual prototyping;XML;Chromium;System testing;Buildings;Software tools;Software testing;Application software},
doi={10.1109/ISMAR.2006.297818},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079281,
author={R. {Grasset} and J. {Looser} and M. {Billinghurst}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Transitional interface: concept, issues and framework},
year={2006},
volume={},
number={},
pages={231-232},
abstract={Transitional Interfaces have emerged as a new way to interact and collaborate between different interactive spaces such as reality, virtual reality and augmented reality environments. In this paper we explore this concept further. We introduce a descriptive model of the concept, its collaborative aspect and how it can be generalized to describe natural and continuous transitions between contexts (e.g. across space, scale, viewpoints, and representation).},
keywords={augmented reality;groupware;user interfaces;collaborative transitional interface;interactive space;continuous perceptual transition;augmented reality;virtual reality;Navigation;Virtual reality;Collaboration;Augmented reality;Collaborative work;Switches;Context modeling;Chemistry;Legged locomotion;Interpolation},
doi={10.1109/ISMAR.2006.297819},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079282,
author={A. {Gunnarsson} and M. {Rauhala} and A. {Henrysson} and A. {Ynnerman}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Visualization of sensor data using mobile phone augmented reality},
year={2006},
volume={},
number={},
pages={233-234},
abstract={We have developed a prototype system for visual inspection of hidden structures using a mobile phone wireless ZigBee sensor network. Data collected from an embedded wireless sensor matrix is used to synthesize graphics in real-time. Combining this with augmented reality technology on a mobile phone yields a novel approach to on-site inspection of a broad range of elements and their current internal states.},
keywords={augmented reality;data visualisation;mobile computing;mobile handsets;sensor data visualization;mobile phone augmented reality;visual inspection;ZigBee sensor network;embedded wireless sensor matrix;Data visualization;Mobile handsets;Augmented reality;Wireless sensor networks;Inspection;Prototypes;ZigBee;Sensor systems;Network synthesis;Graphics},
doi={10.1109/ISMAR.2006.297820},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079283,
author={S. {Guven} and S. {Feiner} and O. {Oda}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Mobile augmented reality interaction techniques for authoring situated media on-site},
year={2006},
volume={},
number={},
pages={235-236},
abstract={We present a set of mobile augmented reality interaction techniques for authoring situated media: multimedia and hypermedia that are embedded within the physical environment. Our techniques are designed for use with a tracked hand-held tablet display with an attached camera, and rely on "freezing" the frame for later editing.},
keywords={augmented reality;hypermedia;interactive systems;mobile computing;mobile augmented reality interaction technique;authoring situated media;multimedia;hypermedia;physical environment;hand-held tablet display;Augmented reality;Cameras;Virtual reality;Mars;Layout;Organizing;Books;Scattering;Mobile computing;Computer science},
doi={10.1109/ISMAR.2006.297821},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079284,
author={Y. {Hong} and S. {Lee} and Y. {Lee} and S. {Kim}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Mobile pointing and input system using active marker},
year={2006},
volume={},
number={},
pages={237-238},
abstract={We present a mobile pointing and input system for mobile phones with minimal hardware parts that allows users who wear an eyeglass display to view and interact with a large virtual image. To use the mobile phone as a pointing and input device, a fiducial marker-based tracking system developed for augmented reality is adapted for our system. The brightness and shape of active marker could be changed according to the state of operating environment and/or the user's intentions in order to increase the detection rate of the marker.},
keywords={augmented reality;interactive devices;mobile computing;mobile pointing;input system;mobile phone;eyeglass display;virtual image;fiducial marker-based tracking system;augmented reality;active marker;input device;Mobile handsets;Hardware;Displays;Augmented reality;Brightness;Shape},
doi={10.1109/ISMAR.2006.297822},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079285,
author={R. {Ichikari} and K. {Kawano} and A. {Kimura} and F. {Shibata} and H. {Tamura}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Mixed reality pre-visualization and camera-work authoring in filmmaking},
year={2006},
volume={},
number={},
pages={239-240},
abstract={In this paper, we introduce the outline of "The MR-PreViz Project" performed in Japan. In the pre-production process of filmmaking, Pre Viz, pre-visualizing the desired scene by CGI, is used as a new technique. As its advanced approach, we propose MR-PreViz that utilized mixed reality technology in current PreViz. MR-PreViz makes it possible to merge the real background and the computer-generated humans and creatures in open set or at outdoor location. The user can consider the camerawork and camera blocking efficiently by using MR-PreViz. This paper introduces the outline of the MR-PreViz project, the design of hardware configuration, camera-work authoring method and the results of prototyping.},
keywords={cameras;cinematography;data visualisation;virtual reality;mixed reality previsualization;camera-work authoring;filmmaking;MR-PreViz Project;computer-generated humans;camera blocking;Virtual reality;Layout;Motion pictures;Character generation;Digital cameras;Prototypes;Application software;Image generation;Animation;High definition video},
doi={10.1109/ISMAR.2006.297823},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079286,
author={T. {Lee} and T. {Hollerer}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Viewpoint stabilization for live collaborative video augmentations},
year={2006},
volume={},
number={},
pages={241-242},
abstract={We present a method for stabilizing live video from a moving camera for the purpose of a tele-meeting, in which a participant with an AR view onto a shared canvas collaborates with a remote user. The AR view is established without markers and using no other tracking equipment than a head-worn camera. The remote user is allowed to directly annotate the local user's view in real time on a desktop or tablet PC. The planar homographies between the reference frame and the other following frames are maintained. In effect, both the local and remote participants can annotate the physical meeting space, the local AR user through physical interaction, the remote user through our stabilized video. When tracking is lost, the remote user can still continue annotating on a frozen video frame. We tested several small demo applications with this new form of transient AR collaboration that can be established easily, on a per need basis, and without complicated equipment or calibration requirements.},
keywords={augmented reality;cameras;computer vision;teleconferencing;video streaming;viewpoint stabilization;live collaborative video augmentations;telemeeting;head-worn camera;remote user;local AR user;video frame;Collaboration;Cameras;Collaborative work;Video sharing;Layout;Application software;Streaming media;Target tracking;Eyes;Laboratories},
doi={10.1109/ISMAR.2006.297824},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079287,
author={Y. {Motokawa} and H. {Saito}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Support system for guitar playing using augmented reality display},
year={2006},
volume={},
number={},
pages={243-244},
abstract={Learning to play the guitar is difficult. We proposed a system that assists people learning to play the guitar using augmented reality. This system shows a learner how to correctly hold the strings by overlaying a virtual hand model and lines onto a real guitar. The player learning to play the guitar can easily understand the required position by overlapping their hand on a visual guide. An important issue for this system to address is the accurate registration between the visual guide and the guitar, therefore we need to track the pose and the position of the guitar. We also proposed a method to track the guitar with a visual marker and natural features of the guitar. Since we used marker information and edge information as natural features, the system could continually track the guitar. Accordingly, our system can constantly display visual guides at the required position to enable a player to learn to play the guitar in a natural manner.},
keywords={augmented reality;computer displays;edge detection;feature extraction;image registration;support system;guitar playing;augmented reality display;virtual hand model;virtual lines;visual guide;visual marker;natural features;edge information;Augmented reality;Instruments;Computer displays;Universal Serial Bus;Cameras;Image edge detection;Keyboards;Computer science;Chromium;Image processing},
doi={10.1109/ISMAR.2006.297825},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079288,
author={J. J. {Neubert} and T. {Drummond}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Using backlight intensity for device identification},
year={2006},
volume={},
number={},
pages={245-246},
abstract={This paper presents a method for identifying handheld devices (e.g. smart phones and pocket PCs) to facilitate the use of these devices as tangible interfaces for desktop augmented reality systems. The proposed system leverages the ability of these handheld devices to programmatically control their backlight intensity to display a binary code. The codes produced are non-intrusive, require no specialized hardware, and can be generated with most handheld devices. This technique is shown to accurately and robustly identify up to 16 different devices in under 500 msec and is easily expandable to 256 or more devices.},
keywords={augmented reality;binary codes;user interfaces;backlight intensity;device identification;handheld devices;desktop augmented reality systems;binary codes;Object recognition;Handheld computers;Smart phones;Personal communication networks;Augmented reality;Control systems;Displays;Binary codes;Hardware;Robustness},
doi={10.1109/ISMAR.2006.297826},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079289,
author={S. {Nilsson} and B. {Johansson}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={User experience and acceptance of a mixed reality system in a naturalistic setting - a case study},
year={2006},
volume={},
number={},
pages={247-248},
abstract={This poster presents a qualitative user study investigating user experience and acceptance of an MR application designed to give instructions on how to start up a diathermy apparatus. The study was conducted in a naturalistic setting on site at a hospital with actual users and their equipment. The analysis of the results has indicated that although there are several ergonomic issues to be solved, the acceptance of an MR system in this user group is high. As a result of the study, the MR system has been redesigned to better fit the ergonomic needs of this user group.},
keywords={virtual reality;mixed reality system;naturalistic setting;diathermy apparatus;ergonomic issues;Virtual reality;Usability;Hospitals;Software tools;Ergonomics;Assembly systems;Computer displays;Cameras;Augmented reality;Industry applications},
doi={10.1109/ISMAR.2006.297827},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079290,
author={S. {Lee} and D. {Kim} and D. {Kim} and T. {Han}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Tag detection algorithm for improving the instability problem of an augmented reality},
year={2006},
volume={},
number={},
pages={257-258},
abstract={Detection technology is a requirement for an Augmented Reality system. One of the problems with detection technology is the instability problem, which occurs when an obstacle occludes a tag while detecting the tag, and the augmented object suddenly disappears. We have proposed a corner detection algorithm to solve this instability problem. The key feature is that if the tag can recognize its position using its four corner cells despite the obstacle being present, then it can maintain its augmented object. We defined the corner case for all types of cases where the instability problem occurs in ARToolkit or ARTag. We have adapted our proposed algorithm to the corner case in ARToolkit, ARTag and ColorCode vision systems and have compared their false detection rates.},
keywords={augmented reality;edge detection;object detection;tag detection algorithm;augmented reality;instability problem;corner detection;Detection algorithms;Augmented reality;Object detection;Machine vision;Error analysis;Laboratories;Computer science;Data gloves;Image recognition;Cameras},
doi={10.1109/ISMAR.2006.297828},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079291,
author={S. {Nolle} and G. {Klinker}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Augmented reality as a comparison tool in automotive industry},
year={2006},
volume={},
number={},
pages={249-250},
abstract={Augmented reality (AR) can be used in the automotive industry to compare real parts of a car with their associated construction data. The real parts have to be checked whether they correspond to the latest version of the design and whether they have been manufactured with appropriate precision. With AR, CAD construction data can be superimposed on the real parts striving for maximum correspondence. The real and the virtual part should both be visible at the same time and in the same place. Therefore, for this kind of overlay, a special method of augmentation is needed. We present and discuss some visualization schemes.},
keywords={augmented reality;automobile industry;CAD;production engineering computing;program visualisation;augmented reality;automotive industry;associated construction data;CAD construction;visualization schemes;Augmented reality;Automotive engineering;Shape;Construction industry;Data visualization;Prototypes;Wire;Layout;Muscles;Bones},
doi={10.1109/ISMAR.2006.297829},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079292,
author={W. {Piekarski} and R. {Smith}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Robust gloves for 3D interaction in mobile outdoor AR environments},
year={2006},
volume={},
number={},
pages={251-252},
abstract={This paper describes the design of hand-worn gloves for interacting with mobile outdoor augmented reality systems. Most existing systems rely on more traditional 2D input devices such as mice and keyboards. Since augmented reality information is typically registered in 3D to the environment, user interfaces need to be designed that are capable of supporting the more complex operations possible. This paper describes how we used metallic thread and adhesive fabric to add conduction sensing to a standard set of gloves which can survive harsh treatment; how Bluetooth and MSP430 microcontrollers are used to build a small circuit that is wireless and highly portable; and how AR-ToolKit is used for 3D tracking of fiducial markers on the thumbs. While we have previously demonstrated this technology with a number of our previous systems, this paper explains the various techniques we use in the implementation.},
keywords={augmented reality;mobile computing;wearable computers;3D interaction;mobile outdoor augmented reality systems;robust gloves;user interfaces;Bluetooth;MSP430 microcontrollers;Robustness;Augmented reality;Mice;Keyboards;User interfaces;Yarn;Fabrics;Bluetooth;Microcontrollers;Circuits},
doi={10.1109/ISMAR.2006.297830},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079293,
author={S. {Siltanen}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Texture generation over the marker area},
year={2006},
volume={},
number={},
pages={253-254},
abstract={In this paper, we present a method for generating a texture for hiding a marker in augmented reality applications. The texture is generated using the neighbourhood of the detected marker area in the image, which enables photorealistic results. The method presented here shows clear potential for real time use.},
keywords={augmented reality;image texture;rendering (computer graphics);texture generation;augmented reality;detected marker area;image inpainting;photorealistic rendering;Application software;Augmented reality;Virtual reality;Image generation;Filling;Automatic voltage control;Chromium;Multimedia systems;Information systems;Computer graphics},
doi={10.1109/ISMAR.2006.297831},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079294,
author={C. {Warrington} and G. {Roth} and E. {Dubois}},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Markerless augmented reality for cubic panorama sequences},
year={2006},
volume={},
number={},
pages={255-256},
abstract={This paper presents a system for introducing augmented reality (AR) enhancements into an image-based cubic panorama sequence. Panoramic cameras, such as the Point Gray Research Ladybug allow rapid capture and generation of panoramic sequences for users to navigate and view. Our AR system provides the ability for authors to add virtual content into the panoramic sequences. First, a user manually selects a planar region over which to add the content. Then the system automatically finds a matching planar region in all the panoramas, allowing the virtual content to propagate. No preconditioning of the imaged scene through the addition of physical markers is necessary. Instead, 3-D position information is obtained by matching interest-point features across the panoramic sequence. This paper presents an application of augmented reality algorithms to the unique case of pre-captured panoramic sequences.},
keywords={augmented reality;image enhancement;image matching;image sequences;augmented reality;image-based cubic panorama sequence;Point Gray Research Ladybug;virtual contents;3D position information;Augmented reality;Cameras;Navigation;Layout;Computer vision;Computer graphics;Councils;Application software;Chromium;Image processing},
doi={10.1109/ISMAR.2006.297832},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4079295,
author={},
booktitle={2006 IEEE/ACM International Symposium on Mixed and Augmented Reality}, title={Author index},
year={2006},
volume={},
number={},
pages={259-260},
abstract={The author index contains an entry for each author and coauthor included in the proceedings record.},
keywords={},
doi={10.1109/ISMAR.2006.297833},
ISSN={},
month={Oct},}